import tvm
from tvm.script import ir as I
from tvm.script import tir as T
from tvm.script import relax as R

metadata = tvm.ir.load_json("{\n  \"root\": 1, \n  \"nodes\": [\n    {\n      \"type_key\": \"\"\n    }, \n    {\n      \"type_key\": \"Map\", \n      \"keys\": [\n        \"relax.expr.Constant\"\n      ], \n      \"data\": [2]\n    }, \n    {\n      \"type_key\": \"Array\", \n      \"data\": [\n        3, \n        3, \n        3, \n        3, \n        3, \n        3, \n        3, \n        3, \n        3, \n        3, \n        3, \n        3, \n        14\n      ]\n    }, \n    {\n      \"type_key\": \"relax.expr.Constant\", \n      \"attrs\": {\n        \"_checked_type_\": \"13\", \n        \"data\": \"0\", \n        \"span\": \"0\", \n        \"struct_info_\": \"4\"\n      }\n    }, \n    {\n      \"type_key\": \"relax.TensorStructInfo\", \n      \"attrs\": {\n        \"dtype\": \"float32\", \n        \"ndim\": \"4\", \n        \"shape\": \"5\", \n        \"span\": \"0\"\n      }\n    }, \n    {\n      \"type_key\": \"relax.expr.ShapeExpr\", \n      \"attrs\": {\n        \"_checked_type_\": \"12\", \n        \"span\": \"0\", \n        \"struct_info_\": \"11\", \n        \"values\": \"6\"\n      }\n    }, \n    {\n      \"type_key\": \"Array\", \n      \"data\": [7, 8, 9, 10]\n    }, \n    {\n      \"type_key\": \"IntImm\", \n      \"attrs\": {\n        \"dtype\": \"int64\", \n        \"span\": \"0\", \n        \"value\": \"1\"\n      }\n    }, \n    {\n      \"type_key\": \"IntImm\", \n      \"attrs\": {\n        \"dtype\": \"int64\", \n        \"span\": \"0\", \n        \"value\": \"1\"\n      }\n    }, \n    {\n      \"type_key\": \"IntImm\", \n      \"attrs\": {\n        \"dtype\": \"int64\", \n        \"span\": \"0\", \n        \"value\": \"77\"\n      }\n    }, \n    {\n      \"type_key\": \"IntImm\", \n      \"attrs\": {\n        \"dtype\": \"int64\", \n        \"span\": \"0\", \n        \"value\": \"77\"\n      }\n    }, \n    {\n      \"type_key\": \"relax.ShapeStructInfo\", \n      \"attrs\": {\n        \"ndim\": \"4\", \n        \"span\": \"0\", \n        \"values\": \"6\"\n      }\n    }, \n    {\n      \"type_key\": \"relax.ShapeType\", \n      \"attrs\": {\n        \"ndim\": \"4\", \n        \"span\": \"0\"\n      }\n    }, \n    {\n      \"type_key\": \"relax.DynTensorType\", \n      \"attrs\": {\n        \"dtype\": \"float32\", \n        \"ndim\": \"4\", \n        \"span\": \"0\"\n      }\n    }, \n    {\n      \"type_key\": \"relax.expr.Constant\", \n      \"attrs\": {\n        \"_checked_type_\": \"22\", \n        \"data\": \"1\", \n        \"span\": \"0\", \n        \"struct_info_\": \"15\"\n      }\n    }, \n    {\n      \"type_key\": \"relax.TensorStructInfo\", \n      \"attrs\": {\n        \"dtype\": \"float32\", \n        \"ndim\": \"2\", \n        \"shape\": \"16\", \n        \"span\": \"0\"\n      }\n    }, \n    {\n      \"type_key\": \"relax.expr.ShapeExpr\", \n      \"attrs\": {\n        \"_checked_type_\": \"21\", \n        \"span\": \"0\", \n        \"struct_info_\": \"20\", \n        \"values\": \"17\"\n      }\n    }, \n    {\n      \"type_key\": \"Array\", \n      \"data\": [18, 19]\n    }, \n    {\n      \"type_key\": \"IntImm\", \n      \"attrs\": {\n        \"dtype\": \"int64\", \n        \"span\": \"0\", \n        \"value\": \"1\"\n      }\n    }, \n    {\n      \"type_key\": \"IntImm\", \n      \"attrs\": {\n        \"dtype\": \"int64\", \n        \"span\": \"0\", \n        \"value\": \"160\"\n      }\n    }, \n    {\n      \"type_key\": \"relax.ShapeStructInfo\", \n      \"attrs\": {\n        \"ndim\": \"2\", \n        \"span\": \"0\", \n        \"values\": \"17\"\n      }\n    }, \n    {\n      \"type_key\": \"relax.ShapeType\", \n      \"attrs\": {\n        \"ndim\": \"2\", \n        \"span\": \"0\"\n      }\n    }, \n    {\n      \"type_key\": \"relax.DynTensorType\", \n      \"attrs\": {\n        \"dtype\": \"float32\", \n        \"ndim\": \"2\", \n        \"span\": \"0\"\n      }\n    }\n  ], \n  \"b64ndarrays\": [\n    \"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\", \n    \"P6G0lvBAXt0AAAAAAAAAAAEAAAAAAAAAAgAAAAIgAQABAAAAAAAAAKAAAAAAAAAAgAIAAAAAAAAAAIA/+a1xPwUpZD+rZVc/F1lLPxD5Pz/uOzU/lRgrP2uGIT9QfRg/mPUPPwroBz/OTQA/4EDyPrSz5D6a6Nc+s9TLPsFtwD4YqrU+loCrPpjooT4B2pg+G02QPqc6iD7Lm4A+INRyPrk+ZT7Ya1g+mFBMPrfiQD6HGDY+1egrPgVLIj7rNhk+0qQQPnaNCD746QA+vmfzPRLK5T1j79g9yMzMPflXwT03h7Y9VFGsPaytoj0MlJk9wPyQPXjgiD1VOIE9sPtzPb5VZj1Dc1k9R0lNPYHNQT0q9jY9E7osPY0QIz1o8Rk93VQRPaozCT3ihgE9AZD0PMPh5jxv99k8EsbNPE1DwjxjZbc8ECOtPK5zozz6Tpo8Oa2RPA+HiTyd1YE8pyR1PB5uZzzve1o8JUNOPGG5QjzY1Dc8VIwtPAPXIzzIrBo8xAUSPKzaCTyKJAI8rbn1O8j65zu7ANs7iMDOO8AvwzuYRLg7zPWtO6Q6pDvOCps7iV6SO3Yuijukc4I7Dk92O8mHaDvdhVs7Lz5PO2ymQzuctDg7kF8uO3KeJDsLaRs7g7cSO3OCCjvxwgI7vuT2OiMV6TpQC9w6KrzPOlwdxDrlJLk6icmuOn0CpTqFx5s6rRCTOqfWijpvEoM61Hp3OtSiaToUkVw6cjpQOoKURDpplTk6yjMvOtRmJTo4Jhw6FGoTOg0rCzodYgM6RhH4OdEw6jkgF905CLnQOQsMxTk7Bro5U56vOWDLpTkkhZw5q8OTOad/izn1sYM5E6h4OS+/ajl9nV056TdROdWDRTlQdzo5/ggwOSIwJjlD5Bw5fh0UOW7UCzn+AQQ5Oz/5OO1N6zg/JN44\"\n  ], \n  \"attrs\": {\"tvm_version\": \"0.11.dev0\"}\n}")

@I.ir_module
class Module:
    @T.prim_func
    def concatenate(rxplaceholder: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), rxplaceholder_1: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), T_concat: T.Buffer((T.int64(2), T.int64(77), T.int64(768)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                    with T.block("T_concat"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(59136))
                        v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(59136) // T.int64(768))
                        v_ax2 = T.axis.spatial(T.int64(768), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(768))
                        T.where((ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) * T.int64(256) + ax0_ax1_ax2_fused_2 < T.int64(118272))
                        T.reads(rxplaceholder_1[v_ax0 - T.int64(1), v_ax1, v_ax2], rxplaceholder[v_ax0, v_ax1, v_ax2])
                        T.writes(T_concat[v_ax0, v_ax1, v_ax2])
                        T_concat[v_ax0, v_ax1, v_ax2] = T.if_then_else(T.int64(1) <= v_ax0, rxplaceholder_1[v_ax0 - T.int64(1), v_ax1, v_ax2], rxplaceholder[v_ax0, v_ax1, v_ax2])

    @T.prim_func
    def concatenate1(rxplaceholder: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), rxplaceholder_1: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_concat: T.Buffer((T.int64(2), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(512), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("T_concat"):
                    v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(16384))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16384) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(rxplaceholder_1[v_ax0 - T.int64(1), v_ax1, v_ax2, v_ax3], rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(1) <= v_ax0, rxplaceholder_1[v_ax0 - T.int64(1), v_ax1, v_ax2, v_ax3], rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def concatenate10(rxplaceholder: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), rxplaceholder_1: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), T_concat: T.Buffer((T.int64(2), T.int64(640), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(80)):
                    with T.block("T_concat"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(2621440))
                        v_ax1 = T.axis.spatial(T.int64(640), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2621440) // T.int64(4096))
                        v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4096) // T.int64(64))
                        v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                        T.reads(rxplaceholder_1[v_ax0, v_ax1 - T.int64(320), v_ax2, v_ax3], rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(320) <= v_ax1, rxplaceholder_1[v_ax0, v_ax1 - T.int64(320), v_ax2, v_ax3], rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def concatenate3(rxplaceholder: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), rxplaceholder_1: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), T_concat: T.Buffer((T.int64(2), T.int64(2560), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                    with T.block("T_concat"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(163840))
                        v_ax1 = T.axis.spatial(T.int64(2560), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(163840) // T.int64(64))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64) // T.int64(8))
                        v_ax3 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(8))
                        T.reads(rxplaceholder_1[v_ax0, v_ax1 - T.int64(1280), v_ax2, v_ax3], rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(1280) <= v_ax1, rxplaceholder_1[v_ax0, v_ax1 - T.int64(1280), v_ax2, v_ax3], rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def concatenate4(rxplaceholder: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), rxplaceholder_1: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), T_concat: T.Buffer((T.int64(2), T.int64(2560), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(20)):
                    with T.block("T_concat"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(655360))
                        v_ax1 = T.axis.spatial(T.int64(2560), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(655360) // T.int64(256))
                        v_ax2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(256) // T.int64(16))
                        v_ax3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                        T.reads(rxplaceholder_1[v_ax0, v_ax1 - T.int64(1280), v_ax2, v_ax3], rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(1280) <= v_ax1, rxplaceholder_1[v_ax0, v_ax1 - T.int64(1280), v_ax2, v_ax3], rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def concatenate5(rxplaceholder: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), rxplaceholder_1: T.Buffer((T.int64(2), T.int64(640), T.int64(16), T.int64(16)), "float32"), T_concat: T.Buffer((T.int64(2), T.int64(1920), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(15)):
                    with T.block("T_concat"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(491520))
                        v_ax1 = T.axis.spatial(T.int64(1920), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(491520) // T.int64(256))
                        v_ax2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(256) // T.int64(16))
                        v_ax3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                        T.reads(rxplaceholder_1[v_ax0, v_ax1 - T.int64(1280), v_ax2, v_ax3], rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(1280) <= v_ax1, rxplaceholder_1[v_ax0, v_ax1 - T.int64(1280), v_ax2, v_ax3], rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def concatenate6(rxplaceholder: T.Buffer((T.int64(2), T.int64(1280), T.int64(32), T.int64(32)), "float32"), rxplaceholder_1: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), T_concat: T.Buffer((T.int64(2), T.int64(1920), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(60)):
                    with T.block("T_concat"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1966080))
                        v_ax1 = T.axis.spatial(T.int64(1920), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1966080) // T.int64(1024))
                        v_ax2 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1024) // T.int64(32))
                        v_ax3 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                        T.reads(rxplaceholder_1[v_ax0, v_ax1 - T.int64(1280), v_ax2, v_ax3], rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(1280) <= v_ax1, rxplaceholder_1[v_ax0, v_ax1 - T.int64(1280), v_ax2, v_ax3], rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def concatenate7(rxplaceholder: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), rxplaceholder_1: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), T_concat: T.Buffer((T.int64(2), T.int64(1280), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(40)):
                    with T.block("T_concat"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1310720))
                        v_ax1 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1310720) // T.int64(1024))
                        v_ax2 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1024) // T.int64(32))
                        v_ax3 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                        T.reads(rxplaceholder_1[v_ax0, v_ax1 - T.int64(640), v_ax2, v_ax3], rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(640) <= v_ax1, rxplaceholder_1[v_ax0, v_ax1 - T.int64(640), v_ax2, v_ax3], rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def concatenate8(rxplaceholder: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), rxplaceholder_1: T.Buffer((T.int64(2), T.int64(320), T.int64(32), T.int64(32)), "float32"), T_concat: T.Buffer((T.int64(2), T.int64(960), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(30)):
                    with T.block("T_concat"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(983040))
                        v_ax1 = T.axis.spatial(T.int64(960), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(983040) // T.int64(1024))
                        v_ax2 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1024) // T.int64(32))
                        v_ax3 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                        T.reads(rxplaceholder_1[v_ax0, v_ax1 - T.int64(640), v_ax2, v_ax3], rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(640) <= v_ax1, rxplaceholder_1[v_ax0, v_ax1 - T.int64(640), v_ax2, v_ax3], rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def concatenate9(rxplaceholder: T.Buffer((T.int64(2), T.int64(640), T.int64(64), T.int64(64)), "float32"), rxplaceholder_1: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), T_concat: T.Buffer((T.int64(2), T.int64(960), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(120)):
                    with T.block("T_concat"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(3932160))
                        v_ax1 = T.axis.spatial(T.int64(960), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3932160) // T.int64(4096))
                        v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4096) // T.int64(64))
                        v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                        T.reads(rxplaceholder_1[v_ax0, v_ax1 - T.int64(640), v_ax2, v_ax3], rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(640) <= v_ax1, rxplaceholder_1[v_ax0, v_ax1 - T.int64(640), v_ax2, v_ax3], rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def fused_add_divide1_multiply5_multiply5_divide_subtract(inp_0: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), inp_5: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), inp_2: T.Buffer((), "float32"), inp_1: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), inp_3: T.Buffer((), "float32"), inp_4: T.Buffer((), "float32"), T_subtract: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(512), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("T_subtract"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(inp_2[()], inp_1[v_ax0, v_ax1, v_ax2, v_ax3], inp_3[()], inp_0[v_ax0, v_ax1, v_ax2, v_ax3], inp_5[v_ax0, v_ax1, v_ax2, v_ax3], inp_4[()])
                    T.writes(T_subtract[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_subtract[v_ax0, v_ax1, v_ax2, v_ax3] = inp_2[()] * inp_1[v_ax0, v_ax1, v_ax2, v_ax3] - inp_3[()] * ((inp_0[v_ax0, v_ax1, v_ax2, v_ax3] + inp_5[v_ax0, v_ax1, v_ax2, v_ax3]) * T.float32(0.5)) / inp_4[()]

    @T.prim_func
    def fused_broadcast_to_strided_slice_reshape9_cast2_multiply3_multiply4_tir_sin_tir_cos_concatenate2_strided_slice1_reshape10_strided_slice2_reshape10_concatenate2(inp_1: T.Buffer((), "int32"), param_0: T.Buffer((T.int64(1), T.int64(160)), "float32"), T_concat: T.Buffer((T.int64(2), T.int64(320)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(5), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("T_concat_1"):
                    v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) // T.int64(320))
                    v_ax1 = T.axis.spatial(T.int64(320), (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) % T.int64(320))
                    T.reads(inp_1[()], param_0[T.int64(0), v_ax1 % T.int64(160) - T.int64(160):v_ax1 % T.int64(160) - T.int64(160) + T.int64(321)])
                    T.writes(T_concat[v_ax0, v_ax1])
                    T_concat[v_ax0, v_ax1] = T.if_then_else(T.int64(160) <= v_ax1, T.if_then_else(T.int64(160) <= (v_ax1 - T.int64(160)) % T.int64(160), T.cos(T.Cast("float32", inp_1[()]) * param_0[T.int64(0), (v_ax1 - T.int64(160)) % T.int64(160) - T.int64(160)]), T.sin(T.Cast("float32", inp_1[()]) * param_0[T.int64(0), (v_ax1 - T.int64(160)) % T.int64(160)])), T.if_then_else(T.int64(160) <= v_ax1 % T.int64(160) + T.int64(160), T.cos(T.Cast("float32", inp_1[()]) * param_0[T.int64(0), v_ax1 % T.int64(160) + T.int64(160) - T.int64(160)]), T.sin(T.Cast("float32", inp_1[()]) * param_0[T.int64(0), v_ax1 % T.int64(160) + T.int64(160)])))

    @T.prim_func
    def fused_conv2d10_add21(lv865: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), param_0: T.Buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(18), T.int64(18)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(160), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(80) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(80) // T.int64(8) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(2) + ff_3_init)
                            v_yy = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(4) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(16), xx_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) + xx_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(1280), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(80))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0)
                                    v2 = T.axis.spatial(T.int64(18), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4))
                                    v3 = T.axis.spatial(T.int64(18), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(72))
                                    T.reads(lv865[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(17) and T.int64(1) <= v3 and v3 < T.int64(17), lv865[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(80) // T.int64(8) * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(9))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0)
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(80) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), ff_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(80) // T.int64(8) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(2) + ff_3)
                                v_yy = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(4) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(16), xx_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) + xx_3)
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_2 + ry_0 * T.int64(3) + ry_1)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(4), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(80) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(80) // T.int64(8) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d10_add21_add22(lv537: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), param_0: T.Buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv546: T.Buffer((T.int64(2), T.int64(1280), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(18), T.int64(18)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(160), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(4)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(80) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(80) // T.int64(4) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(16), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + yy_3_init)
                            v_xx = T.axis.spatial(T.int64(16), nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(8) + xx_3_init * T.int64(4) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(1280), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(80))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0)
                                    v2 = T.axis.spatial(T.int64(18), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(18))
                                    v3 = T.axis.spatial(T.int64(18), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(18))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(108))
                                    T.reads(lv537[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(17) and T.int64(1) <= v3 and v3 < T.int64(17), lv537[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(80) // T.int64(4) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(9))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0)
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(2), T.int64(1), T.int64(4)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(80) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(80) // T.int64(4) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(16), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + yy_3)
                                v_xx = T.axis.spatial(T.int64(16), nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(8) + xx_3 * T.int64(4) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_2 + ry_0 * T.int64(3) + ry_1)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(8)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(80) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(80) // T.int64(4) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(8) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)], lv546[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)] + lv546[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d10_add21_add23_divide2(lv446: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), param_0: T.Buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv452: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), T_divide: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(18), T.int64(18)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init + nn_4_init + nn_2_ff_2_yy_2_xx_2_fused // T.int64(80))
                            v_ff = T.axis.spatial(T.int64(1280), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(80) // T.int64(2) * T.int64(2) + ff_3_init)
                            v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) // T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + xx_3_init * T.int64(4) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(160), T.int64(3), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) // T.int64(320))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(320) // T.int64(40))
                                    v2 = T.axis.spatial(T.int64(18), ry_0 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) // T.int64(2) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(40) // T.int64(10))
                                    v3 = T.axis.spatial(T.int64(18), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(10))
                                    T.reads(lv446[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(17) and T.int64(1) <= v3 and v3 < T.int64(17), lv446[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(12)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(80) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) // T.int64(24))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(24) // T.int64(3))
                                    v2 = T.axis.spatial(T.int64(3), ry_0)
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 + nn_4 + nn_2_ff_2_yy_2_xx_2_fused // T.int64(80))
                                v_ff = T.axis.spatial(T.int64(1280), ff_4 + nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(80) // T.int64(2) * T.int64(2) + ff_3)
                                v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) // T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + xx_3 * T.int64(4) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 * T.int64(8) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(8)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(80) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(80) // T.int64(2) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) // T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + ax3)
                            T.reads(lv452[v0, v1, v2, v3], conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(T_divide[v0, v1, v2, v3])
                            T_divide[v0, v1, v2, v3] = lv452[v0, v1, v2, v3] + (conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)])

    @T.prim_func
    def fused_conv2d11_add21(lv432: T.Buffer((T.int64(2), T.int64(640), T.int64(16), T.int64(16)), "float32"), param_0: T.Buffer((T.int64(1280), T.int64(640), T.int64(1), T.int64(1)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(16), T.int64(16)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(1280), T.int64(640), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(8) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ff_3_init)
                            v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(16), xx_4_init + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + xx_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(80), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160))
                                    v1 = T.axis.spatial(T.int64(640), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32) // T.int64(16))
                                    v3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    T.reads(lv432[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv432[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(8) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(8))
                                    v1 = T.axis.spatial(T.int64(640), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), ff_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(8) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ff_3)
                                v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(16), xx_4 + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + xx_3)
                                v_rc = T.axis.reduce(T.int64(640), rc_0 * T.int64(8) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(8) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d12_add21(lv455: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), param_0: T.Buffer((T.int64(1280), T.int64(1280), T.int64(1), T.int64(1)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(1280), T.int64(1280), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(640), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(320) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(320) // T.int64(4) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ff_3_init)
                            v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(16), xx_3_init + xx_4_init + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16))
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(320), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(320))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(64))
                                    v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64) // T.int64(16))
                                    v3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    T.reads(lv455[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv455[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(320) // T.int64(4) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(320) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), ff_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(320) // T.int64(4) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ff_3)
                                v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(16), xx_3 + xx_4 + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16))
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 * T.int64(4) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(4), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(320) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(320) // T.int64(4) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d12_add21_add23(lv531: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), param_0: T.Buffer((T.int64(1280), T.int64(1280), T.int64(1), T.int64(1)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv454: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(1280), T.int64(1280), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(10), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(4) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(10) + ff_3_init)
                            v_yy = T.axis.spatial(T.int64(16), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(4) + yy_3_init)
                            v_xx = T.axis.spatial(T.int64(16), xx_3_init + xx_4_init + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4))
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(320), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(64))
                                    v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64) // T.int64(16))
                                    v3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    T.reads(lv531[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv531[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(4) * T.int64(40) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(160))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(10), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), ff_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(4) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(10) + ff_3)
                                v_yy = T.axis.spatial(T.int64(16), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(4) + yy_3)
                                v_xx = T.axis.spatial(T.int64(16), xx_3 + xx_4 + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4))
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 * T.int64(4) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(10), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(4) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(10) + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)], lv454[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)] + lv454[v0, v1, v2, v3]

    @T.prim_func
    def fused_conv2d13_add27(lv635: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), param_0: T.Buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(18), T.int64(18)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(40), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_4_init + nn_1_ff_1_yy_1_xx_1_fused + nn_3_init)
                            v_ff = T.axis.spatial(T.int64(1280), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(4) + ff_3_init)
                            v_yy = T.axis.spatial(T.int64(8), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) + yy_3_init)
                            v_xx = T.axis.spatial(T.int64(8), xx_4_init + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(640), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(306))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(306) // T.int64(153))
                                    v2 = T.axis.spatial(T.int64(18), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(153) // T.int64(17))
                                    v3 = T.axis.spatial(T.int64(18), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(17))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(612))
                                    T.reads(lv635[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(17) and T.int64(1) <= v3 and v3 < T.int64(17), lv635[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(18))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(18) // T.int64(9))
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(3), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_4 + nn_1_ff_1_yy_1_xx_1_fused + nn_3)
                                v_ff = T.axis.spatial(T.int64(1280), ff_4 + nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(4) + ff_3)
                                v_yy = T.axis.spatial(T.int64(8), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) + yy_3)
                                v_xx = T.axis.spatial(T.int64(8), xx_4 + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3)
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 * T.int64(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_2 + ry_0 * T.int64(3) + ry_1)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(4)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_1_ff_1_yy_1_xx_1_fused + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d14_add27_add28(lv640: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), param_0: T.Buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv649: T.Buffer((T.int64(2), T.int64(1280), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(10), T.int64(10)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(40), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init + nn_4_init + nn_2_ff_2_yy_2_xx_2_fused // T.int64(128))
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(128) // T.int64(8) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(8), yy_4_init + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + yy_3_init)
                            v_xx = T.axis.spatial(T.int64(8), xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(1280), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(100))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0)
                                    v2 = T.axis.spatial(T.int64(10), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(100) // T.int64(10))
                                    v3 = T.axis.spatial(T.int64(10), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(10))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(200))
                                    T.reads(lv640[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(9) and T.int64(1) <= v3 and v3 < T.int64(9), lv640[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(9))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0)
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(288))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 + nn_4 + nn_2_ff_2_yy_2_xx_2_fused // T.int64(128))
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(128) // T.int64(8) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(8), yy_4 + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + yy_3)
                                v_xx = T.axis.spatial(T.int64(8), xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_2 + ry_0 * T.int64(3) + ry_1)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(8)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(128) // T.int64(8) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + ax2)
                            v3 = T.axis.spatial(T.int64(8), ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)], lv649[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)] + lv649[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d14_add27_add29_divide3(lv652: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), param_0: T.Buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv638: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), T_divide: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(10), T.int64(10)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(40), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(20) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(20) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(4) + ff_3_init)
                            v_yy = T.axis.spatial(T.int64(8), yy_4_init + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + yy_3_init)
                            v_xx = T.axis.spatial(T.int64(8), nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + xx_3_init * T.int64(4) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(640), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(20))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(100))
                                    v2 = T.axis.spatial(T.int64(10), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(100) // T.int64(10))
                                    v3 = T.axis.spatial(T.int64(10), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(10))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(200))
                                    T.reads(lv652[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(9) and T.int64(1) <= v3 and v3 < T.int64(9), lv652[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(20) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(18))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(18) // T.int64(9))
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(3), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(20) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), ff_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(20) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(4) + ff_3)
                                v_yy = T.axis.spatial(T.int64(8), yy_4 + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + yy_3)
                                v_xx = T.axis.spatial(T.int64(8), nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + xx_3 * T.int64(4) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 * T.int64(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_2 + ry_0 * T.int64(3) + ry_1)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(4)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(20) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(20) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + ax2)
                            v3 = T.axis.spatial(T.int64(8), nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + ax3)
                            T.reads(lv638[v0, v1, v2, v3], conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(T_divide[v0, v1, v2, v3])
                            T_divide[v0, v1, v2, v3] = lv638[v0, v1, v2, v3] + (conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)])

    @T.prim_func
    def fused_conv2d15_add27(lv696: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), param_0: T.Buffer((T.int64(1280), T.int64(1280), T.int64(1), T.int64(1)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(1280), T.int64(1280), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(40), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init + nn_4_init + nn_2_ff_2_yy_2_xx_2_fused // T.int64(128))
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(128) // T.int64(16) * T.int64(8) + ff_3_init * T.int64(8) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(4) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(8), xx_3_init + xx_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4))
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(40), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(1024))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(1024) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32) // T.int64(4))
                                    v3 = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4))
                                    T.reads(lv696[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv696[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 + nn_4 + nn_2_ff_2_yy_2_xx_2_fused // T.int64(128))
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(128) // T.int64(16) * T.int64(8) + ff_3 * T.int64(8) + ff_4)
                                v_yy = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(4) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(8), xx_3 + xx_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4))
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 * T.int64(32) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(8), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(128) // T.int64(16) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(4) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d15_add27_add29(lv772: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), param_0: T.Buffer((T.int64(1280), T.int64(1280), T.int64(1), T.int64(1)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv695: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(1280), T.int64(1280), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init * T.int64(2) + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(8) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(8), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(8) + yy_3_init)
                            v_xx = T.axis.spatial(T.int64(8), xx_3_init + xx_4_init + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8))
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(128), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(80) + ax0_ax1_ax2_ax3_fused_1) // T.int64(160))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(10) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(80) + ax0_ax1_ax2_ax3_fused_1) % T.int64(160) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(80) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16) // T.int64(8))
                                    v3 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(80) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    T.reads(lv772[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv772[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(40) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(80) + ax0_ax1_ax2_ax3_fused_1) // T.int64(10))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(10) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(80) + ax0_ax1_ax2_ax3_fused_1) % T.int64(10))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(10), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 * T.int64(2) + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(8) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(8), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(8) + yy_3)
                                v_xx = T.axis.spatial(T.int64(8), xx_3 + xx_4 + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8))
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 * T.int64(10) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(8), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(8) + ax2)
                            v3 = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)], lv695[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)] + lv695[v0, v1, v2, v3]

    @T.prim_func
    def fused_conv2d16_add27_add28(lv798: T.Buffer((T.int64(2), T.int64(2560), T.int64(8), T.int64(8)), "float32"), param_0: T.Buffer((T.int64(1280), T.int64(2560), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv807: T.Buffer((T.int64(2), T.int64(1280), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(2560), T.int64(10), T.int64(10)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(1280), T.int64(2560), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(80), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init + nn_4_init + nn_2_ff_2_yy_2_xx_2_fused // T.int64(64))
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(4) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) * T.int64(4) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(8), xx_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(640), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(240))
                                    v1 = T.axis.spatial(T.int64(2560), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(240) // T.int64(60))
                                    v2 = T.axis.spatial(T.int64(10), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(60) // T.int64(6))
                                    v3 = T.axis.spatial(T.int64(10), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(6))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(480))
                                    T.reads(lv798[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(9) and T.int64(1) <= v3 and v3 < T.int64(9), lv798[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(36))
                                    v1 = T.axis.spatial(T.int64(2560), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(36) // T.int64(9))
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(3), T.int64(3), T.int64(1), T.int64(1), T.int64(4), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 + nn_4 + nn_2_ff_2_yy_2_xx_2_fused // T.int64(64))
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(4) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) * T.int64(4) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(8), xx_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3)
                                v_rc = T.axis.reduce(T.int64(2560), rc_0 * T.int64(4) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_2 + ry_0 * T.int64(3) + ry_1)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(4), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(4) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)], lv807[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)] + lv807[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d17_add27(lv796: T.Buffer((T.int64(2), T.int64(2560), T.int64(8), T.int64(8)), "float32"), param_0: T.Buffer((T.int64(1280), T.int64(2560), T.int64(1), T.int64(1)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(2560), T.int64(8), T.int64(8)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(1280), T.int64(2560), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(160), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(2), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(2) + ff_3_init)
                            v_yy = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(8), xx_4_init + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(2) + xx_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(640), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(2560), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16) // T.int64(8))
                                    v3 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    T.reads(lv796[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv796[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4))
                                    v1 = T.axis.spatial(T.int64(2560), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), ff_4 + nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(2) + ff_3)
                                v_yy = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(8), xx_4 + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(2) + xx_3)
                                v_rc = T.axis.reduce(T.int64(2560), rc_0 * T.int64(4) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(2), T.int64(2), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d18_add21_add22(lv871: T.Buffer((T.int64(2), T.int64(2560), T.int64(16), T.int64(16)), "float32"), param_0: T.Buffer((T.int64(1280), T.int64(2560), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv880: T.Buffer((T.int64(2), T.int64(1280), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(2560), T.int64(18), T.int64(18)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(1280), T.int64(2560), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(4)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init + nn_4_init + nn_2_ff_2_yy_2_xx_2_fused // T.int64(80))
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(80) // T.int64(4) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(16), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) // T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + yy_3_init)
                            v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + xx_3_init * T.int64(4) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(320), T.int64(3), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) // T.int64(320))
                                    v1 = T.axis.spatial(T.int64(2560), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(320) // T.int64(40))
                                    v2 = T.axis.spatial(T.int64(18), ry_0 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) // T.int64(2) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(40) // T.int64(10))
                                    v3 = T.axis.spatial(T.int64(18), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(10))
                                    T.reads(lv871[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(17) and T.int64(1) <= v3 and v3 < T.int64(17), lv871[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(6)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(40) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) // T.int64(24))
                                    v1 = T.axis.spatial(T.int64(2560), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(24) // T.int64(3))
                                    v2 = T.axis.spatial(T.int64(3), ry_0)
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(2), T.int64(1), T.int64(4)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 + nn_4 + nn_2_ff_2_yy_2_xx_2_fused // T.int64(80))
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(80) // T.int64(4) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(16), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) // T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + yy_3)
                                v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + xx_3 * T.int64(4) + xx_4)
                                v_rc = T.axis.reduce(T.int64(2560), rc_0 * T.int64(8) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(8)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(80) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(80) // T.int64(4) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) // T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)], lv880[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)] + lv880[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d19_add21(lv869: T.Buffer((T.int64(2), T.int64(2560), T.int64(16), T.int64(16)), "float32"), param_0: T.Buffer((T.int64(1280), T.int64(2560), T.int64(1), T.int64(1)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(2560), T.int64(16), T.int64(16)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(1280), T.int64(2560), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(4) + ff_3_init)
                            v_yy = T.axis.spatial(T.int64(16), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(8) + yy_3_init)
                            v_xx = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(80), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(13)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) // T.int64(1024))
                                    v1 = T.axis.spatial(T.int64(2560), rc_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(1024) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32) // T.int64(16))
                                    v3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 < T.int64(2048))
                                    T.reads(lv869[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv869[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(40) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(2560), rc_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), ff_4 + nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(4) + ff_3)
                                v_yy = T.axis.spatial(T.int64(16), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(8) + yy_3)
                                v_xx = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(2560), rc_0 * T.int64(32) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(4), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(8) + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d1_add5_add7(lv25: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), param_0: T.Buffer((T.int64(320), T.int64(320), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), lv34: T.Buffer((T.int64(2), T.int64(320), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(66), T.int64(66)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(320), T.int64(320), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(5), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(256) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(16) * T.int64(20) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(32) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(320), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(320), rc_0)
                                    v2 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(66))
                                    v3 = T.axis.spatial(T.int64(66), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(66))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(396))
                                    T.reads(lv25[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(65) and T.int64(1) <= v3 and v3 < T.int64(65), lv25[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(16) * T.int64(20) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(9))
                                    v1 = T.axis.spatial(T.int64(320), rc_0)
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(256) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(16) * T.int64(20) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(32) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(320), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_2 + ry_0 * T.int64(3) + ry_1)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(256) + ax0)
                            v1 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(16) * T.int64(20) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(32) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)], lv34[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)] + lv34[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d1_add5_add8_divide(lv37: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), param_0: T.Buffer((T.int64(320), T.int64(320), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), lv23: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), T_divide: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(66), T.int64(66)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(320), T.int64(320), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(640), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(2), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(320) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(320) // T.int64(64) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(4) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(4) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(4) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(320), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(320))
                                    v1 = T.axis.spatial(T.int64(320), rc_0)
                                    v2 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(18))
                                    v3 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(18))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(108))
                                    T.reads(lv37[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(65) and T.int64(1) <= v3 and v3 < T.int64(65), lv37[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(320) // T.int64(64) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(9))
                                    v1 = T.axis.spatial(T.int64(320), rc_0)
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(3), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(320) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(320) // T.int64(64) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(4) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(4) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(4) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(320), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_2 + ry_0 * T.int64(3) + ry_1)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(2), T.int64(4)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(320) + ax0)
                            v1 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(320) // T.int64(64) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(4) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(4) + ax3)
                            T.reads(lv23[v0, v1, v2, v3], conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(T_divide[v0, v1, v2, v3])
                            T_divide[v0, v1, v2, v3] = lv23[v0, v1, v2, v3] + (conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)])

    @T.prim_func
    def fused_conv2d20_add21_add22(lv1079: T.Buffer((T.int64(2), T.int64(1920), T.int64(16), T.int64(16)), "float32"), param_0: T.Buffer((T.int64(1280), T.int64(1920), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv1088: T.Buffer((T.int64(2), T.int64(1280), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1920), T.int64(18), T.int64(18)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(1280), T.int64(1920), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(80), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_4_init + nn_1_ff_1_yy_1_xx_1_fused + nn_3_init)
                            v_ff = T.axis.spatial(T.int64(1280), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) * T.int64(4) + ff_3_init)
                            v_yy = T.axis.spatial(T.int64(16), yy_4_init + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(4) + yy_3_init)
                            v_xx = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(4) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(1920), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(324))
                                    v1 = T.axis.spatial(T.int64(1920), rc_0)
                                    v2 = T.axis.spatial(T.int64(18), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(324) // T.int64(18))
                                    v3 = T.axis.spatial(T.int64(18), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(18))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(648))
                                    T.reads(lv1079[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(17) and T.int64(1) <= v3 and v3 < T.int64(17), lv1079[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(9))
                                    v1 = T.axis.spatial(T.int64(1920), rc_0)
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(144))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_4 + nn_1_ff_1_yy_1_xx_1_fused + nn_3)
                                v_ff = T.axis.spatial(T.int64(1280), ff_4 + nn_0_ff_0_yy_0_xx_0_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) * T.int64(4) + ff_3)
                                v_yy = T.axis.spatial(T.int64(16), yy_4 + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(4) + yy_3)
                                v_xx = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(4) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1920), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_2 + ry_0 * T.int64(3) + ry_1)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(4)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_1_ff_1_yy_1_xx_1_fused + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(4) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)], lv1088[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)] + lv1088[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d21_add21(lv1077: T.Buffer((T.int64(2), T.int64(1920), T.int64(16), T.int64(16)), "float32"), param_0: T.Buffer((T.int64(1280), T.int64(1920), T.int64(1), T.int64(1)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1920), T.int64(16), T.int64(16)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(1280), T.int64(1920), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(160), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init + nn_4_init + nn_2_ff_2_yy_2_xx_2_fused // T.int64(64))
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(8) * T.int64(4) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(16), xx_3_init + xx_4_init + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8))
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(160), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(6)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(384))
                                    v1 = T.axis.spatial(T.int64(1920), rc_0 * T.int64(12) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(384) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32) // T.int64(16))
                                    v3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    T.reads(lv1077[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv1077[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(6)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(12))
                                    v1 = T.axis.spatial(T.int64(1920), rc_0 * T.int64(12) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(12))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 + nn_4 + nn_2_ff_2_yy_2_xx_2_fused // T.int64(64))
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(8) * T.int64(4) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(16), xx_3 + xx_4 + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8))
                                v_rc = T.axis.reduce(T.int64(1920), rc_0 * T.int64(12) + rc_1 * T.int64(3) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(8) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d22_add33(lv1181: T.Buffer((T.int64(2), T.int64(1280), T.int64(32), T.int64(32)), "float32"), param_0: T.Buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(1280), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(34), T.int64(34)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1280), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(640) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(640) // T.int64(32) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(2) + ff_3_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), xx_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + xx_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(1280), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(640))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0)
                                    v2 = T.axis.spatial(T.int64(34), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(4) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(10))
                                    v3 = T.axis.spatial(T.int64(34), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(10))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(60))
                                    T.reads(lv1181[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(33) and T.int64(1) <= v3 and v3 < T.int64(33), lv1181[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(640) // T.int64(32) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(9))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0)
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(640) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), ff_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(640) // T.int64(32) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(2) + ff_3)
                                v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), xx_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + xx_3)
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_2 + ry_0 * T.int64(3) + ry_1)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(4)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(640) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(640) // T.int64(32) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d23_add13_add15(lv1187: T.Buffer((T.int64(2), T.int64(1920), T.int64(32), T.int64(32)), "float32"), param_0: T.Buffer((T.int64(640), T.int64(1920), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv1196: T.Buffer((T.int64(2), T.int64(640), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1920), T.int64(34), T.int64(34)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(640), T.int64(1920), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(160), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(80) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(80) // T.int64(8) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ff_3_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(960), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(80))
                                    v1 = T.axis.spatial(T.int64(1920), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(204))
                                    v2 = T.axis.spatial(T.int64(34), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(204) // T.int64(6))
                                    v3 = T.axis.spatial(T.int64(34), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(6))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(408))
                                    T.reads(lv1187[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(33) and T.int64(1) <= v3 and v3 < T.int64(33), lv1187[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(80) // T.int64(8) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(18))
                                    v1 = T.axis.spatial(T.int64(1920), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(18) // T.int64(9))
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(80) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), ff_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(80) // T.int64(8) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ff_3)
                                v_yy = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1920), rc_0 * T.int64(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_2 + ry_0 * T.int64(3) + ry_1)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(4)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(80) + ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(80) // T.int64(8) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)], lv1196[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)] + lv1196[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d24_add13(lv1185: T.Buffer((T.int64(2), T.int64(1920), T.int64(32), T.int64(32)), "float32"), param_0: T.Buffer((T.int64(640), T.int64(1920), T.int64(1), T.int64(1)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1920), T.int64(32), T.int64(32)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(640), T.int64(1920), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(5), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_4_init + nn_1_ff_1_yy_1_xx_1_fused + nn_3_init)
                            v_ff = T.axis.spatial(T.int64(640), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(5) + ff_3_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), xx_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + xx_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(96), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(640))
                                    v1 = T.axis.spatial(T.int64(1920), rc_0 * T.int64(20) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(640) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32) // T.int64(16))
                                    v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    T.reads(lv1185[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv1185[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(13)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(80) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(20))
                                    v1 = T.axis.spatial(T.int64(1920), rc_0 * T.int64(20) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(20))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(1600))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(5), T.int64(1), T.int64(1), T.int64(1), T.int64(5), T.int64(2), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_4 + nn_1_ff_1_yy_1_xx_1_fused + nn_3)
                                v_ff = T.axis.spatial(T.int64(640), ff_4 + nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(5) + ff_3)
                                v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), xx_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + xx_3)
                                v_rc = T.axis.reduce(T.int64(1920), rc_0 * T.int64(20) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(5), T.int64(2), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_1_ff_1_yy_1_xx_1_fused + ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(5) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d25_add13_add15(lv1291: T.Buffer((T.int64(2), T.int64(1280), T.int64(32), T.int64(32)), "float32"), param_0: T.Buffer((T.int64(640), T.int64(1280), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv1300: T.Buffer((T.int64(2), T.int64(640), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(34), T.int64(34)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(640), T.int64(1280), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(8) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(4) + ff_3_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(640), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(204))
                                    v2 = T.axis.spatial(T.int64(34), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(204) // T.int64(34))
                                    v3 = T.axis.spatial(T.int64(34), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(34))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(408))
                                    T.reads(lv1291[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(33) and T.int64(1) <= v3 and v3 < T.int64(33), lv1291[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(8) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(18))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(18) // T.int64(9))
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(576))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(2), T.int64(1), T.int64(1), T.int64(3), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), ff_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(8) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(4) + ff_3)
                                v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 * T.int64(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(4), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160) + ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(8) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)], lv1300[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)] + lv1300[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d26_add13(lv1289: T.Buffer((T.int64(2), T.int64(1280), T.int64(32), T.int64(32)), "float32"), param_0: T.Buffer((T.int64(640), T.int64(1280), T.int64(1), T.int64(1)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(32), T.int64(32)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(640), T.int64(1280), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1280), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init * T.int64(2) + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(4) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(32), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(8) + yy_3_init)
                            v_xx = T.axis.spatial(T.int64(32), xx_3_init + xx_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8))
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(160), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(256) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32) // T.int64(16))
                                    v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    T.reads(lv1289[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv1289[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(8))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 * T.int64(2) + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(4) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(32), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(8) + yy_3)
                                v_xx = T.axis.spatial(T.int64(32), xx_3 + xx_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8))
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 * T.int64(8) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(8) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d27_add13_add15(lv1395: T.Buffer((T.int64(2), T.int64(960), T.int64(32), T.int64(32)), "float32"), param_0: T.Buffer((T.int64(640), T.int64(960), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv1404: T.Buffer((T.int64(2), T.int64(640), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(960), T.int64(34), T.int64(34)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(640), T.int64(960), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(160), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(4)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(80) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(80) // T.int64(16) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(4) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(4) + xx_3_init * T.int64(4) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(960), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(80))
                                    v1 = T.axis.spatial(T.int64(960), rc_0)
                                    v2 = T.axis.spatial(T.int64(34), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(2) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(18))
                                    v3 = T.axis.spatial(T.int64(34), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(18))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(108))
                                    T.reads(lv1395[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(33) and T.int64(1) <= v3 and v3 < T.int64(33), lv1395[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(80) // T.int64(16) * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(9))
                                    v1 = T.axis.spatial(T.int64(960), rc_0)
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(4)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(80) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(80) // T.int64(16) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(4) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(4) + xx_3 * T.int64(4) + xx_4)
                                v_rc = T.axis.reduce(T.int64(960), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_2 + ry_0 * T.int64(3) + ry_1)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(4)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(80) + ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(80) // T.int64(16) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(4) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(4) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)], lv1404[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)] + lv1404[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d28_add13(lv1393: T.Buffer((T.int64(2), T.int64(960), T.int64(32), T.int64(32)), "float32"), param_0: T.Buffer((T.int64(640), T.int64(960), T.int64(1), T.int64(1)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(960), T.int64(32), T.int64(32)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(640), T.int64(960), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(640), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(320) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(320) // T.int64(32) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(32), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) + yy_3_init)
                            v_xx = T.axis.spatial(T.int64(32), xx_4_init + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + xx_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(120), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(320))
                                    v1 = T.axis.spatial(T.int64(960), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32))
                                    v3 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    T.reads(lv1393[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv1393[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(320) // T.int64(32) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(8))
                                    v1 = T.axis.spatial(T.int64(960), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(320) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(320) // T.int64(32) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(32), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) + yy_3)
                                v_xx = T.axis.spatial(T.int64(32), xx_4 + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + xx_3)
                                v_rc = T.axis.reduce(T.int64(960), rc_0 * T.int64(8) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(320) + ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(320) // T.int64(32) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d29_add34(lv1497: T.Buffer((T.int64(2), T.int64(640), T.int64(64), T.int64(64)), "float32"), param_0: T.Buffer((T.int64(640), T.int64(640), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(640), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(66), T.int64(66)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(640), T.int64(640), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(5), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(64) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(5) + ff_3_init)
                            v_yy = T.axis.spatial(T.int64(64), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + yy_3_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(320), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512))
                                    v1 = T.axis.spatial(T.int64(640), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(100))
                                    v2 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(8) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(100) // T.int64(10))
                                    v3 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(10))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(200))
                                    T.reads(lv1497[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(65) and T.int64(1) <= v3 and v3 < T.int64(65), lv1497[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(12)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(64) * T.int64(80) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(18))
                                    v1 = T.axis.spatial(T.int64(640), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(18) // T.int64(9))
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(1440))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(3), T.int64(1), T.int64(1), T.int64(5), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), ff_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(64) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(5) + ff_3)
                                v_yy = T.axis.spatial(T.int64(64), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + yy_3)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(640), rc_0 * T.int64(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_2 + ry_0 * T.int64(3) + ry_1)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(5), T.int64(1), T.int64(8)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) + ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(64) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(5) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d2_add5(lv43: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), param_0: T.Buffer((T.int64(320), T.int64(320), T.int64(1), T.int64(1)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(320), T.int64(320), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(10), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init + nn_4_init + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32))
                            v_ff = T.axis.spatial(T.int64(320), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused // T.int64(256) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(4) * T.int64(10) + ff_3_init)
                            v_yy = T.axis.spatial(T.int64(64), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(8) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) + yy_3_init)
                            v_xx = T.axis.spatial(T.int64(64), xx_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(40), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(128) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(8) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16) // T.int64(8))
                                    v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    T.reads(lv43[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv43[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(256) * T.int64(80) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(8))
                                    v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(10), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 + nn_4 + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32))
                                v_ff = T.axis.spatial(T.int64(320), ff_4 + nn_0_ff_0_yy_0_xx_0_fused // T.int64(256) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(4) * T.int64(10) + ff_3)
                                v_yy = T.axis.spatial(T.int64(64), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(8) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) + yy_3)
                                v_xx = T.axis.spatial(T.int64(64), xx_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3)
                                v_rc = T.axis.reduce(T.int64(320), rc_0 * T.int64(8) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(10), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) + ax0)
                            v1 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(256) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(4) * T.int64(10) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(8) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d2_add5_add8(lv119: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), param_0: T.Buffer((T.int64(320), T.int64(320), T.int64(1), T.int64(1)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), lv42: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(320), T.int64(320), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(5), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_4_init + nn_1_ff_1_yy_1_xx_1_fused + nn_3_init)
                            v_ff = T.axis.spatial(T.int64(320), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused // T.int64(256) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(5) + ff_3_init)
                            v_yy = T.axis.spatial(T.int64(64), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(4) + yy_3_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(4) + xx_3_init * T.int64(4) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(20), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(256) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(4))
                                    v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    T.reads(lv119[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv119[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(20)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(256) * T.int64(80) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(5), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_4 + nn_1_ff_1_yy_1_xx_1_fused + nn_3)
                                v_ff = T.axis.spatial(T.int64(320), ff_4 + nn_0_ff_0_yy_0_xx_0_fused // T.int64(256) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(5) + ff_3)
                                v_yy = T.axis.spatial(T.int64(64), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(4) + yy_3)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(4) + xx_3 * T.int64(4) + xx_4)
                                v_rc = T.axis.reduce(T.int64(320), rc_0 * T.int64(16) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(5), T.int64(1), T.int64(4)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_1_ff_1_yy_1_xx_1_fused + ax0)
                            v1 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(256) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(5) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(4) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)], lv42[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)] + lv42[v0, v1, v2, v3]

    @T.prim_func
    def fused_conv2d30_add5_add7(lv1503: T.Buffer((T.int64(2), T.int64(960), T.int64(64), T.int64(64)), "float32"), param_0: T.Buffer((T.int64(320), T.int64(960), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), lv1512: T.Buffer((T.int64(2), T.int64(320), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(960), T.int64(66), T.int64(66)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(320), T.int64(960), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(320), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(32) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(4) + ff_3_init)
                            v_yy = T.axis.spatial(T.int64(64), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(8) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(2) + yy_3_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(480), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160))
                                    v1 = T.axis.spatial(T.int64(960), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(180))
                                    v2 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(8) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(180) // T.int64(10))
                                    v3 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(10))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(360))
                                    T.reads(lv1503[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(65) and T.int64(1) <= v3 and v3 < T.int64(65), lv1503[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(32) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(18))
                                    v1 = T.axis.spatial(T.int64(960), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(18) // T.int64(9))
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(3), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(320), ff_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(32) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(4) + ff_3)
                                v_yy = T.axis.spatial(T.int64(64), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(8) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(2) + yy_3)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(960), rc_0 * T.int64(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_2 + ry_0 * T.int64(3) + ry_1)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(4)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160) + ax0)
                            v1 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(32) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(8) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)], lv1512[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)] + lv1512[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d31_add5(lv1501: T.Buffer((T.int64(2), T.int64(960), T.int64(64), T.int64(64)), "float32"), param_0: T.Buffer((T.int64(320), T.int64(960), T.int64(1), T.int64(1)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(960), T.int64(64), T.int64(64)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(320), T.int64(960), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(640), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init + nn_4_init + nn_2_ff_2_yy_2_xx_2_fused // T.int64(64))
                            v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(8) * T.int64(4) + ff_3_init * T.int64(4) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) + yy_3_init)
                            v_xx = T.axis.spatial(T.int64(64), xx_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(60), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(512))
                                    v1 = T.axis.spatial(T.int64(960), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(512) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(8) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32) // T.int64(8))
                                    v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    T.reads(lv1501[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv1501[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(960), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 + nn_4 + nn_2_ff_2_yy_2_xx_2_fused // T.int64(64))
                                v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(8) * T.int64(4) + ff_3 * T.int64(4) + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) + yy_3)
                                v_xx = T.axis.spatial(T.int64(64), xx_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3)
                                v_rc = T.axis.reduce(T.int64(960), rc_0 * T.int64(16) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(4)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(8) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d32_add5_add7(lv1607: T.Buffer((T.int64(2), T.int64(640), T.int64(64), T.int64(64)), "float32"), param_0: T.Buffer((T.int64(320), T.int64(640), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), lv1616: T.Buffer((T.int64(2), T.int64(320), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(66), T.int64(66)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(320), T.int64(640), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(256) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(64) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(4) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(32) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(4) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(640), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(640), rc_0)
                                    v2 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(32) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4))
                                    v3 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 < T.int64(136))
                                    T.reads(lv1607[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(65) and T.int64(1) <= v3 and v3 < T.int64(65), lv1607[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(64) * T.int64(80) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) // T.int64(9))
                                    v1 = T.axis.spatial(T.int64(640), rc_0)
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 < T.int64(720))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(3), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(256) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(64) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(4) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(32) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(4) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(640), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_2 + ry_0 * T.int64(3) + ry_1)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(4), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(256) + ax0)
                            v1 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(64) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(32) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)], lv1616[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)] + lv1616[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d33_add5(lv1605: T.Buffer((T.int64(2), T.int64(640), T.int64(64), T.int64(64)), "float32"), param_0: T.Buffer((T.int64(320), T.int64(640), T.int64(1), T.int64(1)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(64), T.int64(64)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(320), T.int64(640), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(640), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(320) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(320) // T.int64(64) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(8) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(16) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), xx_3_init + xx_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16))
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(40), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(320))
                                    v1 = T.axis.spatial(T.int64(640), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(64))
                                    v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64) // T.int64(16))
                                    v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    T.reads(lv1605[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv1605[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(320) // T.int64(64) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(640), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(320) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(320) // T.int64(64) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(8) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(16) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), xx_3 + xx_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16))
                                v_rc = T.axis.reduce(T.int64(640), rc_0 * T.int64(16) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(8), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(320) + ax0)
                            v1 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(320) // T.int64(64) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(16) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d34_add35(lv1814: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), param_0: T.Buffer((T.int64(4), T.int64(320), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(4), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(4), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(66), T.int64(66)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(4), T.int64(320), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(4), ff_4_init + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(2) + ff_3_init)
                            v_yy = T.axis.spatial(T.int64(64), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) + yy_3_init)
                            v_xx = T.axis.spatial(T.int64(64), xx_4_init + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) * T.int64(2) + xx_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(80), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(13)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(198))
                                    v2 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(198) // T.int64(66))
                                    v3 = T.axis.spatial(T.int64(66), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(66))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(792))
                                    T.reads(lv1814[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(65) and T.int64(1) <= v3 and v3 < T.int64(65), lv1814[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(36))
                                    v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(36) // T.int64(9))
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(144))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(3), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(4), ff_4 + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(2) + ff_3)
                                v_yy = T.axis.spatial(T.int64(64), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) + yy_3)
                                v_xx = T.axis.spatial(T.int64(64), xx_4 + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) * T.int64(2) + xx_3)
                                v_rc = T.axis.reduce(T.int64(320), rc_0 * T.int64(4) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_2 + ry_0 * T.int64(3) + ry_1)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(4), nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d35_add37(inp_0: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), param_0: T.Buffer((T.int64(4), T.int64(4), T.int64(1), T.int64(1)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(4), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(4), nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), xx_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + xx_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(4), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4), rc_0)
                                    v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4))
                                    v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4))
                                    T.reads(inp_0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = inp_0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1)
                                    v1 = T.axis.spatial(T.int64(4), rc_0)
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1 < T.int64(4))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(4), nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), xx_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + xx_3)
                                v_rc = T.axis.reduce(T.int64(4), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(4)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4), nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d36_add38(lv2: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), param_0: T.Buffer((T.int64(512), T.int64(4), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(4), T.int64(66), T.int64(66)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(512), T.int64(4), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(8) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) * T.int64(4) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), xx_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(4), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4), rc_0)
                                    v2 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(8) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(10))
                                    v3 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(10))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180))
                                    T.reads(lv2[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(65) and T.int64(1) <= v3 and v3 < T.int64(65), lv2[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(9))
                                    v1 = T.axis.spatial(T.int64(4), rc_0)
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(144))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(2), T.int64(1), T.int64(3), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(8) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) * T.int64(4) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), xx_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3)
                                v_rc = T.axis.reduce(T.int64(4), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(8) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d37_add38(lv7: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32"), param_0: T.Buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(66), T.int64(66)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(4) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(4) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(512), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0)
                                    v2 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(18))
                                    v3 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(18))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(108))
                                    T.reads(lv7[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(65) and T.int64(1) <= v3 and v3 < T.int64(65), lv7[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(9))
                                    v1 = T.axis.spatial(T.int64(512), rc_0)
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(4) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(4) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_2 + ry_0 * T.int64(3) + ry_1)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(4)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(4) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(4) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d37_add38_add39_divide4(lv12: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32"), param_0: T.Buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), lv5: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(66), T.int64(66)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(4) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(16) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) * T.int64(4) + yy_3_init * T.int64(4) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(512), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0)
                                    v2 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(16) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(6))
                                    v3 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(6))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(108))
                                    T.reads(lv12[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(65) and T.int64(1) <= v3 and v3 < T.int64(65), lv12[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(9))
                                    v1 = T.axis.spatial(T.int64(512), rc_0)
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(288))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(3), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(4) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(16) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) * T.int64(4) + yy_3 * T.int64(4) + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_2 + ry_0 * T.int64(3) + ry_1)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(4), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(16) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + ax3)
                            T.reads(lv5[v0, v1, v2, v3], conv2d_nchw_local[v0, v1, v2, v3], param_1[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_divide[v0, v1, v2, v3])
                            T_divide[v0, v1, v2, v3] = lv5[v0, v1, v2, v3] + (conv2d_nchw_local[v0, v1, v2, v3] + param_1[v0, v1, T.int64(0), T.int64(0)])

    @T.prim_func
    def fused_conv2d38_add41(lv104: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32"), param_0: T.Buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(130), T.int64(130)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(2048), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(256) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(128), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + yy_3_init)
                            v_xx = T.axis.spatial(T.int64(128), xx_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(8) + xx_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(512), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0)
                                    v2 = T.axis.spatial(T.int64(130), nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(8) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(18))
                                    v3 = T.axis.spatial(T.int64(130), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(18))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(108))
                                    T.reads(lv104[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(129) and T.int64(1) <= v3 and v3 < T.int64(129), lv104[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(256) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(9))
                                    v1 = T.axis.spatial(T.int64(512), rc_0)
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(256) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(128), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + yy_3)
                                v_xx = T.axis.spatial(T.int64(128), xx_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(8) + xx_3)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_2 + ry_0 * T.int64(3) + ry_1)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(8)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(256) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(8) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d38_add41_add42_divide5(lv114: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32"), param_0: T.Buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), lv107: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(130), T.int64(130)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(4096), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(4) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(32) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) * T.int64(4) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(512), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0)
                                    v2 = T.axis.spatial(T.int64(130), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(32) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(6))
                                    v3 = T.axis.spatial(T.int64(130), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(6))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(60))
                                    T.reads(lv114[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(129) and T.int64(1) <= v3 and v3 < T.int64(129), lv114[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(9))
                                    v1 = T.axis.spatial(T.int64(512), rc_0)
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(3), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(4) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(32) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) * T.int64(4) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_2 + ry_0 * T.int64(3) + ry_1)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(4), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(32) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + ax3)
                            T.reads(lv107[v0, v1, v2, v3], conv2d_nchw_local[v0, v1, v2, v3], param_1[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_divide[v0, v1, v2, v3])
                            T_divide[v0, v1, v2, v3] = lv107[v0, v1, v2, v3] + (conv2d_nchw_local[v0, v1, v2, v3] + param_1[v0, v1, T.int64(0), T.int64(0)])

    @T.prim_func
    def fused_conv2d39_add43(lv144: T.Buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)), "float32"), param_0: T.Buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(258), T.int64(258)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(8192), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1024) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(256), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(1024) // T.int64(16) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + yy_3_init)
                            v_xx = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(8) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(512), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0)
                                    v2 = T.axis.spatial(T.int64(258), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1024) // T.int64(16) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(18))
                                    v3 = T.axis.spatial(T.int64(258), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(18))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(108))
                                    T.reads(lv144[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(257) and T.int64(1) <= v3 and v3 < T.int64(257), lv144[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1024) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(9))
                                    v1 = T.axis.spatial(T.int64(512), rc_0)
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1024) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(256), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(1024) // T.int64(16) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + yy_3)
                                v_xx = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(8) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_2 + ry_0 * T.int64(3) + ry_1)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(8)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1024) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1024) // T.int64(16) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(8) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d3_add12(lv223: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), param_0: T.Buffer((T.int64(320), T.int64(320), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(320), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(66), T.int64(66)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(320), T.int64(320), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(32) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(8) + ff_3_init * T.int64(8) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(32), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(4) + yy_3_init)
                            v_xx = T.axis.spatial(T.int64(32), xx_3_init + xx_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4))
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(320), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(320), rc_0)
                                    v2 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(4) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) // T.int64(17))
                                    v3 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(17))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 < T.int64(153))
                                    T.reads(lv223[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(65) and T.int64(1) <= v3 and v3 < T.int64(65), lv223[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(32) * T.int64(80) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) // T.int64(9))
                                    v1 = T.axis.spatial(T.int64(320), rc_0)
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 < T.int64(720))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(32) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(8) + ff_3 * T.int64(8) + ff_4)
                                v_yy = T.axis.spatial(T.int64(32), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(4) + yy_3)
                                v_xx = T.axis.spatial(T.int64(32), xx_3 + xx_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4))
                                v_rc = T.axis.reduce(T.int64(320), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_2 + ry_0 * T.int64(3) + ry_1)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(32) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d40_add44(lv149: T.Buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)), "float32"), param_0: T.Buffer((T.int64(256), T.int64(512), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(258), T.int64(258)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(256), T.int64(512), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(8192), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(2) + ff_3_init)
                            v_yy = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(64) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) * T.int64(4) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(512), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0)
                                    v2 = T.axis.spatial(T.int64(258), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(64) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(6))
                                    v3 = T.axis.spatial(T.int64(258), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(6))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(60))
                                    T.reads(lv149[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(257) and T.int64(1) <= v3 and v3 < T.int64(257), lv149[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(9))
                                    v1 = T.axis.spatial(T.int64(512), rc_0)
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), ff_4 + nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(2) + ff_3)
                                v_yy = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(64) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) * T.int64(4) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_2 + ry_0 * T.int64(3) + ry_1)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(4), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(64) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d41_add44(lv164: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32"), param_0: T.Buffer((T.int64(256), T.int64(256), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(258), T.int64(258)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(256), T.int64(256), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(8192), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(2) + ff_3_init)
                            v_yy = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(64) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) * T.int64(4) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(256), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(256), rc_0)
                                    v2 = T.axis.spatial(T.int64(258), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(64) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(6))
                                    v3 = T.axis.spatial(T.int64(258), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(6))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(60))
                                    T.reads(lv164[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(257) and T.int64(1) <= v3 and v3 < T.int64(257), lv164[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(9))
                                    v1 = T.axis.spatial(T.int64(256), rc_0)
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), ff_4 + nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(2) + ff_3)
                                v_yy = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(64) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) * T.int64(4) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(256), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_2 + ry_0 * T.int64(3) + ry_1)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(4), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(64) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d41_add44_add45_divide6(lv154: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32"), param_0: T.Buffer((T.int64(256), T.int64(256), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), lv160: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(258), T.int64(258)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(256), T.int64(256), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(8192), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(64) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) * T.int64(4) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(256), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(256), rc_0)
                                    v2 = T.axis.spatial(T.int64(258), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(64) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(6))
                                    v3 = T.axis.spatial(T.int64(258), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(6))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(60))
                                    T.reads(lv154[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(257) and T.int64(1) <= v3 and v3 < T.int64(257), lv154[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(9))
                                    v1 = T.axis.spatial(T.int64(256), rc_0)
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(3), T.int64(3), T.int64(1), T.int64(2), T.int64(2), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(64) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) * T.int64(4) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(256), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(4), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(64) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + ax3)
                            T.reads(lv160[v0, v1, v2, v3], conv2d_nchw_local[v0, v1, v2, v3], param_1[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_divide[v0, v1, v2, v3])
                            T_divide[v0, v1, v2, v3] = lv160[v0, v1, v2, v3] + (conv2d_nchw_local[v0, v1, v2, v3] + param_1[v0, v1, T.int64(0), T.int64(0)])

    @T.prim_func
    def fused_conv2d42_add44(lv147: T.Buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)), "float32"), param_0: T.Buffer((T.int64(256), T.int64(512), T.int64(1), T.int64(1)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(256), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(4096), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), ff_4_init + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(8) + ff_3_init)
                            v_yy = T.axis.spatial(T.int64(256), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) + yy_3_init)
                            v_xx = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16) // T.int64(8))
                                    v3 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    T.reads(lv147[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv147[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(8))
                                    v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), ff_4 + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(8) + ff_3)
                                v_yy = T.axis.spatial(T.int64(256), yy_4 + nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) + yy_3)
                                v_xx = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(8) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(4)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d43_add46(lv187: T.Buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)), "float32"), param_0: T.Buffer((T.int64(256), T.int64(256), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(514), T.int64(514)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(256), T.int64(256), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(32768), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused // T.int64(8192) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(2) + ff_3_init)
                            v_yy = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8192) // T.int64(64) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(512), xx_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(8) + xx_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(256), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(256), rc_0)
                                    v2 = T.axis.spatial(T.int64(514), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8192) // T.int64(64) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(10))
                                    v3 = T.axis.spatial(T.int64(514), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(10))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(60))
                                    T.reads(lv187[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(513) and T.int64(1) <= v3 and v3 < T.int64(513), lv187[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8192) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(9))
                                    v1 = T.axis.spatial(T.int64(256), rc_0)
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), ff_4 + nn_0_ff_0_yy_0_xx_0_fused // T.int64(8192) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(2) + ff_3)
                                v_yy = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8192) // T.int64(64) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(512), xx_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(8) + xx_3)
                                v_rc = T.axis.reduce(T.int64(256), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_2 + ry_0 * T.int64(3) + ry_1)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(8)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8192) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8192) // T.int64(64) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(8) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d44_add47(lv192: T.Buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)), "float32"), param_0: T.Buffer((T.int64(128), T.int64(256), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(514), T.int64(514)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(128), T.int64(256), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(4096), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(128), ff_4_init + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(8) + ff_3_init)
                            v_yy = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(4) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(4) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(256), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(256), rc_0)
                                    v2 = T.axis.spatial(T.int64(514), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(18))
                                    v3 = T.axis.spatial(T.int64(514), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(18))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(108))
                                    T.reads(lv192[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(513) and T.int64(1) <= v3 and v3 < T.int64(513), lv192[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(9))
                                    v1 = T.axis.spatial(T.int64(256), rc_0)
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(128), ff_4 + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(8) + ff_3)
                                v_yy = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(4) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(4) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(256), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_2 + ry_0 * T.int64(3) + ry_1)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(8), T.int64(2), T.int64(4)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(128), nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(4) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(4) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d45_add47(lv207: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32"), param_0: T.Buffer((T.int64(128), T.int64(128), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(514), T.int64(514)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(128), T.int64(128), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(8192), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(128), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ff_3_init)
                            v_yy = T.axis.spatial(T.int64(512), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(32) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(2) + yy_3_init)
                            v_xx = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(8) + xx_3_init * T.int64(8) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(128), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(128), rc_0)
                                    v2 = T.axis.spatial(T.int64(514), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(32) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(18))
                                    v3 = T.axis.spatial(T.int64(514), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(18))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180))
                                    T.reads(lv207[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(513) and T.int64(1) <= v3 and v3 < T.int64(513), lv207[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(9))
                                    v1 = T.axis.spatial(T.int64(128), rc_0)
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(288))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(8)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(128), ff_4 + nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ff_3)
                                v_yy = T.axis.spatial(T.int64(512), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(32) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(2) + yy_3)
                                v_xx = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(8) + xx_3 * T.int64(8) + xx_4)
                                v_rc = T.axis.reduce(T.int64(128), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_2 + ry_0 * T.int64(3) + ry_1)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(8)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(32) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(8) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d45_add47_add48_divide7(lv197: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32"), param_0: T.Buffer((T.int64(128), T.int64(128), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), lv203: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(514), T.int64(514)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(128), T.int64(128), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(8192), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(128), nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(4) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) * T.int64(4) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(128), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(128), rc_0)
                                    v2 = T.axis.spatial(T.int64(514), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(6))
                                    v3 = T.axis.spatial(T.int64(514), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(6))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(60))
                                    T.reads(lv197[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(513) and T.int64(1) <= v3 and v3 < T.int64(513), lv197[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(9))
                                    v1 = T.axis.spatial(T.int64(128), rc_0)
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(3), T.int64(3), T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(128), nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(4) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) * T.int64(4) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(128), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(4), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(128), nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + ax3)
                            T.reads(lv203[v0, v1, v2, v3], conv2d_nchw_local[v0, v1, v2, v3], param_1[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_divide[v0, v1, v2, v3])
                            T_divide[v0, v1, v2, v3] = lv203[v0, v1, v2, v3] + (conv2d_nchw_local[v0, v1, v2, v3] + param_1[v0, v1, T.int64(0), T.int64(0)])

    @T.prim_func
    def fused_conv2d46_add47(lv190: T.Buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)), "float32"), param_0: T.Buffer((T.int64(128), T.int64(256), T.int64(1), T.int64(1)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(128), T.int64(256), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(8192), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4096) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(8) + ff_3_init * T.int64(8) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(512), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(4096) // T.int64(128) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + yy_3_init)
                            v_xx = T.axis.spatial(T.int64(512), xx_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused + xx_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(256), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(64))
                                    v2 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4096) // T.int64(128) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64) // T.int64(4))
                                    v3 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4))
                                    T.reads(lv190[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv190[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4096) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4))
                                    v1 = T.axis.spatial(T.int64(256), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4096) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(8) + ff_3 * T.int64(8) + ff_4)
                                v_yy = T.axis.spatial(T.int64(512), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(4096) // T.int64(128) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + yy_3)
                                v_xx = T.axis.spatial(T.int64(512), xx_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused + xx_3)
                                v_rc = T.axis.reduce(T.int64(256), rc_0 * T.int64(4) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4096) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4096) // T.int64(128) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + ax2)
                            v3 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d47_add49_divide8_add50_tir_clip(lv232: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32"), vae_decoder_conv_out_weight: T.Buffer((T.int64(3), T.int64(128), T.int64(3), T.int64(3)), "float32"), lv234: T.Buffer((T.int64(1), T.int64(3), T.int64(1), T.int64(1)), "float32"), compute: T.Buffer((T.int64(1), T.int64(3), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(3), T.int64(512), T.int64(512)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(514), T.int64(514)), scope="shared")
        vae_decoder_conv_out_weight_shared = T.alloc_buffer((T.int64(3), T.int64(128), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(2048), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(3), ff_4_init + ff_3_init)
                            v_yy = T.axis.spatial(T.int64(512), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) + yy_3_init)
                            v_xx = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) * T.int64(4) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(128), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(128), rc_0)
                                    v2 = T.axis.spatial(T.int64(514), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(66))
                                    v3 = T.axis.spatial(T.int64(514), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(66))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1 < T.int64(264))
                                    T.reads(lv232[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(513) and T.int64(1) <= v3 and v3 < T.int64(513), lv232[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("vae.decoder.conv_out.weight_shared"):
                                    v0 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(9))
                                    v1 = T.axis.spatial(T.int64(128), rc_0)
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1 < T.int64(27))
                                    T.reads(vae_decoder_conv_out_weight[v0, v1, v2, v3])
                                    T.writes(vae_decoder_conv_out_weight_shared[v0, v1, v2, v3])
                                    vae_decoder_conv_out_weight_shared[v0, v1, v2, v3] = vae_decoder_conv_out_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(3), T.int64(1), T.int64(2), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(3), ff_4 + ff_3)
                                v_yy = T.axis.spatial(T.int64(512), yy_4 + nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) + yy_3)
                                v_xx = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) * T.int64(4) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(128), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_conv_out_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_conv_out_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(4)):
                        with T.block("conv2d_nchw_local"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) + ax2)
                            v3 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) * T.int64(4) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], lv234[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min((conv2d_nchw_local[v0, v1, v2, v3] + lv234[v0, v1, T.int64(0), T.int64(0)]) * T.float32(0.5) + T.float32(0.5), T.float32(1)), T.float32(0))

    @T.prim_func
    def fused_conv2d4_add13_add15(lv228: T.Buffer((T.int64(2), T.int64(320), T.int64(32), T.int64(32)), "float32"), param_0: T.Buffer((T.int64(640), T.int64(320), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv237: T.Buffer((T.int64(2), T.int64(640), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(34), T.int64(34)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(640), T.int64(320), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(16) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ff_3_init)
                            v_yy = T.axis.spatial(T.int64(32), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(4) + yy_3_init)
                            v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(4) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(320), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160))
                                    v1 = T.axis.spatial(T.int64(320), rc_0)
                                    v2 = T.axis.spatial(T.int64(34), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(2) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(18))
                                    v3 = T.axis.spatial(T.int64(34), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(18))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(108))
                                    T.reads(lv228[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(33) and T.int64(1) <= v3 and v3 < T.int64(33), lv228[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(16) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(9))
                                    v1 = T.axis.spatial(T.int64(320), rc_0)
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(576))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), ff_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(16) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ff_3)
                                v_yy = T.axis.spatial(T.int64(32), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(4) + yy_3)
                                v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(4) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(320), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_2 + ry_0 * T.int64(3) + ry_1)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(4)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160) + ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(16) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(4) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)], lv237[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)] + lv237[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d5_add13_add15(lv331: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), param_0: T.Buffer((T.int64(640), T.int64(640), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv340: T.Buffer((T.int64(2), T.int64(640), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(34), T.int64(34)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(640), T.int64(640), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(5), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(8) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(5) + ff_3_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + yy_3_init * T.int64(4) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(640), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(640), rc_0)
                                    v2 = T.axis.spatial(T.int64(34), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(34))
                                    v3 = T.axis.spatial(T.int64(34), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(34))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(204))
                                    T.reads(lv331[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(33) and T.int64(1) <= v3 and v3 < T.int64(33), lv331[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(8) * T.int64(40) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(9))
                                    v1 = T.axis.spatial(T.int64(640), rc_0)
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(360))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(5), T.int64(1), T.int64(1), T.int64(1), T.int64(3), T.int64(3), T.int64(1), T.int64(1), T.int64(4), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), ff_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(8) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(5) + ff_3)
                                v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + yy_3 * T.int64(4) + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(640), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(5), T.int64(4), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(8) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(5) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)], lv340[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)] + lv340[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d5_add13_add16_divide1(lv240: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), param_0: T.Buffer((T.int64(640), T.int64(640), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv246: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), T_divide: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(34), T.int64(34)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(640), T.int64(640), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(640), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(4), T.int64(2), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(320) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(320) // T.int64(32) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(4) + ff_3_init * T.int64(4) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) * T.int64(4) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(640), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(320))
                                    v1 = T.axis.spatial(T.int64(640), rc_0)
                                    v2 = T.axis.spatial(T.int64(34), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(8) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(6))
                                    v3 = T.axis.spatial(T.int64(34), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(6))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(60))
                                    T.reads(lv240[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(33) and T.int64(1) <= v3 and v3 < T.int64(33), lv240[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(320) // T.int64(32) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(9))
                                    v1 = T.axis.spatial(T.int64(640), rc_0)
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(4), T.int64(2), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(320) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(320) // T.int64(32) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(4) + ff_3 * T.int64(4) + ff_4)
                                v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) * T.int64(4) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(640), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_2 + ry_0 * T.int64(3) + ry_1)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(4), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(320) + ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(320) // T.int64(32) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + ax3)
                            T.reads(lv246[v0, v1, v2, v3], conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(T_divide[v0, v1, v2, v3])
                            T_divide[v0, v1, v2, v3] = lv246[v0, v1, v2, v3] + (conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)])

    @T.prim_func
    def fused_conv2d6_add13(lv226: T.Buffer((T.int64(2), T.int64(320), T.int64(32), T.int64(32)), "float32"), param_0: T.Buffer((T.int64(640), T.int64(320), T.int64(1), T.int64(1)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(32), T.int64(32)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(640), T.int64(320), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init + nn_4_init + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32))
                            v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(32) + ff_3_init * T.int64(32) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(32), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(2) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(16) + yy_3_init)
                            v_xx = T.axis.spatial(T.int64(32), xx_3_init + xx_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16))
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(320), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(320), rc_0)
                                    v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(2) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64) // T.int64(16))
                                    v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    T.reads(lv226[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv226[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1))
                                    v1 = T.axis.spatial(T.int64(320), rc_0)
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(32))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 + nn_4 + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32))
                                v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(32) + ff_3 * T.int64(32) + ff_4)
                                v_yy = T.axis.spatial(T.int64(32), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(2) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(16) + yy_3)
                                v_xx = T.axis.spatial(T.int64(32), xx_3 + xx_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16))
                                v_rc = T.axis.reduce(T.int64(320), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) + ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(32) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(2) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(16) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d7_add13(lv249: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), param_0: T.Buffer((T.int64(640), T.int64(640), T.int64(1), T.int64(1)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(640), T.int64(640), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(5), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_4_init + nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) + nn_3_init)
                            v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(40) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) // T.int64(2) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(5) + ff_3_init * T.int64(5) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(32), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) + yy_3_init)
                            v_xx = T.axis.spatial(T.int64(32), xx_3_init + xx_4_init + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16))
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(80), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(640), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(256) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32))
                                    v3 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    T.reads(lv249[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv249[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(40) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(8))
                                    v1 = T.axis.spatial(T.int64(640), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(5), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_4 + nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) + nn_3)
                                v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(40) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) // T.int64(2) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(5) + ff_3 * T.int64(5) + ff_4)
                                v_yy = T.axis.spatial(T.int64(32), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) + yy_3)
                                v_xx = T.axis.spatial(T.int64(32), xx_3 + xx_4 + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16))
                                v_rc = T.axis.reduce(T.int64(640), rc_0 * T.int64(8) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(5), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) + ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(40) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) // T.int64(2) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(5) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d7_add13_add16(lv325: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), param_0: T.Buffer((T.int64(640), T.int64(640), T.int64(1), T.int64(1)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv248: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(640), T.int64(640), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(32) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(4) + ff_3_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(4) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), xx_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(2) + xx_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(80), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160))
                                    v1 = T.axis.spatial(T.int64(640), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(4) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32) // T.int64(8))
                                    v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    T.reads(lv325[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv325[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(32) * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(8))
                                    v1 = T.axis.spatial(T.int64(640), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(2), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), ff_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(32) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(4) + ff_3)
                                v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(4) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), xx_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(2) + xx_3)
                                v_rc = T.axis.reduce(T.int64(640), rc_0 * T.int64(8) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(2), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160) + ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(32) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(4) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)], lv248[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)] + lv248[v0, v1, v2, v3]

    @T.prim_func
    def fused_conv2d8_add20(lv429: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), param_0: T.Buffer((T.int64(640), T.int64(640), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(640), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(34), T.int64(34)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(640), T.int64(640), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(80), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init * T.int64(2) + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(4) + ff_3_init)
                            v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(640), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(165))
                                    v1 = T.axis.spatial(T.int64(640), rc_0)
                                    v2 = T.axis.spatial(T.int64(34), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(165) // T.int64(33))
                                    v3 = T.axis.spatial(T.int64(34), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(33))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(330))
                                    T.reads(lv429[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(33) and T.int64(1) <= v3 and v3 < T.int64(33), lv429[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(9))
                                    v1 = T.axis.spatial(T.int64(640), rc_0)
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(576))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(3), T.int64(2), T.int64(1), T.int64(2), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 * T.int64(2) + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), ff_4 + nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(4) + ff_3)
                                v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(640), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_2 + ry_0 * T.int64(3) + ry_1)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(4), T.int64(2), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d9_add21_add22(lv434: T.Buffer((T.int64(2), T.int64(640), T.int64(16), T.int64(16)), "float32"), param_0: T.Buffer((T.int64(1280), T.int64(640), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv443: T.Buffer((T.int64(2), T.int64(1280), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(18), T.int64(18)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(1280), T.int64(640), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(5), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(4) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(5) + ff_3_init)
                            v_yy = T.axis.spatial(T.int64(16), yy_4_init + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + yy_3_init)
                            v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(640), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(640), rc_0)
                                    v2 = T.axis.spatial(T.int64(18), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(6))
                                    v3 = T.axis.spatial(T.int64(18), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(6))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(108))
                                    T.reads(lv434[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(17) and T.int64(1) <= v3 and v3 < T.int64(17), lv434[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(4) * T.int64(40) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(9))
                                    v1 = T.axis.spatial(T.int64(640), rc_0)
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(360))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(3), T.int64(1), T.int64(5), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), ff_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(4) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(5) + ff_3)
                                v_yy = T.axis.spatial(T.int64(16), yy_4 + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + yy_3)
                                v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(640), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_2 + ry_0 * T.int64(3) + ry_1)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(5), T.int64(1), T.int64(4)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(4) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(5) + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)], lv443[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)] + lv443[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d_add5(inp_0: T.Buffer((T.int64(2), T.int64(4), T.int64(64), T.int64(64)), "float32"), param_0: T.Buffer((T.int64(320), T.int64(4), T.int64(3), T.int64(3)), "float32"), param_1: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(4), T.int64(66), T.int64(66)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(320), T.int64(4), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(160), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(80) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(80) // T.int64(16) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(8) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(2) * T.int64(4) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(2), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(80))
                                    v1 = T.axis.spatial(T.int64(4), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(340))
                                    v2 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(8) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(340) // T.int64(10))
                                    v3 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(10))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(680))
                                    T.reads(inp_0[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(65) and T.int64(1) <= v3 and v3 < T.int64(65), inp_0[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(80) // T.int64(16) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(18))
                                    v1 = T.axis.spatial(T.int64(4), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(18) // T.int64(9))
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(1152))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(3), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(80) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(80) // T.int64(16) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(8) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(2) * T.int64(4) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(4), rc_0 * T.int64(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(80) + ax0)
                            v1 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(80) // T.int64(16) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(8) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(2) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_group_norm10_silu7(lv796: T.Buffer((T.int64(2), T.int64(2560), T.int64(8), T.int64(8)), "float32"), param_0: T.Buffer((T.int64(2560),), "float32"), param_1: T.Buffer((T.int64(2560),), "float32"), T_multiply: T.Buffer((T.int64(2), T.int64(2560), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(40)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(80), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) // T.int64(64))
                        v_k3 = T.axis.reduce(T.int64(8), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(64) // T.int64(8))
                        v_k4 = T.axis.reduce(T.int64(8), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(8))
                        T.reads(lv796[((v_ax1 * T.int64(80) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(2560) + v_ax0) % T.int64(2), (v_ax1 * T.int64(80) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(2560), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv796[((v_ax1 * T.int64(80) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(2560) + v_ax0) % T.int64(2), (v_ax1 * T.int64(80) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(2560), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv796[((v_ax1 * T.int64(80) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(2560) + v_ax0) % T.int64(2), (v_ax1 * T.int64(80) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(2560), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)] * lv796[((v_ax1 * T.int64(80) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(2560) + v_ax0) % T.int64(2), (v_ax1 * T.int64(80) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(2560), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(163840))
                        v_ax1 = T.axis.spatial(T.int64(2560), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(163840) // T.int64(64))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64) // T.int64(8))
                        v_ax3 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(8))
                        T.reads(lv796[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560), (v_ax3 // T.int64(8) + v_ax2) % T.int64(8), v_ax3 % T.int64(8)], rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80)], rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80)], param_0[((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560)], param_1[((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv796[((((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) // T.int64(8) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(80)) // T.int64(2560) + (((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) // T.int64(8) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(80)) % T.int64(2560), (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) % T.int64(8), v_ax3 % T.int64(8) % T.int64(8)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(0.00019531250000000001)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(0.00019531250000000001) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(0.00019531250000000001) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(0.00019531250000000001)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(80)) % T.int64(2560)] + param_1[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(80)) % T.int64(2560)]) * T.sigmoid((lv796[((((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) // T.int64(8) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(80)) // T.int64(2560) + (((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) // T.int64(8) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(80)) % T.int64(2560), (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) % T.int64(8), v_ax3 % T.int64(8) % T.int64(8)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(0.00019531250000000001)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(0.00019531250000000001) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(0.00019531250000000001) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(0.00019531250000000001)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(80)) % T.int64(2560)] + param_1[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(80)) % T.int64(2560)])

    @T.prim_func
    def fused_group_norm11_silu8(lv869: T.Buffer((T.int64(2), T.int64(2560), T.int64(16), T.int64(16)), "float32"), param_0: T.Buffer((T.int64(2560),), "float32"), param_1: T.Buffer((T.int64(2560),), "float32"), T_multiply: T.Buffer((T.int64(2), T.int64(2560), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for k2_k3_k4_fused_0 in range(T.int64(320)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(80), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) // T.int64(256))
                        v_k3 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) % T.int64(256) // T.int64(16))
                        v_k4 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) % T.int64(16))
                        T.reads(lv869[((v_ax1 * T.int64(80) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(2560) + v_ax0) % T.int64(2), (v_ax1 * T.int64(80) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(2560), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv869[((v_ax1 * T.int64(80) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(2560) + v_ax0) % T.int64(2), (v_ax1 * T.int64(80) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(2560), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv869[((v_ax1 * T.int64(80) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(2560) + v_ax0) % T.int64(2), (v_ax1 * T.int64(80) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(2560), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)] * lv869[((v_ax1 * T.int64(80) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(2560) + v_ax0) % T.int64(2), (v_ax1 * T.int64(80) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(2560), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(20)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(655360))
                        v_ax1 = T.axis.spatial(T.int64(2560), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(655360) // T.int64(256))
                        v_ax2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(256) // T.int64(16))
                        v_ax3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                        T.reads(lv869[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560), (v_ax3 // T.int64(16) + v_ax2) % T.int64(16), v_ax3 % T.int64(16)], rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80)], rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80)], param_0[((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560)], param_1[((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv869[((((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(80)) // T.int64(2560) + (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(80)) % T.int64(2560), (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) % T.int64(16), v_ax3 % T.int64(16) % T.int64(16)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(4.8828125000000003e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(4.8828125000000003e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(4.8828125000000003e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(4.8828125000000003e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(80)) % T.int64(2560)] + param_1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(80)) % T.int64(2560)]) * T.sigmoid((lv869[((((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(80)) // T.int64(2560) + (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(80)) % T.int64(2560), (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) % T.int64(16), v_ax3 % T.int64(16) % T.int64(16)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(4.8828125000000003e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(4.8828125000000003e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(4.8828125000000003e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(4.8828125000000003e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(80)) % T.int64(2560)] + param_1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(80)) % T.int64(2560)])

    @T.prim_func
    def fused_group_norm12_silu9(lv1077: T.Buffer((T.int64(2), T.int64(1920), T.int64(16), T.int64(16)), "float32"), param_0: T.Buffer((T.int64(1920),), "float32"), param_1: T.Buffer((T.int64(1920),), "float32"), T_multiply: T.Buffer((T.int64(2), T.int64(1920), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(60)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(60), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) // T.int64(256))
                        v_k3 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(256) // T.int64(16))
                        v_k4 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(16))
                        T.reads(lv1077[((v_ax1 * T.int64(60) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1920) + v_ax0) % T.int64(2), (v_ax1 * T.int64(60) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1920), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv1077[((v_ax1 * T.int64(60) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1920) + v_ax0) % T.int64(2), (v_ax1 * T.int64(60) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1920), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv1077[((v_ax1 * T.int64(60) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1920) + v_ax0) % T.int64(2), (v_ax1 * T.int64(60) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1920), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)] * lv1077[((v_ax1 * T.int64(60) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1920) + v_ax0) % T.int64(2), (v_ax1 * T.int64(60) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1920), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(15)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(491520))
                        v_ax1 = T.axis.spatial(T.int64(1920), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(491520) // T.int64(256))
                        v_ax2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(256) // T.int64(16))
                        v_ax3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                        T.reads(lv1077[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920), (v_ax3 // T.int64(16) + v_ax2) % T.int64(16), v_ax3 % T.int64(16)], rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60)], rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60)], param_0[((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920)], param_1[((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv1077[((((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(60)) // T.int64(1920) + (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(60)) % T.int64(1920), (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) % T.int64(16), v_ax3 % T.int64(16) % T.int64(16)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(6.5104166666666666e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(6.5104166666666666e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(6.5104166666666666e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(6.5104166666666666e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(60)) % T.int64(1920)] + param_1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(60)) % T.int64(1920)]) * T.sigmoid((lv1077[((((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(60)) // T.int64(1920) + (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(60)) % T.int64(1920), (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) % T.int64(16), v_ax3 % T.int64(16) % T.int64(16)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(6.5104166666666666e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(6.5104166666666666e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(6.5104166666666666e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(6.5104166666666666e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(60)) % T.int64(1920)] + param_1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(60)) % T.int64(1920)])

    @T.prim_func
    def fused_group_norm13_silu10(lv1185: T.Buffer((T.int64(2), T.int64(1920), T.int64(32), T.int64(32)), "float32"), param_0: T.Buffer((T.int64(1920),), "float32"), param_1: T.Buffer((T.int64(1920),), "float32"), T_multiply: T.Buffer((T.int64(2), T.int64(1920), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for k2_k3_k4_fused_0 in range(T.int64(480)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(60), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) // T.int64(1024))
                        v_k3 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(1024) // T.int64(32))
                        v_k4 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(32))
                        T.reads(lv1185[((v_ax1 * T.int64(60) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(1920) + v_ax0) % T.int64(2), (v_ax1 * T.int64(60) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(1920), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv1185[((v_ax1 * T.int64(60) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(1920) + v_ax0) % T.int64(2), (v_ax1 * T.int64(60) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(1920), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv1185[((v_ax1 * T.int64(60) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(1920) + v_ax0) % T.int64(2), (v_ax1 * T.int64(60) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(1920), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)] * lv1185[((v_ax1 * T.int64(60) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(1920) + v_ax0) % T.int64(2), (v_ax1 * T.int64(60) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(1920), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(60)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1966080))
                        v_ax1 = T.axis.spatial(T.int64(1920), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1966080) // T.int64(1024))
                        v_ax2 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1024) // T.int64(32))
                        v_ax3 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                        T.reads(lv1185[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920), (v_ax3 // T.int64(32) + v_ax2) % T.int64(32), v_ax3 % T.int64(32)], rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60)], rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60)], param_0[((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920)], param_1[((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv1185[((((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(60)) // T.int64(1920) + (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(60)) % T.int64(1920), (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) % T.int64(32), v_ax3 % T.int64(32) % T.int64(32)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(1.6276041666666666e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(1.6276041666666666e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(1.6276041666666666e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(1.6276041666666666e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(60)) % T.int64(1920)] + param_1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(60)) % T.int64(1920)]) * T.sigmoid((lv1185[((((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(60)) // T.int64(1920) + (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(60)) % T.int64(1920), (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) % T.int64(32), v_ax3 % T.int64(32) % T.int64(32)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(1.6276041666666666e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(1.6276041666666666e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(1.6276041666666666e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(1.6276041666666666e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(60)) % T.int64(1920)] + param_1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(60)) % T.int64(1920)])

    @T.prim_func
    def fused_group_norm14_silu11(lv1289: T.Buffer((T.int64(2), T.int64(1280), T.int64(32), T.int64(32)), "float32"), param_0: T.Buffer((T.int64(1280),), "float32"), param_1: T.Buffer((T.int64(1280),), "float32"), T_multiply: T.Buffer((T.int64(2), T.int64(1280), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(320)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(40), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) // T.int64(1024))
                        v_k3 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(1024) // T.int64(32))
                        v_k4 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(32))
                        T.reads(lv1289[((v_ax1 * T.int64(40) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(1280), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv1289[((v_ax1 * T.int64(40) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(1280), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv1289[((v_ax1 * T.int64(40) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(1280), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)] * lv1289[((v_ax1 * T.int64(40) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(1280), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(40)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1310720))
                        v_ax1 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1310720) // T.int64(1024))
                        v_ax2 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1024) // T.int64(32))
                        v_ax3 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                        T.reads(lv1289[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280), (v_ax3 // T.int64(32) + v_ax2) % T.int64(32), v_ax3 % T.int64(32)], rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40)], rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40)], param_0[((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280)], param_1[((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv1289[((((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(40)) // T.int64(1280) + (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(40)) % T.int64(1280), (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) % T.int64(32), v_ax3 % T.int64(32) % T.int64(32)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(2.4414062500000001e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(2.4414062500000001e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(2.4414062500000001e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(2.4414062500000001e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(40)) % T.int64(1280)] + param_1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(40)) % T.int64(1280)]) * T.sigmoid((lv1289[((((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(40)) // T.int64(1280) + (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(40)) % T.int64(1280), (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) % T.int64(32), v_ax3 % T.int64(32) % T.int64(32)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(2.4414062500000001e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(2.4414062500000001e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(2.4414062500000001e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(2.4414062500000001e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(40)) % T.int64(1280)] + param_1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(40)) % T.int64(1280)])

    @T.prim_func
    def fused_group_norm15_silu12(lv1393: T.Buffer((T.int64(2), T.int64(960), T.int64(32), T.int64(32)), "float32"), param_0: T.Buffer((T.int64(960),), "float32"), param_1: T.Buffer((T.int64(960),), "float32"), T_multiply: T.Buffer((T.int64(2), T.int64(960), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for k2_k3_k4_fused_0 in range(T.int64(480)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(30), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) // T.int64(1024))
                        v_k3 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) % T.int64(1024) // T.int64(32))
                        v_k4 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) % T.int64(32))
                        T.reads(lv1393[((v_ax1 * T.int64(30) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(960) + v_ax0) % T.int64(2), (v_ax1 * T.int64(30) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(960), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv1393[((v_ax1 * T.int64(30) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(960) + v_ax0) % T.int64(2), (v_ax1 * T.int64(30) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(960), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv1393[((v_ax1 * T.int64(30) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(960) + v_ax0) % T.int64(2), (v_ax1 * T.int64(30) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(960), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)] * lv1393[((v_ax1 * T.int64(30) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(960) + v_ax0) % T.int64(2), (v_ax1 * T.int64(30) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(960), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(30)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(983040))
                        v_ax1 = T.axis.spatial(T.int64(960), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(983040) // T.int64(1024))
                        v_ax2 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1024) // T.int64(32))
                        v_ax3 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                        T.reads(lv1393[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960), (v_ax3 // T.int64(32) + v_ax2) % T.int64(32), v_ax3 % T.int64(32)], rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30)], rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30)], param_0[((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960)], param_1[((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv1393[((((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(30)) // T.int64(960) + (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(30)) % T.int64(960), (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) % T.int64(32), v_ax3 % T.int64(32) % T.int64(32)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(3.2552083333333333e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(3.2552083333333333e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(3.2552083333333333e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(3.2552083333333333e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(30)) % T.int64(960)] + param_1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(30)) % T.int64(960)]) * T.sigmoid((lv1393[((((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(30)) // T.int64(960) + (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(30)) % T.int64(960), (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) % T.int64(32), v_ax3 % T.int64(32) % T.int64(32)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(3.2552083333333333e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(3.2552083333333333e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(3.2552083333333333e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(3.2552083333333333e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(30)) % T.int64(960)] + param_1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(30)) % T.int64(960)])

    @T.prim_func
    def fused_group_norm16_silu13(lv1501: T.Buffer((T.int64(2), T.int64(960), T.int64(64), T.int64(64)), "float32"), param_0: T.Buffer((T.int64(960),), "float32"), param_1: T.Buffer((T.int64(960),), "float32"), T_multiply: T.Buffer((T.int64(2), T.int64(960), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for k2_k3_k4_fused_0 in range(T.int64(1920)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(30), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) // T.int64(4096))
                        v_k3 = T.axis.reduce(T.int64(64), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) % T.int64(4096) // T.int64(64))
                        v_k4 = T.axis.reduce(T.int64(64), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) % T.int64(64))
                        T.reads(lv1501[((v_ax1 * T.int64(30) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(960) + v_ax0) % T.int64(2), (v_ax1 * T.int64(30) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(960), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv1501[((v_ax1 * T.int64(30) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(960) + v_ax0) % T.int64(2), (v_ax1 * T.int64(30) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(960), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv1501[((v_ax1 * T.int64(30) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(960) + v_ax0) % T.int64(2), (v_ax1 * T.int64(30) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(960), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)] * lv1501[((v_ax1 * T.int64(30) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(960) + v_ax0) % T.int64(2), (v_ax1 * T.int64(30) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(960), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(120)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(3932160))
                        v_ax1 = T.axis.spatial(T.int64(960), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3932160) // T.int64(4096))
                        v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4096) // T.int64(64))
                        v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                        T.reads(lv1501[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960), (v_ax3 // T.int64(64) + v_ax2) % T.int64(64), v_ax3 % T.int64(64)], rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30)], rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30)], param_0[((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960)], param_1[((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv1501[((((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(30)) // T.int64(960) + (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(30)) % T.int64(960), (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) % T.int64(64), v_ax3 % T.int64(64) % T.int64(64)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(8.1380208333333332e-06)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(8.1380208333333332e-06) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(8.1380208333333332e-06) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(8.1380208333333332e-06)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(30)) % T.int64(960)] + param_1[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(30)) % T.int64(960)]) * T.sigmoid((lv1501[((((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(30)) // T.int64(960) + (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(30)) % T.int64(960), (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) % T.int64(64), v_ax3 % T.int64(64) % T.int64(64)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(8.1380208333333332e-06)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(8.1380208333333332e-06) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(8.1380208333333332e-06) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(8.1380208333333332e-06)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(30)) % T.int64(960)] + param_1[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(30)) % T.int64(960)])

    @T.prim_func
    def fused_group_norm17_silu14(lv1605: T.Buffer((T.int64(2), T.int64(640), T.int64(64), T.int64(64)), "float32"), param_0: T.Buffer((T.int64(640),), "float32"), param_1: T.Buffer((T.int64(640),), "float32"), T_multiply: T.Buffer((T.int64(2), T.int64(640), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(1280)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(20), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) // T.int64(4096))
                        v_k3 = T.axis.reduce(T.int64(64), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) % T.int64(4096) // T.int64(64))
                        v_k4 = T.axis.reduce(T.int64(64), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) % T.int64(64))
                        T.reads(lv1605[((v_ax1 * T.int64(20) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(640), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv1605[((v_ax1 * T.int64(20) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(640), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv1605[((v_ax1 * T.int64(20) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(640), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)] * lv1605[((v_ax1 * T.int64(20) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(640), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(80)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(2621440))
                        v_ax1 = T.axis.spatial(T.int64(640), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2621440) // T.int64(4096))
                        v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4096) // T.int64(64))
                        v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                        T.reads(lv1605[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640), (v_ax3 // T.int64(64) + v_ax2) % T.int64(64), v_ax3 % T.int64(64)], rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20)], rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20)], param_0[((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640)], param_1[((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv1605[((((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(20)) // T.int64(640) + (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(20)) % T.int64(640), (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) % T.int64(64), v_ax3 % T.int64(64) % T.int64(64)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(1.2207031250000001e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(1.2207031250000001e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(1.2207031250000001e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(1.2207031250000001e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(20)) % T.int64(640)] + param_1[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(20)) % T.int64(640)]) * T.sigmoid((lv1605[((((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(20)) // T.int64(640) + (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(20)) % T.int64(640), (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) % T.int64(64), v_ax3 % T.int64(64) % T.int64(64)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(1.2207031250000001e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(1.2207031250000001e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(1.2207031250000001e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(1.2207031250000001e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(20)) % T.int64(640)] + param_1[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(20)) % T.int64(640)])

    @T.prim_func
    def fused_group_norm18_silu15(lv5: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32"), param_0: T.Buffer((T.int64(512),), "float32"), param_1: T.Buffer((T.int64(512),), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(512)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused)
                        v_k2 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) // T.int64(4096))
                        v_k3 = T.axis.reduce(T.int64(64), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(4096) // T.int64(64))
                        v_k4 = T.axis.reduce(T.int64(64), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(64))
                        T.reads(lv5[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(512), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv5[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(512), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv5[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(512), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)] * lv5[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(512), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(4096))
                        v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4096) // T.int64(64))
                        v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                        T.reads(lv5[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512), (v_ax3 // T.int64(64) + v_ax2) % T.int64(64), v_ax3 % T.int64(64)], rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16)], rxplaceholder_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16)], param_0[((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512)], param_1[((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv5[T.int64(0), (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(16)) % T.int64(512), (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) % T.int64(64), v_ax3 % T.int64(64) % T.int64(64)] - rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(1.52587890625e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(1.52587890625e-05) - rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(1.52587890625e-05) * (rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(1.52587890625e-05)) + T.float32(9.9999999999999995e-07)) * param_0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(16)) % T.int64(512)] + param_1[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(16)) % T.int64(512)]) * T.sigmoid((lv5[T.int64(0), (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(16)) % T.int64(512), (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) % T.int64(64), v_ax3 % T.int64(64) % T.int64(64)] - rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(1.52587890625e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(1.52587890625e-05) - rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(1.52587890625e-05) * (rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(1.52587890625e-05)) + T.float32(9.9999999999999995e-07)) * param_0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(16)) % T.int64(512)] + param_1[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(16)) % T.int64(512)])

    @T.prim_func
    def fused_group_norm19_silu16(lv107: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32"), param_0: T.Buffer((T.int64(512),), "float32"), param_1: T.Buffer((T.int64(512),), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(8192)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused)
                        v_k2 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(32) + k2_k3_k4_fused_1) // T.int64(16384))
                        v_k3 = T.axis.reduce(T.int64(128), (k2_k3_k4_fused_0 * T.int64(32) + k2_k3_k4_fused_1) % T.int64(16384) // T.int64(128))
                        v_k4 = T.axis.reduce(T.int64(128), (k2_k3_k4_fused_0 * T.int64(32) + k2_k3_k4_fused_1) % T.int64(128))
                        T.reads(lv107[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(128) + v_k3) // T.int64(128) + v_k2) % T.int64(512), (v_k4 // T.int64(128) + v_k3) % T.int64(128), v_k4 % T.int64(128)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv107[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(128) + v_k3) // T.int64(128) + v_k2) % T.int64(512), (v_k4 // T.int64(128) + v_k3) % T.int64(128), v_k4 % T.int64(128)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv107[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(128) + v_k3) // T.int64(128) + v_k2) % T.int64(512), (v_k4 // T.int64(128) + v_k3) % T.int64(128), v_k4 % T.int64(128)] * lv107[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(128) + v_k3) // T.int64(128) + v_k2) % T.int64(512), (v_k4 // T.int64(128) + v_k3) % T.int64(128), v_k4 % T.int64(128)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(128)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(16384))
                        v_ax2 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16384) // T.int64(128))
                        v_ax3 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                        T.reads(lv107[T.int64(0), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512), (v_ax3 // T.int64(128) + v_ax2) % T.int64(128), v_ax3 % T.int64(128)], rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16)], rxplaceholder_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16)], param_0[((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512)], param_1[((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv107[T.int64(0), (((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + (v_ax3 % T.int64(128) // T.int64(128) + (v_ax3 // T.int64(128) + v_ax2) % T.int64(128)) // T.int64(128) + ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(16)) % T.int64(512), (v_ax3 % T.int64(128) // T.int64(128) + (v_ax3 // T.int64(128) + v_ax2) % T.int64(128)) % T.int64(128), v_ax3 % T.int64(128) % T.int64(128)] - rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(3.814697265625e-06)) * T.rsqrt(rxplaceholder_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(3.814697265625e-06) - rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(3.814697265625e-06) * (rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(3.814697265625e-06)) + T.float32(9.9999999999999995e-07)) * param_0[(((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(16)) % T.int64(512)] + param_1[(((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(16)) % T.int64(512)]) * T.sigmoid((lv107[T.int64(0), (((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + (v_ax3 % T.int64(128) // T.int64(128) + (v_ax3 // T.int64(128) + v_ax2) % T.int64(128)) // T.int64(128) + ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(16)) % T.int64(512), (v_ax3 % T.int64(128) // T.int64(128) + (v_ax3 // T.int64(128) + v_ax2) % T.int64(128)) % T.int64(128), v_ax3 % T.int64(128) % T.int64(128)] - rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(3.814697265625e-06)) * T.rsqrt(rxplaceholder_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(3.814697265625e-06) - rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(3.814697265625e-06) * (rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(3.814697265625e-06)) + T.float32(9.9999999999999995e-07)) * param_0[(((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(16)) % T.int64(512)] + param_1[(((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(16)) % T.int64(512)])

    @T.prim_func
    def fused_group_norm20_silu17(lv147: T.Buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)), "float32"), param_0: T.Buffer((T.int64(512),), "float32"), param_1: T.Buffer((T.int64(512),), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(32768)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused)
                        v_k2 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(32) + k2_k3_k4_fused_1) // T.int64(65536))
                        v_k3 = T.axis.reduce(T.int64(256), (k2_k3_k4_fused_0 * T.int64(32) + k2_k3_k4_fused_1) % T.int64(65536) // T.int64(256))
                        v_k4 = T.axis.reduce(T.int64(256), (k2_k3_k4_fused_0 * T.int64(32) + k2_k3_k4_fused_1) % T.int64(256))
                        T.reads(lv147[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(256) + v_k3) // T.int64(256) + v_k2) % T.int64(512), (v_k4 // T.int64(256) + v_k3) % T.int64(256), v_k4 % T.int64(256)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv147[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(256) + v_k3) // T.int64(256) + v_k2) % T.int64(512), (v_k4 // T.int64(256) + v_k3) % T.int64(256), v_k4 % T.int64(256)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv147[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(256) + v_k3) // T.int64(256) + v_k2) % T.int64(512), (v_k4 // T.int64(256) + v_k3) % T.int64(256), v_k4 % T.int64(256)] * lv147[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(256) + v_k3) // T.int64(256) + v_k2) % T.int64(512), (v_k4 // T.int64(256) + v_k3) % T.int64(256), v_k4 % T.int64(256)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(512)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(65536))
                        v_ax2 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(65536) // T.int64(256))
                        v_ax3 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(256))
                        T.reads(lv147[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512), (v_ax3 // T.int64(256) + v_ax2) % T.int64(256), v_ax3 % T.int64(256)], rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16)], rxplaceholder_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16)], param_0[((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512)], param_1[((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv147[T.int64(0), (((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + (v_ax3 % T.int64(256) // T.int64(256) + (v_ax3 // T.int64(256) + v_ax2) % T.int64(256)) // T.int64(256) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(16)) % T.int64(512), (v_ax3 % T.int64(256) // T.int64(256) + (v_ax3 // T.int64(256) + v_ax2) % T.int64(256)) % T.int64(256), v_ax3 % T.int64(256) % T.int64(256)] - rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(9.5367431640625e-07)) * T.rsqrt(rxplaceholder_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(9.5367431640625e-07) - rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(9.5367431640625e-07) * (rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(9.5367431640625e-07)) + T.float32(9.9999999999999995e-07)) * param_0[(((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(16)) % T.int64(512)] + param_1[(((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(16)) % T.int64(512)]) * T.sigmoid((lv147[T.int64(0), (((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + (v_ax3 % T.int64(256) // T.int64(256) + (v_ax3 // T.int64(256) + v_ax2) % T.int64(256)) // T.int64(256) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(16)) % T.int64(512), (v_ax3 % T.int64(256) // T.int64(256) + (v_ax3 // T.int64(256) + v_ax2) % T.int64(256)) % T.int64(256), v_ax3 % T.int64(256) % T.int64(256)] - rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(9.5367431640625e-07)) * T.rsqrt(rxplaceholder_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(9.5367431640625e-07) - rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(9.5367431640625e-07) * (rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(9.5367431640625e-07)) + T.float32(9.9999999999999995e-07)) * param_0[(((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(16)) % T.int64(512)] + param_1[(((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(16)) % T.int64(512)])

    @T.prim_func
    def fused_group_norm21_silu18(lv152: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32"), param_0: T.Buffer((T.int64(256),), "float32"), param_1: T.Buffer((T.int64(256),), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(16384)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused)
                        v_k2 = T.axis.reduce(T.int64(8), (k2_k3_k4_fused_0 * T.int64(32) + k2_k3_k4_fused_1) // T.int64(65536))
                        v_k3 = T.axis.reduce(T.int64(256), (k2_k3_k4_fused_0 * T.int64(32) + k2_k3_k4_fused_1) % T.int64(65536) // T.int64(256))
                        v_k4 = T.axis.reduce(T.int64(256), (k2_k3_k4_fused_0 * T.int64(32) + k2_k3_k4_fused_1) % T.int64(256))
                        T.reads(lv152[T.int64(0), (v_ax1 * T.int64(8) + (v_k4 // T.int64(256) + v_k3) // T.int64(256) + v_k2) % T.int64(256), (v_k4 // T.int64(256) + v_k3) % T.int64(256), v_k4 % T.int64(256)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv152[T.int64(0), (v_ax1 * T.int64(8) + (v_k4 // T.int64(256) + v_k3) // T.int64(256) + v_k2) % T.int64(256), (v_k4 // T.int64(256) + v_k3) % T.int64(256), v_k4 % T.int64(256)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv152[T.int64(0), (v_ax1 * T.int64(8) + (v_k4 // T.int64(256) + v_k3) // T.int64(256) + v_k2) % T.int64(256), (v_k4 // T.int64(256) + v_k3) % T.int64(256), v_k4 % T.int64(256)] * lv152[T.int64(0), (v_ax1 * T.int64(8) + (v_k4 // T.int64(256) + v_k3) // T.int64(256) + v_k2) % T.int64(256), (v_k4 // T.int64(256) + v_k3) % T.int64(256), v_k4 % T.int64(256)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(256)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(65536))
                        v_ax2 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(65536) // T.int64(256))
                        v_ax3 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(256))
                        T.reads(lv152[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256), (v_ax3 // T.int64(256) + v_ax2) % T.int64(256), v_ax3 % T.int64(256)], rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8)], rxplaceholder_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8)], param_0[((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256)], param_1[((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv152[T.int64(0), (((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + (v_ax3 % T.int64(256) // T.int64(256) + (v_ax3 // T.int64(256) + v_ax2) % T.int64(256)) // T.int64(256) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(8)) % T.int64(256), (v_ax3 % T.int64(256) // T.int64(256) + (v_ax3 // T.int64(256) + v_ax2) % T.int64(256)) % T.int64(256), v_ax3 % T.int64(256) % T.int64(256)] - rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(1.9073486328125e-06)) * T.rsqrt(rxplaceholder_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(1.9073486328125e-06) - rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(1.9073486328125e-06) * (rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(1.9073486328125e-06)) + T.float32(9.9999999999999995e-07)) * param_0[(((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(8)) % T.int64(256)] + param_1[(((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(8)) % T.int64(256)]) * T.sigmoid((lv152[T.int64(0), (((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + (v_ax3 % T.int64(256) // T.int64(256) + (v_ax3 // T.int64(256) + v_ax2) % T.int64(256)) // T.int64(256) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(8)) % T.int64(256), (v_ax3 % T.int64(256) // T.int64(256) + (v_ax3 // T.int64(256) + v_ax2) % T.int64(256)) % T.int64(256), v_ax3 % T.int64(256) % T.int64(256)] - rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(1.9073486328125e-06)) * T.rsqrt(rxplaceholder_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(1.9073486328125e-06) - rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(1.9073486328125e-06) * (rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(1.9073486328125e-06)) + T.float32(9.9999999999999995e-07)) * param_0[(((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(8)) % T.int64(256)] + param_1[(((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(8)) % T.int64(256)])

    @T.prim_func
    def fused_group_norm22_silu19(lv190: T.Buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)), "float32"), param_0: T.Buffer((T.int64(256),), "float32"), param_1: T.Buffer((T.int64(256),), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(16384)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused)
                        v_k2 = T.axis.reduce(T.int64(8), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) // T.int64(262144))
                        v_k3 = T.axis.reduce(T.int64(512), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(262144) // T.int64(512))
                        v_k4 = T.axis.reduce(T.int64(512), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(512))
                        T.reads(lv190[T.int64(0), (v_ax1 * T.int64(8) + (v_k4 // T.int64(512) + v_k3) // T.int64(512) + v_k2) % T.int64(256), (v_k4 // T.int64(512) + v_k3) % T.int64(512), v_k4 % T.int64(512)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv190[T.int64(0), (v_ax1 * T.int64(8) + (v_k4 // T.int64(512) + v_k3) // T.int64(512) + v_k2) % T.int64(256), (v_k4 // T.int64(512) + v_k3) % T.int64(512), v_k4 % T.int64(512)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv190[T.int64(0), (v_ax1 * T.int64(8) + (v_k4 // T.int64(512) + v_k3) // T.int64(512) + v_k2) % T.int64(256), (v_k4 // T.int64(512) + v_k3) % T.int64(512), v_k4 % T.int64(512)] * lv190[T.int64(0), (v_ax1 * T.int64(8) + (v_k4 // T.int64(512) + v_k3) // T.int64(512) + v_k2) % T.int64(256), (v_k4 // T.int64(512) + v_k3) % T.int64(512), v_k4 % T.int64(512)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1024)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(262144))
                        v_ax2 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(262144) // T.int64(512))
                        v_ax3 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(512))
                        T.reads(lv190[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256), (v_ax3 // T.int64(512) + v_ax2) % T.int64(512), v_ax3 % T.int64(512)], rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8)], rxplaceholder_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8)], param_0[((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256)], param_1[((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv190[T.int64(0), (((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + (v_ax3 % T.int64(512) // T.int64(512) + (v_ax3 // T.int64(512) + v_ax2) % T.int64(512)) // T.int64(512) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(8)) % T.int64(256), (v_ax3 % T.int64(512) // T.int64(512) + (v_ax3 // T.int64(512) + v_ax2) % T.int64(512)) % T.int64(512), v_ax3 % T.int64(512) % T.int64(512)] - rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(4.76837158203125e-07)) * T.rsqrt(rxplaceholder_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(4.76837158203125e-07) - rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(4.76837158203125e-07) * (rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(4.76837158203125e-07)) + T.float32(9.9999999999999995e-07)) * param_0[(((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(8)) % T.int64(256)] + param_1[(((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(8)) % T.int64(256)]) * T.sigmoid((lv190[T.int64(0), (((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + (v_ax3 % T.int64(512) // T.int64(512) + (v_ax3 // T.int64(512) + v_ax2) % T.int64(512)) // T.int64(512) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(8)) % T.int64(256), (v_ax3 % T.int64(512) // T.int64(512) + (v_ax3 // T.int64(512) + v_ax2) % T.int64(512)) % T.int64(512), v_ax3 % T.int64(512) % T.int64(512)] - rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(4.76837158203125e-07)) * T.rsqrt(rxplaceholder_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(4.76837158203125e-07) - rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(4.76837158203125e-07) * (rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(4.76837158203125e-07)) + T.float32(9.9999999999999995e-07)) * param_0[(((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(8)) % T.int64(256)] + param_1[(((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(8)) % T.int64(256)])

    @T.prim_func
    def fused_group_norm23_silu20(lv195: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32"), param_0: T.Buffer((T.int64(128),), "float32"), param_1: T.Buffer((T.int64(128),), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(32768)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused)
                        v_k2 = T.axis.reduce(T.int64(4), (k2_k3_k4_fused_0 * T.int64(32) + k2_k3_k4_fused_1) // T.int64(262144))
                        v_k3 = T.axis.reduce(T.int64(512), (k2_k3_k4_fused_0 * T.int64(32) + k2_k3_k4_fused_1) % T.int64(262144) // T.int64(512))
                        v_k4 = T.axis.reduce(T.int64(512), (k2_k3_k4_fused_0 * T.int64(32) + k2_k3_k4_fused_1) % T.int64(512))
                        T.reads(lv195[T.int64(0), (v_ax1 * T.int64(4) + (v_k4 // T.int64(512) + v_k3) // T.int64(512) + v_k2) % T.int64(128), (v_k4 // T.int64(512) + v_k3) % T.int64(512), v_k4 % T.int64(512)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv195[T.int64(0), (v_ax1 * T.int64(4) + (v_k4 // T.int64(512) + v_k3) // T.int64(512) + v_k2) % T.int64(128), (v_k4 // T.int64(512) + v_k3) % T.int64(512), v_k4 % T.int64(512)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv195[T.int64(0), (v_ax1 * T.int64(4) + (v_k4 // T.int64(512) + v_k3) // T.int64(512) + v_k2) % T.int64(128), (v_k4 // T.int64(512) + v_k3) % T.int64(512), v_k4 % T.int64(512)] * lv195[T.int64(0), (v_ax1 * T.int64(4) + (v_k4 // T.int64(512) + v_k3) // T.int64(512) + v_k2) % T.int64(128), (v_k4 // T.int64(512) + v_k3) % T.int64(512), v_k4 % T.int64(512)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(512)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(262144))
                        v_ax2 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(262144) // T.int64(512))
                        v_ax3 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(512))
                        T.reads(lv195[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128), (v_ax3 // T.int64(512) + v_ax2) % T.int64(512), v_ax3 % T.int64(512)], rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4)], rxplaceholder_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4)], param_0[((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128)], param_1[((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv195[T.int64(0), (((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4) * T.int64(4) + (v_ax3 % T.int64(512) // T.int64(512) + (v_ax3 // T.int64(512) + v_ax2) % T.int64(512)) // T.int64(512) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(4)) % T.int64(128), (v_ax3 % T.int64(512) // T.int64(512) + (v_ax3 // T.int64(512) + v_ax2) % T.int64(512)) % T.int64(512), v_ax3 % T.int64(512) % T.int64(512)] - rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4)] * T.float32(9.5367431640625e-07)) * T.rsqrt(rxplaceholder_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4)] * T.float32(9.5367431640625e-07) - rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4)] * T.float32(9.5367431640625e-07) * (rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4)] * T.float32(9.5367431640625e-07)) + T.float32(9.9999999999999995e-07)) * param_0[(((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4) * T.int64(4) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(4)) % T.int64(128)] + param_1[(((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4) * T.int64(4) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(4)) % T.int64(128)]) * T.sigmoid((lv195[T.int64(0), (((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4) * T.int64(4) + (v_ax3 % T.int64(512) // T.int64(512) + (v_ax3 // T.int64(512) + v_ax2) % T.int64(512)) // T.int64(512) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(4)) % T.int64(128), (v_ax3 % T.int64(512) // T.int64(512) + (v_ax3 // T.int64(512) + v_ax2) % T.int64(512)) % T.int64(512), v_ax3 % T.int64(512) % T.int64(512)] - rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4)] * T.float32(9.5367431640625e-07)) * T.rsqrt(rxplaceholder_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4)] * T.float32(9.5367431640625e-07) - rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4)] * T.float32(9.5367431640625e-07) * (rxplaceholder_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4)] * T.float32(9.5367431640625e-07)) + T.float32(9.9999999999999995e-07)) * param_0[(((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4) * T.int64(4) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(4)) % T.int64(128)] + param_1[(((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4) * T.int64(4) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(4)) % T.int64(128)])

    @T.prim_func
    def fused_group_norm2_silu2(lv226: T.Buffer((T.int64(2), T.int64(320), T.int64(32), T.int64(32)), "float32"), param_0: T.Buffer((T.int64(320),), "float32"), param_1: T.Buffer((T.int64(320),), "float32"), T_multiply: T.Buffer((T.int64(2), T.int64(320), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(80)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(10), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) // T.int64(1024))
                        v_k3 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(1024) // T.int64(32))
                        v_k4 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(32))
                        T.reads(lv226[((v_ax1 * T.int64(10) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(320), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv226[((v_ax1 * T.int64(10) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(320), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv226[((v_ax1 * T.int64(10) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(320), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)] * lv226[((v_ax1 * T.int64(10) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(320), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(327680))
                        v_ax1 = T.axis.spatial(T.int64(320), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(327680) // T.int64(1024))
                        v_ax2 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1024) // T.int64(32))
                        v_ax3 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                        T.reads(lv226[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320), (v_ax3 // T.int64(32) + v_ax2) % T.int64(32), v_ax3 % T.int64(32)], rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10)], rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10)], param_0[((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320)], param_1[((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv226[((((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(10)) // T.int64(320) + (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(10)) % T.int64(320), (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) % T.int64(32), v_ax3 % T.int64(32) % T.int64(32)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(9.7656250000000005e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(9.7656250000000005e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(9.7656250000000005e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(9.7656250000000005e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(10)) % T.int64(320)] + param_1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(10)) % T.int64(320)]) * T.sigmoid((lv226[((((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(10)) // T.int64(320) + (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(10)) % T.int64(320), (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) % T.int64(32), v_ax3 % T.int64(32) % T.int64(32)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(9.7656250000000005e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(9.7656250000000005e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(9.7656250000000005e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(9.7656250000000005e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(10)) % T.int64(320)] + param_1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(10)) % T.int64(320)])

    @T.prim_func
    def fused_group_norm3_silu3(lv238: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), param_0: T.Buffer((T.int64(640),), "float32"), param_1: T.Buffer((T.int64(640),), "float32"), T_multiply: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(320)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(20), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) // T.int64(1024))
                        v_k3 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) % T.int64(1024) // T.int64(32))
                        v_k4 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) % T.int64(32))
                        T.reads(lv238[((v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(640), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv238[((v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(640), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv238[((v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(640), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)] * lv238[((v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(640), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(20)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(655360))
                        v_ax1 = T.axis.spatial(T.int64(640), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(655360) // T.int64(1024))
                        v_ax2 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1024) // T.int64(32))
                        v_ax3 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                        T.reads(lv238[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640), (v_ax3 // T.int64(32) + v_ax2) % T.int64(32), v_ax3 % T.int64(32)], rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20)], rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20)], param_0[((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640)], param_1[((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv238[((((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(20)) // T.int64(640) + (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(20)) % T.int64(640), (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) % T.int64(32), v_ax3 % T.int64(32) % T.int64(32)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(4.8828125000000003e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(4.8828125000000003e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(4.8828125000000003e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(4.8828125000000003e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(20)) % T.int64(640)] + param_1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(20)) % T.int64(640)]) * T.sigmoid((lv238[((((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(20)) // T.int64(640) + (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(20)) % T.int64(640), (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) % T.int64(32), v_ax3 % T.int64(32) % T.int64(32)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(4.8828125000000003e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(4.8828125000000003e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(4.8828125000000003e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(4.8828125000000003e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(20)) % T.int64(640)] + param_1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(20)) % T.int64(640)])

    @T.prim_func
    def fused_group_norm5_silu4(lv432: T.Buffer((T.int64(2), T.int64(640), T.int64(16), T.int64(16)), "float32"), param_0: T.Buffer((T.int64(640),), "float32"), param_1: T.Buffer((T.int64(640),), "float32"), T_multiply: T.Buffer((T.int64(2), T.int64(640), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(20)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(20), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) // T.int64(256))
                        v_k3 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(256) // T.int64(16))
                        v_k4 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(16))
                        T.reads(lv432[((v_ax1 * T.int64(20) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(640), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv432[((v_ax1 * T.int64(20) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(640), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv432[((v_ax1 * T.int64(20) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(640), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)] * lv432[((v_ax1 * T.int64(20) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(640), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(163840))
                        v_ax1 = T.axis.spatial(T.int64(640), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(163840) // T.int64(256))
                        v_ax2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(256) // T.int64(16))
                        v_ax3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                        T.reads(lv432[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640), (v_ax3 // T.int64(16) + v_ax2) % T.int64(16), v_ax3 % T.int64(16)], rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20)], rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20)], param_0[((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640)], param_1[((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv432[((((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(20)) // T.int64(640) + (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(20)) % T.int64(640), (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) % T.int64(16), v_ax3 % T.int64(16) % T.int64(16)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(0.00019531250000000001)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(0.00019531250000000001) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(0.00019531250000000001) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(0.00019531250000000001)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(20)) % T.int64(640)] + param_1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(20)) % T.int64(640)]) * T.sigmoid((lv432[((((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(20)) // T.int64(640) + (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(20)) % T.int64(640), (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) % T.int64(16), v_ax3 % T.int64(16) % T.int64(16)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(0.00019531250000000001)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(0.00019531250000000001) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(0.00019531250000000001) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(0.00019531250000000001)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(20)) % T.int64(640)] + param_1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(20)) % T.int64(640)])

    @T.prim_func
    def fused_group_norm6_silu5(lv444: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), param_0: T.Buffer((T.int64(1280),), "float32"), param_1: T.Buffer((T.int64(1280),), "float32"), T_multiply: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for k2_k3_k4_fused_0 in range(T.int64(160)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(40), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) // T.int64(256))
                        v_k3 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) % T.int64(256) // T.int64(16))
                        v_k4 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) % T.int64(16))
                        T.reads(lv444[((v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1280), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv444[((v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1280), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv444[((v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1280), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)] * lv444[((v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1280), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(327680))
                        v_ax1 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(327680) // T.int64(256))
                        v_ax2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(256) // T.int64(16))
                        v_ax3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                        T.reads(lv444[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280), (v_ax3 // T.int64(16) + v_ax2) % T.int64(16), v_ax3 % T.int64(16)], rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40)], rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40)], param_0[((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280)], param_1[((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv444[((((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(40)) // T.int64(1280) + (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(40)) % T.int64(1280), (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) % T.int64(16), v_ax3 % T.int64(16) % T.int64(16)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(9.7656250000000005e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(9.7656250000000005e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(9.7656250000000005e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(9.7656250000000005e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(40)) % T.int64(1280)] + param_1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(40)) % T.int64(1280)]) * T.sigmoid((lv444[((((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(40)) // T.int64(1280) + (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(40)) % T.int64(1280), (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) % T.int64(16), v_ax3 % T.int64(16) % T.int64(16)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(9.7656250000000005e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(9.7656250000000005e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(9.7656250000000005e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(9.7656250000000005e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(40)) % T.int64(1280)] + param_1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(40)) % T.int64(1280)])

    @T.prim_func
    def fused_group_norm8_silu6(lv638: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), param_0: T.Buffer((T.int64(1280),), "float32"), param_1: T.Buffer((T.int64(1280),), "float32"), T_multiply: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(160)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(40), (k2_k3_k4_fused_0 * T.int64(16) + k2_k3_k4_fused_1) // T.int64(64))
                        v_k3 = T.axis.reduce(T.int64(8), (k2_k3_k4_fused_0 * T.int64(16) + k2_k3_k4_fused_1) % T.int64(64) // T.int64(8))
                        v_k4 = T.axis.reduce(T.int64(8), (k2_k3_k4_fused_0 * T.int64(16) + k2_k3_k4_fused_1) % T.int64(8))
                        T.reads(lv638[((v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(1280), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv638[((v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(1280), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv638[((v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(1280), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)] * lv638[((v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(1280), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(640), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(81920))
                    v_ax1 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(81920) // T.int64(64))
                    v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64) // T.int64(8))
                    v_ax3 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                    T.reads(lv638[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280), (v_ax3 // T.int64(8) + v_ax2) % T.int64(8), v_ax3 % T.int64(8)], rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40)], rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40)], param_0[((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280)], param_1[((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280)])
                    T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv638[((((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) // T.int64(8) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(40)) // T.int64(1280) + (((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) // T.int64(8) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(40)) % T.int64(1280), (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) % T.int64(8), v_ax3 % T.int64(8) % T.int64(8)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(0.00039062500000000002)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(0.00039062500000000002) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(0.00039062500000000002) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(0.00039062500000000002)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(40)) % T.int64(1280)] + param_1[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(40)) % T.int64(1280)]) * T.sigmoid((lv638[((((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) // T.int64(8) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(40)) // T.int64(1280) + (((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) // T.int64(8) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(40)) % T.int64(1280), (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) % T.int64(8), v_ax3 % T.int64(8) % T.int64(8)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(0.00039062500000000002)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(0.00039062500000000002) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(0.00039062500000000002) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(0.00039062500000000002)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(40)) % T.int64(1280)] + param_1[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(40)) % T.int64(1280)])

    @T.prim_func
    def fused_group_norm_silu1(lv23: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), param_0: T.Buffer((T.int64(320),), "float32"), param_1: T.Buffer((T.int64(320),), "float32"), T_multiply: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for k2_k3_k4_fused_0 in range(T.int64(1280)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(10), (k2_k3_k4_fused_0 * T.int64(32) + k2_k3_k4_fused_1) // T.int64(4096))
                        v_k3 = T.axis.reduce(T.int64(64), (k2_k3_k4_fused_0 * T.int64(32) + k2_k3_k4_fused_1) % T.int64(4096) // T.int64(64))
                        v_k4 = T.axis.reduce(T.int64(64), (k2_k3_k4_fused_0 * T.int64(32) + k2_k3_k4_fused_1) % T.int64(64))
                        T.reads(lv23[((v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(320), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv23[((v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(320), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv23[((v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(320), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)] * lv23[((v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(320), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(40)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1310720))
                        v_ax1 = T.axis.spatial(T.int64(320), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1310720) // T.int64(4096))
                        v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4096) // T.int64(64))
                        v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                        T.reads(lv23[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320), (v_ax3 // T.int64(64) + v_ax2) % T.int64(64), v_ax3 % T.int64(64)], rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10)], rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10)], param_0[((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320)], param_1[((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv23[((((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(10)) // T.int64(320) + (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(10)) % T.int64(320), (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) % T.int64(64), v_ax3 % T.int64(64) % T.int64(64)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(2.4414062500000001e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(2.4414062500000001e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(2.4414062500000001e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(2.4414062500000001e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(10)) % T.int64(320)] + param_1[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(10)) % T.int64(320)]) * T.sigmoid((lv23[((((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(10)) // T.int64(320) + (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(10)) % T.int64(320), (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) % T.int64(64), v_ax3 % T.int64(64) % T.int64(64)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(2.4414062500000001e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(2.4414062500000001e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(2.4414062500000001e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(2.4414062500000001e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(10)) % T.int64(320)] + param_1[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(10)) % T.int64(320)])

    @T.prim_func
    def fused_matmul12_multiply6(lv86: T.Buffer((T.int64(16), T.int64(4096), T.int64(40)), "float32"), lv93: T.Buffer((T.int64(16), T.int64(40), T.int64(77)), "float32"), T_multiply: T.Buffer((T.int64(16), T.int64(4096), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(16), T.int64(4096), T.int64(77)), scope="local")
        lv86_shared = T.alloc_buffer((T.int64(16), T.int64(4096), T.int64(40)), scope="shared")
        lv93_shared = T.alloc_buffer((T.int64(16), T.int64(40), T.int64(77)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(2048), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(176), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(7)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_3_init + i0_4_init + i0_0_i1_0_i2_0_fused // T.int64(256) * T.int64(2) + i0_2_i1_2_i2_2_fused // T.int64(88))
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(256) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(88) // T.int64(11) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused % T.int64(11) * T.int64(7) + i2_3_init * T.int64(7) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(2)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(176), thread="threadIdx.x"):
                                with T.block("lv86_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(256) * T.int64(2) + (ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1) // T.int64(320))
                                    v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(256) * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1) % T.int64(320) // T.int64(20))
                                    v2 = T.axis.spatial(T.int64(40), k_0 * T.int64(20) + (ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1) % T.int64(20))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1 < T.int64(640))
                                    T.reads(lv86[v0, v1, v2])
                                    T.writes(lv86_shared[v0, v1, v2])
                                    lv86_shared[v0, v1, v2] = lv86[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(18)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(176), thread="threadIdx.x"):
                                with T.block("lv93_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(256) * T.int64(2) + (ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1) // T.int64(1540))
                                    v1 = T.axis.spatial(T.int64(40), k_0 * T.int64(20) + (ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1) % T.int64(1540) // T.int64(77))
                                    v2 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1) % T.int64(77))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1 < T.int64(3080))
                                    T.reads(lv93[v0, v1, v2])
                                    T.writes(lv93_shared[v0, v1, v2])
                                    lv93_shared[v0, v1, v2] = lv93[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(5), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(7)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_3 + i0_4 + i0_0_i1_0_i2_0_fused // T.int64(256) * T.int64(2) + i0_2_i1_2_i2_2_fused // T.int64(88))
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(256) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(88) // T.int64(11) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused % T.int64(11) * T.int64(7) + i2_3 * T.int64(7) + i2_4)
                                v_k = T.axis.reduce(T.int64(40), k_0 * T.int64(20) + k_1 * T.int64(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv86_shared[v_i0, v_i1, v_k], lv93_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv86_shared[v_i0, v_i1, v_k] * lv93_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(7)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(256) * T.int64(2) + i0_2_i1_2_i2_2_fused // T.int64(88) + ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(256) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(88) // T.int64(11) + ax1)
                            v2 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused % T.int64(11) * T.int64(7) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(T_multiply[v0, v1, v2])
                            T_multiply[v0, v1, v2] = matmul_local[v0, v1, v2] * T.float32(0.15811388194561005)

    @T.prim_func
    def fused_matmul14_add11_gelu(lv105: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32"), param_0: T.Buffer((T.int64(320), T.int64(1280)), "float32"), param_1: T.Buffer((T.int64(1280),), "float32"), T_multiply: T.Buffer((T.int64(2), T.int64(4096), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul = T.alloc_buffer((T.int64(2), T.int64(4096), T.int64(1280)))
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(4096), T.int64(1280)), scope="local")
        lv105_shared = T.alloc_buffer((T.int64(2), T.int64(4096), T.int64(320)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(320), T.int64(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(2560), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(64), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_4_init + i0_0_i1_0_i2_0_fused // T.int64(1280) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(1280) // T.int64(20) * T.int64(64) + i0_1_i1_1_i2_1_fused // T.int64(4) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(16) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(1280), i2_4_init + i0_0_i1_0_i2_0_fused % T.int64(20) * T.int64(64) + i0_1_i1_1_i2_1_fused % T.int64(4) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(80)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("lv105_shared"):
                                    v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(1280))
                                    v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(1280) // T.int64(20) * T.int64(64) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(4))
                                    v2 = T.axis.spatial(T.int64(320), k_0 * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(4))
                                    T.reads(lv105[v0, v1, v2])
                                    T.writes(lv105_shared[v0, v1, v2])
                                    lv105_shared[v0, v1, v2] = lv105[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(320), k_0 * T.int64(4) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(20) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(64))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_4 + i0_0_i1_0_i2_0_fused // T.int64(1280) + i0_3)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(1280) // T.int64(20) * T.int64(64) + i0_1_i1_1_i2_1_fused // T.int64(4) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(16) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(1280), i2_4 + i0_0_i1_0_i2_0_fused % T.int64(20) * T.int64(64) + i0_1_i1_1_i2_1_fused % T.int64(4) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) + i2_3)
                                v_k = T.axis.reduce(T.int64(320), k_0 * T.int64(4) + k_1 * T.int64(2) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv105_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv105_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(1280) + ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(1280) // T.int64(20) * T.int64(64) + i0_1_i1_1_i2_1_fused // T.int64(4) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(20) * T.int64(64) + i0_1_i1_1_i2_1_fused % T.int64(4) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(160)):
                    with T.block("T_multiply_2"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(5242880))
                        v_ax1 = T.axis.spatial(T.int64(4096), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(5242880) // T.int64(1280))
                        v_ax2 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(1280))
                        T.reads(matmul[v_ax0, v_ax1, v_ax2], param_1[v_ax2])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2])
                        T_multiply[v_ax0, v_ax1, v_ax2] = (matmul[v_ax0, v_ax1, v_ax2] + param_1[v_ax2]) * (T.float32(0.5) + T.erf((matmul[v_ax0, v_ax1, v_ax2] + param_1[v_ax2]) * T.float32(0.70710678118654757)) * T.float32(0.5))

    @T.prim_func
    def fused_matmul14_add11_multiply7(lv105: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32"), param_0: T.Buffer((T.int64(320), T.int64(1280)), "float32"), param_1: T.Buffer((T.int64(1280),), "float32"), lv112: T.Buffer((T.int64(2), T.int64(4096), T.int64(1280)), "float32"), T_multiply: T.Buffer((T.int64(2), T.int64(4096), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(4096), T.int64(1280)), scope="local")
        lv105_shared = T.alloc_buffer((T.int64(2), T.int64(4096), T.int64(320)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(320), T.int64(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(5120), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(4), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_4_init + i0_1_i1_1_i2_1_fused // T.int64(2) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(20) * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(4) + i1_3_init * T.int64(4) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(1280), i2_4_init + i0_0_i1_0_i2_0_fused % T.int64(20) * T.int64(64) + i0_1_i1_1_i2_1_fused % T.int64(2) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(2) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(40)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("lv105_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(20) * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(128) // T.int64(8))
                                    v2 = T.axis.spatial(T.int64(320), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(8))
                                    T.reads(lv105[v0, v1, v2])
                                    T.writes(lv105_shared[v0, v1, v2])
                                    lv105_shared[v0, v1, v2] = lv105[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(320), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(20) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(64))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(4), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_4 + i0_1_i1_1_i2_1_fused // T.int64(2) + i0_3)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(20) * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(4) + i1_3 * T.int64(4) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(1280), i2_4 + i0_0_i1_0_i2_0_fused % T.int64(20) * T.int64(64) + i0_1_i1_1_i2_1_fused % T.int64(2) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(2) + i2_3)
                                v_k = T.axis.reduce(T.int64(320), k_0 * T.int64(8) + k_1 * T.int64(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv105_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv105_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(4), T.int64(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_1_i1_1_i2_1_fused // T.int64(2) + ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(20) * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(20) * T.int64(64) + i0_1_i1_1_i2_1_fused % T.int64(2) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2], param_1[v2], lv112[v0, v1, v2])
                            T.writes(T_multiply[v0, v1, v2])
                            T_multiply[v0, v1, v2] = (matmul_local[v0, v1, v2] + param_1[v2]) * lv112[v0, v1, v2]

    @T.prim_func
    def fused_matmul15_add9_add10(lv113: T.Buffer((T.int64(2), T.int64(4096), T.int64(1280)), "float32"), param_0: T.Buffer((T.int64(1280), T.int64(320)), "float32"), param_1: T.Buffer((T.int64(320),), "float32"), lv104: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(4096), T.int64(320)), scope="local")
        lv113_shared = T.alloc_buffer((T.int64(2), T.int64(4096), T.int64(1280)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(1280), T.int64(320)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(1280), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(8), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_3_init + i0_4_init + i0_2_i1_2_i2_2_fused // T.int64(32))
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(16) + i1_3_init * T.int64(8) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(320), i2_4_init + i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(32) * T.int64(2) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(320)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("lv113_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(64) // T.int64(4))
                                    v2 = T.axis.spatial(T.int64(1280), k_0 * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(4))
                                    T.reads(lv113[v0, v1, v2])
                                    T.writes(lv113_shared[v0, v1, v2])
                                    lv113_shared[v0, v1, v2] = lv113[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(4) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(64))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(4), T.int64(1), T.int64(8), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_3 + i0_4 + i0_2_i1_2_i2_2_fused // T.int64(32))
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(16) + i1_3 * T.int64(8) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(320), i2_4 + i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(32) * T.int64(2) + i2_3)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(4) + k_1 * T.int64(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv113_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv113_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(16), T.int64(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(32) + ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(32) * T.int64(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2], param_1[v2], lv104[v0, v1, v2])
                            T.writes(T_add[v0, v1, v2])
                            T_add[v0, v1, v2] = matmul_local[v0, v1, v2] + param_1[v2] + lv104[v0, v1, v2]

    @T.prim_func
    def fused_matmul16_add14_strided_slice4(lv232: T.Buffer((T.int64(2), T.int64(1280)), "float32"), param_0: T.Buffer((T.int64(1280), T.int64(640)), "float32"), param_1: T.Buffer((T.int64(640),), "float32"), T_strided_slice_with_axes: T.Buffer((T.int64(2), T.int64(640)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(640)), scope="local")
        lv232_shared = T.alloc_buffer((T.int64(2), T.int64(1280)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(1280), T.int64(640)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_fused // T.int64(32) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(640), i1_4_init + i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused % T.int64(32) + i1_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("lv232_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(20))
                                    v1 = T.axis.spatial(T.int64(1280), k_0 * T.int64(20) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(20))
                                    T.where(ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 < T.int64(40))
                                    T.reads(lv232[v0, v1])
                                    T.writes(lv232_shared[v0, v1])
                                    lv232_shared[v0, v1] = lv232[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(10)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(20) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(640), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(32))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(20), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_fused // T.int64(32) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(640), i1_4 + i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused % T.int64(32) + i1_3)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(20) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1], lv232_shared[v_i0, v_k], param_0_shared[v_k, v_i1])
                                T.writes(matmul_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1] = matmul_local[v_i0, v_i1] + lv232_shared[v_i0, v_k] * param_0_shared[v_k, v_i1]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_2_i1_2_fused // T.int64(32) + ax0)
                            v1 = T.axis.spatial(T.int64(640), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused % T.int64(32) + ax1)
                            T.reads(matmul_local[v0, v1], param_1[v1])
                            T.writes(T_strided_slice_with_axes[v0, v1])
                            T_strided_slice_with_axes[v0, v1] = matmul_local[v0, v1] + param_1[v1]

    @T.prim_func
    def fused_matmul17_add17_add18(lv278: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32"), param_0: T.Buffer((T.int64(640), T.int64(640)), "float32"), param_1: T.Buffer((T.int64(640),), "float32"), lv254: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(1024), T.int64(640)), scope="local")
        lv278_shared = T.alloc_buffer((T.int64(2), T.int64(1024), T.int64(640)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(640), T.int64(640)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(640), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_3_init + i0_4_init + i0_2_i1_2_i2_2_fused // T.int64(32))
                            v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(10) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(32) // T.int64(16) * T.int64(8) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(2) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(80)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("lv278_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(10) * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(128) // T.int64(8))
                                    v2 = T.axis.spatial(T.int64(640), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(8))
                                    T.reads(lv278[v0, v1, v2])
                                    T.writes(lv278_shared[v0, v1, v2])
                                    lv278_shared[v0, v1, v2] = lv278[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(640), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(64))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(8), T.int64(1), T.int64(2), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_3 + i0_4 + i0_2_i1_2_i2_2_fused // T.int64(32))
                                v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(10) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(32) // T.int64(16) * T.int64(8) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(2) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(640), k_0 * T.int64(8) + k_1 * T.int64(8) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv278_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv278_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(8), T.int64(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(32) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(10) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(32) // T.int64(16) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2], param_1[v2], lv254[v0, v1, v2])
                            T.writes(T_add[v0, v1, v2])
                            T_add[v0, v1, v2] = matmul_local[v0, v1, v2] + param_1[v2] + lv254[v0, v1, v2]

    @T.prim_func
    def fused_matmul18_multiply8(lv264: T.Buffer((T.int64(16), T.int64(1024), T.int64(80)), "float32"), lv271: T.Buffer((T.int64(16), T.int64(80), T.int64(1024)), "float32"), T_multiply: T.Buffer((T.int64(16), T.int64(1024), T.int64(1024)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(16), T.int64(1024), T.int64(1024)), scope="local")
        lv264_shared = T.alloc_buffer((T.int64(16), T.int64(1024), T.int64(80)), scope="shared")
        lv271_shared = T.alloc_buffer((T.int64(16), T.int64(80), T.int64(1024)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(8192), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(16), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_4_init + i0_0_i1_0_i2_0_fused // T.int64(512) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(512) // T.int64(16) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(32) * T.int64(16) + i1_3_init * T.int64(16) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(1024), i2_4_init + i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(32) * T.int64(2) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(20)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("lv264_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(512))
                                    v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(512) // T.int64(16) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(4))
                                    v2 = T.axis.spatial(T.int64(80), k_0 * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(4))
                                    T.reads(lv264[v0, v1, v2])
                                    T.writes(lv264_shared[v0, v1, v2])
                                    lv264_shared[v0, v1, v2] = lv264[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("lv271_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(512))
                                    v1 = T.axis.spatial(T.int64(80), k_0 * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(64))
                                    v2 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(64) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(64))
                                    T.reads(lv271[v0, v1, v2])
                                    T.writes(lv271_shared[v0, v1, v2])
                                    lv271_shared[v0, v1, v2] = lv271[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(16), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_4 + i0_0_i1_0_i2_0_fused // T.int64(512) + i0_3)
                                v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(512) // T.int64(16) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(32) * T.int64(16) + i1_3 * T.int64(16) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(1024), i2_4 + i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(32) * T.int64(2) + i2_3)
                                v_k = T.axis.reduce(T.int64(80), k_0 * T.int64(4) + k_1 * T.int64(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv264_shared[v_i0, v_i1, v_k], lv271_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv264_shared[v_i0, v_i1, v_k] * lv271_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(16), T.int64(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(512) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(512) // T.int64(16) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(32) * T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(32) * T.int64(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(T_multiply[v0, v1, v2])
                            T_multiply[v0, v1, v2] = matmul_local[v0, v1, v2] * T.float32(0.11180339753627777)

    @T.prim_func
    def fused_matmul21_multiply9(lv292: T.Buffer((T.int64(16), T.int64(1024), T.int64(80)), "float32"), lv299: T.Buffer((T.int64(16), T.int64(80), T.int64(77)), "float32"), T_multiply: T.Buffer((T.int64(16), T.int64(1024), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(16), T.int64(1024), T.int64(77)), scope="local")
        lv292_shared = T.alloc_buffer((T.int64(16), T.int64(1024), T.int64(80)), scope="shared")
        lv299_shared = T.alloc_buffer((T.int64(16), T.int64(80), T.int64(77)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(154), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_4_init + i0_0_i1_0_i2_0_fused // T.int64(32) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(77) * T.int64(4) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(77), i2_4_init + i0_2_i1_2_i2_2_fused % T.int64(77) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(5)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(154), thread="threadIdx.x"):
                                with T.block("lv292_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(154) + ax0_ax1_ax2_fused_1) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(80), k_0 * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(154) + ax0_ax1_ax2_fused_1) % T.int64(16))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int64(154) + ax0_ax1_ax2_fused_1 < T.int64(512))
                                    T.reads(lv292[v0, v1, v2])
                                    T.writes(lv292_shared[v0, v1, v2])
                                    lv292_shared[v0, v1, v2] = lv292[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(154), thread="threadIdx.x"):
                                with T.block("lv299_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(80), k_0 * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(154) + ax0_ax1_ax2_fused_1) // T.int64(77))
                                    v2 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_fused_0 * T.int64(154) + ax0_ax1_ax2_fused_1) % T.int64(77))
                                    T.reads(lv299[v0, v1, v2])
                                    T.writes(lv299_shared[v0, v1, v2])
                                    lv299_shared[v0, v1, v2] = lv299[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(4), T.int64(1), T.int64(4), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_4 + i0_0_i1_0_i2_0_fused // T.int64(32) + i0_3)
                                v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(77) * T.int64(4) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(77), i2_4 + i0_2_i1_2_i2_2_fused % T.int64(77) + i2_3)
                                v_k = T.axis.reduce(T.int64(80), k_0 * T.int64(16) + k_1 * T.int64(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv292_shared[v_i0, v_i1, v_k], lv299_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv292_shared[v_i0, v_i1, v_k] * lv299_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(4), T.int64(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(32) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(77) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused % T.int64(77) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(T_multiply[v0, v1, v2])
                            T_multiply[v0, v1, v2] = matmul_local[v0, v1, v2] * T.float32(0.11180339753627777)

    @T.prim_func
    def fused_matmul23_add19_gelu1(lv311: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32"), param_0: T.Buffer((T.int64(640), T.int64(2560)), "float32"), param_1: T.Buffer((T.int64(2560),), "float32"), T_multiply: T.Buffer((T.int64(2), T.int64(1024), T.int64(2560)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul = T.alloc_buffer((T.int64(2), T.int64(1024), T.int64(2560)))
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(1024), T.int64(2560)), scope="local")
        lv311_shared = T.alloc_buffer((T.int64(2), T.int64(1024), T.int64(640)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(640), T.int64(2560)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(1280), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(64), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_4_init + i0_0_i1_0_i2_0_fused // T.int64(640) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(640) // T.int64(40) * T.int64(64) + i0_1_i1_1_i2_1_fused // T.int64(4) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(16) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(2560), i2_4_init + i0_0_i1_0_i2_0_fused % T.int64(40) * T.int64(64) + i0_1_i1_1_i2_1_fused % T.int64(4) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(160)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("lv311_shared"):
                                    v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(640))
                                    v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(640) // T.int64(40) * T.int64(64) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(4))
                                    v2 = T.axis.spatial(T.int64(640), k_0 * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(4))
                                    T.reads(lv311[v0, v1, v2])
                                    T.writes(lv311_shared[v0, v1, v2])
                                    lv311_shared[v0, v1, v2] = lv311[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(640), k_0 * T.int64(4) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(2560), i0_0_i1_0_i2_0_fused % T.int64(40) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(64))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_4 + i0_0_i1_0_i2_0_fused // T.int64(640) + i0_3)
                                v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(640) // T.int64(40) * T.int64(64) + i0_1_i1_1_i2_1_fused // T.int64(4) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(16) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(2560), i2_4 + i0_0_i1_0_i2_0_fused % T.int64(40) * T.int64(64) + i0_1_i1_1_i2_1_fused % T.int64(4) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) + i2_3)
                                v_k = T.axis.reduce(T.int64(640), k_0 * T.int64(4) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv311_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv311_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(640) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(640) // T.int64(40) * T.int64(64) + i0_1_i1_1_i2_1_fused // T.int64(4) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(2560), i0_0_i1_0_i2_0_fused % T.int64(40) * T.int64(64) + i0_1_i1_1_i2_1_fused % T.int64(4) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(80)):
                    with T.block("T_multiply_2"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(2621440))
                        v_ax1 = T.axis.spatial(T.int64(1024), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(2621440) // T.int64(2560))
                        v_ax2 = T.axis.spatial(T.int64(2560), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(2560))
                        T.reads(matmul[v_ax0, v_ax1, v_ax2], param_1[v_ax2])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2])
                        T_multiply[v_ax0, v_ax1, v_ax2] = (matmul[v_ax0, v_ax1, v_ax2] + param_1[v_ax2]) * (T.float32(0.5) + T.erf((matmul[v_ax0, v_ax1, v_ax2] + param_1[v_ax2]) * T.float32(0.70710678118654757)) * T.float32(0.5))

    @T.prim_func
    def fused_matmul23_add19_multiply10(lv311: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32"), param_0: T.Buffer((T.int64(640), T.int64(2560)), "float32"), param_1: T.Buffer((T.int64(2560),), "float32"), lv318: T.Buffer((T.int64(2), T.int64(1024), T.int64(2560)), "float32"), T_multiply: T.Buffer((T.int64(2), T.int64(1024), T.int64(2560)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(1024), T.int64(2560)), scope="local")
        lv311_shared = T.alloc_buffer((T.int64(2), T.int64(1024), T.int64(640)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(640), T.int64(2560)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(2048), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(4), T.int64(5), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_4_init + i0_0_i1_0_i2_0_fused // T.int64(1024) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(1024) // T.int64(32) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(4) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(2560), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(80) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(10) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(160)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("lv311_shared"):
                                    v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(1024))
                                    v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(1024) // T.int64(32) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(4))
                                    v2 = T.axis.spatial(T.int64(640), k_0 * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(4))
                                    T.reads(lv311[v0, v1, v2])
                                    T.writes(lv311_shared[v0, v1, v2])
                                    lv311_shared[v0, v1, v2] = lv311[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(5)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(640), k_0 * T.int64(4) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(80))
                                    v1 = T.axis.spatial(T.int64(2560), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(80) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(80))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(5), T.int64(4), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_4 + i0_0_i1_0_i2_0_fused // T.int64(1024) + i0_3)
                                v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(1024) // T.int64(32) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(4) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(2560), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(80) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(10) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(640), k_0 * T.int64(4) + k_1 * T.int64(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv311_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv311_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(4), T.int64(10)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(1024) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(1024) // T.int64(32) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(2560), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(80) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(10) + ax2)
                            T.reads(matmul_local[v0, v1, v2], param_1[v2], lv318[v0, v1, v2])
                            T.writes(T_multiply[v0, v1, v2])
                            T_multiply[v0, v1, v2] = (matmul_local[v0, v1, v2] + param_1[v2]) * lv318[v0, v1, v2]

    @T.prim_func
    def fused_matmul24_add17_add18(lv319: T.Buffer((T.int64(2), T.int64(1024), T.int64(2560)), "float32"), param_0: T.Buffer((T.int64(2560), T.int64(640)), "float32"), param_1: T.Buffer((T.int64(640),), "float32"), lv310: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(1024), T.int64(640)), scope="local")
        lv319_shared = T.alloc_buffer((T.int64(2), T.int64(1024), T.int64(2560)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(2560), T.int64(640)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(1280), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(8), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_4_init + i0_0_i1_0_i2_0_fused // T.int64(640) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(640) // T.int64(20) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(8) + i1_3_init * T.int64(8) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(640), i2_4_init + i0_0_i1_0_i2_0_fused % T.int64(20) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(320)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("lv319_shared"):
                                    v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(640))
                                    v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(640) // T.int64(20) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) // T.int64(8))
                                    v2 = T.axis.spatial(T.int64(2560), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) % T.int64(8))
                                    T.reads(lv319[v0, v1, v2])
                                    T.writes(lv319_shared[v0, v1, v2])
                                    lv319_shared[v0, v1, v2] = lv319[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(2560), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(20) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(32))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(8), T.int64(1), T.int64(8), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_4 + i0_0_i1_0_i2_0_fused // T.int64(640) + i0_3)
                                v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(640) // T.int64(20) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(8) + i1_3 * T.int64(8) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(640), i2_4 + i0_0_i1_0_i2_0_fused % T.int64(20) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + i2_3)
                                v_k = T.axis.reduce(T.int64(2560), k_0 * T.int64(8) + k_1 * T.int64(8) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv319_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv319_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(8), T.int64(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(640) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(640) // T.int64(20) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(20) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2], param_1[v2], lv310[v0, v1, v2])
                            T.writes(T_add[v0, v1, v2])
                            T_add[v0, v1, v2] = matmul_local[v0, v1, v2] + param_1[v2] + lv310[v0, v1, v2]

    @T.prim_func
    def fused_matmul25_add24_add25(lv484: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32"), param_0: T.Buffer((T.int64(1280), T.int64(1280)), "float32"), param_1: T.Buffer((T.int64(1280),), "float32"), lv460: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(256), T.int64(1280)), scope="local")
        lv484_shared = T.alloc_buffer((T.int64(2), T.int64(256), T.int64(1280)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(1280), T.int64(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(640), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(2), T.int64(4), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_3_init * T.int64(2) + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(40) * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(8) + i1_3_init * T.int64(4) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(1280), i2_4_init + i0_0_i1_0_i2_0_fused % T.int64(40) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(2) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(320)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("lv484_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(40) * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) % T.int64(64) // T.int64(4))
                                    v2 = T.axis.spatial(T.int64(1280), k_0 * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) % T.int64(4))
                                    T.reads(lv484[v0, v1, v2])
                                    T.writes(lv484_shared[v0, v1, v2])
                                    lv484_shared[v0, v1, v2] = lv484[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(4) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(40) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(32))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(2), T.int64(1), T.int64(2), T.int64(2), T.int64(2), T.int64(2), T.int64(4), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_3 * T.int64(2) + i0_4)
                                v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(40) * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(8) + i1_3 * T.int64(4) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(1280), i2_4 + i0_0_i1_0_i2_0_fused % T.int64(40) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(2) + i2_3)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(4) + k_1 * T.int64(2) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv484_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv484_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(2), T.int64(8), T.int64(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(40) * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(40) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2], param_1[v2], lv460[v0, v1, v2])
                            T.writes(T_add[v0, v1, v2])
                            T_add[v0, v1, v2] = matmul_local[v0, v1, v2] + param_1[v2] + lv460[v0, v1, v2]

    @T.prim_func
    def fused_matmul26_multiply11(lv470: T.Buffer((T.int64(16), T.int64(256), T.int64(160)), "float32"), lv477: T.Buffer((T.int64(16), T.int64(160), T.int64(256)), "float32"), T_multiply: T.Buffer((T.int64(16), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(16), T.int64(256), T.int64(256)), scope="local")
        lv470_shared = T.alloc_buffer((T.int64(16), T.int64(256), T.int64(160)), scope="shared")
        lv477_shared = T.alloc_buffer((T.int64(16), T.int64(160), T.int64(256)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_4_init + i0_0_i1_0_i2_0_fused // T.int64(16) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(16) // T.int64(4) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(2) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(4) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(4) + i2_3_init * T.int64(4) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(5)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("lv470_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(16) // T.int64(4) * T.int64(64) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(160), k_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) % T.int64(32))
                                    T.reads(lv470[v0, v1, v2])
                                    T.writes(lv470_shared[v0, v1, v2])
                                    lv470_shared[v0, v1, v2] = lv470[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("lv477_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(160), k_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) // T.int64(64))
                                    v2 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(4) * T.int64(64) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) % T.int64(64))
                                    T.reads(lv477[v0, v1, v2])
                                    T.writes(lv477_shared[v0, v1, v2])
                                    lv477_shared[v0, v1, v2] = lv477[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(8), T.int64(1), T.int64(2), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(4)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_4 + i0_0_i1_0_i2_0_fused // T.int64(16) + i0_3)
                                v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(16) // T.int64(4) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(2) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(4) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(4) + i2_3 * T.int64(4) + i2_4)
                                v_k = T.axis.reduce(T.int64(160), k_0 * T.int64(32) + k_1 * T.int64(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv470_shared[v_i0, v_i1, v_k], lv477_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv470_shared[v_i0, v_i1, v_k] * lv477_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(2), T.int64(4)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(16) + ax0)
                            v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(16) // T.int64(4) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(4) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(4) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(T_multiply[v0, v1, v2])
                            T_multiply[v0, v1, v2] = matmul_local[v0, v1, v2] * T.float32(0.079056940972805023)

    @T.prim_func
    def fused_matmul29_multiply12(lv498: T.Buffer((T.int64(16), T.int64(256), T.int64(160)), "float32"), lv505: T.Buffer((T.int64(16), T.int64(160), T.int64(77)), "float32"), T_multiply: T.Buffer((T.int64(16), T.int64(256), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(16), T.int64(256), T.int64(77)), scope="local")
        lv498_shared = T.alloc_buffer((T.int64(16), T.int64(256), T.int64(160)), scope="shared")
        lv505_shared = T.alloc_buffer((T.int64(16), T.int64(160), T.int64(77)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(11), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_4_init + i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(2) + i0_1_i1_1_i2_1_fused + i0_3_init)
                            v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(7) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(77), i2_4_init + i0_2_i1_2_i2_2_fused % T.int64(7) * T.int64(11) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(10)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("lv498_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(2) + (ax0_ax1_ax2_fused_0 * T.int64(224) + ax0_ax1_ax2_fused_1) // T.int64(512))
                                    v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(224) + ax0_ax1_ax2_fused_1) % T.int64(512) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(160), k_0 * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(224) + ax0_ax1_ax2_fused_1) % T.int64(16))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int64(224) + ax0_ax1_ax2_fused_1 < T.int64(1024))
                                    T.reads(lv498[v0, v1, v2])
                                    T.writes(lv498_shared[v0, v1, v2])
                                    lv498_shared[v0, v1, v2] = lv498[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(11)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("lv505_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(2) + (ax0_ax1_ax2_fused_0 * T.int64(224) + ax0_ax1_ax2_fused_1) // T.int64(1232))
                                    v1 = T.axis.spatial(T.int64(160), k_0 * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(224) + ax0_ax1_ax2_fused_1) % T.int64(1232) // T.int64(77))
                                    v2 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_fused_0 * T.int64(224) + ax0_ax1_ax2_fused_1) % T.int64(77))
                                    T.reads(lv505[v0, v1, v2])
                                    T.writes(lv505_shared[v0, v1, v2])
                                    lv505_shared[v0, v1, v2] = lv505[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(11), T.int64(4), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_4 + i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(2) + i0_1_i1_1_i2_1_fused + i0_3)
                                v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(7) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(77), i2_4 + i0_2_i1_2_i2_2_fused % T.int64(7) * T.int64(11) + i2_3)
                                v_k = T.axis.reduce(T.int64(160), k_0 * T.int64(16) + k_1 * T.int64(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv498_shared[v_i0, v_i1, v_k], lv505_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv498_shared[v_i0, v_i1, v_k] * lv505_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(11)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(2) + i0_1_i1_1_i2_1_fused + ax0)
                            v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(7) + ax1)
                            v2 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused % T.int64(7) * T.int64(11) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(T_multiply[v0, v1, v2])
                            T_multiply[v0, v1, v2] = matmul_local[v0, v1, v2] * T.float32(0.079056940972805023)

    @T.prim_func
    def fused_matmul31_add26_gelu2(lv517: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32"), param_0: T.Buffer((T.int64(1280), T.int64(5120)), "float32"), param_1: T.Buffer((T.int64(5120),), "float32"), T_multiply: T.Buffer((T.int64(2), T.int64(256), T.int64(5120)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul = T.alloc_buffer((T.int64(2), T.int64(256), T.int64(5120)))
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(256), T.int64(5120)), scope="local")
        lv517_shared = T.alloc_buffer((T.int64(2), T.int64(256), T.int64(1280)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(1280), T.int64(5120)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(640), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(16), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_4_init + i0_3_init)
                            v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(32) + i0_1_i1_1_i2_1_fused // T.int64(4) * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(2) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(5120), i2_4_init + i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(64) + i0_1_i1_1_i2_1_fused % T.int64(4) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(320)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("lv517_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(128) // T.int64(4))
                                    v2 = T.axis.spatial(T.int64(1280), k_0 * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(4))
                                    T.reads(lv517[v0, v1, v2])
                                    T.writes(lv517_shared[v0, v1, v2])
                                    lv517_shared[v0, v1, v2] = lv517[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(4) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(64))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(4), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_4 + i0_3)
                                v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(32) + i0_1_i1_1_i2_1_fused // T.int64(4) * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(2) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(5120), i2_4 + i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(64) + i0_1_i1_1_i2_1_fused % T.int64(4) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) + i2_3)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(4) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv517_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv517_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(2), T.int64(2), T.int64(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(32) + i0_1_i1_1_i2_1_fused // T.int64(4) * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(64) + i0_1_i1_1_i2_1_fused % T.int64(4) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(40)):
                    with T.block("T_multiply_2"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(1310720))
                        v_ax1 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(1310720) // T.int64(5120))
                        v_ax2 = T.axis.spatial(T.int64(5120), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(5120))
                        T.reads(matmul[v_ax0, v_ax1, v_ax2], param_1[v_ax2])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2])
                        T_multiply[v_ax0, v_ax1, v_ax2] = (matmul[v_ax0, v_ax1, v_ax2] + param_1[v_ax2]) * (T.float32(0.5) + T.erf((matmul[v_ax0, v_ax1, v_ax2] + param_1[v_ax2]) * T.float32(0.70710678118654757)) * T.float32(0.5))

    @T.prim_func
    def fused_matmul31_add26_multiply13(lv517: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32"), param_0: T.Buffer((T.int64(1280), T.int64(5120)), "float32"), param_1: T.Buffer((T.int64(5120),), "float32"), lv524: T.Buffer((T.int64(2), T.int64(256), T.int64(5120)), "float32"), T_multiply: T.Buffer((T.int64(2), T.int64(256), T.int64(5120)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(256), T.int64(5120)), scope="local")
        lv517_shared = T.alloc_buffer((T.int64(2), T.int64(256), T.int64(1280)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(1280), T.int64(5120)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(1280), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_4_init + i0_3_init)
                            v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(160) * T.int64(32) + i0_1_i1_1_i2_1_fused // T.int64(2) * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(5120), i2_4_init + i0_0_i1_0_i2_0_fused % T.int64(160) * T.int64(32) + i0_1_i1_1_i2_1_fused % T.int64(2) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(160)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("lv517_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(160) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(256) // T.int64(8))
                                    v2 = T.axis.spatial(T.int64(1280), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(8))
                                    T.reads(lv517[v0, v1, v2])
                                    T.writes(lv517_shared[v0, v1, v2])
                                    lv517_shared[v0, v1, v2] = lv517[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(160) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(32))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(4), T.int64(2), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_4 + i0_3)
                                v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(160) * T.int64(32) + i0_1_i1_1_i2_1_fused // T.int64(2) * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(5120), i2_4 + i0_0_i1_0_i2_0_fused % T.int64(160) * T.int64(32) + i0_1_i1_1_i2_1_fused % T.int64(2) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + i2_3)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(8) + k_1 * T.int64(2) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv517_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv517_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(2), T.int64(2), T.int64(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(160) * T.int64(32) + i0_1_i1_1_i2_1_fused // T.int64(2) * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(160) * T.int64(32) + i0_1_i1_1_i2_1_fused % T.int64(2) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2], param_1[v2], lv524[v0, v1, v2])
                            T.writes(T_multiply[v0, v1, v2])
                            T_multiply[v0, v1, v2] = (matmul_local[v0, v1, v2] + param_1[v2]) * lv524[v0, v1, v2]

    @T.prim_func
    def fused_matmul32_add24_add25(lv525: T.Buffer((T.int64(2), T.int64(256), T.int64(5120)), "float32"), param_0: T.Buffer((T.int64(5120), T.int64(1280)), "float32"), param_1: T.Buffer((T.int64(1280),), "float32"), lv516: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(256), T.int64(1280)), scope="local")
        lv525_shared = T.alloc_buffer((T.int64(2), T.int64(256), T.int64(5120)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(5120), T.int64(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(2), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_3_init * T.int64(2) + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(32) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(10) * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(10) * T.int64(4) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(256)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                with T.block("lv525_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(160) + ax0_ax1_ax2_fused_1) // T.int64(1280))
                                    v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(32) * T.int64(64) + (ax0_ax1_ax2_fused_0 * T.int64(160) + ax0_ax1_ax2_fused_1) % T.int64(1280) // T.int64(20))
                                    v2 = T.axis.spatial(T.int64(5120), k_0 * T.int64(20) + (ax0_ax1_ax2_fused_0 * T.int64(160) + ax0_ax1_ax2_fused_1) % T.int64(20))
                                    T.reads(lv525[v0, v1, v2])
                                    T.writes(lv525_shared[v0, v1, v2])
                                    lv525_shared[v0, v1, v2] = lv525[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(5)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(5120), k_0 * T.int64(20) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1) // T.int64(40))
                                    v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1) % T.int64(40))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(5), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(2), T.int64(2), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_3 * T.int64(2) + i0_4)
                                v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(32) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(10) * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(10) * T.int64(4) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(5120), k_0 * T.int64(20) + k_1 * T.int64(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv525_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv525_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(2), T.int64(2), T.int64(4)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(32) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(10) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(10) * T.int64(4) + ax2)
                            T.reads(matmul_local[v0, v1, v2], param_1[v2], lv516[v0, v1, v2])
                            T.writes(T_add[v0, v1, v2])
                            T_add[v0, v1, v2] = matmul_local[v0, v1, v2] + param_1[v2] + lv516[v0, v1, v2]

    @T.prim_func
    def fused_matmul33_add30_add31(lv725: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32"), param_0: T.Buffer((T.int64(1280), T.int64(1280)), "float32"), param_1: T.Buffer((T.int64(1280),), "float32"), lv701: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(64), T.int64(1280)), scope="local")
        lv725_shared = T.alloc_buffer((T.int64(2), T.int64(64), T.int64(1280)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(1280), T.int64(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_4_init + i0_0_i1_0_i2_0_fused // T.int64(10) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(10) // T.int64(5) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(128) * T.int64(8) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(256) + i0_2_i1_2_i2_2_fused % T.int64(128) * T.int64(2) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(80)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("lv725_shared"):
                                    v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(10))
                                    v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(10) // T.int64(5) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(1280), k_0 * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) % T.int64(16))
                                    T.reads(lv725[v0, v1, v2])
                                    T.writes(lv725_shared[v0, v1, v2])
                                    lv725_shared[v0, v1, v2] = lv725[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1) // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(256) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1) % T.int64(256))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(8), T.int64(1), T.int64(4), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_4 + i0_0_i1_0_i2_0_fused // T.int64(10) + i0_3)
                                v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(10) // T.int64(5) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(128) * T.int64(8) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(256) + i0_2_i1_2_i2_2_fused % T.int64(128) * T.int64(2) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(16) + k_1 * T.int64(2) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv725_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv725_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(8), T.int64(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(10) + ax0)
                            v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(10) // T.int64(5) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(128) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(256) + i0_2_i1_2_i2_2_fused % T.int64(128) * T.int64(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2], param_1[v2], lv701[v0, v1, v2])
                            T.writes(T_add[v0, v1, v2])
                            T_add[v0, v1, v2] = matmul_local[v0, v1, v2] + param_1[v2] + lv701[v0, v1, v2]

    @T.prim_func
    def fused_matmul34_multiply14(lv711: T.Buffer((T.int64(16), T.int64(64), T.int64(160)), "float32"), lv718: T.Buffer((T.int64(16), T.int64(160), T.int64(64)), "float32"), T_multiply: T.Buffer((T.int64(16), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(16), T.int64(64), T.int64(64)), scope="local")
        lv711_shared = T.alloc_buffer((T.int64(16), T.int64(64), T.int64(160)), scope="shared")
        lv718_shared = T.alloc_buffer((T.int64(16), T.int64(160), T.int64(64)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(16), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(2) * T.int64(2) + i0_3_init * T.int64(2) + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(64), i0_2_i1_2_i2_2_fused // T.int64(4) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(2) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(8) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(8)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("lv711_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(2) * T.int64(2) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) // T.int64(1280))
                                    v1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) % T.int64(1280) // T.int64(20))
                                    v2 = T.axis.spatial(T.int64(160), k_0 * T.int64(20) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) % T.int64(20))
                                    T.reads(lv711[v0, v1, v2])
                                    T.writes(lv711_shared[v0, v1, v2])
                                    lv711_shared[v0, v1, v2] = lv711[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("lv718_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(2) * T.int64(2) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) // T.int64(640))
                                    v1 = T.axis.spatial(T.int64(160), k_0 * T.int64(20) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) % T.int64(640) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(2) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) % T.int64(32))
                                    T.reads(lv718[v0, v1, v2])
                                    T.writes(lv718_shared[v0, v1, v2])
                                    lv718_shared[v0, v1, v2] = lv718[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(20), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(2) * T.int64(2) + i0_3 * T.int64(2) + i0_4)
                                v_i1 = T.axis.spatial(T.int64(64), i0_2_i1_2_i2_2_fused // T.int64(4) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(2) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(8) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(160), k_0 * T.int64(20) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv711_shared[v_i0, v_i1, v_k], lv718_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv711_shared[v_i0, v_i1, v_k] * lv718_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(2), T.int64(1), T.int64(8)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(2) * T.int64(2) + ax0)
                            v1 = T.axis.spatial(T.int64(64), i0_2_i1_2_i2_2_fused // T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(2) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(8) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(T_multiply[v0, v1, v2])
                            T_multiply[v0, v1, v2] = matmul_local[v0, v1, v2] * T.float32(0.079056940972805023)

    @T.prim_func
    def fused_matmul36_multiply15(lv739: T.Buffer((T.int64(16), T.int64(64), T.int64(160)), "float32"), lv746: T.Buffer((T.int64(16), T.int64(160), T.int64(77)), "float32"), T_multiply: T.Buffer((T.int64(16), T.int64(64), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(16), T.int64(64), T.int64(77)), scope="local")
        lv739_shared = T.alloc_buffer((T.int64(16), T.int64(64), T.int64(160)), scope="shared")
        lv746_shared = T.alloc_buffer((T.int64(16), T.int64(160), T.int64(77)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(28), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(176), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_4_init + i0_0_i1_0_i2_0_fused // T.int64(7) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(88) * T.int64(2) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int64(64), i0_2_i1_2_i2_2_fused % T.int64(88) // T.int64(11) * T.int64(8) + i1_3_init * T.int64(8) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(77), i2_4_init + i0_0_i1_0_i2_0_fused % T.int64(7) * T.int64(11) + i0_2_i1_2_i2_2_fused % T.int64(11) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(20)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(12)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(176), thread="threadIdx.x"):
                                with T.block("lv739_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(7) * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1) // T.int64(512))
                                    v1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1) % T.int64(512) // T.int64(8))
                                    v2 = T.axis.spatial(T.int64(160), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1) % T.int64(8))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1 < T.int64(2048))
                                    T.reads(lv739[v0, v1, v2])
                                    T.writes(lv739_shared[v0, v1, v2])
                                    lv739_shared[v0, v1, v2] = lv739[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(176), thread="threadIdx.x"):
                                with T.block("lv746_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(7) * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1) // T.int64(88))
                                    v1 = T.axis.spatial(T.int64(160), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1) % T.int64(88) // T.int64(11))
                                    v2 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(7) * T.int64(11) + (ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1) % T.int64(11))
                                    T.reads(lv746[v0, v1, v2])
                                    T.writes(lv746_shared[v0, v1, v2])
                                    lv746_shared[v0, v1, v2] = lv746[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(8), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_4 + i0_0_i1_0_i2_0_fused // T.int64(7) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(88) * T.int64(2) + i0_3)
                                v_i1 = T.axis.spatial(T.int64(64), i0_2_i1_2_i2_2_fused % T.int64(88) // T.int64(11) * T.int64(8) + i1_3 * T.int64(8) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(77), i2_4 + i0_0_i1_0_i2_0_fused % T.int64(7) * T.int64(11) + i0_2_i1_2_i2_2_fused % T.int64(11) + i2_3)
                                v_k = T.axis.reduce(T.int64(160), k_0 * T.int64(8) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv739_shared[v_i0, v_i1, v_k], lv746_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv739_shared[v_i0, v_i1, v_k] * lv746_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(2), T.int64(8), T.int64(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(7) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(88) * T.int64(2) + ax0)
                            v1 = T.axis.spatial(T.int64(64), i0_2_i1_2_i2_2_fused % T.int64(88) // T.int64(11) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(7) * T.int64(11) + i0_2_i1_2_i2_2_fused % T.int64(11) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(T_multiply[v0, v1, v2])
                            T_multiply[v0, v1, v2] = matmul_local[v0, v1, v2] * T.float32(0.079056940972805023)

    @T.prim_func
    def fused_matmul38_add32_gelu3(lv758: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32"), param_0: T.Buffer((T.int64(1280), T.int64(5120)), "float32"), param_1: T.Buffer((T.int64(5120),), "float32"), T_multiply: T.Buffer((T.int64(2), T.int64(64), T.int64(5120)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul = T.alloc_buffer((T.int64(2), T.int64(64), T.int64(5120)))
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(64), T.int64(5120)), scope="local")
        lv758_shared = T.alloc_buffer((T.int64(2), T.int64(64), T.int64(1280)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(1280), T.int64(5120)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_4_init + i0_0_i1_0_i2_0_fused // T.int64(160) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int64(64), i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(5120), i2_4_init + i0_0_i1_0_i2_0_fused % T.int64(160) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(4) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(160)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("lv758_shared"):
                                    v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(160))
                                    v1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(8))
                                    v2 = T.axis.spatial(T.int64(1280), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(8))
                                    T.reads(lv758[v0, v1, v2])
                                    T.writes(lv758_shared[v0, v1, v2])
                                    lv758_shared[v0, v1, v2] = lv758[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(160) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(32))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(8), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_4 + i0_0_i1_0_i2_0_fused // T.int64(160) + i0_3)
                                v_i1 = T.axis.spatial(T.int64(64), i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(5120), i2_4 + i0_0_i1_0_i2_0_fused % T.int64(160) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(4) + i2_3)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(8) + k_1 * T.int64(8) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv758_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv758_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(2), T.int64(4)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(160) + ax0)
                            v1 = T.axis.spatial(T.int64(64), i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(160) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(4) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(10)):
                    with T.block("T_multiply_2"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(327680))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(327680) // T.int64(5120))
                        v_ax2 = T.axis.spatial(T.int64(5120), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(5120))
                        T.reads(matmul[v_ax0, v_ax1, v_ax2], param_1[v_ax2])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2])
                        T_multiply[v_ax0, v_ax1, v_ax2] = (matmul[v_ax0, v_ax1, v_ax2] + param_1[v_ax2]) * (T.float32(0.5) + T.erf((matmul[v_ax0, v_ax1, v_ax2] + param_1[v_ax2]) * T.float32(0.70710678118654757)) * T.float32(0.5))

    @T.prim_func
    def fused_matmul38_add32_multiply16(lv758: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32"), param_0: T.Buffer((T.int64(1280), T.int64(5120)), "float32"), param_1: T.Buffer((T.int64(5120),), "float32"), lv765: T.Buffer((T.int64(2), T.int64(64), T.int64(5120)), "float32"), T_multiply: T.Buffer((T.int64(2), T.int64(64), T.int64(5120)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(64), T.int64(5120)), scope="local")
        lv758_shared = T.alloc_buffer((T.int64(2), T.int64(64), T.int64(1280)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(1280), T.int64(5120)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_4_init + i0_1_i1_1_i2_1_fused + i0_3_init)
                            v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused // T.int64(160) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(8) + i1_3_init * T.int64(8) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(160) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(2) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(320)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("lv758_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused // T.int64(160) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(128) // T.int64(4))
                                    v2 = T.axis.spatial(T.int64(1280), k_0 * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(4))
                                    T.reads(lv758[v0, v1, v2])
                                    T.writes(lv758_shared[v0, v1, v2])
                                    lv758_shared[v0, v1, v2] = lv758[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(4) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(160) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(32))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(8), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_4 + i0_1_i1_1_i2_1_fused + i0_3)
                                v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused // T.int64(160) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(8) + i1_3 * T.int64(8) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(160) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(2) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(4) + k_1 * T.int64(2) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv758_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv758_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(8), T.int64(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_1_i1_1_i2_1_fused + ax0)
                            v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused // T.int64(160) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(160) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2], param_1[v2], lv765[v0, v1, v2])
                            T.writes(T_multiply[v0, v1, v2])
                            T_multiply[v0, v1, v2] = (matmul_local[v0, v1, v2] + param_1[v2]) * lv765[v0, v1, v2]

    @T.prim_func
    def fused_matmul39_add30_add31(lv766: T.Buffer((T.int64(2), T.int64(64), T.int64(5120)), "float32"), param_0: T.Buffer((T.int64(5120), T.int64(1280)), "float32"), param_1: T.Buffer((T.int64(1280),), "float32"), lv757: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(64), T.int64(1280)), scope="local")
        lv766_shared = T.alloc_buffer((T.int64(2), T.int64(64), T.int64(5120)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(5120), T.int64(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(40), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(8), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_3_init + i0_4_init + i0_2_i1_2_i2_2_fused // T.int64(64))
                            v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused // T.int64(10) * T.int64(16) + i1_3_init * T.int64(8) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(1280), i2_4_init + i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(128) + i0_2_i1_2_i2_2_fused % T.int64(64) * T.int64(2) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(640)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("lv766_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused // T.int64(10) * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) % T.int64(128) // T.int64(8))
                                    v2 = T.axis.spatial(T.int64(5120), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) % T.int64(8))
                                    T.reads(lv766[v0, v1, v2])
                                    T.writes(lv766_shared[v0, v1, v2])
                                    lv766_shared[v0, v1, v2] = lv766[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(5120), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(128) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) % T.int64(128))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(8), T.int64(1), T.int64(8), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_3 + i0_4 + i0_2_i1_2_i2_2_fused // T.int64(64))
                                v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused // T.int64(10) * T.int64(16) + i1_3 * T.int64(8) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(1280), i2_4 + i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(128) + i0_2_i1_2_i2_2_fused % T.int64(64) * T.int64(2) + i2_3)
                                v_k = T.axis.reduce(T.int64(5120), k_0 * T.int64(8) + k_1 * T.int64(8) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv766_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv766_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(16), T.int64(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused // T.int64(10) * T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(128) + i0_2_i1_2_i2_2_fused % T.int64(64) * T.int64(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2], param_1[v2], lv757[v0, v1, v2])
                            T.writes(T_add[v0, v1, v2])
                            T_add[v0, v1, v2] = matmul_local[v0, v1, v2] + param_1[v2] + lv757[v0, v1, v2]

    @T.prim_func
    def fused_matmul3_add3_multiply1_tir_sigmoid_multiply2(lv48: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), param_0: T.Buffer((T.int64(768), T.int64(3072)), "float32"), param_1: T.Buffer((T.int64(3072),), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(77), T.int64(3072)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(3072)))
        matmul_local = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(3072)), scope="local")
        lv48_shared = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(768)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(768), T.int64(3072)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(48), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(11), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_4_init + i0_3_init)
                            v_i1 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused // T.int64(32) * T.int64(11) + i1_3_init * T.int64(11) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(3072), i0_0_i1_0_i2_0_fused * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(32) * T.int64(2) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(24)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(11)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("lv48_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_fused_0 * T.int64(224) + ax0_ax1_ax2_fused_1) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(768), k_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(224) + ax0_ax1_ax2_fused_1) % T.int64(32))
                                    T.reads(lv48[v0, v1, v2])
                                    T.writes(lv48_shared[v0, v1, v2])
                                    lv48_shared[v0, v1, v2] = lv48[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(10)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(768), k_0 * T.int64(32) + (ax0_ax1_fused_0 * T.int64(224) + ax0_ax1_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(3072), i0_0_i1_0_i2_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(224) + ax0_ax1_fused_1) % T.int64(64))
                                    T.where(ax0_ax1_fused_0 * T.int64(224) + ax0_ax1_fused_1 < T.int64(2048))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(11), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_4 + i0_3)
                                v_i1 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused // T.int64(32) * T.int64(11) + i1_3 * T.int64(11) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(3072), i0_0_i1_0_i2_0_fused * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(32) * T.int64(2) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(768), k_0 * T.int64(32) + k_1 * T.int64(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv48_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv48_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(11), T.int64(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused // T.int64(32) * T.int64(11) + ax1)
                            v2 = T.axis.spatial(T.int64(3072), i0_0_i1_0_i2_0_fused * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(32) * T.int64(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(3696), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("T_multiply_1"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(3072))
                    v_ax2 = T.axis.spatial(T.int64(3072), (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(3072))
                    T.reads(matmul[v_ax0, v_ax1, v_ax2], param_1[v_ax2])
                    T.writes(T_multiply[v_ax0, v_ax1, v_ax2])
                    T_multiply[v_ax0, v_ax1, v_ax2] = (matmul[v_ax0, v_ax1, v_ax2] + param_1[v_ax2]) * T.sigmoid(T.float32(1.7020000219345093) * (matmul[v_ax0, v_ax1, v_ax2] + param_1[v_ax2]))

    @T.prim_func
    def fused_matmul40_add40(lv20: T.Buffer((T.int64(1), T.int64(4096), T.int64(512)), "float32"), param_0: T.Buffer((T.int64(512), T.int64(512)), "float32"), param_1: T.Buffer((T.int64(512),), "float32"), T_add: T.Buffer((T.int64(1), T.int64(4096), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(1), T.int64(4096), T.int64(512)), scope="local")
        lv20_shared = T.alloc_buffer((T.int64(1), T.int64(4096), T.int64(512)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(512), T.int64(512)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_4_init + i0_3_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(4) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(512), i2_4_init + i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(4) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("lv20_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(64) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(512), k_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) % T.int64(32))
                                    T.reads(lv20[v0, v1, v2])
                                    T.writes(lv20_shared[v0, v1, v2])
                                    lv20_shared[v0, v1, v2] = lv20[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(512), k_0 * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(512), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) % T.int64(32))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(8), T.int64(1), T.int64(2), T.int64(4), T.int64(4), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_4 + i0_3)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(4) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(512), i2_4 + i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(4) + i2_3)
                                v_k = T.axis.reduce(T.int64(512), k_0 * T.int64(32) + k_1 * T.int64(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv20_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv20_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(512), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(4) + ax2)
                            T.reads(matmul_local[v0, v1, v2], param_1[v2])
                            T.writes(T_add[v0, v1, v2])
                            T_add[v0, v1, v2] = matmul_local[v0, v1, v2] + param_1[v2]

    @T.prim_func
    def fused_matmul41_multiply19_cast3(lv32: T.Buffer((T.int64(1), T.int64(4096), T.int64(512)), "float32"), lv39: T.Buffer((T.int64(1), T.int64(512), T.int64(4096)), "float32"), compute: T.Buffer((T.int64(1), T.int64(4096), T.int64(4096)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(1), T.int64(4096), T.int64(4096)), scope="local")
        lv32_shared = T.alloc_buffer((T.int64(1), T.int64(4096), T.int64(512)), scope="shared")
        lv39_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(4096)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(4096), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_4_init + i0_3_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(64) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(16) + i1_3_init * T.int64(16) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(4096), i2_4_init + i0_0_i1_0_i2_0_fused % T.int64(64) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(128)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("lv32_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(64) * T.int64(64) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(4))
                                    v2 = T.axis.spatial(T.int64(512), k_0 * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(4))
                                    T.reads(lv32[v0, v1, v2])
                                    T.writes(lv32_shared[v0, v1, v2])
                                    lv32_shared[v0, v1, v2] = lv32[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("lv39_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(64))
                                    v2 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(64) * T.int64(64) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(64))
                                    T.reads(lv39[v0, v1, v2])
                                    T.writes(lv39_shared[v0, v1, v2])
                                    lv39_shared[v0, v1, v2] = lv39[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_4 + i0_3)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(64) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(16) + i1_3 * T.int64(16) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(4096), i2_4 + i0_0_i1_0_i2_0_fused % T.int64(64) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) + i2_3)
                                v_k = T.axis.reduce(T.int64(512), k_0 * T.int64(4) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv32_shared[T.int64(0), v_i1, v_k], lv39_shared[T.int64(0), v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv32_shared[T.int64(0), v_i1, v_k] * lv39_shared[T.int64(0), v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(16), T.int64(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(64) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(64) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(compute[v0, v1, v2])
                            compute[v0, v1, v2] = matmul_local[v0, v1, v2] * T.float32(0.044194173067808151)

    @T.prim_func
    def fused_matmul4_add1_add(lv54: T.Buffer((T.int64(1), T.int64(77), T.int64(3072)), "float32"), param_0: T.Buffer((T.int64(3072), T.int64(768)), "float32"), param_1: T.Buffer((T.int64(768),), "float32"), lv47: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(768)), scope="local")
        lv54_shared = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(3072)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(3072), T.int64(768)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(48), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(88), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(7), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_4_init + i0_3_init)
                            v_i1 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(7) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(768), i2_4_init + i0_0_i1_0_i2_0_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(192)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(14)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(88), thread="threadIdx.x"):
                                with T.block("lv54_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_fused_0 * T.int64(88) + ax0_ax1_ax2_fused_1) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(3072), k_0 * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(88) + ax0_ax1_ax2_fused_1) % T.int64(16))
                                    T.reads(lv54[v0, v1, v2])
                                    T.writes(lv54_shared[v0, v1, v2])
                                    lv54_shared[v0, v1, v2] = lv54[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(3)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(88), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(3072), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(88) + ax0_ax1_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused * T.int64(16) + (ax0_ax1_fused_0 * T.int64(88) + ax0_ax1_fused_1) % T.int64(16))
                                    T.where(ax0_ax1_fused_0 * T.int64(88) + ax0_ax1_fused_1 < T.int64(256))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(1), T.int64(7), T.int64(2), T.int64(16), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_4 + i0_3)
                                v_i1 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(7) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(768), i2_4 + i0_0_i1_0_i2_0_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + i2_3)
                                v_k = T.axis.reduce(T.int64(3072), k_0 * T.int64(16) + k_1 * T.int64(16) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv54_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv54_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(7), T.int64(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(7) + ax1)
                            v2 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + ax2)
                            T.reads(lv47[v0, v1, v2], matmul_local[v0, v1, v2], param_1[v2])
                            T.writes(T_add[v0, v1, v2])
                            T_add[v0, v1, v2] = lv47[v0, v1, v2] + (matmul_local[v0, v1, v2] + param_1[v2])

    @T.prim_func
    def fused_matmul5_add4_silu(lv13: T.Buffer((T.int64(2), T.int64(320)), "float32"), param_0: T.Buffer((T.int64(320), T.int64(1280)), "float32"), param_1: T.Buffer((T.int64(1280),), "float32"), T_multiply: T.Buffer((T.int64(2), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul = T.alloc_buffer((T.int64(2), T.int64(1280)))
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(1280)), scope="local")
        lv13_shared = T.alloc_buffer((T.int64(2), T.int64(320)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(320), T.int64(1280)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_3_init + i0_4_init + i0_0_i1_0_fused // T.int64(10))
                            v_i1 = T.axis.spatial(T.int64(1280), i1_4_init + i0_0_i1_0_fused % T.int64(10) * T.int64(128) + i0_2_i1_2_fused + i1_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("lv13_shared"):
                                    v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_fused // T.int64(10))
                                    v1 = T.axis.spatial(T.int64(320), k_0 * T.int64(10) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 < T.int64(10))
                                    T.reads(lv13[v0, v1])
                                    T.writes(lv13_shared[v0, v1])
                                    lv13_shared[v0, v1] = lv13[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(10)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(320), k_0 * T.int64(10) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_fused % T.int64(10) * T.int64(128) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) % T.int64(128))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(10), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_3 + i0_4 + i0_0_i1_0_fused // T.int64(10))
                                v_i1 = T.axis.spatial(T.int64(1280), i1_4 + i0_0_i1_0_fused % T.int64(10) * T.int64(128) + i0_2_i1_2_fused + i1_3)
                                v_k = T.axis.reduce(T.int64(320), k_0 * T.int64(10) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1], lv13_shared[v_i0, v_k], param_0_shared[v_k, v_i1])
                                T.writes(matmul_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1] = matmul_local[v_i0, v_i1] + lv13_shared[v_i0, v_k] * param_0_shared[v_k, v_i1]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_fused // T.int64(10) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_fused % T.int64(10) * T.int64(128) + i0_2_i1_2_fused + ax1)
                            T.reads(matmul_local[v0, v1])
                            T.writes(matmul[v0, v1])
                            matmul[v0, v1] = matmul_local[v0, v1]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(10), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1) // T.int64(1280))
                    v_ax1 = T.axis.spatial(T.int64(1280), (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1) % T.int64(1280))
                    T.reads(matmul[v_ax0, v_ax1], param_1[v_ax1])
                    T.writes(T_multiply[v_ax0, v_ax1])
                    T_multiply[v_ax0, v_ax1] = (matmul[v_ax0, v_ax1] + param_1[v_ax1]) * T.sigmoid(matmul[v_ax0, v_ax1] + param_1[v_ax1])

    @T.prim_func
    def fused_matmul6_add4(lv17: T.Buffer((T.int64(2), T.int64(1280)), "float32"), param_0: T.Buffer((T.int64(1280), T.int64(1280)), "float32"), param_1: T.Buffer((T.int64(1280),), "float32"), T_add: T.Buffer((T.int64(2), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(1280)), scope="local")
        lv17_shared = T.alloc_buffer((T.int64(2), T.int64(1280)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(1280), T.int64(1280)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_3_init + i0_4_init + i0_0_i1_0_fused // T.int64(10))
                            v_i1 = T.axis.spatial(T.int64(1280), i1_4_init + i0_0_i1_0_fused % T.int64(10) * T.int64(128) + i0_2_i1_2_fused + i1_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(128)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("lv17_shared"):
                                    v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_fused // T.int64(10))
                                    v1 = T.axis.spatial(T.int64(1280), k_0 * T.int64(10) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 < T.int64(10))
                                    T.reads(lv17[v0, v1])
                                    T.writes(lv17_shared[v0, v1])
                                    lv17_shared[v0, v1] = lv17[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(10)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(10) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_fused % T.int64(10) * T.int64(128) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) % T.int64(128))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(10), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_3 + i0_4 + i0_0_i1_0_fused // T.int64(10))
                                v_i1 = T.axis.spatial(T.int64(1280), i1_4 + i0_0_i1_0_fused % T.int64(10) * T.int64(128) + i0_2_i1_2_fused + i1_3)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(10) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1], lv17_shared[v_i0, v_k], param_0_shared[v_k, v_i1])
                                T.writes(matmul_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1] = matmul_local[v_i0, v_i1] + lv17_shared[v_i0, v_k] * param_0_shared[v_k, v_i1]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_fused // T.int64(10) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_fused % T.int64(10) * T.int64(128) + i0_2_i1_2_fused + ax1)
                            T.reads(matmul_local[v0, v1], param_1[v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = matmul_local[v0, v1] + param_1[v1]

    @T.prim_func
    def fused_matmul6_add4_strided_slice5(lv438: T.Buffer((T.int64(2), T.int64(1280)), "float32"), param_0: T.Buffer((T.int64(1280), T.int64(1280)), "float32"), param_1: T.Buffer((T.int64(1280),), "float32"), T_strided_slice_with_axes: T.Buffer((T.int64(2), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(1280)), scope="local")
        lv438_shared = T.alloc_buffer((T.int64(2), T.int64(1280)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(1280), T.int64(1280)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(40), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_fused // T.int64(32) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1280), i1_4_init + i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused % T.int64(32) + i1_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(20)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("lv438_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(1280), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(64))
                                    T.reads(lv438[v0, v1])
                                    T.writes(lv438_shared[v0, v1])
                                    lv438_shared[v0, v1] = lv438[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(32))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_fused // T.int64(32) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1280), i1_4 + i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused % T.int64(32) + i1_3)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(64) + k_1 * T.int64(16) + k_2)
                                T.reads(matmul_local[v_i0, v_i1], lv438_shared[v_i0, v_k], param_0_shared[v_k, v_i1])
                                T.writes(matmul_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1] = matmul_local[v_i0, v_i1] + lv438_shared[v_i0, v_k] * param_0_shared[v_k, v_i1]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_2_i1_2_fused // T.int64(32) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused % T.int64(32) + ax1)
                            T.reads(matmul_local[v0, v1], param_1[v1])
                            T.writes(T_strided_slice_with_axes[v0, v1])
                            T_strided_slice_with_axes[v0, v1] = matmul_local[v0, v1] + param_1[v1]

    @T.prim_func
    def fused_matmul7_add6_strided_slice3(lv29: T.Buffer((T.int64(2), T.int64(1280)), "float32"), param_0: T.Buffer((T.int64(1280), T.int64(320)), "float32"), param_1: T.Buffer((T.int64(320),), "float32"), T_strided_slice_with_axes: T.Buffer((T.int64(2), T.int64(320)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(320)), scope="local")
        lv29_shared = T.alloc_buffer((T.int64(2), T.int64(1280)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(1280), T.int64(320)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(10), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_fused // T.int64(32) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(320), i1_4_init + i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused % T.int64(32) + i1_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(10)):
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("lv29_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(1280), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(128))
                                    T.reads(lv29[v0, v1])
                                    T.writes(lv29_shared[v0, v1])
                                    lv29_shared[v0, v1] = lv29[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(320), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(32))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_fused // T.int64(32) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(320), i1_4 + i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused % T.int64(32) + i1_3)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(128) + k_1 * T.int64(2) + k_2)
                                T.reads(matmul_local[v_i0, v_i1], lv29_shared[v_i0, v_k], param_0_shared[v_k, v_i1])
                                T.writes(matmul_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1] = matmul_local[v_i0, v_i1] + lv29_shared[v_i0, v_k] * param_0_shared[v_k, v_i1]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_2_i1_2_fused // T.int64(32) + ax0)
                            v1 = T.axis.spatial(T.int64(320), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused % T.int64(32) + ax1)
                            T.reads(matmul_local[v0, v1], param_1[v1])
                            T.writes(T_strided_slice_with_axes[v0, v1])
                            T_strided_slice_with_axes[v0, v1] = matmul_local[v0, v1] + param_1[v1]

    @T.prim_func
    def fused_matmul8_add9_add10(lv72: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32"), param_0: T.Buffer((T.int64(320), T.int64(320)), "float32"), param_1: T.Buffer((T.int64(320),), "float32"), lv48: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32"), T_add: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(4096), T.int64(320)), scope="local")
        lv72_shared = T.alloc_buffer((T.int64(2), T.int64(4096), T.int64(320)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(320), T.int64(320)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(1280), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_4_init + i0_3_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(4) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(64) + i0_2_i1_2_i2_2_fused * T.int64(2) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(80)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("lv72_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) % T.int64(64) // T.int64(4))
                                    v2 = T.axis.spatial(T.int64(320), k_0 * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) % T.int64(4))
                                    T.reads(lv72[v0, v1, v2])
                                    T.writes(lv72_shared[v0, v1, v2])
                                    lv72_shared[v0, v1, v2] = lv72[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(320), k_0 * T.int64(4) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(64))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_4 + i0_3)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(4) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(64) + i0_2_i1_2_i2_2_fused * T.int64(2) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(320), k_0 * T.int64(4) + k_1 * T.int64(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv72_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv72_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(2), T.int64(4), T.int64(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(64) + i0_2_i1_2_i2_2_fused * T.int64(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2], param_1[v2], lv48[v0, v1, v2])
                            T.writes(T_add[v0, v1, v2])
                            T_add[v0, v1, v2] = matmul_local[v0, v1, v2] + param_1[v2] + lv48[v0, v1, v2]

    @T.prim_func
    def fused_matmul9_multiply5(lv58: T.Buffer((T.int64(16), T.int64(4096), T.int64(40)), "float32"), lv65: T.Buffer((T.int64(16), T.int64(40), T.int64(4096)), "float32"), T_multiply: T.Buffer((T.int64(16), T.int64(4096), T.int64(4096)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(16), T.int64(4096), T.int64(4096)), scope="local")
        lv58_shared = T.alloc_buffer((T.int64(16), T.int64(4096), T.int64(40)), scope="shared")
        lv65_shared = T.alloc_buffer((T.int64(16), T.int64(40), T.int64(4096)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(131072), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(32), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_4_init + i0_0_i1_0_i2_0_fused // T.int64(8192) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(8192) // T.int64(64) * T.int64(32) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(4096), i2_4_init + i0_0_i1_0_i2_0_fused % T.int64(64) * T.int64(64) + i0_2_i1_2_i2_2_fused * T.int64(2) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(10)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("lv58_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(8192))
                                    v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(8192) // T.int64(64) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) // T.int64(4))
                                    v2 = T.axis.spatial(T.int64(40), k_0 * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) % T.int64(4))
                                    T.reads(lv58[v0, v1, v2])
                                    T.writes(lv58_shared[v0, v1, v2])
                                    lv58_shared[v0, v1, v2] = lv58[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("lv65_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(8192))
                                    v1 = T.axis.spatial(T.int64(40), k_0 * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) // T.int64(64))
                                    v2 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(64) * T.int64(64) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) % T.int64(64))
                                    T.reads(lv65[v0, v1, v2])
                                    T.writes(lv65_shared[v0, v1, v2])
                                    lv65_shared[v0, v1, v2] = lv65[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(1), T.int64(32), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_4 + i0_0_i1_0_i2_0_fused // T.int64(8192) + i0_3)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(8192) // T.int64(64) * T.int64(32) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(4096), i2_4 + i0_0_i1_0_i2_0_fused % T.int64(64) * T.int64(64) + i0_2_i1_2_i2_2_fused * T.int64(2) + i2_3)
                                v_k = T.axis.reduce(T.int64(40), k_0 * T.int64(4) + k_1 * T.int64(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv58_shared[v_i0, v_i1, v_k], lv65_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv58_shared[v_i0, v_i1, v_k] * lv65_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(32), T.int64(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(8192) + ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(8192) // T.int64(64) * T.int64(32) + ax1)
                            v2 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(64) * T.int64(64) + i0_2_i1_2_i2_2_fused * T.int64(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(T_multiply[v0, v1, v2])
                            T_multiply[v0, v1, v2] = matmul_local[v0, v1, v2] * T.float32(0.15811388194561005)

    @T.prim_func
    def fused_matmul_add1(lv14: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), param_0: T.Buffer((T.int64(768), T.int64(768)), "float32"), param_1: T.Buffer((T.int64(768),), "float32"), T_add: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(768)), scope="local")
        lv14_shared = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(768)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(768), T.int64(768)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(48), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(88), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(7), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_4_init + i0_3_init)
                            v_i1 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(7) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(768), i2_4_init + i0_0_i1_0_i2_0_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(48)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(14)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(88), thread="threadIdx.x"):
                                with T.block("lv14_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_fused_0 * T.int64(88) + ax0_ax1_ax2_fused_1) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(768), k_0 * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(88) + ax0_ax1_ax2_fused_1) % T.int64(16))
                                    T.reads(lv14[v0, v1, v2])
                                    T.writes(lv14_shared[v0, v1, v2])
                                    lv14_shared[v0, v1, v2] = lv14[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(3)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(88), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(768), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(88) + ax0_ax1_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused * T.int64(16) + (ax0_ax1_fused_0 * T.int64(88) + ax0_ax1_fused_1) % T.int64(16))
                                    T.where(ax0_ax1_fused_0 * T.int64(88) + ax0_ax1_fused_1 < T.int64(256))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(1), T.int64(7), T.int64(2), T.int64(16), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_4 + i0_3)
                                v_i1 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(7) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(768), i2_4 + i0_0_i1_0_i2_0_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + i2_3)
                                v_k = T.axis.reduce(T.int64(768), k_0 * T.int64(16) + k_1 * T.int64(16) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv14_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv14_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(7), T.int64(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(7) + ax1)
                            v2 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2], param_1[v2])
                            T.writes(T_add[v0, v1, v2])
                            T_add[v0, v1, v2] = matmul_local[v0, v1, v2] + param_1[v2]

    @T.prim_func
    def fused_matmul_add1_add(lv43: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), param_0: T.Buffer((T.int64(768), T.int64(768)), "float32"), param_1: T.Buffer((T.int64(768),), "float32"), lv9: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(768)), scope="local")
        lv43_shared = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(768)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(768), T.int64(768)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(48), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(88), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(7), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_4_init + i0_3_init)
                            v_i1 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(7) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(768), i2_4_init + i0_0_i1_0_i2_0_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(48)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(14)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(88), thread="threadIdx.x"):
                                with T.block("lv43_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_fused_0 * T.int64(88) + ax0_ax1_ax2_fused_1) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(768), k_0 * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(88) + ax0_ax1_ax2_fused_1) % T.int64(16))
                                    T.reads(lv43[v0, v1, v2])
                                    T.writes(lv43_shared[v0, v1, v2])
                                    lv43_shared[v0, v1, v2] = lv43[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(3)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(88), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(768), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(88) + ax0_ax1_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused * T.int64(16) + (ax0_ax1_fused_0 * T.int64(88) + ax0_ax1_fused_1) % T.int64(16))
                                    T.where(ax0_ax1_fused_0 * T.int64(88) + ax0_ax1_fused_1 < T.int64(256))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(1), T.int64(7), T.int64(2), T.int64(16), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_4 + i0_3)
                                v_i1 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(7) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(768), i2_4 + i0_0_i1_0_i2_0_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + i2_3)
                                v_k = T.axis.reduce(T.int64(768), k_0 * T.int64(16) + k_1 * T.int64(16) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv43_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv43_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(7), T.int64(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(7) + ax1)
                            v2 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + ax2)
                            T.reads(lv9[v0, v1, v2], matmul_local[v0, v1, v2], param_1[v2])
                            T.writes(T_add[v0, v1, v2])
                            T_add[v0, v1, v2] = lv9[v0, v1, v2] + (matmul_local[v0, v1, v2] + param_1[v2])

    @T.prim_func
    def fused_matmul_add1_multiply(lv14: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), param_0: T.Buffer((T.int64(768), T.int64(768)), "float32"), param_1: T.Buffer((T.int64(768),), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(768)), scope="local")
        lv14_shared = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(768)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int64(768), T.int64(768)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(48), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(88), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(7), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_4_init + i0_3_init)
                            v_i1 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(7) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(768), i2_4_init + i0_0_i1_0_i2_0_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(48)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(14)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(88), thread="threadIdx.x"):
                                with T.block("lv14_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_fused_0 * T.int64(88) + ax0_ax1_ax2_fused_1) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(768), k_0 * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(88) + ax0_ax1_ax2_fused_1) % T.int64(16))
                                    T.reads(lv14[v0, v1, v2])
                                    T.writes(lv14_shared[v0, v1, v2])
                                    lv14_shared[v0, v1, v2] = lv14[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(3)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(88), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int64(768), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(88) + ax0_ax1_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused * T.int64(16) + (ax0_ax1_fused_0 * T.int64(88) + ax0_ax1_fused_1) % T.int64(16))
                                    T.where(ax0_ax1_fused_0 * T.int64(88) + ax0_ax1_fused_1 < T.int64(256))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(1), T.int64(7), T.int64(2), T.int64(16), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_4 + i0_3)
                                v_i1 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(7) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(768), i2_4 + i0_0_i1_0_i2_0_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + i2_3)
                                v_k = T.axis.reduce(T.int64(768), k_0 * T.int64(16) + k_1 * T.int64(16) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv14_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv14_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(7), T.int64(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(7) + ax1)
                            v2 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2], param_1[v2])
                            T.writes(T_multiply[v0, v1, v2])
                            T_multiply[v0, v1, v2] = (matmul_local[v0, v1, v2] + param_1[v2]) * T.float32(0.125)

    @T.prim_func
    def fused_multiply5_multiply5_divide_subtract(inp_2: T.Buffer((), "float32"), inp_1: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), inp_3: T.Buffer((), "float32"), inp_0: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), inp_4: T.Buffer((), "float32"), T_subtract: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("T_subtract"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(inp_2[()], inp_1[v_ax0, v_ax1, v_ax2, v_ax3], inp_3[()], inp_0[v_ax0, v_ax1, v_ax2, v_ax3], inp_4[()])
                    T.writes(T_subtract[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_subtract[v_ax0, v_ax1, v_ax2, v_ax3] = inp_2[()] * inp_1[v_ax0, v_ax1, v_ax2, v_ax3] - inp_3[()] * inp_0[v_ax0, v_ax1, v_ax2, v_ax3] / inp_4[()]

    @T.prim_func
    def fused_multiply6_subtract_divide1_multiply5_multiply5_divide_subtract(inp_5: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), inp_4: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), inp_1: T.Buffer((), "float32"), inp_0: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), inp_2: T.Buffer((), "float32"), inp_3: T.Buffer((), "float32"), T_subtract: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("T_subtract_1"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(inp_1[()], inp_0[v_ax0, v_ax1, v_ax2, v_ax3], inp_2[()], inp_5[v_ax0, v_ax1, v_ax2, v_ax3], inp_4[v_ax0, v_ax1, v_ax2, v_ax3], inp_3[()])
                    T.writes(T_subtract[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_subtract[v_ax0, v_ax1, v_ax2, v_ax3] = inp_1[()] * inp_0[v_ax0, v_ax1, v_ax2, v_ax3] - inp_2[()] * ((T.float32(3) * inp_5[v_ax0, v_ax1, v_ax2, v_ax3] - inp_4[v_ax0, v_ax1, v_ax2, v_ax3]) * T.float32(0.5)) / inp_3[()]

    @T.prim_func
    def fused_multiply7_multiply8_subtract_multiply9_add_divide2_multiply5_multiply5_divide_subtract(inp_6: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), inp_5: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), inp_4: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), inp_1: T.Buffer((), "float32"), inp_0: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), inp_2: T.Buffer((), "float32"), inp_3: T.Buffer((), "float32"), T_subtract: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("T_subtract_1"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(inp_1[()], inp_0[v_ax0, v_ax1, v_ax2, v_ax3], inp_2[()], inp_6[v_ax0, v_ax1, v_ax2, v_ax3], inp_5[v_ax0, v_ax1, v_ax2, v_ax3], inp_4[v_ax0, v_ax1, v_ax2, v_ax3], inp_3[()])
                    T.writes(T_subtract[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_subtract[v_ax0, v_ax1, v_ax2, v_ax3] = inp_1[()] * inp_0[v_ax0, v_ax1, v_ax2, v_ax3] - inp_2[()] * ((T.float32(23) * inp_6[v_ax0, v_ax1, v_ax2, v_ax3] - T.float32(16) * inp_5[v_ax0, v_ax1, v_ax2, v_ax3] + T.float32(5) * inp_4[v_ax0, v_ax1, v_ax2, v_ax3]) * T.float32(0.083333333333333329)) / inp_3[()]

    @T.prim_func
    def fused_multiply_multiply1_subtract_multiply2_add_multiply3_subtract_multiply4_multiply5_multiply5_divide_subtract(inp_7: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), inp_6: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), inp_5: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), inp_4: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), inp_1: T.Buffer((), "float32"), inp_0: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), inp_2: T.Buffer((), "float32"), inp_3: T.Buffer((), "float32"), T_subtract: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("T_subtract_2"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(inp_1[()], inp_0[v_ax0, v_ax1, v_ax2, v_ax3], inp_2[()], inp_7[v_ax0, v_ax1, v_ax2, v_ax3], inp_6[v_ax0, v_ax1, v_ax2, v_ax3], inp_5[v_ax0, v_ax1, v_ax2, v_ax3], inp_4[v_ax0, v_ax1, v_ax2, v_ax3], inp_3[()])
                    T.writes(T_subtract[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_subtract[v_ax0, v_ax1, v_ax2, v_ax3] = inp_1[()] * inp_0[v_ax0, v_ax1, v_ax2, v_ax3] - inp_2[()] * (T.float32(0.041666667908430099) * (T.float32(55) * inp_7[v_ax0, v_ax1, v_ax2, v_ax3] - T.float32(59) * inp_6[v_ax0, v_ax1, v_ax2, v_ax3] + T.float32(37) * inp_5[v_ax0, v_ax1, v_ax2, v_ax3] - T.float32(9) * inp_4[v_ax0, v_ax1, v_ax2, v_ax3])) / inp_3[()]

    @T.prim_func
    def fused_reshape14_transpose11_reshape15(lv51: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32"), T_reshape: T.Buffer((T.int64(16), T.int64(4096), T.int64(40)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(40)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1310720))
                        v_ax1 = T.axis.spatial(T.int64(4096), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1310720) // T.int64(320))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(320) // T.int64(40))
                        v_ax3 = T.axis.spatial(T.int64(40), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(40))
                        T.reads(lv51[(((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) // T.int64(4096) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) % T.int64(4096), (v_ax2 * T.int64(40) + v_ax3) % T.int64(320)])
                        T.writes(T_reshape[v_ax0 * T.int64(8) + v_ax2, v_ax1, v_ax3])
                        T_reshape[v_ax2 + v_ax0 * T.int64(8), v_ax1, v_ax3] = lv51[(((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) // T.int64(4096) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) % T.int64(4096), (v_ax2 * T.int64(40) + v_ax3) % T.int64(320)]

    @T.prim_func
    def fused_reshape14_transpose11_reshape15_transpose12(lv53: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32"), T_transpose: T.Buffer((T.int64(16), T.int64(40), T.int64(4096)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(40)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1310720))
                        v_ax1 = T.axis.spatial(T.int64(4096), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1310720) // T.int64(320))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(320) // T.int64(40))
                        v_ax3 = T.axis.spatial(T.int64(40), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(40))
                        T.reads(lv53[(((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) // T.int64(4096) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) % T.int64(4096), (v_ax2 * T.int64(40) + v_ax3) % T.int64(320)])
                        T.writes(T_transpose[v_ax0 * T.int64(8) + v_ax2, v_ax3, v_ax1])
                        T_transpose[v_ax2 + v_ax0 * T.int64(8), v_ax3, v_ax1] = lv53[(((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) // T.int64(4096) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) % T.int64(4096), (v_ax2 * T.int64(40) + v_ax3) % T.int64(320)]

    @T.prim_func
    def fused_reshape16_transpose13_reshape17(lv69: T.Buffer((T.int64(16), T.int64(4096), T.int64(40)), "float32"), T_reshape: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(40)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1310720))
                        v_ax1 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1310720) // T.int64(163840))
                        v_ax2 = T.axis.spatial(T.int64(4096), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(163840) // T.int64(40))
                        v_ax3 = T.axis.spatial(T.int64(40), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(40))
                        T.reads(lv69[(v_ax0 * T.int64(8) + (v_ax3 // T.int64(40) + v_ax2) // T.int64(4096) + v_ax1) % T.int64(16), (v_ax3 // T.int64(40) + v_ax2) % T.int64(4096), v_ax3 % T.int64(40)])
                        T.writes(T_reshape[v_ax0, v_ax2, v_ax1 * T.int64(40) + v_ax3])
                        T_reshape[v_ax0, v_ax2, v_ax3 + v_ax1 * T.int64(40)] = lv69[(v_ax0 * T.int64(8) + (v_ax3 // T.int64(40) + v_ax2) // T.int64(4096) + v_ax1) % T.int64(16), (v_ax3 // T.int64(40) + v_ax2) % T.int64(4096), v_ax3 % T.int64(40)]

    @T.prim_func
    def fused_reshape18_transpose15_reshape19(lv83: T.Buffer((T.int64(2), T.int64(77), T.int64(320)), "float32"), T_reshape: T.Buffer((T.int64(16), T.int64(77), T.int64(40)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(770), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(24640))
                    v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(24640) // T.int64(320))
                    v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(320) // T.int64(40))
                    v_ax3 = T.axis.spatial(T.int64(40), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(40))
                    T.reads(lv83[(((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) % T.int64(77), (v_ax2 * T.int64(40) + v_ax3) % T.int64(320)])
                    T.writes(T_reshape[v_ax0 * T.int64(8) + v_ax2, v_ax1, v_ax3])
                    T_reshape[v_ax2 + v_ax0 * T.int64(8), v_ax1, v_ax3] = lv83[(((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) % T.int64(77), (v_ax2 * T.int64(40) + v_ax3) % T.int64(320)]

    @T.prim_func
    def fused_reshape18_transpose15_reshape19_transpose16(lv81: T.Buffer((T.int64(2), T.int64(77), T.int64(320)), "float32"), T_transpose: T.Buffer((T.int64(16), T.int64(40), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(770), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(24640))
                    v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(24640) // T.int64(320))
                    v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(320) // T.int64(40))
                    v_ax3 = T.axis.spatial(T.int64(40), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(40))
                    T.reads(lv81[(((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) % T.int64(77), (v_ax2 * T.int64(40) + v_ax3) % T.int64(320)])
                    T.writes(T_transpose[v_ax0 * T.int64(8) + v_ax2, v_ax3, v_ax1])
                    T_transpose[v_ax2 + v_ax0 * T.int64(8), v_ax3, v_ax1] = lv81[(((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) % T.int64(77), (v_ax2 * T.int64(40) + v_ax3) % T.int64(320)]

    @T.prim_func
    def fused_reshape20_transpose17(lv117: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32"), T_transpose: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(40)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1310720))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1310720) // T.int64(20480))
                        v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(20480) // T.int64(320))
                        v_ax3 = T.axis.spatial(T.int64(320), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(320))
                        T.reads(lv117[((v_ax1 * T.int64(64) + v_ax3 // T.int64(320) + v_ax2) // T.int64(4096) + v_ax0) % T.int64(2), (v_ax1 * T.int64(64) + v_ax3 // T.int64(320) + v_ax2) % T.int64(4096), v_ax3 % T.int64(320)])
                        T.writes(T_transpose[v_ax0, v_ax3, v_ax1, v_ax2])
                        T_transpose[v_ax0, v_ax3, v_ax1, v_ax2] = lv117[((v_ax1 * T.int64(64) + v_ax3 // T.int64(320) + v_ax2) // T.int64(4096) + v_ax0) % T.int64(2), (v_ax1 * T.int64(64) + v_ax3 // T.int64(320) + v_ax2) % T.int64(4096), v_ax3 % T.int64(320)]

    @T.prim_func
    def fused_reshape24_transpose21_reshape25(lv257: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32"), T_reshape: T.Buffer((T.int64(16), T.int64(1024), T.int64(80)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(20)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(655360))
                        v_ax1 = T.axis.spatial(T.int64(1024), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(655360) // T.int64(640))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(640) // T.int64(80))
                        v_ax3 = T.axis.spatial(T.int64(80), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(80))
                        T.reads(lv257[(((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) // T.int64(1024) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) % T.int64(1024), (v_ax2 * T.int64(80) + v_ax3) % T.int64(640)])
                        T.writes(T_reshape[v_ax0 * T.int64(8) + v_ax2, v_ax1, v_ax3])
                        T_reshape[v_ax2 + v_ax0 * T.int64(8), v_ax1, v_ax3] = lv257[(((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) // T.int64(1024) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) % T.int64(1024), (v_ax2 * T.int64(80) + v_ax3) % T.int64(640)]

    @T.prim_func
    def fused_reshape24_transpose21_reshape25_transpose22(lv259: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32"), T_transpose: T.Buffer((T.int64(16), T.int64(80), T.int64(1024)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(20)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(655360))
                        v_ax1 = T.axis.spatial(T.int64(1024), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(655360) // T.int64(640))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(640) // T.int64(80))
                        v_ax3 = T.axis.spatial(T.int64(80), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(80))
                        T.reads(lv259[(((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) // T.int64(1024) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) % T.int64(1024), (v_ax2 * T.int64(80) + v_ax3) % T.int64(640)])
                        T.writes(T_transpose[v_ax0 * T.int64(8) + v_ax2, v_ax3, v_ax1])
                        T_transpose[v_ax2 + v_ax0 * T.int64(8), v_ax3, v_ax1] = lv259[(((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) // T.int64(1024) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) % T.int64(1024), (v_ax2 * T.int64(80) + v_ax3) % T.int64(640)]

    @T.prim_func
    def fused_reshape26_transpose23_reshape27(lv275: T.Buffer((T.int64(16), T.int64(1024), T.int64(80)), "float32"), T_reshape: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(20)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(655360))
                        v_ax1 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(655360) // T.int64(81920))
                        v_ax2 = T.axis.spatial(T.int64(1024), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(81920) // T.int64(80))
                        v_ax3 = T.axis.spatial(T.int64(80), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(80))
                        T.reads(lv275[(v_ax0 * T.int64(8) + (v_ax3 // T.int64(80) + v_ax2) // T.int64(1024) + v_ax1) % T.int64(16), (v_ax3 // T.int64(80) + v_ax2) % T.int64(1024), v_ax3 % T.int64(80)])
                        T.writes(T_reshape[v_ax0, v_ax2, v_ax1 * T.int64(80) + v_ax3])
                        T_reshape[v_ax0, v_ax2, v_ax3 + v_ax1 * T.int64(80)] = lv275[(v_ax0 * T.int64(8) + (v_ax3 // T.int64(80) + v_ax2) // T.int64(1024) + v_ax1) % T.int64(16), (v_ax3 // T.int64(80) + v_ax2) % T.int64(1024), v_ax3 % T.int64(80)]

    @T.prim_func
    def fused_reshape28_transpose25_reshape29(lv289: T.Buffer((T.int64(2), T.int64(77), T.int64(640)), "float32"), T_reshape: T.Buffer((T.int64(16), T.int64(77), T.int64(80)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(3080), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(49280))
                    v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(49280) // T.int64(640))
                    v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(640) // T.int64(80))
                    v_ax3 = T.axis.spatial(T.int64(80), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(80))
                    T.reads(lv289[(((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) % T.int64(77), (v_ax2 * T.int64(80) + v_ax3) % T.int64(640)])
                    T.writes(T_reshape[v_ax0 * T.int64(8) + v_ax2, v_ax1, v_ax3])
                    T_reshape[v_ax2 + v_ax0 * T.int64(8), v_ax1, v_ax3] = lv289[(((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) % T.int64(77), (v_ax2 * T.int64(80) + v_ax3) % T.int64(640)]

    @T.prim_func
    def fused_reshape28_transpose25_reshape29_transpose26(lv287: T.Buffer((T.int64(2), T.int64(77), T.int64(640)), "float32"), T_transpose: T.Buffer((T.int64(16), T.int64(80), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(3080), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(49280))
                    v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(49280) // T.int64(640))
                    v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(640) // T.int64(80))
                    v_ax3 = T.axis.spatial(T.int64(80), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(80))
                    T.reads(lv287[(((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) % T.int64(77), (v_ax2 * T.int64(80) + v_ax3) % T.int64(640)])
                    T.writes(T_transpose[v_ax0 * T.int64(8) + v_ax2, v_ax3, v_ax1])
                    T_transpose[v_ax2 + v_ax0 * T.int64(8), v_ax3, v_ax1] = lv287[(((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) % T.int64(77), (v_ax2 * T.int64(80) + v_ax3) % T.int64(640)]

    @T.prim_func
    def fused_reshape2_reshape2_add(lv3: T.Buffer((T.int64(77), T.int64(768)), "float32"), lv7: T.Buffer((T.int64(77), T.int64(768)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(924), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("T_add"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(768))
                    v_ax2 = T.axis.spatial(T.int64(768), (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(768))
                    T.reads(lv3[(v_ax2 // T.int64(768) + v_ax1) % T.int64(77), v_ax2 % T.int64(768)], lv7[(v_ax2 // T.int64(768) + v_ax1) % T.int64(77), v_ax2 % T.int64(768)])
                    T.writes(T_add[v_ax0, v_ax1, v_ax2])
                    T_add[v_ax0, v_ax1, v_ax2] = lv3[(v_ax2 // T.int64(768) + v_ax1) % T.int64(77), v_ax2 % T.int64(768)] + lv7[(v_ax2 // T.int64(768) + v_ax1) % T.int64(77), v_ax2 % T.int64(768)]

    @T.prim_func
    def fused_reshape30_transpose29(lv323: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32"), T_transpose: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(20)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(655360))
                        v_ax1 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(655360) // T.int64(20480))
                        v_ax2 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(20480) // T.int64(640))
                        v_ax3 = T.axis.spatial(T.int64(640), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(640))
                        T.reads(lv323[((v_ax1 * T.int64(32) + v_ax3 // T.int64(640) + v_ax2) // T.int64(1024) + v_ax0) % T.int64(2), (v_ax1 * T.int64(32) + v_ax3 // T.int64(640) + v_ax2) % T.int64(1024), v_ax3 % T.int64(640)])
                        T.writes(T_transpose[v_ax0, v_ax3, v_ax1, v_ax2])
                        T_transpose[v_ax0, v_ax3, v_ax1, v_ax2] = lv323[((v_ax1 * T.int64(32) + v_ax3 // T.int64(640) + v_ax2) // T.int64(1024) + v_ax0) % T.int64(2), (v_ax1 * T.int64(32) + v_ax3 // T.int64(640) + v_ax2) % T.int64(1024), v_ax3 % T.int64(640)]

    @T.prim_func
    def fused_reshape34_transpose31_reshape35(lv463: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32"), T_reshape: T.Buffer((T.int64(16), T.int64(256), T.int64(160)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(327680))
                        v_ax1 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(327680) // T.int64(1280))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1280) // T.int64(160))
                        v_ax3 = T.axis.spatial(T.int64(160), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(160))
                        T.reads(lv463[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(256) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(256), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)])
                        T.writes(T_reshape[v_ax0 * T.int64(8) + v_ax2, v_ax1, v_ax3])
                        T_reshape[v_ax2 + v_ax0 * T.int64(8), v_ax1, v_ax3] = lv463[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(256) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(256), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)]

    @T.prim_func
    def fused_reshape34_transpose31_reshape35_transpose32(lv465: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32"), T_transpose: T.Buffer((T.int64(16), T.int64(160), T.int64(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(327680))
                        v_ax1 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(327680) // T.int64(1280))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1280) // T.int64(160))
                        v_ax3 = T.axis.spatial(T.int64(160), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(160))
                        T.reads(lv465[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(256) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(256), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)])
                        T.writes(T_transpose[v_ax0 * T.int64(8) + v_ax2, v_ax3, v_ax1])
                        T_transpose[v_ax2 + v_ax0 * T.int64(8), v_ax3, v_ax1] = lv465[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(256) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(256), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)]

    @T.prim_func
    def fused_reshape36_transpose33_reshape37(lv481: T.Buffer((T.int64(16), T.int64(256), T.int64(160)), "float32"), T_reshape: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(327680))
                        v_ax1 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(327680) // T.int64(40960))
                        v_ax2 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(40960) // T.int64(160))
                        v_ax3 = T.axis.spatial(T.int64(160), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(160))
                        T.reads(lv481[(v_ax0 * T.int64(8) + (v_ax3 // T.int64(160) + v_ax2) // T.int64(256) + v_ax1) % T.int64(16), (v_ax3 // T.int64(160) + v_ax2) % T.int64(256), v_ax3 % T.int64(160)])
                        T.writes(T_reshape[v_ax0, v_ax2, v_ax1 * T.int64(160) + v_ax3])
                        T_reshape[v_ax0, v_ax2, v_ax3 + v_ax1 * T.int64(160)] = lv481[(v_ax0 * T.int64(8) + (v_ax3 // T.int64(160) + v_ax2) // T.int64(256) + v_ax1) % T.int64(16), (v_ax3 // T.int64(160) + v_ax2) % T.int64(256), v_ax3 % T.int64(160)]

    @T.prim_func
    def fused_reshape38_transpose35_reshape39(lv495: T.Buffer((T.int64(2), T.int64(77), T.int64(1280)), "float32"), T_reshape: T.Buffer((T.int64(16), T.int64(77), T.int64(160)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(6160), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(98560))
                    v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(98560) // T.int64(1280))
                    v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(1280) // T.int64(160))
                    v_ax3 = T.axis.spatial(T.int64(160), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(160))
                    T.reads(lv495[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(77), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)])
                    T.writes(T_reshape[v_ax0 * T.int64(8) + v_ax2, v_ax1, v_ax3])
                    T_reshape[v_ax2 + v_ax0 * T.int64(8), v_ax1, v_ax3] = lv495[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(77), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)]

    @T.prim_func
    def fused_reshape38_transpose35_reshape39_transpose36(lv493: T.Buffer((T.int64(2), T.int64(77), T.int64(1280)), "float32"), T_transpose: T.Buffer((T.int64(16), T.int64(160), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(3080), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(98560))
                    v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(98560) // T.int64(1280))
                    v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(1280) // T.int64(160))
                    v_ax3 = T.axis.spatial(T.int64(160), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(160))
                    T.reads(lv493[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(77), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)])
                    T.writes(T_transpose[v_ax0 * T.int64(8) + v_ax2, v_ax3, v_ax1])
                    T_transpose[v_ax2 + v_ax0 * T.int64(8), v_ax3, v_ax1] = lv493[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(77), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)]

    @T.prim_func
    def fused_reshape3_transpose1_reshape4(lv26: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), T_reshape: T.Buffer((T.int64(12), T.int64(77), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(1848), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(768))
                    v_ax2 = T.axis.spatial(T.int64(12), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(768) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(lv26[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(768) + v_ax1) % T.int64(77), (v_ax2 * T.int64(64) + v_ax3) % T.int64(768)])
                    T.writes(T_reshape[v_ax2, v_ax1, v_ax3])
                    T_reshape[v_ax2, v_ax1, v_ax3] = lv26[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(768) + v_ax1) % T.int64(77), (v_ax2 * T.int64(64) + v_ax3) % T.int64(768)]

    @T.prim_func
    def fused_reshape3_transpose1_reshape4_transpose2(lv21: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), T_transpose: T.Buffer((T.int64(12), T.int64(64), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(1848), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(768))
                    v_ax2 = T.axis.spatial(T.int64(12), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(768) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(lv21[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(768) + v_ax1) % T.int64(77), (v_ax2 * T.int64(64) + v_ax3) % T.int64(768)])
                    T.writes(T_transpose[v_ax2, v_ax3, v_ax1])
                    T_transpose[v_ax2, v_ax3, v_ax1] = lv21[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(768) + v_ax1) % T.int64(77), (v_ax2 * T.int64(64) + v_ax3) % T.int64(768)]

    @T.prim_func
    def fused_reshape40_transpose39(lv529: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32"), T_transpose: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(327680))
                        v_ax1 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(327680) // T.int64(20480))
                        v_ax2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(20480) // T.int64(1280))
                        v_ax3 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1280))
                        T.reads(lv529[((v_ax1 * T.int64(16) + v_ax3 // T.int64(1280) + v_ax2) // T.int64(256) + v_ax0) % T.int64(2), (v_ax1 * T.int64(16) + v_ax3 // T.int64(1280) + v_ax2) % T.int64(256), v_ax3 % T.int64(1280)])
                        T.writes(T_transpose[v_ax0, v_ax3, v_ax1, v_ax2])
                        T_transpose[v_ax0, v_ax3, v_ax1, v_ax2] = lv529[((v_ax1 * T.int64(16) + v_ax3 // T.int64(1280) + v_ax2) // T.int64(256) + v_ax0) % T.int64(2), (v_ax1 * T.int64(16) + v_ax3 // T.int64(1280) + v_ax2) % T.int64(256), v_ax3 % T.int64(1280)]

    @T.prim_func
    def fused_reshape42_transpose41_reshape43(lv704: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32"), T_reshape: T.Buffer((T.int64(16), T.int64(64), T.int64(160)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(1280), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(81920))
                    v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(81920) // T.int64(1280))
                    v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(1280) // T.int64(160))
                    v_ax3 = T.axis.spatial(T.int64(160), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(160))
                    T.reads(lv704[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(64) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(64), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)])
                    T.writes(T_reshape[v_ax0 * T.int64(8) + v_ax2, v_ax1, v_ax3])
                    T_reshape[v_ax2 + v_ax0 * T.int64(8), v_ax1, v_ax3] = lv704[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(64) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(64), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)]

    @T.prim_func
    def fused_reshape42_transpose41_reshape43_transpose42(lv706: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32"), T_transpose: T.Buffer((T.int64(16), T.int64(160), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(81920))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(81920) // T.int64(1280))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1280) // T.int64(160))
                        v_ax3 = T.axis.spatial(T.int64(160), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(160))
                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(256) + ax0_ax1_ax2_ax3_fused_2 < T.int64(163840))
                        T.reads(lv706[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(64) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(64), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)])
                        T.writes(T_transpose[v_ax0 * T.int64(8) + v_ax2, v_ax3, v_ax1])
                        T_transpose[v_ax2 + v_ax0 * T.int64(8), v_ax3, v_ax1] = lv706[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(64) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(64), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)]

    @T.prim_func
    def fused_reshape44_transpose43_reshape45(lv722: T.Buffer((T.int64(16), T.int64(64), T.int64(160)), "float32"), T_reshape: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(81920))
                        v_ax1 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(81920) // T.int64(10240))
                        v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(10240) // T.int64(160))
                        v_ax3 = T.axis.spatial(T.int64(160), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(160))
                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(256) + ax0_ax1_ax2_ax3_fused_2 < T.int64(163840))
                        T.reads(lv722[(v_ax0 * T.int64(8) + (v_ax3 // T.int64(160) + v_ax2) // T.int64(64) + v_ax1) % T.int64(16), (v_ax3 // T.int64(160) + v_ax2) % T.int64(64), v_ax3 % T.int64(160)])
                        T.writes(T_reshape[v_ax0, v_ax2, v_ax1 * T.int64(160) + v_ax3])
                        T_reshape[v_ax0, v_ax2, v_ax3 + v_ax1 * T.int64(160)] = lv722[(v_ax0 * T.int64(8) + (v_ax3 // T.int64(160) + v_ax2) // T.int64(64) + v_ax1) % T.int64(16), (v_ax3 // T.int64(160) + v_ax2) % T.int64(64), v_ax3 % T.int64(160)]

    @T.prim_func
    def fused_reshape46_transpose44(lv770: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32"), T_transpose: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(81920))
                        v_ax1 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(81920) // T.int64(10240))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(10240) // T.int64(1280))
                        v_ax3 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1280))
                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(256) + ax0_ax1_ax2_ax3_fused_2 < T.int64(163840))
                        T.reads(lv770[((v_ax1 * T.int64(8) + v_ax3 // T.int64(1280) + v_ax2) // T.int64(64) + v_ax0) % T.int64(2), (v_ax1 * T.int64(8) + v_ax3 // T.int64(1280) + v_ax2) % T.int64(64), v_ax3 % T.int64(1280)])
                        T.writes(T_transpose[v_ax0, v_ax3, v_ax1, v_ax2])
                        T_transpose[v_ax0, v_ax3, v_ax1, v_ax2] = lv770[((v_ax1 * T.int64(8) + v_ax3 // T.int64(1280) + v_ax2) // T.int64(64) + v_ax0) % T.int64(2), (v_ax1 * T.int64(8) + v_ax3 // T.int64(1280) + v_ax2) % T.int64(64), v_ax3 % T.int64(1280)]

    @T.prim_func
    def fused_reshape49_transpose45(lv18: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32"), T_transpose: T.Buffer((T.int64(1), T.int64(4096), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(32)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(4096))
                        v_ax2 = T.axis.spatial(T.int64(4096), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(4096))
                        T.reads(lv18[T.int64(0), (v_ax2 // T.int64(4096) + v_ax1) % T.int64(512), v_ax2 % T.int64(4096) // T.int64(64), v_ax2 % T.int64(64)])
                        T.writes(T_transpose[v_ax0, v_ax2, v_ax1])
                        T_transpose[v_ax0, v_ax2, v_ax1] = lv18[T.int64(0), (v_ax2 // T.int64(4096) + v_ax1) % T.int64(512), v_ax2 % T.int64(4096) // T.int64(64), v_ax2 % T.int64(64)]

    @T.prim_func
    def fused_reshape50_transpose47_reshape51(lv23: T.Buffer((T.int64(1), T.int64(4096), T.int64(512)), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(4096), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(4096), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(512))
                        v_ax2 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax3 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(512))
                        T.reads(lv23[T.int64(0), (v_ax3 // T.int64(512) + v_ax1 + v_ax2) % T.int64(4096), v_ax3 % T.int64(512)])
                        T.writes(T_reshape[T.int64(0), v_ax1, v_ax3])
                        T_reshape[T.int64(0), v_ax1, v_ax3] = lv23[T.int64(0), (v_ax3 // T.int64(512) + v_ax1 + v_ax2) % T.int64(4096), v_ax3 % T.int64(512)]

    @T.prim_func
    def fused_reshape50_transpose47_reshape51_transpose48(lv26: T.Buffer((T.int64(1), T.int64(4096), T.int64(512)), "float32"), T_transpose: T.Buffer((T.int64(1), T.int64(512), T.int64(4096)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(4096), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(512))
                        v_ax2 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax3 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(512))
                        T.reads(lv26[T.int64(0), (v_ax3 // T.int64(512) + v_ax1 + v_ax2) % T.int64(4096), v_ax3 % T.int64(512)])
                        T.writes(T_transpose[T.int64(0), v_ax3, v_ax1])
                        T_transpose[T.int64(0), v_ax3, v_ax1] = lv26[T.int64(0), (v_ax3 // T.int64(512) + v_ax1 + v_ax2) % T.int64(4096), v_ax3 % T.int64(512)]

    @T.prim_func
    def fused_reshape52_transpose49_reshape53(lv45: T.Buffer((T.int64(1), T.int64(4096), T.int64(512)), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(4096), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax2 = T.axis.spatial(T.int64(4096), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(512))
                        v_ax3 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(512))
                        T.reads(lv45[T.int64(0), (v_ax3 // T.int64(512) + v_ax2) % T.int64(4096), v_ax3 % T.int64(512)])
                        T.writes(T_reshape[T.int64(0), v_ax2, v_ax3])
                        T_reshape[T.int64(0), v_ax2, v_ax3] = lv45[T.int64(0), (v_ax3 // T.int64(512) + v_ax2) % T.int64(4096), v_ax3 % T.int64(512)]

    @T.prim_func
    def fused_reshape5_add2_reshape6(lv35: T.Buffer((T.int64(12), T.int64(77), T.int64(77)), "float32"), param_0: T.Buffer((T.int64(1), T.int64(1), T.int64(77), T.int64(77)), "float32"), T_reshape: T.Buffer((T.int64(12), T.int64(77), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(1112), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(12), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(5929))
                    v_ax2 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(5929) // T.int64(77))
                    v_ax3 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(77))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(71148))
                    T.reads(lv35[((v_ax3 // T.int64(77) + v_ax2) // T.int64(77) + v_ax1) % T.int64(12), (v_ax3 // T.int64(77) + v_ax2) % T.int64(77), v_ax3 % T.int64(77)], param_0[v_ax0, T.int64(0), v_ax2, v_ax3])
                    T.writes(T_reshape[v_ax1, v_ax2, v_ax3])
                    T_reshape[v_ax1, v_ax2, v_ax3] = lv35[((v_ax3 // T.int64(77) + v_ax2) // T.int64(77) + v_ax1) % T.int64(12), (v_ax3 // T.int64(77) + v_ax2) % T.int64(77), v_ax3 % T.int64(77)] + param_0[v_ax0, T.int64(0), v_ax2, v_ax3]

    @T.prim_func
    def fused_reshape7_transpose3_reshape8(lv40: T.Buffer((T.int64(12), T.int64(77), T.int64(64)), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(924), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(12), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4928))
                    v_ax2 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4928) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(lv40[((v_ax3 // T.int64(64) + v_ax2) // T.int64(77) + v_ax1) % T.int64(12), (v_ax3 // T.int64(64) + v_ax2) % T.int64(77), v_ax3 % T.int64(64)])
                    T.writes(T_reshape[T.int64(0), v_ax2, v_ax1 * T.int64(64) + v_ax3])
                    T_reshape[T.int64(0), v_ax2, v_ax3 + v_ax1 * T.int64(64)] = lv40[((v_ax3 // T.int64(64) + v_ax2) // T.int64(77) + v_ax1) % T.int64(12), (v_ax3 // T.int64(64) + v_ax2) % T.int64(77), v_ax3 % T.int64(64)]

    @T.prim_func
    def fused_reshape_cast_reshape1(inp_0: T.Buffer((T.int64(1), T.int64(77)), "int32"), T_reshape: T.Buffer((T.int64(77),), "int32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(2), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(77), ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 < T.int64(77))
                    T.reads(inp_0[T.int64(0), v_ax1 % T.int64(77)])
                    T.writes(T_reshape[v_ax1])
                    T_reshape[v_ax1] = inp_0[T.int64(0), v_ax1 % T.int64(77)]

    @T.prim_func
    def fused_softmax9_cast3(lv42: T.Buffer((T.int64(1), T.int64(4096), T.int64(4096)), "float32"), compute: T.Buffer((T.int64(1), T.int64(4096), T.int64(4096)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int64(4096), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(16)):
                for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int64(1), ax0)
                        v_i1 = T.axis.spatial(T.int64(4096), i0_i1_fused + ax1)
                        v_k = T.axis.reduce(T.int64(4096), ax2_0 * T.int64(256) + ax2_1)
                        T.reads(lv42[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], lv42[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(16)):
                for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int64(1), ax0)
                        v_i1 = T.axis.spatial(T.int64(4096), i0_i1_fused + ax1)
                        v_k = T.axis.reduce(T.int64(4096), ax2_0 * T.int64(256) + ax2_1)
                        T.reads(lv42[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(lv42[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int64(16)):
                for i2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_i1 = T.axis.spatial(T.int64(4096), i0_i1_fused)
                        v_i2 = T.axis.spatial(T.int64(4096), i2_0 * T.int64(256) + i2_1)
                        T.reads(lv42[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(compute[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        compute[v_i0, v_i1, v_i2] = T.exp(lv42[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def fused_split_subtract_multiply17_add36(lv1818: T.Buffer((T.int64(2), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("T_add"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(lv1818[v_ax0:v_ax0 + T.int64(2), v_ax1, v_ax2, v_ax3])
                    T.writes(T_add[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_add[v_ax0, v_ax1, v_ax2, v_ax3] = lv1818[v_ax0, v_ax1, v_ax2, v_ax3] + T.float32(7.5) * (lv1818[v_ax0 + T.int64(1), v_ax1, v_ax2, v_ax3] - lv1818[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def fused_transpose19_reshape23(lv252: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), T_reshape: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(20)):
                    with T.block("T_transpose"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(655360))
                        v_ax1 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(655360) // T.int64(20480))
                        v_ax2 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(20480) // T.int64(640))
                        v_ax3 = T.axis.spatial(T.int64(640), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(640))
                        T.reads(lv252[v_ax0, v_ax3, v_ax1, v_ax2])
                        T.writes(T_reshape[v_ax0, v_ax1 * T.int64(32) + v_ax2, v_ax3])
                        T_reshape[v_ax0, v_ax2 + v_ax1 * T.int64(32), v_ax3] = lv252[v_ax0, v_ax3, v_ax1, v_ax2]

    @T.prim_func
    def fused_transpose30_reshape33(lv458: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), T_reshape: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                    with T.block("T_transpose"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(327680))
                        v_ax1 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(327680) // T.int64(20480))
                        v_ax2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(20480) // T.int64(1280))
                        v_ax3 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1280))
                        T.reads(lv458[v_ax0, v_ax3, v_ax1, v_ax2])
                        T.writes(T_reshape[v_ax0, v_ax1 * T.int64(16) + v_ax2, v_ax3])
                        T_reshape[v_ax0, v_ax2 + v_ax1 * T.int64(16), v_ax3] = lv458[v_ax0, v_ax3, v_ax1, v_ax2]

    @T.prim_func
    def fused_transpose40_reshape41(lv699: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), T_reshape: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                    with T.block("T_transpose"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(81920))
                        v_ax1 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(81920) // T.int64(10240))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(10240) // T.int64(1280))
                        v_ax3 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1280))
                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(256) + ax0_ax1_ax2_ax3_fused_2 < T.int64(163840))
                        T.reads(lv699[v_ax0, v_ax3, v_ax1, v_ax2])
                        T.writes(T_reshape[v_ax0, v_ax1 * T.int64(8) + v_ax2, v_ax3])
                        T_reshape[v_ax0, v_ax2 + v_ax1 * T.int64(8), v_ax3] = lv699[v_ax0, v_ax3, v_ax1, v_ax2]

    @T.prim_func
    def fused_transpose48_reshape54_add39_divide4(lv51: T.Buffer((T.int64(1), T.int64(4096), T.int64(512)), "float32"), lv17: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(32)):
                    with T.block("T_transpose"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(4096))
                        v_ax2 = T.axis.spatial(T.int64(4096), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(4096))
                        T.reads(lv51[v_ax0, v_ax2, v_ax1], lv17[T.int64(0), v_ax1, v_ax2 // T.int64(64), v_ax2 % T.int64(64)])
                        T.writes(T_divide[T.int64(0), v_ax1, v_ax2 // T.int64(64), v_ax2 % T.int64(64)])
                        T_divide[T.int64(0), v_ax1, v_ax2 // T.int64(64), v_ax2 % T.int64(64)] = lv51[v_ax0, v_ax2, v_ax1] + lv17[T.int64(0), v_ax1, v_ax2 // T.int64(64), v_ax2 % T.int64(64)]

    @T.prim_func
    def fused_transpose9_reshape13(lv46: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), T_reshape: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(40)):
                    with T.block("T_transpose"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1310720))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1310720) // T.int64(20480))
                        v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(20480) // T.int64(320))
                        v_ax3 = T.axis.spatial(T.int64(320), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(320))
                        T.reads(lv46[v_ax0, v_ax3, v_ax1, v_ax2])
                        T.writes(T_reshape[v_ax0, v_ax1 * T.int64(64) + v_ax2, v_ax3])
                        T_reshape[v_ax0, v_ax2 + v_ax1 * T.int64(64), v_ax3] = lv46[v_ax0, v_ax3, v_ax1, v_ax2]

    @T.prim_func
    def group_norm1(rxplaceholder: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), rxplaceholder_1: T.Buffer((T.int64(320),), "float32"), rxplaceholder_2: T.Buffer((T.int64(320),), "float32"), T_reshape: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for k2_k3_k4_fused_0 in range(T.int64(640)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(10), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) // T.int64(4096))
                        v_k3 = T.axis.reduce(T.int64(64), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) % T.int64(4096) // T.int64(64))
                        v_k4 = T.axis.reduce(T.int64(64), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) % T.int64(64))
                        T.reads(rxplaceholder[((v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(320), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + rxplaceholder[((v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(320), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + rxplaceholder[((v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(320), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)] * rxplaceholder[((v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(320), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_ax4_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_ax4_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_ax4_fused_0 in range(T.int64(40)):
                    with T.block("T_group_norm"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) // T.int64(1310720))
                        v_ax1 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(1310720) // T.int64(40960))
                        v_ax2 = T.axis.spatial(T.int64(10), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(40960) // T.int64(4096))
                        v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(4096) // T.int64(64))
                        v_ax4 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(64))
                        T.reads(rxplaceholder[((v_ax1 * T.int64(10) + (v_ax4 // T.int64(64) + v_ax3) // T.int64(64) + v_ax2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_ax4 // T.int64(64) + v_ax3) // T.int64(64) + v_ax2) % T.int64(320), (v_ax4 // T.int64(64) + v_ax3) % T.int64(64), v_ax4 % T.int64(64)], rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1], rxplaceholder_1[(v_ax1 * T.int64(10) + v_ax2) % T.int64(320)], rxplaceholder_2[(v_ax1 * T.int64(10) + v_ax2) % T.int64(320)])
                        T.writes(T_reshape[v_ax0, v_ax1 * T.int64(10) + v_ax2, v_ax3, v_ax4])
                        T_reshape[v_ax0, v_ax2 + v_ax1 * T.int64(10), v_ax3, v_ax4] = (rxplaceholder[((v_ax1 * T.int64(10) + (v_ax4 // T.int64(64) + v_ax3) // T.int64(64) + v_ax2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_ax4 // T.int64(64) + v_ax3) // T.int64(64) + v_ax2) % T.int64(320), (v_ax4 // T.int64(64) + v_ax3) % T.int64(64), v_ax4 % T.int64(64)] - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(2.4414062500000001e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[v_ax0, v_ax1] * T.float32(2.4414062500000001e-05) - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(2.4414062500000001e-05) * (rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(2.4414062500000001e-05)) + T.float32(9.9999999999999995e-07)) * rxplaceholder_1[(v_ax1 * T.int64(10) + v_ax2) % T.int64(320)] + rxplaceholder_2[(v_ax1 * T.int64(10) + v_ax2) % T.int64(320)]

    @T.prim_func
    def group_norm18(rxplaceholder: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32"), rxplaceholder_1: T.Buffer((T.int64(512),), "float32"), rxplaceholder_2: T.Buffer((T.int64(512),), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(32), thread="blockIdx.x"):
            for k2_k3_k4_fused_0 in range(T.int64(256)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused)
                        v_k2 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) // T.int64(4096))
                        v_k3 = T.axis.reduce(T.int64(64), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(4096) // T.int64(64))
                        v_k4 = T.axis.reduce(T.int64(64), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(64))
                        T.reads(rxplaceholder[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(512), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + rxplaceholder[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(512), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + rxplaceholder[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(512), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)] * rxplaceholder[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(512), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_ax4_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_ax4_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_ax4_fused_0 in range(T.int64(32)):
                    with T.block("T_group_norm"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) // T.int64(65536))
                        v_ax2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(65536) // T.int64(4096))
                        v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(4096) // T.int64(64))
                        v_ax4 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(64))
                        T.reads(rxplaceholder[T.int64(0), (v_ax1 * T.int64(16) + (v_ax4 // T.int64(64) + v_ax3) // T.int64(64) + v_ax2) % T.int64(512), (v_ax4 // T.int64(64) + v_ax3) % T.int64(64), v_ax4 % T.int64(64)], rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1], rxplaceholder_1[(v_ax1 * T.int64(16) + v_ax2) % T.int64(512)], rxplaceholder_2[(v_ax1 * T.int64(16) + v_ax2) % T.int64(512)])
                        T.writes(T_reshape[T.int64(0), v_ax1 * T.int64(16) + v_ax2, v_ax3, v_ax4])
                        T_reshape[T.int64(0), v_ax2 + v_ax1 * T.int64(16), v_ax3, v_ax4] = (rxplaceholder[T.int64(0), (v_ax1 * T.int64(16) + (v_ax4 // T.int64(64) + v_ax3) // T.int64(64) + v_ax2) % T.int64(512), (v_ax4 // T.int64(64) + v_ax3) % T.int64(64), v_ax4 % T.int64(64)] - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(1.52587890625e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[v_ax0, v_ax1] * T.float32(1.52587890625e-05) - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(1.52587890625e-05) * (rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(1.52587890625e-05)) + T.float32(9.9999999999999995e-07)) * rxplaceholder_1[(v_ax1 * T.int64(16) + v_ax2) % T.int64(512)] + rxplaceholder_2[(v_ax1 * T.int64(16) + v_ax2) % T.int64(512)]

    @T.prim_func
    def group_norm4(rxplaceholder: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), rxplaceholder_1: T.Buffer((T.int64(640),), "float32"), rxplaceholder_2: T.Buffer((T.int64(640),), "float32"), T_reshape: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(80)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(20), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) // T.int64(1024))
                        v_k3 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(1024) // T.int64(32))
                        v_k4 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(32))
                        T.reads(rxplaceholder[((v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(640), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + rxplaceholder[((v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(640), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + rxplaceholder[((v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(640), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)] * rxplaceholder[((v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(640), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_ax4_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for ax0_ax1_ax2_ax3_ax4_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_ax4_fused_0 in range(T.int64(20)):
                    with T.block("T_group_norm"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) // T.int64(655360))
                        v_ax1 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(655360) // T.int64(20480))
                        v_ax2 = T.axis.spatial(T.int64(20), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(20480) // T.int64(1024))
                        v_ax3 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(1024) // T.int64(32))
                        v_ax4 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(32))
                        T.reads(rxplaceholder[((v_ax1 * T.int64(20) + (v_ax4 // T.int64(32) + v_ax3) // T.int64(32) + v_ax2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_ax4 // T.int64(32) + v_ax3) // T.int64(32) + v_ax2) % T.int64(640), (v_ax4 // T.int64(32) + v_ax3) % T.int64(32), v_ax4 % T.int64(32)], rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1], rxplaceholder_1[(v_ax1 * T.int64(20) + v_ax2) % T.int64(640)], rxplaceholder_2[(v_ax1 * T.int64(20) + v_ax2) % T.int64(640)])
                        T.writes(T_reshape[v_ax0, v_ax1 * T.int64(20) + v_ax2, v_ax3, v_ax4])
                        T_reshape[v_ax0, v_ax2 + v_ax1 * T.int64(20), v_ax3, v_ax4] = (rxplaceholder[((v_ax1 * T.int64(20) + (v_ax4 // T.int64(32) + v_ax3) // T.int64(32) + v_ax2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_ax4 // T.int64(32) + v_ax3) // T.int64(32) + v_ax2) % T.int64(640), (v_ax4 // T.int64(32) + v_ax3) % T.int64(32), v_ax4 % T.int64(32)] - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(4.8828125000000003e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[v_ax0, v_ax1] * T.float32(4.8828125000000003e-05) - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(4.8828125000000003e-05) * (rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(4.8828125000000003e-05)) + T.float32(9.9999999999999995e-07)) * rxplaceholder_1[(v_ax1 * T.int64(20) + v_ax2) % T.int64(640)] + rxplaceholder_2[(v_ax1 * T.int64(20) + v_ax2) % T.int64(640)]

    @T.prim_func
    def group_norm7(rxplaceholder: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), rxplaceholder_1: T.Buffer((T.int64(1280),), "float32"), rxplaceholder_2: T.Buffer((T.int64(1280),), "float32"), T_reshape: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(160)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(40), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) // T.int64(256))
                        v_k3 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) % T.int64(256) // T.int64(16))
                        v_k4 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) % T.int64(16))
                        T.reads(rxplaceholder[((v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1280), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + rxplaceholder[((v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1280), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + rxplaceholder[((v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1280), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)] * rxplaceholder[((v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1280), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_ax4_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for ax0_ax1_ax2_ax3_ax4_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_ax4_fused_0 in range(T.int64(10)):
                    with T.block("T_group_norm"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) // T.int64(327680))
                        v_ax1 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(327680) // T.int64(10240))
                        v_ax2 = T.axis.spatial(T.int64(40), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(10240) // T.int64(256))
                        v_ax3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(256) // T.int64(16))
                        v_ax4 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(16))
                        T.reads(rxplaceholder[((v_ax1 * T.int64(40) + (v_ax4 // T.int64(16) + v_ax3) // T.int64(16) + v_ax2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_ax4 // T.int64(16) + v_ax3) // T.int64(16) + v_ax2) % T.int64(1280), (v_ax4 // T.int64(16) + v_ax3) % T.int64(16), v_ax4 % T.int64(16)], rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1], rxplaceholder_1[(v_ax1 * T.int64(40) + v_ax2) % T.int64(1280)], rxplaceholder_2[(v_ax1 * T.int64(40) + v_ax2) % T.int64(1280)])
                        T.writes(T_reshape[v_ax0, v_ax1 * T.int64(40) + v_ax2, v_ax3, v_ax4])
                        T_reshape[v_ax0, v_ax2 + v_ax1 * T.int64(40), v_ax3, v_ax4] = (rxplaceholder[((v_ax1 * T.int64(40) + (v_ax4 // T.int64(16) + v_ax3) // T.int64(16) + v_ax2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_ax4 // T.int64(16) + v_ax3) // T.int64(16) + v_ax2) % T.int64(1280), (v_ax4 // T.int64(16) + v_ax3) % T.int64(16), v_ax4 % T.int64(16)] - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(9.7656250000000005e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[v_ax0, v_ax1] * T.float32(9.7656250000000005e-05) - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(9.7656250000000005e-05) * (rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(9.7656250000000005e-05)) + T.float32(9.9999999999999995e-07)) * rxplaceholder_1[(v_ax1 * T.int64(40) + v_ax2) % T.int64(1280)] + rxplaceholder_2[(v_ax1 * T.int64(40) + v_ax2) % T.int64(1280)]

    @T.prim_func
    def group_norm9(rxplaceholder: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), rxplaceholder_1: T.Buffer((T.int64(1280),), "float32"), rxplaceholder_2: T.Buffer((T.int64(1280),), "float32"), T_reshape: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(80)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(40), (k2_k3_k4_fused_0 * T.int64(32) + k2_k3_k4_fused_1) // T.int64(64))
                        v_k3 = T.axis.reduce(T.int64(8), (k2_k3_k4_fused_0 * T.int64(32) + k2_k3_k4_fused_1) % T.int64(64) // T.int64(8))
                        v_k4 = T.axis.reduce(T.int64(8), (k2_k3_k4_fused_0 * T.int64(32) + k2_k3_k4_fused_1) % T.int64(8))
                        T.reads(rxplaceholder[((v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(1280), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + rxplaceholder[((v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(1280), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + rxplaceholder[((v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(1280), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)] * rxplaceholder[((v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(1280), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_ax4_fused_0 in T.thread_binding(T.int64(640), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for ax0_ax1_ax2_ax3_ax4_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_group_norm"):
                    v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_1) // T.int64(81920))
                    v_ax1 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_1) % T.int64(81920) // T.int64(2560))
                    v_ax2 = T.axis.spatial(T.int64(40), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_1) % T.int64(2560) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_1) % T.int64(64) // T.int64(8))
                    v_ax4 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_1) % T.int64(8))
                    T.reads(rxplaceholder[((v_ax1 * T.int64(40) + (v_ax4 // T.int64(8) + v_ax3) // T.int64(8) + v_ax2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_ax4 // T.int64(8) + v_ax3) // T.int64(8) + v_ax2) % T.int64(1280), (v_ax4 // T.int64(8) + v_ax3) % T.int64(8), v_ax4 % T.int64(8)], rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1], rxplaceholder_1[(v_ax1 * T.int64(40) + v_ax2) % T.int64(1280)], rxplaceholder_2[(v_ax1 * T.int64(40) + v_ax2) % T.int64(1280)])
                    T.writes(T_reshape[v_ax0, v_ax1 * T.int64(40) + v_ax2, v_ax3, v_ax4])
                    T_reshape[v_ax0, v_ax2 + v_ax1 * T.int64(40), v_ax3, v_ax4] = (rxplaceholder[((v_ax1 * T.int64(40) + (v_ax4 // T.int64(8) + v_ax3) // T.int64(8) + v_ax2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_ax4 // T.int64(8) + v_ax3) // T.int64(8) + v_ax2) % T.int64(1280), (v_ax4 // T.int64(8) + v_ax3) % T.int64(8), v_ax4 % T.int64(8)] - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00039062500000000002)) * T.rsqrt(rxplaceholder_red_temp_v1[v_ax0, v_ax1] * T.float32(0.00039062500000000002) - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00039062500000000002) * (rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00039062500000000002)) + T.float32(9.9999999999999995e-07)) * rxplaceholder_1[(v_ax1 * T.int64(40) + v_ax2) % T.int64(1280)] + rxplaceholder_2[(v_ax1 * T.int64(40) + v_ax2) % T.int64(1280)]

    @T.prim_func
    def layer_norm(rxplaceholder: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), rxplaceholder_1: T.Buffer((T.int64(768),), "float32"), rxplaceholder_2: T.Buffer((T.int64(768),), "float32"), T_layer_norm: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(77)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(77)))
        for ax0_ax1_fused in T.thread_binding(T.int64(77), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for k2_0 in range(T.int64(96)):
                for k2_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(77), ax0_ax1_fused)
                        v_k2 = T.axis.reduce(T.int64(768), k2_0 * T.int64(8) + k2_1)
                        T.reads(rxplaceholder[v_ax0, v_ax1, v_k2])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + rxplaceholder[v_ax0, v_ax1, v_k2]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + rxplaceholder[v_ax0, v_ax1, v_k2] * rxplaceholder[v_ax0, v_ax1, v_k2]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(1848), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("T_layer_norm"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) // T.int64(768))
                    v_ax2 = T.axis.spatial(T.int64(768), (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) % T.int64(768))
                    T.reads(rxplaceholder[v_ax0, v_ax1, v_ax2], rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1], rxplaceholder_1[v_ax2], rxplaceholder_2[v_ax2])
                    T.writes(T_layer_norm[v_ax0, v_ax1, v_ax2])
                    T_layer_norm[v_ax0, v_ax1, v_ax2] = (rxplaceholder[v_ax0, v_ax1, v_ax2] - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0013020833333333333)) * T.rsqrt(rxplaceholder_red_temp_v1[v_ax0, v_ax1] * T.float32(0.0013020833333333333) - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0013020833333333333) * (rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0013020833333333333)) + T.float32(1.0000000000000001e-05)) * rxplaceholder_1[v_ax2] + rxplaceholder_2[v_ax2]

    @T.prim_func
    def layer_norm1(rxplaceholder: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32"), rxplaceholder_1: T.Buffer((T.int64(320),), "float32"), rxplaceholder_2: T.Buffer((T.int64(320),), "float32"), T_layer_norm: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(4096)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(4096)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("rxplaceholder_red_temp_init"):
                    v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(4096))
                    v_ax1 = T.axis.spatial(T.int64(4096), (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(4096))
                    T.reads()
                    T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                    rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                    rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                for k2 in range(T.int64(320)):
                    with T.block("rxplaceholder_red_temp_update"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(4096))
                        v_ax1 = T.axis.spatial(T.int64(4096), (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(4096))
                        v_k2 = T.axis.reduce(T.int64(320), k2)
                        T.reads(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1], rxplaceholder[v_ax0, v_ax1, v_k2])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + rxplaceholder[v_ax0, v_ax1, v_k2]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + rxplaceholder[v_ax0, v_ax1, v_k2] * rxplaceholder[v_ax0, v_ax1, v_k2]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(40)):
                    with T.block("T_layer_norm"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(1310720))
                        v_ax1 = T.axis.spatial(T.int64(4096), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(1310720) // T.int64(320))
                        v_ax2 = T.axis.spatial(T.int64(320), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(320))
                        T.reads(rxplaceholder[v_ax0, v_ax1, v_ax2], rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1], rxplaceholder_1[v_ax2], rxplaceholder_2[v_ax2])
                        T.writes(T_layer_norm[v_ax0, v_ax1, v_ax2])
                        T_layer_norm[v_ax0, v_ax1, v_ax2] = (rxplaceholder[v_ax0, v_ax1, v_ax2] - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0031250000000000002)) * T.rsqrt(rxplaceholder_red_temp_v1[v_ax0, v_ax1] * T.float32(0.0031250000000000002) - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0031250000000000002) * (rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0031250000000000002)) + T.float32(1.0000000000000001e-05)) * rxplaceholder_1[v_ax2] + rxplaceholder_2[v_ax2]

    @T.prim_func
    def layer_norm2(rxplaceholder: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32"), rxplaceholder_1: T.Buffer((T.int64(640),), "float32"), rxplaceholder_2: T.Buffer((T.int64(640),), "float32"), T_layer_norm: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(1024)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(1024)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("rxplaceholder_red_temp_init"):
                    v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(1024))
                    v_ax1 = T.axis.spatial(T.int64(1024), (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(1024))
                    T.reads()
                    T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                    rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                    rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                for k2 in range(T.int64(640)):
                    with T.block("rxplaceholder_red_temp_update"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(1024))
                        v_ax1 = T.axis.spatial(T.int64(1024), (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(1024))
                        v_k2 = T.axis.reduce(T.int64(640), k2)
                        T.reads(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1], rxplaceholder[v_ax0, v_ax1, v_k2])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + rxplaceholder[v_ax0, v_ax1, v_k2]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + rxplaceholder[v_ax0, v_ax1, v_k2] * rxplaceholder[v_ax0, v_ax1, v_k2]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(20)):
                    with T.block("T_layer_norm"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(655360))
                        v_ax1 = T.axis.spatial(T.int64(1024), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(655360) // T.int64(640))
                        v_ax2 = T.axis.spatial(T.int64(640), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(640))
                        T.reads(rxplaceholder[v_ax0, v_ax1, v_ax2], rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1], rxplaceholder_1[v_ax2], rxplaceholder_2[v_ax2])
                        T.writes(T_layer_norm[v_ax0, v_ax1, v_ax2])
                        T_layer_norm[v_ax0, v_ax1, v_ax2] = (rxplaceholder[v_ax0, v_ax1, v_ax2] - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0015625000000000001)) * T.rsqrt(rxplaceholder_red_temp_v1[v_ax0, v_ax1] * T.float32(0.0015625000000000001) - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0015625000000000001) * (rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0015625000000000001)) + T.float32(1.0000000000000001e-05)) * rxplaceholder_1[v_ax2] + rxplaceholder_2[v_ax2]

    @T.prim_func
    def layer_norm3(rxplaceholder: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32"), rxplaceholder_1: T.Buffer((T.int64(1280),), "float32"), rxplaceholder_2: T.Buffer((T.int64(1280),), "float32"), T_layer_norm: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(256)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(256)))
        for ax0_ax1_fused in T.thread_binding(T.int64(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for k2_0 in range(T.int64(20)):
                for k2_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(256))
                        v_ax1 = T.axis.spatial(T.int64(256), ax0_ax1_fused % T.int64(256))
                        v_k2 = T.axis.reduce(T.int64(1280), k2_0 * T.int64(64) + k2_1)
                        T.reads(rxplaceholder[v_ax0, v_ax1, v_k2])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + rxplaceholder[v_ax0, v_ax1, v_k2]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + rxplaceholder[v_ax0, v_ax1, v_k2] * rxplaceholder[v_ax0, v_ax1, v_k2]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(10)):
                    with T.block("T_layer_norm"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(327680))
                        v_ax1 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(327680) // T.int64(1280))
                        v_ax2 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(1280))
                        T.reads(rxplaceholder[v_ax0, v_ax1, v_ax2], rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1], rxplaceholder_1[v_ax2], rxplaceholder_2[v_ax2])
                        T.writes(T_layer_norm[v_ax0, v_ax1, v_ax2])
                        T_layer_norm[v_ax0, v_ax1, v_ax2] = (rxplaceholder[v_ax0, v_ax1, v_ax2] - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00078125000000000004)) * T.rsqrt(rxplaceholder_red_temp_v1[v_ax0, v_ax1] * T.float32(0.00078125000000000004) - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00078125000000000004) * (rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00078125000000000004)) + T.float32(1.0000000000000001e-05)) * rxplaceholder_1[v_ax2] + rxplaceholder_2[v_ax2]

    @T.prim_func
    def layer_norm4(rxplaceholder: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32"), rxplaceholder_1: T.Buffer((T.int64(1280),), "float32"), rxplaceholder_2: T.Buffer((T.int64(1280),), "float32"), T_layer_norm: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(64)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(64)))
        for ax0_ax1_fused in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for k2_0 in range(T.int64(80)):
                for k2_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(64))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_fused % T.int64(64))
                        v_k2 = T.axis.reduce(T.int64(1280), k2_0 * T.int64(16) + k2_1)
                        T.reads(rxplaceholder[v_ax0, v_ax1, v_k2])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + rxplaceholder[v_ax0, v_ax1, v_k2]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + rxplaceholder[v_ax0, v_ax1, v_k2] * rxplaceholder[v_ax0, v_ax1, v_k2]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(3)):
                    with T.block("T_layer_norm"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(81920))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(81920) // T.int64(1280))
                        v_ax2 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(1280))
                        T.where((ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) * T.int64(256) + ax0_ax1_ax2_fused_2 < T.int64(163840))
                        T.reads(rxplaceholder[v_ax0, v_ax1, v_ax2], rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1], rxplaceholder_1[v_ax2], rxplaceholder_2[v_ax2])
                        T.writes(T_layer_norm[v_ax0, v_ax1, v_ax2])
                        T_layer_norm[v_ax0, v_ax1, v_ax2] = (rxplaceholder[v_ax0, v_ax1, v_ax2] - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00078125000000000004)) * T.rsqrt(rxplaceholder_red_temp_v1[v_ax0, v_ax1] * T.float32(0.00078125000000000004) - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00078125000000000004) * (rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00078125000000000004)) + T.float32(1.0000000000000001e-05)) * rxplaceholder_1[v_ax2] + rxplaceholder_2[v_ax2]

    @T.prim_func
    def matmul1(rxplaceholder: T.Buffer((T.int64(12), T.int64(77), T.int64(64)), "float32"), rxplaceholder_1: T.Buffer((T.int64(12), T.int64(64), T.int64(77)), "float32"), matmul: T.Buffer((T.int64(12), T.int64(77), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(12), T.int64(77), T.int64(77)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int64(12), T.int64(77), T.int64(64)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int64(12), T.int64(64), T.int64(77)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(33), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(154), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(12), i0_3_init + i0_4_init + i0_0_i1_0_i2_0_fused // T.int64(11) * T.int64(4) + i0_1_i1_1_i2_1_fused * T.int64(2) + i0_2_i1_2_i2_2_fused // T.int64(77))
                            v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(11) * T.int64(7) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(77), i2_4_init + i0_2_i1_2_i2_2_fused % T.int64(77) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(8)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(154), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(12), i0_0_i1_0_i2_0_fused // T.int64(11) * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(154) + ax0_ax1_ax2_fused_1) // T.int64(56))
                                    v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(11) * T.int64(7) + (ax0_ax1_ax2_fused_0 * T.int64(154) + ax0_ax1_ax2_fused_1) % T.int64(56) // T.int64(8))
                                    v2 = T.axis.spatial(T.int64(64), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(154) + ax0_ax1_ax2_fused_1) % T.int64(8))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int64(154) + ax0_ax1_ax2_fused_1 < T.int64(224))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(154), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(12), i0_0_i1_0_i2_0_fused // T.int64(11) * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(154) + ax0_ax1_ax2_fused_1) // T.int64(616))
                                    v1 = T.axis.spatial(T.int64(64), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(154) + ax0_ax1_ax2_fused_1) % T.int64(616) // T.int64(77))
                                    v2 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_fused_0 * T.int64(154) + ax0_ax1_ax2_fused_1) % T.int64(77))
                                    T.reads(rxplaceholder_1[v0, v1, v2])
                                    T.writes(rxplaceholder_shared_1[v0, v1, v2])
                                    rxplaceholder_shared_1[v0, v1, v2] = rxplaceholder_1[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(4), T.int64(1), T.int64(7), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(12), i0_3 + i0_4 + i0_0_i1_0_i2_0_fused // T.int64(11) * T.int64(4) + i0_1_i1_1_i2_1_fused * T.int64(2) + i0_2_i1_2_i2_2_fused // T.int64(77))
                                v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(11) * T.int64(7) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(77), i2_4 + i0_2_i1_2_i2_2_fused % T.int64(77) + i2_3)
                                v_k = T.axis.reduce(T.int64(64), k_0 * T.int64(8) + k_1 * T.int64(2) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[v_i0, v_i1, v_k], rxplaceholder_shared_1[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[v_i0, v_i1, v_k] * rxplaceholder_shared_1[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(7), T.int64(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(12), i0_0_i1_0_i2_0_fused // T.int64(11) * T.int64(4) + i0_1_i1_1_i2_1_fused * T.int64(2) + i0_2_i1_2_i2_2_fused // T.int64(77) + ax0)
                            v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(11) * T.int64(7) + ax1)
                            v2 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused % T.int64(77) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul10(rxplaceholder: T.Buffer((T.int64(16), T.int64(4096), T.int64(4096)), "float32"), rxplaceholder_1: T.Buffer((T.int64(16), T.int64(4096), T.int64(40)), "float32"), matmul: T.Buffer((T.int64(16), T.int64(4096), T.int64(40)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(16), T.int64(4096), T.int64(40)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int64(16), T.int64(4096), T.int64(4096)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int64(16), T.int64(4096), T.int64(40)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(5), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_4_init + i0_0_i1_0_i2_0_fused // T.int64(64) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(64) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(40), i2_4_init + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(5) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(128)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(64) * T.int64(64) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(4096), k_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) % T.int64(32))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) // T.int64(40))
                                    v2 = T.axis.spatial(T.int64(40), (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) % T.int64(40))
                                    T.reads(rxplaceholder_1[v0, v1, v2])
                                    T.writes(rxplaceholder_shared_1[v0, v1, v2])
                                    rxplaceholder_shared_1[v0, v1, v2] = rxplaceholder_1[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(5), T.int64(4), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_4 + i0_0_i1_0_i2_0_fused // T.int64(64) + i0_3)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(64) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(40), i2_4 + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(5) + i2_3)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(32) + k_1 * T.int64(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[v_i0, v_i1, v_k], rxplaceholder_shared_1[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[v_i0, v_i1, v_k] * rxplaceholder_shared_1[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(2), T.int64(5)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(64) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(40), i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(5) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul11(rxplaceholder: T.Buffer((T.int64(2), T.int64(77), T.int64(768)), "float32"), rxplaceholder_1: T.Buffer((T.int64(768), T.int64(320)), "float32"), matmul: T.Buffer((T.int64(2), T.int64(77), T.int64(320)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(77), T.int64(320)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int64(2), T.int64(77), T.int64(768)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int64(768), T.int64(320)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(35), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(176), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_4_init + i0_1_i1_1_i2_1_fused + i0_3_init)
                            v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(11) + i0_2_i1_2_i2_2_fused // T.int64(16) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(320), i2_4_init + i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(4) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(176), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1) // T.int64(264))
                                    v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(11) + (ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1) % T.int64(264) // T.int64(24))
                                    v2 = T.axis.spatial(T.int64(768), k_0 * T.int64(24) + (ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1) % T.int64(24))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(9)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(176), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(768), k_0 * T.int64(24) + (ax0_ax1_fused_0 * T.int64(176) + ax0_ax1_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(176) + ax0_ax1_fused_1) % T.int64(64))
                                    T.where(ax0_ax1_fused_0 * T.int64(176) + ax0_ax1_fused_1 < T.int64(1536))
                                    T.reads(rxplaceholder_1[v0, v1])
                                    T.writes(rxplaceholder_shared_1[v0, v1])
                                    rxplaceholder_shared_1[v0, v1] = rxplaceholder_1[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(24), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_4 + i0_1_i1_1_i2_1_fused + i0_3)
                                v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(11) + i0_2_i1_2_i2_2_fused // T.int64(16) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(320), i2_4 + i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(4) + i2_3)
                                v_k = T.axis.reduce(T.int64(768), k_0 * T.int64(24) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[v_i0, v_i1, v_k], rxplaceholder_shared_1[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[v_i0, v_i1, v_k] * rxplaceholder_shared_1[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_1_i1_1_i2_1_fused + ax0)
                            v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(11) + i0_2_i1_2_i2_2_fused // T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(4) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul13(rxplaceholder: T.Buffer((T.int64(16), T.int64(4096), T.int64(77)), "float32"), rxplaceholder_1: T.Buffer((T.int64(16), T.int64(77), T.int64(40)), "float32"), matmul: T.Buffer((T.int64(16), T.int64(4096), T.int64(40)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(16), T.int64(4096), T.int64(40)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int64(16), T.int64(4096), T.int64(77)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int64(16), T.int64(77), T.int64(40)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(5), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_4_init + i0_0_i1_0_i2_0_fused // T.int64(64) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(64) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(2) * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(40), i2_4_init + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(2) * T.int64(4) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(11)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(7)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(64) * T.int64(64) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(7))
                                    v2 = T.axis.spatial(T.int64(77), k_0 * T.int64(7) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(7))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(77), k_0 * T.int64(7) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(40))
                                    v2 = T.axis.spatial(T.int64(40), (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(40))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1 < T.int64(280))
                                    T.reads(rxplaceholder_1[v0, v1, v2])
                                    T.writes(rxplaceholder_shared_1[v0, v1, v2])
                                    rxplaceholder_shared_1[v0, v1, v2] = rxplaceholder_1[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(7), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_4 + i0_0_i1_0_i2_0_fused // T.int64(64) + i0_3)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(64) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(2) * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(40), i2_4 + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(2) * T.int64(4) + i2_3)
                                v_k = T.axis.reduce(T.int64(77), k_0 * T.int64(7) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[v_i0, v_i1, v_k], rxplaceholder_shared_1[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[v_i0, v_i1, v_k] * rxplaceholder_shared_1[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(2), T.int64(4)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(64) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(2) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(40), i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(2) * T.int64(4) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul17(rxplaceholder: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32"), rxplaceholder_1: T.Buffer((T.int64(640), T.int64(640)), "float32"), matmul: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(1024), T.int64(640)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int64(2), T.int64(1024), T.int64(640)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int64(640), T.int64(640)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(640), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_3_init * T.int64(2) + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(10) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(4) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(64) + i0_2_i1_2_i2_2_fused * T.int64(2) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(160)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(10) * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) % T.int64(64) // T.int64(4))
                                    v2 = T.axis.spatial(T.int64(640), k_0 * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) % T.int64(4))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(640), k_0 * T.int64(4) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(64))
                                    T.reads(rxplaceholder_1[v0, v1])
                                    T.writes(rxplaceholder_shared_1[v0, v1])
                                    rxplaceholder_shared_1[v0, v1] = rxplaceholder_1[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(4), T.int64(2), T.int64(1), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_3 * T.int64(2) + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(10) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(4) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(64) + i0_2_i1_2_i2_2_fused * T.int64(2) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(640), k_0 * T.int64(4) + k_1 * T.int64(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[v_i0, v_i1, v_k], rxplaceholder_shared_1[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[v_i0, v_i1, v_k] * rxplaceholder_shared_1[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(2), T.int64(4), T.int64(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(10) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(64) + i0_2_i1_2_i2_2_fused * T.int64(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul19(rxplaceholder: T.Buffer((T.int64(16), T.int64(1024), T.int64(1024)), "float32"), rxplaceholder_1: T.Buffer((T.int64(16), T.int64(1024), T.int64(80)), "float32"), matmul: T.Buffer((T.int64(16), T.int64(1024), T.int64(80)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(16), T.int64(1024), T.int64(80)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int64(16), T.int64(1024), T.int64(1024)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int64(16), T.int64(1024), T.int64(80)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(2560), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_3_init + i0_4_init + i0_0_i1_0_i2_0_fused // T.int64(320) * T.int64(2) + i0_2_i1_2_i2_2_fused // T.int64(16))
                            v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(320) // T.int64(5) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) // T.int64(8) * T.int64(8) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(80), i2_4_init + i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(8) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(128)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(320) * T.int64(2) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(320) // T.int64(5) * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) % T.int64(128) // T.int64(8))
                                    v2 = T.axis.spatial(T.int64(1024), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) % T.int64(8))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(320) * T.int64(2) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(1024), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) % T.int64(128) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(80), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) % T.int64(16))
                                    T.reads(rxplaceholder_1[v0, v1, v2])
                                    T.writes(rxplaceholder_shared_1[v0, v1, v2])
                                    rxplaceholder_shared_1[v0, v1, v2] = rxplaceholder_1[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_3 + i0_4 + i0_0_i1_0_i2_0_fused // T.int64(320) * T.int64(2) + i0_2_i1_2_i2_2_fused // T.int64(16))
                                v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(320) // T.int64(5) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) // T.int64(8) * T.int64(8) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(80), i2_4 + i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(8) + i2_3)
                                v_k = T.axis.reduce(T.int64(1024), k_0 * T.int64(8) + k_1 * T.int64(8) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[v_i0, v_i1, v_k], rxplaceholder_shared_1[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[v_i0, v_i1, v_k] * rxplaceholder_shared_1[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(8), T.int64(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(320) * T.int64(2) + i0_2_i1_2_i2_2_fused // T.int64(16) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(320) // T.int64(5) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) // T.int64(8) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(80), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(8) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul2(rxplaceholder: T.Buffer((T.int64(12), T.int64(77), T.int64(77)), "float32"), rxplaceholder_1: T.Buffer((T.int64(12), T.int64(77), T.int64(64)), "float32"), matmul: T.Buffer((T.int64(12), T.int64(77), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(12), T.int64(77), T.int64(64)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int64(12), T.int64(77), T.int64(77)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int64(12), T.int64(77), T.int64(64)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(132), thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(12), i0_4_init + i0_0_i1_0_i2_0_fused // T.int64(11) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(11) * T.int64(7) + i0_2_i1_2_i2_2_fused // T.int64(32) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(64), i2_4_init + i0_1_i1_1_i2_1_fused * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(32) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(11)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(12), i0_0_i1_0_i2_0_fused // T.int64(11))
                                    v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(11) * T.int64(7) + (ax0_ax1_ax2_fused_0 * T.int64(224) + ax0_ax1_ax2_fused_1) // T.int64(7))
                                    v2 = T.axis.spatial(T.int64(77), k_0 * T.int64(7) + (ax0_ax1_ax2_fused_0 * T.int64(224) + ax0_ax1_ax2_fused_1) % T.int64(7))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int64(224) + ax0_ax1_ax2_fused_1 < T.int64(49))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(12), i0_0_i1_0_i2_0_fused // T.int64(11))
                                    v1 = T.axis.spatial(T.int64(77), k_0 * T.int64(7) + (ax0_ax1_ax2_fused_0 * T.int64(224) + ax0_ax1_ax2_fused_1) // T.int64(64))
                                    v2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(224) + ax0_ax1_ax2_fused_1) % T.int64(64))
                                    T.reads(rxplaceholder_1[v0, v1, v2])
                                    T.writes(rxplaceholder_shared_1[v0, v1, v2])
                                    rxplaceholder_shared_1[v0, v1, v2] = rxplaceholder_1[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(7), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(12), i0_4 + i0_0_i1_0_i2_0_fused // T.int64(11) + i0_3)
                                v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(11) * T.int64(7) + i0_2_i1_2_i2_2_fused // T.int64(32) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(64), i2_4 + i0_1_i1_1_i2_1_fused * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(32) + i2_3)
                                v_k = T.axis.reduce(T.int64(77), k_0 * T.int64(7) + k_1 * T.int64(7) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[v_i0, v_i1, v_k], rxplaceholder_shared_1[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[v_i0, v_i1, v_k] * rxplaceholder_shared_1[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(12), i0_0_i1_0_i2_0_fused // T.int64(11) + ax0)
                            v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(11) * T.int64(7) + i0_2_i1_2_i2_2_fused // T.int64(32) + ax1)
                            v2 = T.axis.spatial(T.int64(64), i0_1_i1_1_i2_1_fused * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(32) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul20(rxplaceholder: T.Buffer((T.int64(2), T.int64(77), T.int64(768)), "float32"), rxplaceholder_1: T.Buffer((T.int64(768), T.int64(640)), "float32"), matmul: T.Buffer((T.int64(2), T.int64(77), T.int64(640)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(77), T.int64(640)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int64(2), T.int64(77), T.int64(768)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int64(768), T.int64(640)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(110), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(14), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_3_init + i0_4_init + i0_2_i1_2_i2_2_fused // T.int64(32))
                            v_i1 = T.axis.spatial(T.int64(77), i1_4_init + i0_0_i1_0_i2_0_fused // T.int64(10) * T.int64(7) + i0_1_i1_1_i2_1_fused // T.int64(2) + i1_3_init)
                            v_i2 = T.axis.spatial(T.int64(640), i2_4_init + i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(64) + i0_1_i1_1_i2_1_fused % T.int64(2) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(32) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(96)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(56))
                                    v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(10) * T.int64(7) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(56) // T.int64(8))
                                    v2 = T.axis.spatial(T.int64(768), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(8))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1 < T.int64(112))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(768), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(64))
                                    T.reads(rxplaceholder_1[v0, v1])
                                    T.writes(rxplaceholder_shared_1[v0, v1])
                                    rxplaceholder_shared_1[v0, v1] = rxplaceholder_1[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_3 + i0_4 + i0_2_i1_2_i2_2_fused // T.int64(32))
                                v_i1 = T.axis.spatial(T.int64(77), i1_4 + i0_0_i1_0_i2_0_fused // T.int64(10) * T.int64(7) + i0_1_i1_1_i2_1_fused // T.int64(2) + i1_3)
                                v_i2 = T.axis.spatial(T.int64(640), i2_4 + i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(64) + i0_1_i1_1_i2_1_fused % T.int64(2) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(32) + i2_3)
                                v_k = T.axis.reduce(T.int64(768), k_0 * T.int64(8) + k_1 * T.int64(8) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[v_i0, v_i1, v_k], rxplaceholder_shared_1[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[v_i0, v_i1, v_k] * rxplaceholder_shared_1[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(32) + ax0)
                            v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(10) * T.int64(7) + i0_1_i1_1_i2_1_fused // T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(64) + i0_1_i1_1_i2_1_fused % T.int64(2) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(32) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul22(rxplaceholder: T.Buffer((T.int64(16), T.int64(1024), T.int64(77)), "float32"), rxplaceholder_1: T.Buffer((T.int64(16), T.int64(77), T.int64(80)), "float32"), matmul: T.Buffer((T.int64(16), T.int64(1024), T.int64(80)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(16), T.int64(1024), T.int64(80)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int64(16), T.int64(1024), T.int64(77)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int64(16), T.int64(77), T.int64(80)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(1280), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_3_init + i0_4_init + i0_0_i1_0_i2_0_fused // T.int64(320) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(16))
                            v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(320) // T.int64(5) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) // T.int64(8) * T.int64(8) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(80), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(11)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(7)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(320) * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(112))
                                    v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(320) // T.int64(5) * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(112) // T.int64(7))
                                    v2 = T.axis.spatial(T.int64(77), k_0 * T.int64(7) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(7))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(7)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(320) * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(112))
                                    v1 = T.axis.spatial(T.int64(77), k_0 * T.int64(7) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(112) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(80), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(16))
                                    T.reads(rxplaceholder_1[v0, v1, v2])
                                    T.writes(rxplaceholder_shared_1[v0, v1, v2])
                                    rxplaceholder_shared_1[v0, v1, v2] = rxplaceholder_1[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(7), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_3 + i0_4 + i0_0_i1_0_i2_0_fused // T.int64(320) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(16))
                                v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(320) // T.int64(5) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) // T.int64(8) * T.int64(8) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(80), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(77), k_0 * T.int64(7) + k_1 * T.int64(7) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[v_i0, v_i1, v_k], rxplaceholder_shared_1[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[v_i0, v_i1, v_k] * rxplaceholder_shared_1[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(8), T.int64(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(320) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(16) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(320) // T.int64(5) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) // T.int64(8) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(80), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul25(rxplaceholder: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32"), rxplaceholder_1: T.Buffer((T.int64(1280), T.int64(1280)), "float32"), matmul: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(256), T.int64(1280)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int64(2), T.int64(256), T.int64(1280)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int64(1280), T.int64(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_4_init + i0_1_i1_1_i2_1_fused // T.int64(4) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(20) * T.int64(16) + i0_1_i1_1_i2_1_fused % T.int64(4) * T.int64(4) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(20) * T.int64(64) + i0_2_i1_2_i2_2_fused * T.int64(2) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(320)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(20) * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) % T.int64(64) // T.int64(4))
                                    v2 = T.axis.spatial(T.int64(1280), k_0 * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) % T.int64(4))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(4) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(20) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(64))
                                    T.reads(rxplaceholder_1[v0, v1])
                                    T.writes(rxplaceholder_shared_1[v0, v1])
                                    rxplaceholder_shared_1[v0, v1] = rxplaceholder_1[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_4 + i0_1_i1_1_i2_1_fused // T.int64(4) + i0_3)
                                v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(20) * T.int64(16) + i0_1_i1_1_i2_1_fused % T.int64(4) * T.int64(4) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(20) * T.int64(64) + i0_2_i1_2_i2_2_fused * T.int64(2) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(4) + k_1 * T.int64(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[v_i0, v_i1, v_k], rxplaceholder_shared_1[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[v_i0, v_i1, v_k] * rxplaceholder_shared_1[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(4), T.int64(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_1_i1_1_i2_1_fused // T.int64(4) + ax0)
                            v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(20) * T.int64(16) + i0_1_i1_1_i2_1_fused % T.int64(4) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(20) * T.int64(64) + i0_2_i1_2_i2_2_fused * T.int64(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul27(rxplaceholder: T.Buffer((T.int64(16), T.int64(256), T.int64(256)), "float32"), rxplaceholder_1: T.Buffer((T.int64(16), T.int64(256), T.int64(160)), "float32"), matmul: T.Buffer((T.int64(16), T.int64(256), T.int64(160)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(16), T.int64(256), T.int64(160)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int64(16), T.int64(256), T.int64(256)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int64(16), T.int64(256), T.int64(160)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(160), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_4_init + i0_0_i1_0_i2_0_fused // T.int64(10) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(10) // T.int64(5) * T.int64(128) + i0_1_i1_1_i2_1_fused * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(8) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(2) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(10))
                                    v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(10) // T.int64(5) * T.int64(128) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(256), k_0 * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) % T.int64(16))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(10))
                                    v1 = T.axis.spatial(T.int64(256), k_0 * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) % T.int64(32))
                                    T.reads(rxplaceholder_1[v0, v1, v2])
                                    T.writes(rxplaceholder_shared_1[v0, v1, v2])
                                    rxplaceholder_shared_1[v0, v1, v2] = rxplaceholder_1[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(8), T.int64(1), T.int64(4), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_4 + i0_0_i1_0_i2_0_fused // T.int64(10) + i0_3)
                                v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(10) // T.int64(5) * T.int64(128) + i0_1_i1_1_i2_1_fused * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(8) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(2) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(256), k_0 * T.int64(16) + k_1 * T.int64(2) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[v_i0, v_i1, v_k], rxplaceholder_shared_1[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[v_i0, v_i1, v_k] * rxplaceholder_shared_1[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(8), T.int64(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(10) + ax0)
                            v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(10) // T.int64(5) * T.int64(128) + i0_1_i1_1_i2_1_fused * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul28(rxplaceholder: T.Buffer((T.int64(2), T.int64(77), T.int64(768)), "float32"), rxplaceholder_1: T.Buffer((T.int64(768), T.int64(1280)), "float32"), matmul: T.Buffer((T.int64(2), T.int64(77), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(77), T.int64(1280)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int64(2), T.int64(77), T.int64(768)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int64(768), T.int64(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(44), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_3_init + i0_4_init + i0_2_i1_2_i2_2_fused // T.int64(112))
                            v_i1 = T.axis.spatial(T.int64(77), i0_1_i1_1_i2_1_fused // T.int64(4) * T.int64(7) + i0_2_i1_2_i2_2_fused % T.int64(112) // T.int64(16) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(1280), i2_4_init + i0_0_i1_0_i2_0_fused * T.int64(64) + i0_1_i1_1_i2_1_fused % T.int64(4) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(17)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(224) + ax0_ax1_ax2_fused_1) // T.int64(1848))
                                    v1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_fused_0 * T.int64(224) + ax0_ax1_ax2_fused_1) % T.int64(1848) // T.int64(24))
                                    v2 = T.axis.spatial(T.int64(768), k_0 * T.int64(24) + (ax0_ax1_ax2_fused_0 * T.int64(224) + ax0_ax1_ax2_fused_1) % T.int64(24))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int64(224) + ax0_ax1_ax2_fused_1 < T.int64(3696))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(7)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(768), k_0 * T.int64(24) + (ax0_ax1_fused_0 * T.int64(224) + ax0_ax1_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(224) + ax0_ax1_fused_1) % T.int64(64))
                                    T.where(ax0_ax1_fused_0 * T.int64(224) + ax0_ax1_fused_1 < T.int64(1536))
                                    T.reads(rxplaceholder_1[v0, v1])
                                    T.writes(rxplaceholder_shared_1[v0, v1])
                                    rxplaceholder_shared_1[v0, v1] = rxplaceholder_1[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_3 + i0_4 + i0_2_i1_2_i2_2_fused // T.int64(112))
                                v_i1 = T.axis.spatial(T.int64(77), i0_1_i1_1_i2_1_fused // T.int64(4) * T.int64(7) + i0_2_i1_2_i2_2_fused % T.int64(112) // T.int64(16) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(1280), i2_4 + i0_0_i1_0_i2_0_fused * T.int64(64) + i0_1_i1_1_i2_1_fused % T.int64(4) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) + i2_3)
                                v_k = T.axis.reduce(T.int64(768), k_0 * T.int64(24) + k_1 * T.int64(3) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[v_i0, v_i1, v_k], rxplaceholder_shared_1[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[v_i0, v_i1, v_k] * rxplaceholder_shared_1[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(112) + ax0)
                            v1 = T.axis.spatial(T.int64(77), i0_1_i1_1_i2_1_fused // T.int64(4) * T.int64(7) + i0_2_i1_2_i2_2_fused % T.int64(112) // T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused * T.int64(64) + i0_1_i1_1_i2_1_fused % T.int64(4) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul30(rxplaceholder: T.Buffer((T.int64(16), T.int64(256), T.int64(77)), "float32"), rxplaceholder_1: T.Buffer((T.int64(16), T.int64(77), T.int64(160)), "float32"), matmul: T.Buffer((T.int64(16), T.int64(256), T.int64(160)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(16), T.int64(256), T.int64(160)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int64(16), T.int64(256), T.int64(77)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int64(16), T.int64(77), T.int64(160)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(4)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_3_init + i0_4_init + i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(16))
                            v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(80) // T.int64(10) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(16) // T.int64(2) * T.int64(4) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(2) * T.int64(4) + i2_3_init * T.int64(4) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(7)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(22)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(352))
                                    v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(80) // T.int64(10) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(352) // T.int64(11))
                                    v2 = T.axis.spatial(T.int64(77), k_0 * T.int64(11) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(11))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(11)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(176))
                                    v1 = T.axis.spatial(T.int64(77), k_0 * T.int64(11) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(176) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(16))
                                    T.reads(rxplaceholder_1[v0, v1, v2])
                                    T.writes(rxplaceholder_shared_1[v0, v1, v2])
                                    rxplaceholder_shared_1[v0, v1, v2] = rxplaceholder_1[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(11), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(4)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_3 + i0_4 + i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(16))
                                v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(80) // T.int64(10) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(16) // T.int64(2) * T.int64(4) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(2) * T.int64(4) + i2_3 * T.int64(4) + i2_4)
                                v_k = T.axis.reduce(T.int64(77), k_0 * T.int64(11) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[v_i0, v_i1, v_k], rxplaceholder_shared_1[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[v_i0, v_i1, v_k] * rxplaceholder_shared_1[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(16) + ax0)
                            v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(80) // T.int64(10) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(16) // T.int64(2) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(2) * T.int64(4) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul33(rxplaceholder: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32"), rxplaceholder_1: T.Buffer((T.int64(1280), T.int64(1280)), "float32"), matmul: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(64), T.int64(1280)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int64(2), T.int64(64), T.int64(1280)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int64(1280), T.int64(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(160), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_4_init + i0_0_i1_0_i2_0_fused // T.int64(80) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(80) // T.int64(40) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(8) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(40) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(2) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(320)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(80))
                                    v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(80) // T.int64(40) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(4))
                                    v2 = T.axis.spatial(T.int64(1280), k_0 * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(4))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(4) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(40) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(32))
                                    T.reads(rxplaceholder_1[v0, v1])
                                    T.writes(rxplaceholder_shared_1[v0, v1])
                                    rxplaceholder_shared_1[v0, v1] = rxplaceholder_1[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(2), T.int64(1), T.int64(4), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_4 + i0_0_i1_0_i2_0_fused // T.int64(80) + i0_3)
                                v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(80) // T.int64(40) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(8) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(40) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(2) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(4) + k_1 * T.int64(2) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[v_i0, v_i1, v_k], rxplaceholder_shared_1[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[v_i0, v_i1, v_k] * rxplaceholder_shared_1[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(8), T.int64(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(80) + ax0)
                            v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(80) // T.int64(40) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(40) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul35(rxplaceholder: T.Buffer((T.int64(16), T.int64(64), T.int64(64)), "float32"), rxplaceholder_1: T.Buffer((T.int64(16), T.int64(64), T.int64(160)), "float32"), matmul: T.Buffer((T.int64(16), T.int64(64), T.int64(160)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(16), T.int64(64), T.int64(160)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int64(16), T.int64(64), T.int64(64)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int64(16), T.int64(64), T.int64(160)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(5), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_3_init + i0_4_init + i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(64))
                            v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(8) // T.int64(2) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(64) // T.int64(4) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(2) * T.int64(80) + i0_1_i1_1_i2_1_fused * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(10) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(8)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(8) // T.int64(2) * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) % T.int64(128) // T.int64(8))
                                    v2 = T.axis.spatial(T.int64(64), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) % T.int64(8))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) // T.int64(640))
                                    v1 = T.axis.spatial(T.int64(64), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) % T.int64(640) // T.int64(80))
                                    v2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(2) * T.int64(80) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) % T.int64(80))
                                    T.reads(rxplaceholder_1[v0, v1, v2])
                                    T.writes(rxplaceholder_shared_1[v0, v1, v2])
                                    rxplaceholder_shared_1[v0, v1, v2] = rxplaceholder_1[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(5), T.int64(2), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_3 + i0_4 + i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(64))
                                v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(8) // T.int64(2) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(64) // T.int64(4) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(2) * T.int64(80) + i0_1_i1_1_i2_1_fused * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(10) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(64), k_0 * T.int64(8) + k_1 * T.int64(2) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[v_i0, v_i1, v_k], rxplaceholder_shared_1[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[v_i0, v_i1, v_k] * rxplaceholder_shared_1[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(10)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(8) // T.int64(2) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(64) // T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(2) * T.int64(80) + i0_1_i1_1_i2_1_fused * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(10) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul37(rxplaceholder: T.Buffer((T.int64(16), T.int64(64), T.int64(77)), "float32"), rxplaceholder_1: T.Buffer((T.int64(16), T.int64(77), T.int64(160)), "float32"), matmul: T.Buffer((T.int64(16), T.int64(64), T.int64(160)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(16), T.int64(64), T.int64(160)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int64(16), T.int64(64), T.int64(77)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int64(16), T.int64(77), T.int64(160)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_4_init + i0_0_i1_0_i2_0_fused // T.int64(4) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(4) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(2) + i0_2_i1_2_i2_2_fused // T.int64(20) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(160), i0_2_i1_2_i2_2_fused % T.int64(20) * T.int64(8) + i2_3_init * T.int64(4) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(7)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(4))
                                    v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(4) * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(40) + ax0_ax1_ax2_fused_1) // T.int64(11))
                                    v2 = T.axis.spatial(T.int64(77), k_0 * T.int64(11) + (ax0_ax1_ax2_fused_0 * T.int64(40) + ax0_ax1_ax2_fused_1) % T.int64(11))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int64(40) + ax0_ax1_ax2_fused_1 < T.int64(176))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(44)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(4))
                                    v1 = T.axis.spatial(T.int64(77), k_0 * T.int64(11) + (ax0_ax1_ax2_fused_0 * T.int64(40) + ax0_ax1_ax2_fused_1) // T.int64(160))
                                    v2 = T.axis.spatial(T.int64(160), (ax0_ax1_ax2_fused_0 * T.int64(40) + ax0_ax1_ax2_fused_1) % T.int64(160))
                                    T.reads(rxplaceholder_1[v0, v1, v2])
                                    T.writes(rxplaceholder_shared_1[v0, v1, v2])
                                    rxplaceholder_shared_1[v0, v1, v2] = rxplaceholder_1[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(11), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_4 + i0_0_i1_0_i2_0_fused // T.int64(4) + i0_3)
                                v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(4) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(2) + i0_2_i1_2_i2_2_fused // T.int64(20) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(160), i0_2_i1_2_i2_2_fused % T.int64(20) * T.int64(8) + i2_3 * T.int64(4) + i2_4)
                                v_k = T.axis.reduce(T.int64(77), k_0 * T.int64(11) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[v_i0, v_i1, v_k], rxplaceholder_shared_1[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[v_i0, v_i1, v_k] * rxplaceholder_shared_1[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(8)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(4) + ax0)
                            v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(4) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(2) + i0_2_i1_2_i2_2_fused // T.int64(20) + ax1)
                            v2 = T.axis.spatial(T.int64(160), i0_2_i1_2_i2_2_fused % T.int64(20) * T.int64(8) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul42(rxplaceholder: T.Buffer((T.int64(1), T.int64(4096), T.int64(4096)), "float32"), rxplaceholder_1: T.Buffer((T.int64(1), T.int64(4096), T.int64(512)), "float32"), matmul: T.Buffer((T.int64(1), T.int64(4096), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(1), T.int64(4096), T.int64(512)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int64(1), T.int64(4096), T.int64(4096)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int64(1), T.int64(4096), T.int64(512)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_4_init + i0_3_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(2) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(512), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(4) + i2_3_init * T.int64(4) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(512)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(8))
                                    v2 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(8))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(64))
                                    v2 = T.axis.spatial(T.int64(512), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(64) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(64))
                                    T.reads(rxplaceholder_1[v0, v1, v2])
                                    T.writes(rxplaceholder_shared_1[v0, v1, v2])
                                    rxplaceholder_shared_1[v0, v1, v2] = rxplaceholder_1[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(4)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_4 + i0_3)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(2) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(512), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(4) + i2_3 * T.int64(4) + i2_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(8) + k_1 * T.int64(8) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[T.int64(0), v_i1, v_k], rxplaceholder_shared_1[T.int64(0), v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[T.int64(0), v_i1, v_k] * rxplaceholder_shared_1[T.int64(0), v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(2), T.int64(4)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(512), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(4) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul8(rxplaceholder: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32"), rxplaceholder_1: T.Buffer((T.int64(320), T.int64(320)), "float32"), matmul: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(4096), T.int64(320)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int64(2), T.int64(4096), T.int64(320)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int64(320), T.int64(320)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(1280), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_3_init * T.int64(2) + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(8) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(320), i2_4_init + i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(80)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) % T.int64(64) // T.int64(4))
                                    v2 = T.axis.spatial(T.int64(320), k_0 * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) % T.int64(4))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int64(320), k_0 * T.int64(4) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(64))
                                    T.reads(rxplaceholder_1[v0, v1])
                                    T.writes(rxplaceholder_shared_1[v0, v1])
                                    rxplaceholder_shared_1[v0, v1] = rxplaceholder_1[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(4), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_3 * T.int64(2) + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(8) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(320), i2_4 + i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) + i2_3)
                                v_k = T.axis.reduce(T.int64(320), k_0 * T.int64(4) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[v_i0, v_i1, v_k], rxplaceholder_shared_1[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[v_i0, v_i1, v_k] * rxplaceholder_shared_1[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(2), T.int64(8), T.int64(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def multiply18(rxplaceholder: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(5.4899806976318359) * rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func
    def reshape12(rxplaceholder: T.Buffer((T.int64(2), T.int64(320)), "float32"), T_reshape: T.Buffer((T.int64(2), T.int64(320), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(3), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(320))
                    v_ax1 = T.axis.spatial(T.int64(320), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(320))
                    v_ax2 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax3 = T.axis.spatial(T.int64(1), T.int64(0))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(640))
                    T.reads(rxplaceholder[((v_ax1 + v_ax2 + v_ax3) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 + v_ax2 + v_ax3) % T.int64(320)])
                    T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_reshape[v_ax0, v_ax1, v_ax2, v_ax3] = rxplaceholder[((v_ax1 + v_ax2 + v_ax3) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 + v_ax2 + v_ax3) % T.int64(320)]

    @T.prim_func
    def reshape22(rxplaceholder: T.Buffer((T.int64(2), T.int64(640)), "float32"), T_reshape: T.Buffer((T.int64(2), T.int64(640), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(10), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(640))
                    v_ax1 = T.axis.spatial(T.int64(640), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(640))
                    v_ax2 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax3 = T.axis.spatial(T.int64(1), T.int64(0))
                    T.reads(rxplaceholder[((v_ax1 + v_ax2 + v_ax3) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 + v_ax2 + v_ax3) % T.int64(640)])
                    T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_reshape[v_ax0, v_ax1, v_ax2, v_ax3] = rxplaceholder[((v_ax1 + v_ax2 + v_ax3) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 + v_ax2 + v_ax3) % T.int64(640)]

    @T.prim_func
    def reshape32(rxplaceholder: T.Buffer((T.int64(2), T.int64(1280)), "float32"), T_reshape: T.Buffer((T.int64(2), T.int64(1280), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(80), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(1280))
                    v_ax1 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(1280))
                    v_ax2 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax3 = T.axis.spatial(T.int64(1), T.int64(0))
                    T.reads(rxplaceholder[((v_ax1 + v_ax2 + v_ax3) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 + v_ax2 + v_ax3) % T.int64(1280)])
                    T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_reshape[v_ax0, v_ax1, v_ax2, v_ax3] = rxplaceholder[((v_ax1 + v_ax2 + v_ax3) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 + v_ax2 + v_ax3) % T.int64(1280)]

    @T.prim_func
    def resize2d(rxplaceholder: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), resize: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for i0_i1_i2_i3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for i0_i1_i2_i3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for i0_i1_i2_i3_fused_0 in range(T.int64(10)):
                    with T.block("resize"):
                        v_i0 = T.axis.spatial(T.int64(2), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) // T.int64(327680))
                        v_i1 = T.axis.spatial(T.int64(1280), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(327680) // T.int64(256))
                        v_i2 = T.axis.spatial(T.int64(16), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(256) // T.int64(16))
                        v_i3 = T.axis.spatial(T.int64(16), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(16))
                        T.reads(rxplaceholder[v_i0, v_i1, T.max(T.min(v_i2 // T.int64(2), T.int64(7)), T.int64(0)), T.max(T.min(v_i3 // T.int64(2), T.int64(7)), T.int64(0))])
                        T.writes(resize[v_i0, v_i1, v_i2, v_i3])
                        resize[v_i0, v_i1, v_i2, v_i3] = rxplaceholder[v_i0, v_i1, T.max(T.min(v_i2 // T.int64(2), T.int64(7)), T.int64(0)), T.max(T.min(v_i3 // T.int64(2), T.int64(7)), T.int64(0))]

    @T.prim_func
    def resize2d1(rxplaceholder: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), resize: T.Buffer((T.int64(2), T.int64(1280), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for i0_i1_i2_i3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for i0_i1_i2_i3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for i0_i1_i2_i3_fused_0 in range(T.int64(40)):
                    with T.block("resize"):
                        v_i0 = T.axis.spatial(T.int64(2), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) // T.int64(1310720))
                        v_i1 = T.axis.spatial(T.int64(1280), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(1310720) // T.int64(1024))
                        v_i2 = T.axis.spatial(T.int64(32), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(1024) // T.int64(32))
                        v_i3 = T.axis.spatial(T.int64(32), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(32))
                        T.reads(rxplaceholder[v_i0, v_i1, T.max(T.min(v_i2 // T.int64(2), T.int64(15)), T.int64(0)), T.max(T.min(v_i3 // T.int64(2), T.int64(15)), T.int64(0))])
                        T.writes(resize[v_i0, v_i1, v_i2, v_i3])
                        resize[v_i0, v_i1, v_i2, v_i3] = rxplaceholder[v_i0, v_i1, T.max(T.min(v_i2 // T.int64(2), T.int64(15)), T.int64(0)), T.max(T.min(v_i3 // T.int64(2), T.int64(15)), T.int64(0))]

    @T.prim_func
    def resize2d2(rxplaceholder: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), resize: T.Buffer((T.int64(2), T.int64(640), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for i0_i1_i2_i3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for i0_i1_i2_i3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for i0_i1_i2_i3_fused_0 in range(T.int64(80)):
                    with T.block("resize"):
                        v_i0 = T.axis.spatial(T.int64(2), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) // T.int64(2621440))
                        v_i1 = T.axis.spatial(T.int64(640), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(2621440) // T.int64(4096))
                        v_i2 = T.axis.spatial(T.int64(64), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(4096) // T.int64(64))
                        v_i3 = T.axis.spatial(T.int64(64), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(64))
                        T.reads(rxplaceholder[v_i0, v_i1, T.max(T.min(v_i2 // T.int64(2), T.int64(31)), T.int64(0)), T.max(T.min(v_i3 // T.int64(2), T.int64(31)), T.int64(0))])
                        T.writes(resize[v_i0, v_i1, v_i2, v_i3])
                        resize[v_i0, v_i1, v_i2, v_i3] = rxplaceholder[v_i0, v_i1, T.max(T.min(v_i2 // T.int64(2), T.int64(31)), T.int64(0)), T.max(T.min(v_i3 // T.int64(2), T.int64(31)), T.int64(0))]

    @T.prim_func
    def resize2d3(rxplaceholder: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32"), resize: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for i0_i1_i2_i3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for i0_i1_i2_i3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for i0_i1_i2_i3_fused_0 in range(T.int64(128)):
                    with T.block("resize"):
                        v_i0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_i1 = T.axis.spatial(T.int64(512), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) // T.int64(16384))
                        v_i2 = T.axis.spatial(T.int64(128), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(16384) // T.int64(128))
                        v_i3 = T.axis.spatial(T.int64(128), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(128))
                        T.reads(rxplaceholder[v_i0, v_i1, T.max(T.min(v_i2 // T.int64(2), T.int64(63)), T.int64(0)), T.max(T.min(v_i3 // T.int64(2), T.int64(63)), T.int64(0))])
                        T.writes(resize[v_i0, v_i1, v_i2, v_i3])
                        resize[v_i0, v_i1, v_i2, v_i3] = rxplaceholder[v_i0, v_i1, T.max(T.min(v_i2 // T.int64(2), T.int64(63)), T.int64(0)), T.max(T.min(v_i3 // T.int64(2), T.int64(63)), T.int64(0))]

    @T.prim_func
    def resize2d4(rxplaceholder: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32"), resize: T.Buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for i0_i1_i2_i3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for i0_i1_i2_i3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for i0_i1_i2_i3_fused_0 in range(T.int64(512)):
                    with T.block("resize"):
                        v_i0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_i1 = T.axis.spatial(T.int64(512), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) // T.int64(65536))
                        v_i2 = T.axis.spatial(T.int64(256), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(65536) // T.int64(256))
                        v_i3 = T.axis.spatial(T.int64(256), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(256))
                        T.reads(rxplaceholder[v_i0, v_i1, T.max(T.min(v_i2 // T.int64(2), T.int64(127)), T.int64(0)), T.max(T.min(v_i3 // T.int64(2), T.int64(127)), T.int64(0))])
                        T.writes(resize[v_i0, v_i1, v_i2, v_i3])
                        resize[v_i0, v_i1, v_i2, v_i3] = rxplaceholder[v_i0, v_i1, T.max(T.min(v_i2 // T.int64(2), T.int64(127)), T.int64(0)), T.max(T.min(v_i3 // T.int64(2), T.int64(127)), T.int64(0))]

    @T.prim_func
    def resize2d5(rxplaceholder: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32"), resize: T.Buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for i0_i1_i2_i3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for i0_i1_i2_i3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for i0_i1_i2_i3_fused_0 in range(T.int64(1024)):
                    with T.block("resize"):
                        v_i0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_i1 = T.axis.spatial(T.int64(256), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) // T.int64(262144))
                        v_i2 = T.axis.spatial(T.int64(512), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(262144) // T.int64(512))
                        v_i3 = T.axis.spatial(T.int64(512), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(512))
                        T.reads(rxplaceholder[v_i0, v_i1, T.max(T.min(v_i2 // T.int64(2), T.int64(255)), T.int64(0)), T.max(T.min(v_i3 // T.int64(2), T.int64(255)), T.int64(0))])
                        T.writes(resize[v_i0, v_i1, v_i2, v_i3])
                        resize[v_i0, v_i1, v_i2, v_i3] = rxplaceholder[v_i0, v_i1, T.max(T.min(v_i2 // T.int64(2), T.int64(255)), T.int64(0)), T.max(T.min(v_i3 // T.int64(2), T.int64(255)), T.int64(0))]

    @T.prim_func
    def silu(rxplaceholder: T.Buffer((T.int64(2), T.int64(1280)), "float32"), T_multiply: T.Buffer((T.int64(2), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.noalias": True})
        # with T.block("root"):
        for i0_i1_fused_0 in T.thread_binding(T.int64(80), thread="blockIdx.x"):
            for i0_i1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("compute"):
                    v_i0 = T.axis.spatial(T.int64(2), (i0_i1_fused_0 * T.int64(32) + i0_i1_fused_1) // T.int64(1280))
                    v_i1 = T.axis.spatial(T.int64(1280), (i0_i1_fused_0 * T.int64(32) + i0_i1_fused_1) % T.int64(1280))
                    T.reads(rxplaceholder[v_i0, v_i1])
                    T.writes(T_multiply[v_i0, v_i1])
                    T_multiply[v_i0, v_i1] = rxplaceholder[v_i0, v_i1] * T.sigmoid(rxplaceholder[v_i0, v_i1])

    @T.prim_func
    def softmax(rxplaceholder: T.Buffer((T.int64(12), T.int64(77), T.int64(77)), "float32"), T_softmax_norm: T.Buffer((T.int64(12), T.int64(77), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(12), T.int64(77)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(12), T.int64(77)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int64(924), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(3)):
                for ax2_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int64(12), i0_i1_fused // T.int64(77) + ax0)
                        v_i1 = T.axis.spatial(T.int64(77), i0_i1_fused % T.int64(77) + ax1)
                        v_k = T.axis.reduce(T.int64(77), ax2_0 * T.int64(32) + ax2_1)
                        T.where(ax2_0 * T.int64(32) + ax2_1 < T.int64(77))
                        T.reads(rxplaceholder[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], rxplaceholder[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(3)):
                for ax2_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int64(12), i0_i1_fused // T.int64(77) + ax0)
                        v_i1 = T.axis.spatial(T.int64(77), i0_i1_fused % T.int64(77) + ax1)
                        v_k = T.axis.reduce(T.int64(77), ax2_0 * T.int64(32) + ax2_1)
                        T.where(ax2_0 * T.int64(32) + ax2_1 < T.int64(77))
                        T.reads(rxplaceholder[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(rxplaceholder[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int64(3)):
                for i2_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int64(12), i0_i1_fused // T.int64(77))
                        v_i1 = T.axis.spatial(T.int64(77), i0_i1_fused % T.int64(77))
                        v_i2 = T.axis.spatial(T.int64(77), i2_0 * T.int64(32) + i2_1)
                        T.where(i2_0 * T.int64(32) + i2_1 < T.int64(77))
                        T.reads(rxplaceholder[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(rxplaceholder[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def softmax1(rxplaceholder: T.Buffer((T.int64(16), T.int64(4096), T.int64(4096)), "float32"), T_softmax_norm: T.Buffer((T.int64(16), T.int64(4096), T.int64(4096)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(16), T.int64(4096)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(16), T.int64(4096)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int64(65536), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(16)):
                for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(4096) + ax0)
                        v_i1 = T.axis.spatial(T.int64(4096), i0_i1_fused % T.int64(4096) + ax1)
                        v_k = T.axis.reduce(T.int64(4096), ax2_0 * T.int64(256) + ax2_1)
                        T.reads(rxplaceholder[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], rxplaceholder[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(16)):
                for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(4096) + ax0)
                        v_i1 = T.axis.spatial(T.int64(4096), i0_i1_fused % T.int64(4096) + ax1)
                        v_k = T.axis.reduce(T.int64(4096), ax2_0 * T.int64(256) + ax2_1)
                        T.reads(rxplaceholder[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(rxplaceholder[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int64(16)):
                for i2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(4096))
                        v_i1 = T.axis.spatial(T.int64(4096), i0_i1_fused % T.int64(4096))
                        v_i2 = T.axis.spatial(T.int64(4096), i2_0 * T.int64(256) + i2_1)
                        T.reads(rxplaceholder[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(rxplaceholder[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def softmax2(rxplaceholder: T.Buffer((T.int64(16), T.int64(4096), T.int64(77)), "float32"), T_softmax_norm: T.Buffer((T.int64(16), T.int64(4096), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(16), T.int64(4096)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(16), T.int64(4096)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int64(65536), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(10)):
                for ax2_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(4096) + ax0)
                        v_i1 = T.axis.spatial(T.int64(4096), i0_i1_fused % T.int64(4096) + ax1)
                        v_k = T.axis.reduce(T.int64(77), ax2_0 * T.int64(8) + ax2_1)
                        T.where(ax2_0 * T.int64(8) + ax2_1 < T.int64(77))
                        T.reads(rxplaceholder[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], rxplaceholder[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(10)):
                for ax2_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(4096) + ax0)
                        v_i1 = T.axis.spatial(T.int64(4096), i0_i1_fused % T.int64(4096) + ax1)
                        v_k = T.axis.reduce(T.int64(77), ax2_0 * T.int64(8) + ax2_1)
                        T.where(ax2_0 * T.int64(8) + ax2_1 < T.int64(77))
                        T.reads(rxplaceholder[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(rxplaceholder[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int64(10)):
                for i2_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(4096))
                        v_i1 = T.axis.spatial(T.int64(4096), i0_i1_fused % T.int64(4096))
                        v_i2 = T.axis.spatial(T.int64(77), i2_0 * T.int64(8) + i2_1)
                        T.where(i2_0 * T.int64(8) + i2_1 < T.int64(77))
                        T.reads(rxplaceholder[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(rxplaceholder[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def softmax3(rxplaceholder: T.Buffer((T.int64(16), T.int64(1024), T.int64(1024)), "float32"), T_softmax_norm: T.Buffer((T.int64(16), T.int64(1024), T.int64(1024)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(16), T.int64(1024)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(16), T.int64(1024)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int64(16384), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(16)):
                for ax2_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(1024) + ax0)
                        v_i1 = T.axis.spatial(T.int64(1024), i0_i1_fused % T.int64(1024) + ax1)
                        v_k = T.axis.reduce(T.int64(1024), ax2_0 * T.int64(64) + ax2_1)
                        T.reads(rxplaceholder[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], rxplaceholder[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(16)):
                for ax2_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(1024) + ax0)
                        v_i1 = T.axis.spatial(T.int64(1024), i0_i1_fused % T.int64(1024) + ax1)
                        v_k = T.axis.reduce(T.int64(1024), ax2_0 * T.int64(64) + ax2_1)
                        T.reads(rxplaceholder[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(rxplaceholder[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int64(16)):
                for i2_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(1024))
                        v_i1 = T.axis.spatial(T.int64(1024), i0_i1_fused % T.int64(1024))
                        v_i2 = T.axis.spatial(T.int64(1024), i2_0 * T.int64(64) + i2_1)
                        T.reads(rxplaceholder[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(rxplaceholder[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def softmax4(rxplaceholder: T.Buffer((T.int64(16), T.int64(1024), T.int64(77)), "float32"), T_softmax_norm: T.Buffer((T.int64(16), T.int64(1024), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(16), T.int64(1024)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(16), T.int64(1024)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int64(16384), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(10)):
                for ax2_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(1024) + ax0)
                        v_i1 = T.axis.spatial(T.int64(1024), i0_i1_fused % T.int64(1024) + ax1)
                        v_k = T.axis.reduce(T.int64(77), ax2_0 * T.int64(8) + ax2_1)
                        T.where(ax2_0 * T.int64(8) + ax2_1 < T.int64(77))
                        T.reads(rxplaceholder[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], rxplaceholder[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(10)):
                for ax2_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(1024) + ax0)
                        v_i1 = T.axis.spatial(T.int64(1024), i0_i1_fused % T.int64(1024) + ax1)
                        v_k = T.axis.reduce(T.int64(77), ax2_0 * T.int64(8) + ax2_1)
                        T.where(ax2_0 * T.int64(8) + ax2_1 < T.int64(77))
                        T.reads(rxplaceholder[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(rxplaceholder[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int64(10)):
                for i2_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(1024))
                        v_i1 = T.axis.spatial(T.int64(1024), i0_i1_fused % T.int64(1024))
                        v_i2 = T.axis.spatial(T.int64(77), i2_0 * T.int64(8) + i2_1)
                        T.where(i2_0 * T.int64(8) + i2_1 < T.int64(77))
                        T.reads(rxplaceholder[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(rxplaceholder[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def softmax5(rxplaceholder: T.Buffer((T.int64(16), T.int64(256), T.int64(256)), "float32"), T_softmax_norm: T.Buffer((T.int64(16), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(16), T.int64(256)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(16), T.int64(256)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int64(4096), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(8)):
                for ax2_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(256) + ax0)
                        v_i1 = T.axis.spatial(T.int64(256), i0_i1_fused % T.int64(256) + ax1)
                        v_k = T.axis.reduce(T.int64(256), ax2_0 * T.int64(32) + ax2_1)
                        T.reads(rxplaceholder[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], rxplaceholder[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(8)):
                for ax2_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(256) + ax0)
                        v_i1 = T.axis.spatial(T.int64(256), i0_i1_fused % T.int64(256) + ax1)
                        v_k = T.axis.reduce(T.int64(256), ax2_0 * T.int64(32) + ax2_1)
                        T.reads(rxplaceholder[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(rxplaceholder[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int64(8)):
                for i2_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(256))
                        v_i1 = T.axis.spatial(T.int64(256), i0_i1_fused % T.int64(256))
                        v_i2 = T.axis.spatial(T.int64(256), i2_0 * T.int64(32) + i2_1)
                        T.reads(rxplaceholder[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(rxplaceholder[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def softmax6(rxplaceholder: T.Buffer((T.int64(16), T.int64(256), T.int64(77)), "float32"), T_softmax_norm: T.Buffer((T.int64(16), T.int64(256), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(16), T.int64(256)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(16), T.int64(256)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int64(4096), thread="blockIdx.x"):
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(3)):
                for ax2_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(256) + ax0)
                        v_i1 = T.axis.spatial(T.int64(256), i0_i1_fused % T.int64(256) + ax1)
                        v_k = T.axis.reduce(T.int64(77), ax2_0 * T.int64(32) + ax2_1)
                        T.where(ax2_0 * T.int64(32) + ax2_1 < T.int64(77))
                        T.reads(rxplaceholder[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], rxplaceholder[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(3)):
                for ax2_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(256) + ax0)
                        v_i1 = T.axis.spatial(T.int64(256), i0_i1_fused % T.int64(256) + ax1)
                        v_k = T.axis.reduce(T.int64(77), ax2_0 * T.int64(32) + ax2_1)
                        T.where(ax2_0 * T.int64(32) + ax2_1 < T.int64(77))
                        T.reads(rxplaceholder[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(rxplaceholder[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int64(3)):
                for i2_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(256))
                        v_i1 = T.axis.spatial(T.int64(256), i0_i1_fused % T.int64(256))
                        v_i2 = T.axis.spatial(T.int64(77), i2_0 * T.int64(32) + i2_1)
                        T.where(i2_0 * T.int64(32) + i2_1 < T.int64(77))
                        T.reads(rxplaceholder[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(rxplaceholder[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def softmax7(rxplaceholder: T.Buffer((T.int64(16), T.int64(64), T.int64(64)), "float32"), T_softmax_norm: T.Buffer((T.int64(16), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(16), T.int64(64)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(16), T.int64(64)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x"):
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(4)):
                for ax2_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(64) + ax0)
                        v_i1 = T.axis.spatial(T.int64(64), i0_i1_fused % T.int64(64) + ax1)
                        v_k = T.axis.reduce(T.int64(64), ax2_0 * T.int64(16) + ax2_1)
                        T.reads(rxplaceholder[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], rxplaceholder[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(4)):
                for ax2_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(64) + ax0)
                        v_i1 = T.axis.spatial(T.int64(64), i0_i1_fused % T.int64(64) + ax1)
                        v_k = T.axis.reduce(T.int64(64), ax2_0 * T.int64(16) + ax2_1)
                        T.reads(rxplaceholder[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(rxplaceholder[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int64(4)):
                for i2_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(64))
                        v_i1 = T.axis.spatial(T.int64(64), i0_i1_fused % T.int64(64))
                        v_i2 = T.axis.spatial(T.int64(64), i2_0 * T.int64(16) + i2_1)
                        T.reads(rxplaceholder[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(rxplaceholder[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def softmax8(rxplaceholder: T.Buffer((T.int64(16), T.int64(64), T.int64(77)), "float32"), T_softmax_norm: T.Buffer((T.int64(16), T.int64(64), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(16), T.int64(64)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(16), T.int64(64)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(2)):
                for ax2_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(64) + ax0)
                        v_i1 = T.axis.spatial(T.int64(64), i0_i1_fused % T.int64(64) + ax1)
                        v_k = T.axis.reduce(T.int64(77), ax2_0 * T.int64(64) + ax2_1)
                        T.where(ax2_0 * T.int64(64) + ax2_1 < T.int64(77))
                        T.reads(rxplaceholder[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], rxplaceholder[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(2)):
                for ax2_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(64) + ax0)
                        v_i1 = T.axis.spatial(T.int64(64), i0_i1_fused % T.int64(64) + ax1)
                        v_k = T.axis.reduce(T.int64(77), ax2_0 * T.int64(64) + ax2_1)
                        T.where(ax2_0 * T.int64(64) + ax2_1 < T.int64(77))
                        T.reads(rxplaceholder[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(rxplaceholder[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int64(2)):
                for i2_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(64))
                        v_i1 = T.axis.spatial(T.int64(64), i0_i1_fused % T.int64(64))
                        v_i2 = T.axis.spatial(T.int64(77), i2_0 * T.int64(64) + i2_1)
                        T.where(i2_0 * T.int64(64) + i2_1 < T.int64(77))
                        T.reads(rxplaceholder[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(rxplaceholder[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def take(rxplaceholder: T.Buffer((T.int64(49408), T.int64(768)), "float32"), rxplaceholder_1: T.Buffer((T.int64(77),), "int32"), T_take: T.Buffer((T.int64(77), T.int64(768)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 8, "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(462), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("T_take"):
                    v_ax0 = T.axis.spatial(T.int64(77), (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) // T.int64(768))
                    v_ax1 = T.axis.spatial(T.int64(768), (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) % T.int64(768))
                    T.reads(rxplaceholder[rxplaceholder_1[v_ax0], v_ax1], rxplaceholder_1[v_ax0])
                    T.writes(T_take[v_ax0, v_ax1])
                    T_take[v_ax0, v_ax1] = rxplaceholder[rxplaceholder_1[v_ax0], v_ax1]

    @T.prim_func
    def transpose50(rxplaceholder: T.Buffer((T.int64(1), T.int64(3), T.int64(512), T.int64(512)), "float32"), T_transpose: T.Buffer((T.int64(1), T.int64(512), T.int64(512), T.int64(3)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(12)):
                    with T.block("T_transpose"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1536))
                        v_ax2 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1536) // T.int64(3))
                        v_ax3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                        T.reads(rxplaceholder[v_ax0, v_ax3, v_ax1, v_ax2])
                        T.writes(T_transpose[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_transpose[v_ax0, v_ax1, v_ax2, v_ax3] = rxplaceholder[v_ax0, v_ax3, v_ax1, v_ax2]

    @R.function
    def clip(inp_0: R.Tensor((1, 77), dtype="int32"), params: R.Tuple(R.Tensor((1, 77, 768), dtype="float32"), R.Tensor((49408, 768), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((77, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"))) -> R.Tensor((2, 77, 768), dtype="float32"):
        with R.dataflow():
            lv518 = R.call_tir(fused_reshape_cast_reshape1, (inp_0,), out_sinfo=R.Tensor((77,), dtype="int32"))
            lv: R.Tensor((49408, 768), dtype="float32") = params[1]
            lv3 = R.call_tir(take, (lv, lv518), out_sinfo=R.Tensor((77, 768), dtype="float32"))
            lv1: R.Tensor((77, 768), dtype="float32") = params[124]
            lv519 = R.call_tir(fused_reshape2_reshape2_add, (lv3, lv1), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2: R.Tensor((768,), dtype="float32") = params[2]
            lv3_1: R.Tensor((768,), dtype="float32") = params[3]
            lv14 = R.call_tir(layer_norm, (lv519, lv2, lv3_1), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv4: R.Tensor((768, 768), dtype="float32") = params[125]
            lv5: R.Tensor((768,), dtype="float32") = params[4]
            lv520 = R.call_tir(fused_matmul_add1_multiply, (lv14, lv4, lv5), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv6: R.Tensor((768, 768), dtype="float32") = params[126]
            lv7: R.Tensor((768,), dtype="float32") = params[5]
            lv521 = R.call_tir(fused_matmul_add1, (lv14, lv6, lv7), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv522 = R.call_tir(fused_reshape3_transpose1_reshape4_transpose2, (lv521,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv8: R.Tensor((768, 768), dtype="float32") = params[127]
            lv9: R.Tensor((768,), dtype="float32") = params[6]
            lv523 = R.call_tir(fused_matmul_add1, (lv14, lv8, lv9), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv524 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv523,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv525 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv520,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv35 = R.call_tir(matmul1, (lv525, lv522), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv526 = R.call_tir(fused_reshape5_add2_reshape6, (lv35, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv39 = R.call_tir(softmax, (lv526,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv40 = R.call_tir(matmul2, (lv39, lv524), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv527 = R.call_tir(fused_reshape7_transpose3_reshape8, (lv40,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv10: R.Tensor((768, 768), dtype="float32") = params[128]
            lv11: R.Tensor((768,), dtype="float32") = params[7]
            lv528 = R.call_tir(fused_matmul_add1_add, (lv527, lv10, lv11, lv519), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv12: R.Tensor((768,), dtype="float32") = params[8]
            lv13: R.Tensor((768,), dtype="float32") = params[9]
            lv48 = R.call_tir(layer_norm, (lv528, lv12, lv13), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv14_1: R.Tensor((768, 3072), dtype="float32") = params[129]
            lv15: R.Tensor((3072,), dtype="float32") = params[10]
            lv529 = R.call_tir(fused_matmul3_add3_multiply1_tir_sigmoid_multiply2, (lv48, lv14_1, lv15), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv16: R.Tensor((3072, 768), dtype="float32") = params[130]
            lv17: R.Tensor((768,), dtype="float32") = params[11]
            lv530 = R.call_tir(fused_matmul4_add1_add, (lv529, lv16, lv17, lv528), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv18: R.Tensor((768,), dtype="float32") = params[12]
            lv19: R.Tensor((768,), dtype="float32") = params[13]
            lv59 = R.call_tir(layer_norm, (lv530, lv18, lv19), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv20: R.Tensor((768, 768), dtype="float32") = params[131]
            lv21: R.Tensor((768,), dtype="float32") = params[14]
            lv531 = R.call_tir(fused_matmul_add1_multiply, (lv59, lv20, lv21), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv22: R.Tensor((768, 768), dtype="float32") = params[132]
            lv23: R.Tensor((768,), dtype="float32") = params[15]
            lv532 = R.call_tir(fused_matmul_add1, (lv59, lv22, lv23), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv533 = R.call_tir(fused_reshape3_transpose1_reshape4_transpose2, (lv532,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv24: R.Tensor((768, 768), dtype="float32") = params[133]
            lv25: R.Tensor((768,), dtype="float32") = params[16]
            lv534 = R.call_tir(fused_matmul_add1, (lv59, lv24, lv25), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv535 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv534,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv536 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv531,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv80 = R.call_tir(matmul1, (lv536, lv533), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv537 = R.call_tir(fused_reshape5_add2_reshape6, (lv80, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv84 = R.call_tir(softmax, (lv537,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv85 = R.call_tir(matmul2, (lv84, lv535), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv538 = R.call_tir(fused_reshape7_transpose3_reshape8, (lv85,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv26: R.Tensor((768, 768), dtype="float32") = params[134]
            lv27: R.Tensor((768,), dtype="float32") = params[17]
            lv539 = R.call_tir(fused_matmul_add1_add, (lv538, lv26, lv27, lv530), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv28: R.Tensor((768,), dtype="float32") = params[18]
            lv29: R.Tensor((768,), dtype="float32") = params[19]
            lv93 = R.call_tir(layer_norm, (lv539, lv28, lv29), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv30: R.Tensor((768, 3072), dtype="float32") = params[135]
            lv31: R.Tensor((3072,), dtype="float32") = params[20]
            lv540 = R.call_tir(fused_matmul3_add3_multiply1_tir_sigmoid_multiply2, (lv93, lv30, lv31), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv32: R.Tensor((3072, 768), dtype="float32") = params[136]
            lv33: R.Tensor((768,), dtype="float32") = params[21]
            lv541 = R.call_tir(fused_matmul4_add1_add, (lv540, lv32, lv33, lv539), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv34: R.Tensor((768,), dtype="float32") = params[22]
            lv35_1: R.Tensor((768,), dtype="float32") = params[23]
            lv104 = R.call_tir(layer_norm, (lv541, lv34, lv35_1), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv36: R.Tensor((768, 768), dtype="float32") = params[137]
            lv37: R.Tensor((768,), dtype="float32") = params[24]
            lv542 = R.call_tir(fused_matmul_add1_multiply, (lv104, lv36, lv37), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv38: R.Tensor((768, 768), dtype="float32") = params[138]
            lv39_1: R.Tensor((768,), dtype="float32") = params[25]
            lv543 = R.call_tir(fused_matmul_add1, (lv104, lv38, lv39_1), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv544 = R.call_tir(fused_reshape3_transpose1_reshape4_transpose2, (lv543,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv40_1: R.Tensor((768, 768), dtype="float32") = params[139]
            lv41: R.Tensor((768,), dtype="float32") = params[26]
            lv545 = R.call_tir(fused_matmul_add1, (lv104, lv40_1, lv41), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv546 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv545,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv547 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv542,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv125 = R.call_tir(matmul1, (lv547, lv544), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv548 = R.call_tir(fused_reshape5_add2_reshape6, (lv125, metadata["relax.expr.Constant"][2]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv129 = R.call_tir(softmax, (lv548,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv130 = R.call_tir(matmul2, (lv129, lv546), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv549 = R.call_tir(fused_reshape7_transpose3_reshape8, (lv130,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv42: R.Tensor((768, 768), dtype="float32") = params[140]
            lv43: R.Tensor((768,), dtype="float32") = params[27]
            lv550 = R.call_tir(fused_matmul_add1_add, (lv549, lv42, lv43, lv541), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv44: R.Tensor((768,), dtype="float32") = params[28]
            lv45: R.Tensor((768,), dtype="float32") = params[29]
            lv138 = R.call_tir(layer_norm, (lv550, lv44, lv45), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv46: R.Tensor((768, 3072), dtype="float32") = params[141]
            lv47: R.Tensor((3072,), dtype="float32") = params[30]
            lv551 = R.call_tir(fused_matmul3_add3_multiply1_tir_sigmoid_multiply2, (lv138, lv46, lv47), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv48_1: R.Tensor((3072, 768), dtype="float32") = params[142]
            lv49: R.Tensor((768,), dtype="float32") = params[31]
            lv552 = R.call_tir(fused_matmul4_add1_add, (lv551, lv48_1, lv49, lv550), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv50: R.Tensor((768,), dtype="float32") = params[32]
            lv51: R.Tensor((768,), dtype="float32") = params[33]
            lv149 = R.call_tir(layer_norm, (lv552, lv50, lv51), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv52: R.Tensor((768, 768), dtype="float32") = params[143]
            lv53: R.Tensor((768,), dtype="float32") = params[34]
            lv553 = R.call_tir(fused_matmul_add1_multiply, (lv149, lv52, lv53), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv54: R.Tensor((768, 768), dtype="float32") = params[144]
            lv55: R.Tensor((768,), dtype="float32") = params[35]
            lv554 = R.call_tir(fused_matmul_add1, (lv149, lv54, lv55), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv555 = R.call_tir(fused_reshape3_transpose1_reshape4_transpose2, (lv554,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv56: R.Tensor((768, 768), dtype="float32") = params[145]
            lv57: R.Tensor((768,), dtype="float32") = params[36]
            lv556 = R.call_tir(fused_matmul_add1, (lv149, lv56, lv57), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv557 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv556,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv558 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv553,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv170 = R.call_tir(matmul1, (lv558, lv555), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv559 = R.call_tir(fused_reshape5_add2_reshape6, (lv170, metadata["relax.expr.Constant"][3]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv174 = R.call_tir(softmax, (lv559,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv175 = R.call_tir(matmul2, (lv174, lv557), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv560 = R.call_tir(fused_reshape7_transpose3_reshape8, (lv175,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv58: R.Tensor((768, 768), dtype="float32") = params[146]
            lv59_1: R.Tensor((768,), dtype="float32") = params[37]
            lv561 = R.call_tir(fused_matmul_add1_add, (lv560, lv58, lv59_1, lv552), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv60: R.Tensor((768,), dtype="float32") = params[38]
            lv61: R.Tensor((768,), dtype="float32") = params[39]
            lv183 = R.call_tir(layer_norm, (lv561, lv60, lv61), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv62: R.Tensor((768, 3072), dtype="float32") = params[147]
            lv63: R.Tensor((3072,), dtype="float32") = params[40]
            lv562 = R.call_tir(fused_matmul3_add3_multiply1_tir_sigmoid_multiply2, (lv183, lv62, lv63), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv64: R.Tensor((3072, 768), dtype="float32") = params[148]
            lv65: R.Tensor((768,), dtype="float32") = params[41]
            lv563 = R.call_tir(fused_matmul4_add1_add, (lv562, lv64, lv65, lv561), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv66: R.Tensor((768,), dtype="float32") = params[42]
            lv67: R.Tensor((768,), dtype="float32") = params[43]
            lv194 = R.call_tir(layer_norm, (lv563, lv66, lv67), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv68: R.Tensor((768, 768), dtype="float32") = params[149]
            lv69: R.Tensor((768,), dtype="float32") = params[44]
            lv564 = R.call_tir(fused_matmul_add1_multiply, (lv194, lv68, lv69), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv70: R.Tensor((768, 768), dtype="float32") = params[150]
            lv71: R.Tensor((768,), dtype="float32") = params[45]
            lv565 = R.call_tir(fused_matmul_add1, (lv194, lv70, lv71), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv566 = R.call_tir(fused_reshape3_transpose1_reshape4_transpose2, (lv565,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv72: R.Tensor((768, 768), dtype="float32") = params[151]
            lv73: R.Tensor((768,), dtype="float32") = params[46]
            lv567 = R.call_tir(fused_matmul_add1, (lv194, lv72, lv73), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv568 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv567,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv569 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv564,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv215 = R.call_tir(matmul1, (lv569, lv566), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv570 = R.call_tir(fused_reshape5_add2_reshape6, (lv215, metadata["relax.expr.Constant"][4]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv219 = R.call_tir(softmax, (lv570,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv220 = R.call_tir(matmul2, (lv219, lv568), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv571 = R.call_tir(fused_reshape7_transpose3_reshape8, (lv220,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv74: R.Tensor((768, 768), dtype="float32") = params[152]
            lv75: R.Tensor((768,), dtype="float32") = params[47]
            lv572 = R.call_tir(fused_matmul_add1_add, (lv571, lv74, lv75, lv563), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv76: R.Tensor((768,), dtype="float32") = params[48]
            lv77: R.Tensor((768,), dtype="float32") = params[49]
            lv228 = R.call_tir(layer_norm, (lv572, lv76, lv77), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv78: R.Tensor((768, 3072), dtype="float32") = params[153]
            lv79: R.Tensor((3072,), dtype="float32") = params[50]
            lv573 = R.call_tir(fused_matmul3_add3_multiply1_tir_sigmoid_multiply2, (lv228, lv78, lv79), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv80_1: R.Tensor((3072, 768), dtype="float32") = params[154]
            lv81: R.Tensor((768,), dtype="float32") = params[51]
            lv574 = R.call_tir(fused_matmul4_add1_add, (lv573, lv80_1, lv81, lv572), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv82: R.Tensor((768,), dtype="float32") = params[52]
            lv83: R.Tensor((768,), dtype="float32") = params[53]
            lv239 = R.call_tir(layer_norm, (lv574, lv82, lv83), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv84_1: R.Tensor((768, 768), dtype="float32") = params[155]
            lv85_1: R.Tensor((768,), dtype="float32") = params[54]
            lv575 = R.call_tir(fused_matmul_add1_multiply, (lv239, lv84_1, lv85_1), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv86: R.Tensor((768, 768), dtype="float32") = params[156]
            lv87: R.Tensor((768,), dtype="float32") = params[55]
            lv576 = R.call_tir(fused_matmul_add1, (lv239, lv86, lv87), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv577 = R.call_tir(fused_reshape3_transpose1_reshape4_transpose2, (lv576,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv88: R.Tensor((768, 768), dtype="float32") = params[157]
            lv89: R.Tensor((768,), dtype="float32") = params[56]
            lv578 = R.call_tir(fused_matmul_add1, (lv239, lv88, lv89), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv579 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv578,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv580 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv575,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv260 = R.call_tir(matmul1, (lv580, lv577), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv581 = R.call_tir(fused_reshape5_add2_reshape6, (lv260, metadata["relax.expr.Constant"][5]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv264 = R.call_tir(softmax, (lv581,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv265 = R.call_tir(matmul2, (lv264, lv579), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv582 = R.call_tir(fused_reshape7_transpose3_reshape8, (lv265,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv90: R.Tensor((768, 768), dtype="float32") = params[158]
            lv91: R.Tensor((768,), dtype="float32") = params[57]
            lv583 = R.call_tir(fused_matmul_add1_add, (lv582, lv90, lv91, lv574), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv92: R.Tensor((768,), dtype="float32") = params[58]
            lv93_1: R.Tensor((768,), dtype="float32") = params[59]
            lv273 = R.call_tir(layer_norm, (lv583, lv92, lv93_1), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv94: R.Tensor((768, 3072), dtype="float32") = params[159]
            lv95: R.Tensor((3072,), dtype="float32") = params[60]
            lv584 = R.call_tir(fused_matmul3_add3_multiply1_tir_sigmoid_multiply2, (lv273, lv94, lv95), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv96: R.Tensor((3072, 768), dtype="float32") = params[160]
            lv97: R.Tensor((768,), dtype="float32") = params[61]
            lv585 = R.call_tir(fused_matmul4_add1_add, (lv584, lv96, lv97, lv583), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv98: R.Tensor((768,), dtype="float32") = params[62]
            lv99: R.Tensor((768,), dtype="float32") = params[63]
            lv284 = R.call_tir(layer_norm, (lv585, lv98, lv99), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv100: R.Tensor((768, 768), dtype="float32") = params[161]
            lv101: R.Tensor((768,), dtype="float32") = params[64]
            lv586 = R.call_tir(fused_matmul_add1_multiply, (lv284, lv100, lv101), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv102: R.Tensor((768, 768), dtype="float32") = params[162]
            lv103: R.Tensor((768,), dtype="float32") = params[65]
            lv587 = R.call_tir(fused_matmul_add1, (lv284, lv102, lv103), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv588 = R.call_tir(fused_reshape3_transpose1_reshape4_transpose2, (lv587,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv104_1: R.Tensor((768, 768), dtype="float32") = params[163]
            lv105: R.Tensor((768,), dtype="float32") = params[66]
            lv589 = R.call_tir(fused_matmul_add1, (lv284, lv104_1, lv105), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv590 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv589,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv591 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv586,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv305 = R.call_tir(matmul1, (lv591, lv588), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv592 = R.call_tir(fused_reshape5_add2_reshape6, (lv305, metadata["relax.expr.Constant"][6]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv309 = R.call_tir(softmax, (lv592,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv310 = R.call_tir(matmul2, (lv309, lv590), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv593 = R.call_tir(fused_reshape7_transpose3_reshape8, (lv310,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv106: R.Tensor((768, 768), dtype="float32") = params[164]
            lv107: R.Tensor((768,), dtype="float32") = params[67]
            lv594 = R.call_tir(fused_matmul_add1_add, (lv593, lv106, lv107, lv585), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv108: R.Tensor((768,), dtype="float32") = params[68]
            lv109: R.Tensor((768,), dtype="float32") = params[69]
            lv318 = R.call_tir(layer_norm, (lv594, lv108, lv109), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv110: R.Tensor((768, 3072), dtype="float32") = params[165]
            lv111: R.Tensor((3072,), dtype="float32") = params[70]
            lv595 = R.call_tir(fused_matmul3_add3_multiply1_tir_sigmoid_multiply2, (lv318, lv110, lv111), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv112: R.Tensor((3072, 768), dtype="float32") = params[166]
            lv113: R.Tensor((768,), dtype="float32") = params[71]
            lv596 = R.call_tir(fused_matmul4_add1_add, (lv595, lv112, lv113, lv594), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv114: R.Tensor((768,), dtype="float32") = params[72]
            lv115: R.Tensor((768,), dtype="float32") = params[73]
            lv329 = R.call_tir(layer_norm, (lv596, lv114, lv115), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv116: R.Tensor((768, 768), dtype="float32") = params[167]
            lv117: R.Tensor((768,), dtype="float32") = params[74]
            lv597 = R.call_tir(fused_matmul_add1_multiply, (lv329, lv116, lv117), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv118: R.Tensor((768, 768), dtype="float32") = params[168]
            lv119: R.Tensor((768,), dtype="float32") = params[75]
            lv598 = R.call_tir(fused_matmul_add1, (lv329, lv118, lv119), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv599 = R.call_tir(fused_reshape3_transpose1_reshape4_transpose2, (lv598,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv120: R.Tensor((768, 768), dtype="float32") = params[169]
            lv121: R.Tensor((768,), dtype="float32") = params[76]
            lv600 = R.call_tir(fused_matmul_add1, (lv329, lv120, lv121), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv601 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv600,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv602 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv597,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv350 = R.call_tir(matmul1, (lv602, lv599), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv603 = R.call_tir(fused_reshape5_add2_reshape6, (lv350, metadata["relax.expr.Constant"][7]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv354 = R.call_tir(softmax, (lv603,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv355 = R.call_tir(matmul2, (lv354, lv601), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv604 = R.call_tir(fused_reshape7_transpose3_reshape8, (lv355,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv122: R.Tensor((768, 768), dtype="float32") = params[170]
            lv123: R.Tensor((768,), dtype="float32") = params[77]
            lv605 = R.call_tir(fused_matmul_add1_add, (lv604, lv122, lv123, lv596), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv124: R.Tensor((768,), dtype="float32") = params[78]
            lv125_1: R.Tensor((768,), dtype="float32") = params[79]
            lv363 = R.call_tir(layer_norm, (lv605, lv124, lv125_1), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv126: R.Tensor((768, 3072), dtype="float32") = params[171]
            lv127: R.Tensor((3072,), dtype="float32") = params[80]
            lv606 = R.call_tir(fused_matmul3_add3_multiply1_tir_sigmoid_multiply2, (lv363, lv126, lv127), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv128: R.Tensor((3072, 768), dtype="float32") = params[172]
            lv129_1: R.Tensor((768,), dtype="float32") = params[81]
            lv607 = R.call_tir(fused_matmul4_add1_add, (lv606, lv128, lv129_1, lv605), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv130_1: R.Tensor((768,), dtype="float32") = params[82]
            lv131: R.Tensor((768,), dtype="float32") = params[83]
            lv374 = R.call_tir(layer_norm, (lv607, lv130_1, lv131), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv132: R.Tensor((768, 768), dtype="float32") = params[173]
            lv133: R.Tensor((768,), dtype="float32") = params[84]
            lv608 = R.call_tir(fused_matmul_add1_multiply, (lv374, lv132, lv133), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv134: R.Tensor((768, 768), dtype="float32") = params[174]
            lv135: R.Tensor((768,), dtype="float32") = params[85]
            lv609 = R.call_tir(fused_matmul_add1, (lv374, lv134, lv135), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv610 = R.call_tir(fused_reshape3_transpose1_reshape4_transpose2, (lv609,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv136: R.Tensor((768, 768), dtype="float32") = params[175]
            lv137: R.Tensor((768,), dtype="float32") = params[86]
            lv611 = R.call_tir(fused_matmul_add1, (lv374, lv136, lv137), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv612 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv611,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv613 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv608,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv395 = R.call_tir(matmul1, (lv613, lv610), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv614 = R.call_tir(fused_reshape5_add2_reshape6, (lv395, metadata["relax.expr.Constant"][8]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv399 = R.call_tir(softmax, (lv614,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv400 = R.call_tir(matmul2, (lv399, lv612), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv615 = R.call_tir(fused_reshape7_transpose3_reshape8, (lv400,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv138_1: R.Tensor((768, 768), dtype="float32") = params[176]
            lv139: R.Tensor((768,), dtype="float32") = params[87]
            lv616 = R.call_tir(fused_matmul_add1_add, (lv615, lv138_1, lv139, lv607), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv140: R.Tensor((768,), dtype="float32") = params[88]
            lv141: R.Tensor((768,), dtype="float32") = params[89]
            lv408 = R.call_tir(layer_norm, (lv616, lv140, lv141), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv142: R.Tensor((768, 3072), dtype="float32") = params[177]
            lv143: R.Tensor((3072,), dtype="float32") = params[90]
            lv617 = R.call_tir(fused_matmul3_add3_multiply1_tir_sigmoid_multiply2, (lv408, lv142, lv143), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv144: R.Tensor((3072, 768), dtype="float32") = params[178]
            lv145: R.Tensor((768,), dtype="float32") = params[91]
            lv618 = R.call_tir(fused_matmul4_add1_add, (lv617, lv144, lv145, lv616), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv146: R.Tensor((768,), dtype="float32") = params[92]
            lv147: R.Tensor((768,), dtype="float32") = params[93]
            lv419 = R.call_tir(layer_norm, (lv618, lv146, lv147), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv148: R.Tensor((768, 768), dtype="float32") = params[179]
            lv149_1: R.Tensor((768,), dtype="float32") = params[94]
            lv619 = R.call_tir(fused_matmul_add1_multiply, (lv419, lv148, lv149_1), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv150: R.Tensor((768, 768), dtype="float32") = params[180]
            lv151: R.Tensor((768,), dtype="float32") = params[95]
            lv620 = R.call_tir(fused_matmul_add1, (lv419, lv150, lv151), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv621 = R.call_tir(fused_reshape3_transpose1_reshape4_transpose2, (lv620,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv152: R.Tensor((768, 768), dtype="float32") = params[181]
            lv153: R.Tensor((768,), dtype="float32") = params[96]
            lv622 = R.call_tir(fused_matmul_add1, (lv419, lv152, lv153), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv623 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv622,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv624 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv619,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv440 = R.call_tir(matmul1, (lv624, lv621), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv625 = R.call_tir(fused_reshape5_add2_reshape6, (lv440, metadata["relax.expr.Constant"][9]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv444 = R.call_tir(softmax, (lv625,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv445 = R.call_tir(matmul2, (lv444, lv623), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv626 = R.call_tir(fused_reshape7_transpose3_reshape8, (lv445,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv154: R.Tensor((768, 768), dtype="float32") = params[182]
            lv155: R.Tensor((768,), dtype="float32") = params[97]
            lv627 = R.call_tir(fused_matmul_add1_add, (lv626, lv154, lv155, lv618), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv156: R.Tensor((768,), dtype="float32") = params[98]
            lv157: R.Tensor((768,), dtype="float32") = params[99]
            lv453 = R.call_tir(layer_norm, (lv627, lv156, lv157), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv158: R.Tensor((768, 3072), dtype="float32") = params[183]
            lv159: R.Tensor((3072,), dtype="float32") = params[100]
            lv628 = R.call_tir(fused_matmul3_add3_multiply1_tir_sigmoid_multiply2, (lv453, lv158, lv159), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv160: R.Tensor((3072, 768), dtype="float32") = params[184]
            lv161: R.Tensor((768,), dtype="float32") = params[101]
            lv629 = R.call_tir(fused_matmul4_add1_add, (lv628, lv160, lv161, lv627), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv162: R.Tensor((768,), dtype="float32") = params[102]
            lv163: R.Tensor((768,), dtype="float32") = params[103]
            lv464 = R.call_tir(layer_norm, (lv629, lv162, lv163), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv164: R.Tensor((768, 768), dtype="float32") = params[185]
            lv165: R.Tensor((768,), dtype="float32") = params[104]
            lv630 = R.call_tir(fused_matmul_add1_multiply, (lv464, lv164, lv165), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv166: R.Tensor((768, 768), dtype="float32") = params[186]
            lv167: R.Tensor((768,), dtype="float32") = params[105]
            lv631 = R.call_tir(fused_matmul_add1, (lv464, lv166, lv167), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv632 = R.call_tir(fused_reshape3_transpose1_reshape4_transpose2, (lv631,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv168: R.Tensor((768, 768), dtype="float32") = params[187]
            lv169: R.Tensor((768,), dtype="float32") = params[106]
            lv633 = R.call_tir(fused_matmul_add1, (lv464, lv168, lv169), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv634 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv633,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv635 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv630,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv485 = R.call_tir(matmul1, (lv635, lv632), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv636 = R.call_tir(fused_reshape5_add2_reshape6, (lv485, metadata["relax.expr.Constant"][10]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv489 = R.call_tir(softmax, (lv636,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv490 = R.call_tir(matmul2, (lv489, lv634), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv637 = R.call_tir(fused_reshape7_transpose3_reshape8, (lv490,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv170_1: R.Tensor((768, 768), dtype="float32") = params[188]
            lv171: R.Tensor((768,), dtype="float32") = params[107]
            lv638 = R.call_tir(fused_matmul_add1_add, (lv637, lv170_1, lv171, lv629), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv172: R.Tensor((768,), dtype="float32") = params[108]
            lv173: R.Tensor((768,), dtype="float32") = params[109]
            lv498 = R.call_tir(layer_norm, (lv638, lv172, lv173), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv174_1: R.Tensor((768, 3072), dtype="float32") = params[189]
            lv175_1: R.Tensor((3072,), dtype="float32") = params[110]
            lv639 = R.call_tir(fused_matmul3_add3_multiply1_tir_sigmoid_multiply2, (lv498, lv174_1, lv175_1), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv176: R.Tensor((3072, 768), dtype="float32") = params[190]
            lv177: R.Tensor((768,), dtype="float32") = params[111]
            lv640 = R.call_tir(fused_matmul4_add1_add, (lv639, lv176, lv177, lv638), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv178: R.Tensor((768,), dtype="float32") = params[112]
            lv179: R.Tensor((768,), dtype="float32") = params[113]
            lv509 = R.call_tir(layer_norm, (lv640, lv178, lv179), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv180: R.Tensor((768, 768), dtype="float32") = params[191]
            lv181: R.Tensor((768,), dtype="float32") = params[114]
            lv641 = R.call_tir(fused_matmul_add1_multiply, (lv509, lv180, lv181), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv182: R.Tensor((768, 768), dtype="float32") = params[192]
            lv183_1: R.Tensor((768,), dtype="float32") = params[115]
            lv642 = R.call_tir(fused_matmul_add1, (lv509, lv182, lv183_1), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv643 = R.call_tir(fused_reshape3_transpose1_reshape4_transpose2, (lv642,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv184: R.Tensor((768, 768), dtype="float32") = params[193]
            lv185: R.Tensor((768,), dtype="float32") = params[116]
            lv644 = R.call_tir(fused_matmul_add1, (lv509, lv184, lv185), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv645 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv644,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv646 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv641,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv530_1 = R.call_tir(matmul1, (lv646, lv643), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv647 = R.call_tir(fused_reshape5_add2_reshape6, (lv530_1, metadata["relax.expr.Constant"][11]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv534_1 = R.call_tir(softmax, (lv647,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv535_1 = R.call_tir(matmul2, (lv534_1, lv645), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv648 = R.call_tir(fused_reshape7_transpose3_reshape8, (lv535_1,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv186: R.Tensor((768, 768), dtype="float32") = params[194]
            lv187: R.Tensor((768,), dtype="float32") = params[117]
            lv649 = R.call_tir(fused_matmul_add1_add, (lv648, lv186, lv187, lv640), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv188: R.Tensor((768,), dtype="float32") = params[118]
            lv189: R.Tensor((768,), dtype="float32") = params[119]
            lv543_1 = R.call_tir(layer_norm, (lv649, lv188, lv189), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv190: R.Tensor((768, 3072), dtype="float32") = params[195]
            lv191: R.Tensor((3072,), dtype="float32") = params[120]
            lv650 = R.call_tir(fused_matmul3_add3_multiply1_tir_sigmoid_multiply2, (lv543_1, lv190, lv191), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv192: R.Tensor((3072, 768), dtype="float32") = params[196]
            lv193: R.Tensor((768,), dtype="float32") = params[121]
            lv651 = R.call_tir(fused_matmul4_add1_add, (lv650, lv192, lv193, lv649), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv194_1: R.Tensor((768,), dtype="float32") = params[122]
            lv195: R.Tensor((768,), dtype="float32") = params[123]
            lv554_1 = R.call_tir(layer_norm, (lv651, lv194_1, lv195), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv196: R.Tensor((1, 77, 768), dtype="float32") = params[0]
            gv = R.call_tir(concatenate, (lv196, lv554_1), out_sinfo=R.Tensor((2, 77, 768), dtype="float32"))
            R.output(gv)
        return gv

    @R.function
    def scheduler_step_0(inp_0: R.Tensor((1, 4, 64, 64), dtype="float32"), inp_1: R.Tensor((1, 4, 64, 64), dtype="float32"), inp_2: R.Tensor((), dtype="float32"), inp_3: R.Tensor((), dtype="float32"), inp_4: R.Tensor((), dtype="float32")) -> R.Tuple(R.Tensor((1, 4, 64, 64), dtype="float32")):
        with R.dataflow():
            lv2 = R.call_tir(fused_multiply5_multiply5_divide_subtract, (inp_2, inp_1, inp_3, inp_0, inp_4), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
            gv: R.Tuple(R.Tensor((1, 4, 64, 64), dtype="float32")) = (lv2,)
            R.output(gv)
        return gv

    @R.function
    def scheduler_step_1(inp_0: R.Tensor((1, 4, 64, 64), dtype="float32"), inp_1: R.Tensor((1, 4, 64, 64), dtype="float32"), inp_2: R.Tensor((), dtype="float32"), inp_3: R.Tensor((), dtype="float32"), inp_4: R.Tensor((), dtype="float32"), inp_5: R.Tensor((1, 4, 64, 64), dtype="float32")) -> R.Tuple(R.Tensor((1, 4, 64, 64), dtype="float32")):
        with R.dataflow():
            lv4 = R.call_tir(fused_add_divide1_multiply5_multiply5_divide_subtract, (inp_0, inp_5, inp_2, inp_1, inp_3, inp_4), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
            gv: R.Tuple(R.Tensor((1, 4, 64, 64), dtype="float32")) = (lv4,)
            R.output(gv)
        return gv

    @R.function
    def scheduler_step_2(inp_0: R.Tensor((1, 4, 64, 64), dtype="float32"), inp_1: R.Tensor((), dtype="float32"), inp_2: R.Tensor((), dtype="float32"), inp_3: R.Tensor((), dtype="float32"), inp_4: R.Tensor((1, 4, 64, 64), dtype="float32"), inp_5: R.Tensor((1, 4, 64, 64), dtype="float32")) -> R.Tuple(R.Tensor((1, 4, 64, 64), dtype="float32")):
        with R.dataflow():
            lv1 = R.call_tir(fused_multiply6_subtract_divide1_multiply5_multiply5_divide_subtract, (inp_5, inp_4, inp_1, inp_0, inp_2, inp_3), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
            gv: R.Tuple(R.Tensor((1, 4, 64, 64), dtype="float32")) = (lv1,)
            R.output(gv)
        return gv

    @R.function
    def scheduler_step_3(inp_0: R.Tensor((1, 4, 64, 64), dtype="float32"), inp_1: R.Tensor((), dtype="float32"), inp_2: R.Tensor((), dtype="float32"), inp_3: R.Tensor((), dtype="float32"), inp_4: R.Tensor((1, 4, 64, 64), dtype="float32"), inp_5: R.Tensor((1, 4, 64, 64), dtype="float32"), inp_6: R.Tensor((1, 4, 64, 64), dtype="float32")) -> R.Tuple(R.Tensor((1, 4, 64, 64), dtype="float32")):
        with R.dataflow():
            lv3 = R.call_tir(fused_multiply7_multiply8_subtract_multiply9_add_divide2_multiply5_multiply5_divide_subtract, (inp_6, inp_5, inp_4, inp_1, inp_0, inp_2, inp_3), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
            gv: R.Tuple(R.Tensor((1, 4, 64, 64), dtype="float32")) = (lv3,)
            R.output(gv)
        return gv

    @R.function
    def scheduler_step_4(inp_0: R.Tensor((1, 4, 64, 64), dtype="float32"), inp_1: R.Tensor((), dtype="float32"), inp_2: R.Tensor((), dtype="float32"), inp_3: R.Tensor((), dtype="float32"), inp_4: R.Tensor((1, 4, 64, 64), dtype="float32"), inp_5: R.Tensor((1, 4, 64, 64), dtype="float32"), inp_6: R.Tensor((1, 4, 64, 64), dtype="float32"), inp_7: R.Tensor((1, 4, 64, 64), dtype="float32")) -> R.Tuple(R.Tensor((1, 4, 64, 64), dtype="float32")):
        with R.dataflow():
            lv = R.call_tir(fused_multiply_multiply1_subtract_multiply2_add_multiply3_subtract_multiply4_multiply5_multiply5_divide_subtract, (inp_7, inp_6, inp_5, inp_4, inp_1, inp_0, inp_2, inp_3), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
            gv: R.Tuple(R.Tensor((1, 4, 64, 64), dtype="float32")) = (lv,)
            R.output(gv)
        return gv

    @R.function
    def unet(inp_0: R.Tensor((1, 4, 64, 64), dtype="float32"), inp_1: R.Tensor((), dtype="int32"), inp_2: R.Tensor((2, 77, 768), dtype="float32"), params: R.Tuple(R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((320, 4, 3, 3), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 3, 3), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 3, 3), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 3, 3), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 3, 3), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 1, 1), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 1, 1), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 1, 1), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 1, 1), dtype="float32"), R.Tensor((320, 320, 3, 3), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((640, 320, 3, 3), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 3, 3), dtype="float32"), R.Tensor((640, 320, 1, 1), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 3, 3), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 3, 3), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 1, 1), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 1, 1), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 1, 1), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 1, 1), dtype="float32"), R.Tensor((640, 640, 3, 3), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((1280, 640, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280, 640, 1, 1), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((1280, 2560, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280, 2560, 1, 1), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((1280, 2560, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280, 2560, 1, 1), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((1280, 2560, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280, 2560, 1, 1), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((1280, 2560, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280, 2560, 1, 1), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((1280, 2560, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280, 2560, 1, 1), dtype="float32"), R.Tensor((1920,), dtype="float32"), R.Tensor((1920,), dtype="float32"), R.Tensor((1280, 1920, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280, 1920, 1, 1), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1920,), dtype="float32"), R.Tensor((1920,), dtype="float32"), R.Tensor((640, 1920, 3, 3), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 3, 3), dtype="float32"), R.Tensor((640, 1920, 1, 1), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((640, 1280, 3, 3), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 3, 3), dtype="float32"), R.Tensor((640, 1280, 1, 1), dtype="float32"), R.Tensor((960,), dtype="float32"), R.Tensor((960,), dtype="float32"), R.Tensor((640, 960, 3, 3), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 3, 3), dtype="float32"), R.Tensor((640, 960, 1, 1), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 1, 1), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 1, 1), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 1, 1), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 1, 1), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 1, 1), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 1, 1), dtype="float32"), R.Tensor((640, 640, 3, 3), dtype="float32"), R.Tensor((960,), dtype="float32"), R.Tensor((960,), dtype="float32"), R.Tensor((320, 960, 3, 3), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 3, 3), dtype="float32"), R.Tensor((320, 960, 1, 1), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((320, 640, 3, 3), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 3, 3), dtype="float32"), R.Tensor((320, 640, 1, 1), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((320, 640, 3, 3), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 3, 3), dtype="float32"), R.Tensor((320, 640, 1, 1), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 1, 1), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 1, 1), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 1, 1), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 1, 1), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 1, 1), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 1, 1), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((4, 320, 3, 3), dtype="float32"), R.Tensor((320, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1280, 320), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((768, 320), dtype="float32"), R.Tensor((768, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 1280), dtype="float32"), R.Tensor((320, 1280), dtype="float32"), R.Tensor((1280, 320), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1280, 320), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((768, 320), dtype="float32"), R.Tensor((768, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 1280), dtype="float32"), R.Tensor((320, 1280), dtype="float32"), R.Tensor((1280, 320), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1280, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((768, 640), dtype="float32"), R.Tensor((768, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 2560), dtype="float32"), R.Tensor((640, 2560), dtype="float32"), R.Tensor((2560, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1280, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((768, 640), dtype="float32"), R.Tensor((768, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 2560), dtype="float32"), R.Tensor((640, 2560), dtype="float32"), R.Tensor((2560, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1280, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((768, 640), dtype="float32"), R.Tensor((768, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 2560), dtype="float32"), R.Tensor((640, 2560), dtype="float32"), R.Tensor((2560, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1280, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((768, 640), dtype="float32"), R.Tensor((768, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 2560), dtype="float32"), R.Tensor((640, 2560), dtype="float32"), R.Tensor((2560, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1280, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((768, 640), dtype="float32"), R.Tensor((768, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 2560), dtype="float32"), R.Tensor((640, 2560), dtype="float32"), R.Tensor((2560, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1280, 320), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((768, 320), dtype="float32"), R.Tensor((768, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 1280), dtype="float32"), R.Tensor((320, 1280), dtype="float32"), R.Tensor((1280, 320), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1280, 320), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((768, 320), dtype="float32"), R.Tensor((768, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 1280), dtype="float32"), R.Tensor((320, 1280), dtype="float32"), R.Tensor((1280, 320), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1280, 320), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((768, 320), dtype="float32"), R.Tensor((768, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 1280), dtype="float32"), R.Tensor((320, 1280), dtype="float32"), R.Tensor((1280, 320), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 4, 1, 1), dtype="float32"))) -> R.Tensor((1, 4, 64, 64), dtype="float32"):
        with R.dataflow():
            lv = R.call_tir(concatenate1, (inp_0, inp_0), out_sinfo=R.Tensor((2, 4, 64, 64), dtype="float32"))
            lv77 = R.call_tir(fused_broadcast_to_strided_slice_reshape9_cast2_multiply3_multiply4_tir_sin_tir_cos_concatenate2_strided_slice1_reshape10_strided_slice2_reshape10_concatenate2, (inp_1, metadata["relax.expr.Constant"][12]), out_sinfo=R.Tensor((2, 320), dtype="float32"))
            lv197: R.Tensor((320, 1280), dtype="float32") = params[420]
            lv198: R.Tensor((1280,), dtype="float32") = params[0]
            lv78 = R.call_tir(fused_matmul5_add4_silu, (lv77, lv197, lv198), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv199: R.Tensor((1280, 1280), dtype="float32") = params[421]
            lv200: R.Tensor((1280,), dtype="float32") = params[1]
            lv79 = R.call_tir(fused_matmul6_add4, (lv78, lv199, lv200), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv201: R.Tensor((320, 4, 3, 3), dtype="float32") = params[2]
            lv202: R.Tensor((1, 320, 1, 1), dtype="float32") = params[422]
            lv80 = R.call_tir(fused_conv2d_add5, (lv, lv201, lv202), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv203: R.Tensor((320,), dtype="float32") = params[3]
            lv204: R.Tensor((320,), dtype="float32") = params[4]
            lv81 = R.call_tir(fused_group_norm_silu1, (lv80, lv203, lv204), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv30 = R.call_tir(silu, (lv79,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv205: R.Tensor((1280, 320), dtype="float32") = params[424]
            lv206: R.Tensor((320,), dtype="float32") = params[6]
            lv82 = R.call_tir(fused_matmul7_add6_strided_slice3, (lv30, lv205, lv206), out_sinfo=R.Tensor((2, 320), dtype="float32"))
            lv35 = R.call_tir(reshape12, (lv82,), out_sinfo=R.Tensor((2, 320, 1, 1), dtype="float32"))
            lv207: R.Tensor((320, 320, 3, 3), dtype="float32") = params[5]
            lv208: R.Tensor((1, 320, 1, 1), dtype="float32") = params[423]
            lv83 = R.call_tir(fused_conv2d1_add5_add7, (lv81, lv207, lv208, lv35), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv209: R.Tensor((320,), dtype="float32") = params[7]
            lv210: R.Tensor((320,), dtype="float32") = params[8]
            lv84 = R.call_tir(fused_group_norm_silu1, (lv83, lv209, lv210), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv211: R.Tensor((320, 320, 3, 3), dtype="float32") = params[9]
            lv212: R.Tensor((1, 320, 1, 1), dtype="float32") = params[425]
            lv85 = R.call_tir(fused_conv2d1_add5_add8_divide, (lv84, lv211, lv212, lv80), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv213: R.Tensor((320,), dtype="float32") = params[17]
            lv214: R.Tensor((320,), dtype="float32") = params[18]
            lv44 = R.call_tir(group_norm1, (lv85, lv213, lv214), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv215: R.Tensor((320, 320, 1, 1), dtype="float32") = params[19]
            lv216: R.Tensor((1, 320, 1, 1), dtype="float32") = params[426]
            lv86 = R.call_tir(fused_conv2d2_add5, (lv44, lv215, lv216), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv87 = R.call_tir(fused_transpose9_reshape13, (lv86,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv217: R.Tensor((320,), dtype="float32") = params[20]
            lv218: R.Tensor((320,), dtype="float32") = params[21]
            lv50 = R.call_tir(layer_norm1, (lv87, lv217, lv218), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv219: R.Tensor((320, 320), dtype="float32") = params[427]
            lv52 = R.call_tir(matmul8, (lv50, lv219), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv220: R.Tensor((320, 320), dtype="float32") = params[428]
            lv54 = R.call_tir(matmul8, (lv50, lv220), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv221: R.Tensor((320, 320), dtype="float32") = params[429]
            lv56 = R.call_tir(matmul8, (lv50, lv221), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv88 = R.call_tir(fused_reshape14_transpose11_reshape15, (lv52,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv89 = R.call_tir(fused_reshape14_transpose11_reshape15_transpose12, (lv54,), out_sinfo=R.Tensor((16, 40, 4096), dtype="float32"))
            lv90 = R.call_tir(fused_reshape14_transpose11_reshape15, (lv56,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv91 = R.call_tir(fused_matmul9_multiply5, (lv88, lv89), out_sinfo=R.Tensor((16, 4096, 4096), dtype="float32"))
            lv69 = R.call_tir(softmax1, (lv91,), out_sinfo=R.Tensor((16, 4096, 4096), dtype="float32"))
            lv70 = R.call_tir(matmul10, (lv69, lv90), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv92 = R.call_tir(fused_reshape16_transpose13_reshape17, (lv70,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv222: R.Tensor((320, 320), dtype="float32") = params[430]
            lv223: R.Tensor((320,), dtype="float32") = params[22]
            lv93 = R.call_tir(fused_matmul8_add9_add10, (lv92, lv222, lv223, lv87), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv224: R.Tensor((320,), dtype="float32") = params[23]
            lv225: R.Tensor((320,), dtype="float32") = params[24]
            lv78_1 = R.call_tir(layer_norm1, (lv93, lv224, lv225), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv226: R.Tensor((320, 320), dtype="float32") = params[431]
            lv80_1 = R.call_tir(matmul8, (lv78_1, lv226), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv227: R.Tensor((768, 320), dtype="float32") = params[432]
            lv82_1 = R.call_tir(matmul11, (inp_2, lv227), out_sinfo=R.Tensor((2, 77, 320), dtype="float32"))
            lv228: R.Tensor((768, 320), dtype="float32") = params[433]
            lv84_1 = R.call_tir(matmul11, (inp_2, lv228), out_sinfo=R.Tensor((2, 77, 320), dtype="float32"))
            lv94 = R.call_tir(fused_reshape14_transpose11_reshape15, (lv80_1,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv95 = R.call_tir(fused_reshape18_transpose15_reshape19_transpose16, (lv82_1,), out_sinfo=R.Tensor((16, 40, 77), dtype="float32"))
            lv96 = R.call_tir(fused_reshape18_transpose15_reshape19, (lv84_1,), out_sinfo=R.Tensor((16, 77, 40), dtype="float32"))
            lv97 = R.call_tir(fused_matmul12_multiply6, (lv94, lv95), out_sinfo=R.Tensor((16, 4096, 77), dtype="float32"))
            lv97_1 = R.call_tir(softmax2, (lv97,), out_sinfo=R.Tensor((16, 4096, 77), dtype="float32"))
            lv98 = R.call_tir(matmul13, (lv97_1, lv96), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv98_1 = R.call_tir(fused_reshape16_transpose13_reshape17, (lv98,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv229: R.Tensor((320, 320), dtype="float32") = params[434]
            lv230: R.Tensor((320,), dtype="float32") = params[25]
            lv99 = R.call_tir(fused_matmul8_add9_add10, (lv98_1, lv229, lv230, lv93), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv231: R.Tensor((320,), dtype="float32") = params[26]
            lv232: R.Tensor((320,), dtype="float32") = params[27]
            lv106 = R.call_tir(layer_norm1, (lv99, lv231, lv232), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv233: R.Tensor((320, 1280), dtype="float32") = params[436]
            lv234: R.Tensor((1280,), dtype="float32") = params[29]
            lv100 = R.call_tir(fused_matmul14_add11_gelu, (lv106, lv233, lv234), out_sinfo=R.Tensor((2, 4096, 1280), dtype="float32"))
            lv235: R.Tensor((320, 1280), dtype="float32") = params[435]
            lv236: R.Tensor((1280,), dtype="float32") = params[28]
            lv101 = R.call_tir(fused_matmul14_add11_multiply7, (lv106, lv235, lv236, lv100), out_sinfo=R.Tensor((2, 4096, 1280), dtype="float32"))
            lv237: R.Tensor((1280, 320), dtype="float32") = params[437]
            lv238: R.Tensor((320,), dtype="float32") = params[30]
            lv102 = R.call_tir(fused_matmul15_add9_add10, (lv101, lv237, lv238, lv99), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv103 = R.call_tir(fused_reshape20_transpose17, (lv102,), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv239: R.Tensor((320, 320, 1, 1), dtype="float32") = params[31]
            lv240: R.Tensor((1, 320, 1, 1), dtype="float32") = params[438]
            lv104 = R.call_tir(fused_conv2d2_add5_add8, (lv103, lv239, lv240, lv85), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv241: R.Tensor((320,), dtype="float32") = params[10]
            lv242: R.Tensor((320,), dtype="float32") = params[11]
            lv105 = R.call_tir(fused_group_norm_silu1, (lv104, lv241, lv242), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv130 = R.call_tir(silu, (lv79,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv243: R.Tensor((1280, 320), dtype="float32") = params[440]
            lv244: R.Tensor((320,), dtype="float32") = params[13]
            lv106_1 = R.call_tir(fused_matmul7_add6_strided_slice3, (lv130, lv243, lv244), out_sinfo=R.Tensor((2, 320), dtype="float32"))
            lv135 = R.call_tir(reshape12, (lv106_1,), out_sinfo=R.Tensor((2, 320, 1, 1), dtype="float32"))
            lv245: R.Tensor((320, 320, 3, 3), dtype="float32") = params[12]
            lv246: R.Tensor((1, 320, 1, 1), dtype="float32") = params[439]
            lv107 = R.call_tir(fused_conv2d1_add5_add7, (lv105, lv245, lv246, lv135), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv247: R.Tensor((320,), dtype="float32") = params[14]
            lv248: R.Tensor((320,), dtype="float32") = params[15]
            lv108 = R.call_tir(fused_group_norm_silu1, (lv107, lv247, lv248), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv249: R.Tensor((320, 320, 3, 3), dtype="float32") = params[16]
            lv250: R.Tensor((1, 320, 1, 1), dtype="float32") = params[441]
            lv109 = R.call_tir(fused_conv2d1_add5_add8_divide, (lv108, lv249, lv250, lv104), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv251: R.Tensor((320,), dtype="float32") = params[32]
            lv252: R.Tensor((320,), dtype="float32") = params[33]
            lv144 = R.call_tir(group_norm1, (lv109, lv251, lv252), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv253: R.Tensor((320, 320, 1, 1), dtype="float32") = params[34]
            lv254: R.Tensor((1, 320, 1, 1), dtype="float32") = params[442]
            lv110 = R.call_tir(fused_conv2d2_add5, (lv144, lv253, lv254), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv111 = R.call_tir(fused_transpose9_reshape13, (lv110,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv255: R.Tensor((320,), dtype="float32") = params[35]
            lv256: R.Tensor((320,), dtype="float32") = params[36]
            lv150 = R.call_tir(layer_norm1, (lv111, lv255, lv256), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv257: R.Tensor((320, 320), dtype="float32") = params[443]
            lv152 = R.call_tir(matmul8, (lv150, lv257), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv258: R.Tensor((320, 320), dtype="float32") = params[444]
            lv154 = R.call_tir(matmul8, (lv150, lv258), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv259: R.Tensor((320, 320), dtype="float32") = params[445]
            lv156 = R.call_tir(matmul8, (lv150, lv259), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv112 = R.call_tir(fused_reshape14_transpose11_reshape15, (lv152,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv113 = R.call_tir(fused_reshape14_transpose11_reshape15_transpose12, (lv154,), out_sinfo=R.Tensor((16, 40, 4096), dtype="float32"))
            lv114 = R.call_tir(fused_reshape14_transpose11_reshape15, (lv156,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv115 = R.call_tir(fused_matmul9_multiply5, (lv112, lv113), out_sinfo=R.Tensor((16, 4096, 4096), dtype="float32"))
            lv169 = R.call_tir(softmax1, (lv115,), out_sinfo=R.Tensor((16, 4096, 4096), dtype="float32"))
            lv170 = R.call_tir(matmul10, (lv169, lv114), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv116 = R.call_tir(fused_reshape16_transpose13_reshape17, (lv170,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv260: R.Tensor((320, 320), dtype="float32") = params[446]
            lv261: R.Tensor((320,), dtype="float32") = params[37]
            lv117 = R.call_tir(fused_matmul8_add9_add10, (lv116, lv260, lv261, lv111), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv262: R.Tensor((320,), dtype="float32") = params[38]
            lv263: R.Tensor((320,), dtype="float32") = params[39]
            lv178 = R.call_tir(layer_norm1, (lv117, lv262, lv263), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv264: R.Tensor((320, 320), dtype="float32") = params[447]
            lv180 = R.call_tir(matmul8, (lv178, lv264), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv265: R.Tensor((768, 320), dtype="float32") = params[448]
            lv182 = R.call_tir(matmul11, (inp_2, lv265), out_sinfo=R.Tensor((2, 77, 320), dtype="float32"))
            lv266: R.Tensor((768, 320), dtype="float32") = params[449]
            lv184 = R.call_tir(matmul11, (inp_2, lv266), out_sinfo=R.Tensor((2, 77, 320), dtype="float32"))
            lv118 = R.call_tir(fused_reshape14_transpose11_reshape15, (lv180,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv119 = R.call_tir(fused_reshape18_transpose15_reshape19_transpose16, (lv182,), out_sinfo=R.Tensor((16, 40, 77), dtype="float32"))
            lv120 = R.call_tir(fused_reshape18_transpose15_reshape19, (lv184,), out_sinfo=R.Tensor((16, 77, 40), dtype="float32"))
            lv121 = R.call_tir(fused_matmul12_multiply6, (lv118, lv119), out_sinfo=R.Tensor((16, 4096, 77), dtype="float32"))
            lv197_1 = R.call_tir(softmax2, (lv121,), out_sinfo=R.Tensor((16, 4096, 77), dtype="float32"))
            lv198_1 = R.call_tir(matmul13, (lv197_1, lv120), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv122 = R.call_tir(fused_reshape16_transpose13_reshape17, (lv198_1,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv267: R.Tensor((320, 320), dtype="float32") = params[450]
            lv268: R.Tensor((320,), dtype="float32") = params[40]
            lv123 = R.call_tir(fused_matmul8_add9_add10, (lv122, lv267, lv268, lv117), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv269: R.Tensor((320,), dtype="float32") = params[41]
            lv270: R.Tensor((320,), dtype="float32") = params[42]
            lv206_1 = R.call_tir(layer_norm1, (lv123, lv269, lv270), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv271: R.Tensor((320, 1280), dtype="float32") = params[452]
            lv272: R.Tensor((1280,), dtype="float32") = params[44]
            lv124 = R.call_tir(fused_matmul14_add11_gelu, (lv206_1, lv271, lv272), out_sinfo=R.Tensor((2, 4096, 1280), dtype="float32"))
            lv273: R.Tensor((320, 1280), dtype="float32") = params[451]
            lv274: R.Tensor((1280,), dtype="float32") = params[43]
            lv125 = R.call_tir(fused_matmul14_add11_multiply7, (lv206_1, lv273, lv274, lv124), out_sinfo=R.Tensor((2, 4096, 1280), dtype="float32"))
            lv275: R.Tensor((1280, 320), dtype="float32") = params[453]
            lv276: R.Tensor((320,), dtype="float32") = params[45]
            lv126 = R.call_tir(fused_matmul15_add9_add10, (lv125, lv275, lv276, lv123), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv127 = R.call_tir(fused_reshape20_transpose17, (lv126,), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv277: R.Tensor((320, 320, 1, 1), dtype="float32") = params[46]
            lv278: R.Tensor((1, 320, 1, 1), dtype="float32") = params[454]
            lv128 = R.call_tir(fused_conv2d2_add5_add8, (lv127, lv277, lv278, lv109), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv279: R.Tensor((320, 320, 3, 3), dtype="float32") = params[47]
            lv280: R.Tensor((1, 320, 1, 1), dtype="float32") = params[455]
            lv129 = R.call_tir(fused_conv2d3_add12, (lv128, lv279, lv280), out_sinfo=R.Tensor((2, 320, 32, 32), dtype="float32"))
            lv281: R.Tensor((320,), dtype="float32") = params[48]
            lv282: R.Tensor((320,), dtype="float32") = params[49]
            lv130_1 = R.call_tir(fused_group_norm2_silu2, (lv129, lv281, lv282), out_sinfo=R.Tensor((2, 320, 32, 32), dtype="float32"))
            lv233_1 = R.call_tir(silu, (lv79,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv283: R.Tensor((1280, 640), dtype="float32") = params[456]
            lv284: R.Tensor((640,), dtype="float32") = params[51]
            lv131 = R.call_tir(fused_matmul16_add14_strided_slice4, (lv233_1, lv283, lv284), out_sinfo=R.Tensor((2, 640), dtype="float32"))
            lv238_1 = R.call_tir(reshape22, (lv131,), out_sinfo=R.Tensor((2, 640, 1, 1), dtype="float32"))
            lv285: R.Tensor((640, 320, 3, 3), dtype="float32") = params[50]
            lv286: R.Tensor((1, 640, 1, 1), dtype="float32") = params[457]
            lv132 = R.call_tir(fused_conv2d4_add13_add15, (lv130_1, lv285, lv286, lv238_1), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv287: R.Tensor((640,), dtype="float32") = params[52]
            lv288: R.Tensor((640,), dtype="float32") = params[53]
            lv133 = R.call_tir(fused_group_norm3_silu3, (lv132, lv287, lv288), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv289: R.Tensor((640, 320, 1, 1), dtype="float32") = params[55]
            lv290: R.Tensor((1, 640, 1, 1), dtype="float32") = params[459]
            lv134 = R.call_tir(fused_conv2d6_add13, (lv129, lv289, lv290), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv291: R.Tensor((640, 640, 3, 3), dtype="float32") = params[54]
            lv292: R.Tensor((1, 640, 1, 1), dtype="float32") = params[458]
            lv135_1 = R.call_tir(fused_conv2d5_add13_add16_divide1, (lv133, lv291, lv292, lv134), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv293: R.Tensor((640,), dtype="float32") = params[63]
            lv294: R.Tensor((640,), dtype="float32") = params[64]
            lv250_1 = R.call_tir(group_norm4, (lv135_1, lv293, lv294), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv295: R.Tensor((640, 640, 1, 1), dtype="float32") = params[65]
            lv296: R.Tensor((1, 640, 1, 1), dtype="float32") = params[460]
            lv136 = R.call_tir(fused_conv2d7_add13, (lv250_1, lv295, lv296), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv137 = R.call_tir(fused_transpose19_reshape23, (lv136,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv297: R.Tensor((640,), dtype="float32") = params[66]
            lv298: R.Tensor((640,), dtype="float32") = params[67]
            lv256_1 = R.call_tir(layer_norm2, (lv137, lv297, lv298), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv299: R.Tensor((640, 640), dtype="float32") = params[461]
            lv258_1 = R.call_tir(matmul17, (lv256_1, lv299), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv300: R.Tensor((640, 640), dtype="float32") = params[462]
            lv260_1 = R.call_tir(matmul17, (lv256_1, lv300), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv301: R.Tensor((640, 640), dtype="float32") = params[463]
            lv262_1 = R.call_tir(matmul17, (lv256_1, lv301), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv138 = R.call_tir(fused_reshape24_transpose21_reshape25, (lv258_1,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv139 = R.call_tir(fused_reshape24_transpose21_reshape25_transpose22, (lv260_1,), out_sinfo=R.Tensor((16, 80, 1024), dtype="float32"))
            lv140 = R.call_tir(fused_reshape24_transpose21_reshape25, (lv262_1,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv141 = R.call_tir(fused_matmul18_multiply8, (lv138, lv139), out_sinfo=R.Tensor((16, 1024, 1024), dtype="float32"))
            lv275_1 = R.call_tir(softmax3, (lv141,), out_sinfo=R.Tensor((16, 1024, 1024), dtype="float32"))
            lv276_1 = R.call_tir(matmul19, (lv275_1, lv140), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv142 = R.call_tir(fused_reshape26_transpose23_reshape27, (lv276_1,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv302: R.Tensor((640, 640), dtype="float32") = params[464]
            lv303: R.Tensor((640,), dtype="float32") = params[68]
            lv143 = R.call_tir(fused_matmul17_add17_add18, (lv142, lv302, lv303, lv137), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv304: R.Tensor((640,), dtype="float32") = params[69]
            lv305: R.Tensor((640,), dtype="float32") = params[70]
            lv284_1 = R.call_tir(layer_norm2, (lv143, lv304, lv305), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv306: R.Tensor((640, 640), dtype="float32") = params[465]
            lv286_1 = R.call_tir(matmul17, (lv284_1, lv306), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv307: R.Tensor((768, 640), dtype="float32") = params[466]
            lv288_1 = R.call_tir(matmul20, (inp_2, lv307), out_sinfo=R.Tensor((2, 77, 640), dtype="float32"))
            lv308: R.Tensor((768, 640), dtype="float32") = params[467]
            lv290_1 = R.call_tir(matmul20, (inp_2, lv308), out_sinfo=R.Tensor((2, 77, 640), dtype="float32"))
            lv144_1 = R.call_tir(fused_reshape24_transpose21_reshape25, (lv286_1,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv145 = R.call_tir(fused_reshape28_transpose25_reshape29_transpose26, (lv288_1,), out_sinfo=R.Tensor((16, 80, 77), dtype="float32"))
            lv146 = R.call_tir(fused_reshape28_transpose25_reshape29, (lv290_1,), out_sinfo=R.Tensor((16, 77, 80), dtype="float32"))
            lv147 = R.call_tir(fused_matmul21_multiply9, (lv144_1, lv145), out_sinfo=R.Tensor((16, 1024, 77), dtype="float32"))
            lv303_1 = R.call_tir(softmax4, (lv147,), out_sinfo=R.Tensor((16, 1024, 77), dtype="float32"))
            lv304_1 = R.call_tir(matmul22, (lv303_1, lv146), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv148 = R.call_tir(fused_reshape26_transpose23_reshape27, (lv304_1,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv309: R.Tensor((640, 640), dtype="float32") = params[468]
            lv310: R.Tensor((640,), dtype="float32") = params[71]
            lv149 = R.call_tir(fused_matmul17_add17_add18, (lv148, lv309, lv310, lv143), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv311: R.Tensor((640,), dtype="float32") = params[72]
            lv312: R.Tensor((640,), dtype="float32") = params[73]
            lv312_1 = R.call_tir(layer_norm2, (lv149, lv311, lv312), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv313: R.Tensor((640, 2560), dtype="float32") = params[470]
            lv314: R.Tensor((2560,), dtype="float32") = params[75]
            lv150_1 = R.call_tir(fused_matmul23_add19_gelu1, (lv312_1, lv313, lv314), out_sinfo=R.Tensor((2, 1024, 2560), dtype="float32"))
            lv315: R.Tensor((640, 2560), dtype="float32") = params[469]
            lv316: R.Tensor((2560,), dtype="float32") = params[74]
            lv151 = R.call_tir(fused_matmul23_add19_multiply10, (lv312_1, lv315, lv316, lv150_1), out_sinfo=R.Tensor((2, 1024, 2560), dtype="float32"))
            lv317: R.Tensor((2560, 640), dtype="float32") = params[471]
            lv318: R.Tensor((640,), dtype="float32") = params[76]
            lv152_1 = R.call_tir(fused_matmul24_add17_add18, (lv151, lv317, lv318, lv149), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv153 = R.call_tir(fused_reshape30_transpose29, (lv152_1,), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv319: R.Tensor((640, 640, 1, 1), dtype="float32") = params[77]
            lv320: R.Tensor((1, 640, 1, 1), dtype="float32") = params[472]
            lv154_1 = R.call_tir(fused_conv2d7_add13_add16, (lv153, lv319, lv320, lv135_1), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv321: R.Tensor((640,), dtype="float32") = params[56]
            lv322: R.Tensor((640,), dtype="float32") = params[57]
            lv155 = R.call_tir(fused_group_norm3_silu3, (lv154_1, lv321, lv322), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv336 = R.call_tir(silu, (lv79,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv323: R.Tensor((1280, 640), dtype="float32") = params[473]
            lv324: R.Tensor((640,), dtype="float32") = params[59]
            lv156_1 = R.call_tir(fused_matmul16_add14_strided_slice4, (lv336, lv323, lv324), out_sinfo=R.Tensor((2, 640), dtype="float32"))
            lv341 = R.call_tir(reshape22, (lv156_1,), out_sinfo=R.Tensor((2, 640, 1, 1), dtype="float32"))
            lv325: R.Tensor((640, 640, 3, 3), dtype="float32") = params[58]
            lv326: R.Tensor((1, 640, 1, 1), dtype="float32") = params[474]
            lv157 = R.call_tir(fused_conv2d5_add13_add15, (lv155, lv325, lv326, lv341), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv327: R.Tensor((640,), dtype="float32") = params[60]
            lv328: R.Tensor((640,), dtype="float32") = params[61]
            lv158 = R.call_tir(fused_group_norm3_silu3, (lv157, lv327, lv328), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv329: R.Tensor((640, 640, 3, 3), dtype="float32") = params[62]
            lv330: R.Tensor((1, 640, 1, 1), dtype="float32") = params[475]
            lv159 = R.call_tir(fused_conv2d5_add13_add16_divide1, (lv158, lv329, lv330, lv154_1), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv331: R.Tensor((640,), dtype="float32") = params[78]
            lv332: R.Tensor((640,), dtype="float32") = params[79]
            lv350 = R.call_tir(group_norm4, (lv159, lv331, lv332), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv333: R.Tensor((640, 640, 1, 1), dtype="float32") = params[80]
            lv334: R.Tensor((1, 640, 1, 1), dtype="float32") = params[476]
            lv160 = R.call_tir(fused_conv2d7_add13, (lv350, lv333, lv334), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv161 = R.call_tir(fused_transpose19_reshape23, (lv160,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv335: R.Tensor((640,), dtype="float32") = params[81]
            lv336_1: R.Tensor((640,), dtype="float32") = params[82]
            lv356 = R.call_tir(layer_norm2, (lv161, lv335, lv336_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv337: R.Tensor((640, 640), dtype="float32") = params[477]
            lv358 = R.call_tir(matmul17, (lv356, lv337), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv338: R.Tensor((640, 640), dtype="float32") = params[478]
            lv360 = R.call_tir(matmul17, (lv356, lv338), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv339: R.Tensor((640, 640), dtype="float32") = params[479]
            lv362 = R.call_tir(matmul17, (lv356, lv339), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv162 = R.call_tir(fused_reshape24_transpose21_reshape25, (lv358,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv163 = R.call_tir(fused_reshape24_transpose21_reshape25_transpose22, (lv360,), out_sinfo=R.Tensor((16, 80, 1024), dtype="float32"))
            lv164 = R.call_tir(fused_reshape24_transpose21_reshape25, (lv362,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv165 = R.call_tir(fused_matmul18_multiply8, (lv162, lv163), out_sinfo=R.Tensor((16, 1024, 1024), dtype="float32"))
            lv375 = R.call_tir(softmax3, (lv165,), out_sinfo=R.Tensor((16, 1024, 1024), dtype="float32"))
            lv376 = R.call_tir(matmul19, (lv375, lv164), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv166 = R.call_tir(fused_reshape26_transpose23_reshape27, (lv376,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv340: R.Tensor((640, 640), dtype="float32") = params[480]
            lv341_1: R.Tensor((640,), dtype="float32") = params[83]
            lv167 = R.call_tir(fused_matmul17_add17_add18, (lv166, lv340, lv341_1, lv161), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv342: R.Tensor((640,), dtype="float32") = params[84]
            lv343: R.Tensor((640,), dtype="float32") = params[85]
            lv384 = R.call_tir(layer_norm2, (lv167, lv342, lv343), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv344: R.Tensor((640, 640), dtype="float32") = params[481]
            lv386 = R.call_tir(matmul17, (lv384, lv344), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv345: R.Tensor((768, 640), dtype="float32") = params[482]
            lv388 = R.call_tir(matmul20, (inp_2, lv345), out_sinfo=R.Tensor((2, 77, 640), dtype="float32"))
            lv346: R.Tensor((768, 640), dtype="float32") = params[483]
            lv390 = R.call_tir(matmul20, (inp_2, lv346), out_sinfo=R.Tensor((2, 77, 640), dtype="float32"))
            lv168 = R.call_tir(fused_reshape24_transpose21_reshape25, (lv386,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv169_1 = R.call_tir(fused_reshape28_transpose25_reshape29_transpose26, (lv388,), out_sinfo=R.Tensor((16, 80, 77), dtype="float32"))
            lv170_1 = R.call_tir(fused_reshape28_transpose25_reshape29, (lv390,), out_sinfo=R.Tensor((16, 77, 80), dtype="float32"))
            lv171 = R.call_tir(fused_matmul21_multiply9, (lv168, lv169_1), out_sinfo=R.Tensor((16, 1024, 77), dtype="float32"))
            lv403 = R.call_tir(softmax4, (lv171,), out_sinfo=R.Tensor((16, 1024, 77), dtype="float32"))
            lv404 = R.call_tir(matmul22, (lv403, lv170_1), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv172 = R.call_tir(fused_reshape26_transpose23_reshape27, (lv404,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv347: R.Tensor((640, 640), dtype="float32") = params[484]
            lv348: R.Tensor((640,), dtype="float32") = params[86]
            lv173 = R.call_tir(fused_matmul17_add17_add18, (lv172, lv347, lv348, lv167), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv349: R.Tensor((640,), dtype="float32") = params[87]
            lv350_1: R.Tensor((640,), dtype="float32") = params[88]
            lv412 = R.call_tir(layer_norm2, (lv173, lv349, lv350_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv351: R.Tensor((640, 2560), dtype="float32") = params[486]
            lv352: R.Tensor((2560,), dtype="float32") = params[90]
            lv174 = R.call_tir(fused_matmul23_add19_gelu1, (lv412, lv351, lv352), out_sinfo=R.Tensor((2, 1024, 2560), dtype="float32"))
            lv353: R.Tensor((640, 2560), dtype="float32") = params[485]
            lv354: R.Tensor((2560,), dtype="float32") = params[89]
            lv175 = R.call_tir(fused_matmul23_add19_multiply10, (lv412, lv353, lv354, lv174), out_sinfo=R.Tensor((2, 1024, 2560), dtype="float32"))
            lv355: R.Tensor((2560, 640), dtype="float32") = params[487]
            lv356_1: R.Tensor((640,), dtype="float32") = params[91]
            lv176 = R.call_tir(fused_matmul24_add17_add18, (lv175, lv355, lv356_1, lv173), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv177 = R.call_tir(fused_reshape30_transpose29, (lv176,), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv357: R.Tensor((640, 640, 1, 1), dtype="float32") = params[92]
            lv358_1: R.Tensor((1, 640, 1, 1), dtype="float32") = params[488]
            lv178_1 = R.call_tir(fused_conv2d7_add13_add16, (lv177, lv357, lv358_1, lv159), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv359: R.Tensor((640, 640, 3, 3), dtype="float32") = params[93]
            lv360_1: R.Tensor((1, 640, 1, 1), dtype="float32") = params[489]
            lv179 = R.call_tir(fused_conv2d8_add20, (lv178_1, lv359, lv360_1), out_sinfo=R.Tensor((2, 640, 16, 16), dtype="float32"))
            lv361: R.Tensor((640,), dtype="float32") = params[94]
            lv362_1: R.Tensor((640,), dtype="float32") = params[95]
            lv180_1 = R.call_tir(fused_group_norm5_silu4, (lv179, lv361, lv362_1), out_sinfo=R.Tensor((2, 640, 16, 16), dtype="float32"))
            lv439 = R.call_tir(silu, (lv79,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv363: R.Tensor((1280, 1280), dtype="float32") = params[491]
            lv364: R.Tensor((1280,), dtype="float32") = params[97]
            lv181 = R.call_tir(fused_matmul6_add4_strided_slice5, (lv439, lv363, lv364), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv444 = R.call_tir(reshape32, (lv181,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv365: R.Tensor((1280, 640, 3, 3), dtype="float32") = params[96]
            lv366: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[490]
            lv182_1 = R.call_tir(fused_conv2d9_add21_add22, (lv180_1, lv365, lv366, lv444), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv367: R.Tensor((1280,), dtype="float32") = params[98]
            lv368: R.Tensor((1280,), dtype="float32") = params[99]
            lv183 = R.call_tir(fused_group_norm6_silu5, (lv182_1, lv367, lv368), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv369: R.Tensor((1280, 640, 1, 1), dtype="float32") = params[101]
            lv370: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[492]
            lv184_1 = R.call_tir(fused_conv2d11_add21, (lv179, lv369, lv370), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv371: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[100]
            lv372: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[493]
            lv185 = R.call_tir(fused_conv2d10_add21_add23_divide2, (lv183, lv371, lv372, lv184_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv373: R.Tensor((1280,), dtype="float32") = params[109]
            lv374: R.Tensor((1280,), dtype="float32") = params[110]
            lv456 = R.call_tir(group_norm7, (lv185, lv373, lv374), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv375_1: R.Tensor((1280, 1280, 1, 1), dtype="float32") = params[111]
            lv376_1: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[494]
            lv186 = R.call_tir(fused_conv2d12_add21, (lv456, lv375_1, lv376_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv187 = R.call_tir(fused_transpose30_reshape33, (lv186,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv377: R.Tensor((1280,), dtype="float32") = params[112]
            lv378: R.Tensor((1280,), dtype="float32") = params[113]
            lv462 = R.call_tir(layer_norm3, (lv187, lv377, lv378), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv379: R.Tensor((1280, 1280), dtype="float32") = params[495]
            lv464 = R.call_tir(matmul25, (lv462, lv379), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv380: R.Tensor((1280, 1280), dtype="float32") = params[496]
            lv466 = R.call_tir(matmul25, (lv462, lv380), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv381: R.Tensor((1280, 1280), dtype="float32") = params[497]
            lv468 = R.call_tir(matmul25, (lv462, lv381), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv188 = R.call_tir(fused_reshape34_transpose31_reshape35, (lv464,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv189 = R.call_tir(fused_reshape34_transpose31_reshape35_transpose32, (lv466,), out_sinfo=R.Tensor((16, 160, 256), dtype="float32"))
            lv190 = R.call_tir(fused_reshape34_transpose31_reshape35, (lv468,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv191 = R.call_tir(fused_matmul26_multiply11, (lv188, lv189), out_sinfo=R.Tensor((16, 256, 256), dtype="float32"))
            lv481 = R.call_tir(softmax5, (lv191,), out_sinfo=R.Tensor((16, 256, 256), dtype="float32"))
            lv482 = R.call_tir(matmul27, (lv481, lv190), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv192 = R.call_tir(fused_reshape36_transpose33_reshape37, (lv482,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv382: R.Tensor((1280, 1280), dtype="float32") = params[498]
            lv383: R.Tensor((1280,), dtype="float32") = params[114]
            lv193 = R.call_tir(fused_matmul25_add24_add25, (lv192, lv382, lv383, lv187), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv384_1: R.Tensor((1280,), dtype="float32") = params[115]
            lv385: R.Tensor((1280,), dtype="float32") = params[116]
            lv490 = R.call_tir(layer_norm3, (lv193, lv384_1, lv385), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv386_1: R.Tensor((1280, 1280), dtype="float32") = params[499]
            lv492 = R.call_tir(matmul25, (lv490, lv386_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv387: R.Tensor((768, 1280), dtype="float32") = params[500]
            lv494 = R.call_tir(matmul28, (inp_2, lv387), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv388_1: R.Tensor((768, 1280), dtype="float32") = params[501]
            lv496 = R.call_tir(matmul28, (inp_2, lv388_1), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv194 = R.call_tir(fused_reshape34_transpose31_reshape35, (lv492,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv195 = R.call_tir(fused_reshape38_transpose35_reshape39_transpose36, (lv494,), out_sinfo=R.Tensor((16, 160, 77), dtype="float32"))
            lv196 = R.call_tir(fused_reshape38_transpose35_reshape39, (lv496,), out_sinfo=R.Tensor((16, 77, 160), dtype="float32"))
            lv197_2 = R.call_tir(fused_matmul29_multiply12, (lv194, lv195), out_sinfo=R.Tensor((16, 256, 77), dtype="float32"))
            lv509 = R.call_tir(softmax6, (lv197_2,), out_sinfo=R.Tensor((16, 256, 77), dtype="float32"))
            lv510 = R.call_tir(matmul30, (lv509, lv196), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv198_2 = R.call_tir(fused_reshape36_transpose33_reshape37, (lv510,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv389: R.Tensor((1280, 1280), dtype="float32") = params[502]
            lv390_1: R.Tensor((1280,), dtype="float32") = params[117]
            lv199_1 = R.call_tir(fused_matmul25_add24_add25, (lv198_2, lv389, lv390_1, lv193), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv391: R.Tensor((1280,), dtype="float32") = params[118]
            lv392: R.Tensor((1280,), dtype="float32") = params[119]
            lv518 = R.call_tir(layer_norm3, (lv199_1, lv391, lv392), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv393: R.Tensor((1280, 5120), dtype="float32") = params[504]
            lv394: R.Tensor((5120,), dtype="float32") = params[121]
            lv200_1 = R.call_tir(fused_matmul31_add26_gelu2, (lv518, lv393, lv394), out_sinfo=R.Tensor((2, 256, 5120), dtype="float32"))
            lv395: R.Tensor((1280, 5120), dtype="float32") = params[503]
            lv396: R.Tensor((5120,), dtype="float32") = params[120]
            lv201_1 = R.call_tir(fused_matmul31_add26_multiply13, (lv518, lv395, lv396, lv200_1), out_sinfo=R.Tensor((2, 256, 5120), dtype="float32"))
            lv397: R.Tensor((5120, 1280), dtype="float32") = params[505]
            lv398: R.Tensor((1280,), dtype="float32") = params[122]
            lv202_1 = R.call_tir(fused_matmul32_add24_add25, (lv201_1, lv397, lv398, lv199_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv203_1 = R.call_tir(fused_reshape40_transpose39, (lv202_1,), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv399: R.Tensor((1280, 1280, 1, 1), dtype="float32") = params[123]
            lv400: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[506]
            lv204_1 = R.call_tir(fused_conv2d12_add21_add23, (lv203_1, lv399, lv400, lv185), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv401: R.Tensor((1280,), dtype="float32") = params[102]
            lv402: R.Tensor((1280,), dtype="float32") = params[103]
            lv205_1 = R.call_tir(fused_group_norm6_silu5, (lv204_1, lv401, lv402), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv542 = R.call_tir(silu, (lv79,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv403_1: R.Tensor((1280, 1280), dtype="float32") = params[507]
            lv404_1: R.Tensor((1280,), dtype="float32") = params[105]
            lv206_2 = R.call_tir(fused_matmul6_add4_strided_slice5, (lv542, lv403_1, lv404_1), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv547 = R.call_tir(reshape32, (lv206_2,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv405: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[104]
            lv406: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[508]
            lv207_1 = R.call_tir(fused_conv2d10_add21_add22, (lv205_1, lv405, lv406, lv547), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv407: R.Tensor((1280,), dtype="float32") = params[106]
            lv408: R.Tensor((1280,), dtype="float32") = params[107]
            lv208_1 = R.call_tir(fused_group_norm6_silu5, (lv207_1, lv407, lv408), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv409: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[108]
            lv410: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[509]
            lv209_1 = R.call_tir(fused_conv2d10_add21_add23_divide2, (lv208_1, lv409, lv410, lv204_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv411: R.Tensor((1280,), dtype="float32") = params[124]
            lv412_1: R.Tensor((1280,), dtype="float32") = params[125]
            lv556 = R.call_tir(group_norm7, (lv209_1, lv411, lv412_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv413: R.Tensor((1280, 1280, 1, 1), dtype="float32") = params[126]
            lv414: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[510]
            lv210_1 = R.call_tir(fused_conv2d12_add21, (lv556, lv413, lv414), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv211_1 = R.call_tir(fused_transpose30_reshape33, (lv210_1,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv415: R.Tensor((1280,), dtype="float32") = params[127]
            lv416: R.Tensor((1280,), dtype="float32") = params[128]
            lv562 = R.call_tir(layer_norm3, (lv211_1, lv415, lv416), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv417: R.Tensor((1280, 1280), dtype="float32") = params[511]
            lv564 = R.call_tir(matmul25, (lv562, lv417), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv418: R.Tensor((1280, 1280), dtype="float32") = params[512]
            lv566 = R.call_tir(matmul25, (lv562, lv418), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv419: R.Tensor((1280, 1280), dtype="float32") = params[513]
            lv568 = R.call_tir(matmul25, (lv562, lv419), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv212_1 = R.call_tir(fused_reshape34_transpose31_reshape35, (lv564,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv213_1 = R.call_tir(fused_reshape34_transpose31_reshape35_transpose32, (lv566,), out_sinfo=R.Tensor((16, 160, 256), dtype="float32"))
            lv214_1 = R.call_tir(fused_reshape34_transpose31_reshape35, (lv568,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv215_1 = R.call_tir(fused_matmul26_multiply11, (lv212_1, lv213_1), out_sinfo=R.Tensor((16, 256, 256), dtype="float32"))
            lv581 = R.call_tir(softmax5, (lv215_1,), out_sinfo=R.Tensor((16, 256, 256), dtype="float32"))
            lv582 = R.call_tir(matmul27, (lv581, lv214_1), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv216_1 = R.call_tir(fused_reshape36_transpose33_reshape37, (lv582,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv420: R.Tensor((1280, 1280), dtype="float32") = params[514]
            lv421: R.Tensor((1280,), dtype="float32") = params[129]
            lv217_1 = R.call_tir(fused_matmul25_add24_add25, (lv216_1, lv420, lv421, lv211_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv422: R.Tensor((1280,), dtype="float32") = params[130]
            lv423: R.Tensor((1280,), dtype="float32") = params[131]
            lv590 = R.call_tir(layer_norm3, (lv217_1, lv422, lv423), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv424: R.Tensor((1280, 1280), dtype="float32") = params[515]
            lv592 = R.call_tir(matmul25, (lv590, lv424), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv425: R.Tensor((768, 1280), dtype="float32") = params[516]
            lv594 = R.call_tir(matmul28, (inp_2, lv425), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv426: R.Tensor((768, 1280), dtype="float32") = params[517]
            lv596 = R.call_tir(matmul28, (inp_2, lv426), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv218_1 = R.call_tir(fused_reshape34_transpose31_reshape35, (lv592,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv219_1 = R.call_tir(fused_reshape38_transpose35_reshape39_transpose36, (lv594,), out_sinfo=R.Tensor((16, 160, 77), dtype="float32"))
            lv220_1 = R.call_tir(fused_reshape38_transpose35_reshape39, (lv596,), out_sinfo=R.Tensor((16, 77, 160), dtype="float32"))
            lv221_1 = R.call_tir(fused_matmul29_multiply12, (lv218_1, lv219_1), out_sinfo=R.Tensor((16, 256, 77), dtype="float32"))
            lv609 = R.call_tir(softmax6, (lv221_1,), out_sinfo=R.Tensor((16, 256, 77), dtype="float32"))
            lv610 = R.call_tir(matmul30, (lv609, lv220_1), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv222_1 = R.call_tir(fused_reshape36_transpose33_reshape37, (lv610,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv427: R.Tensor((1280, 1280), dtype="float32") = params[518]
            lv428: R.Tensor((1280,), dtype="float32") = params[132]
            lv223_1 = R.call_tir(fused_matmul25_add24_add25, (lv222_1, lv427, lv428, lv217_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv429: R.Tensor((1280,), dtype="float32") = params[133]
            lv430: R.Tensor((1280,), dtype="float32") = params[134]
            lv618 = R.call_tir(layer_norm3, (lv223_1, lv429, lv430), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv431: R.Tensor((1280, 5120), dtype="float32") = params[520]
            lv432: R.Tensor((5120,), dtype="float32") = params[136]
            lv224_1 = R.call_tir(fused_matmul31_add26_gelu2, (lv618, lv431, lv432), out_sinfo=R.Tensor((2, 256, 5120), dtype="float32"))
            lv433: R.Tensor((1280, 5120), dtype="float32") = params[519]
            lv434: R.Tensor((5120,), dtype="float32") = params[135]
            lv225_1 = R.call_tir(fused_matmul31_add26_multiply13, (lv618, lv433, lv434, lv224_1), out_sinfo=R.Tensor((2, 256, 5120), dtype="float32"))
            lv435: R.Tensor((5120, 1280), dtype="float32") = params[521]
            lv436: R.Tensor((1280,), dtype="float32") = params[137]
            lv226_1 = R.call_tir(fused_matmul32_add24_add25, (lv225_1, lv435, lv436, lv223_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv227_1 = R.call_tir(fused_reshape40_transpose39, (lv226_1,), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv437: R.Tensor((1280, 1280, 1, 1), dtype="float32") = params[138]
            lv438: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[522]
            lv228_1 = R.call_tir(fused_conv2d12_add21_add23, (lv227_1, lv437, lv438, lv209_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv439_1: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[139]
            lv440: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[523]
            lv229_1 = R.call_tir(fused_conv2d13_add27, (lv228_1, lv439_1, lv440), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv441: R.Tensor((1280,), dtype="float32") = params[140]
            lv442: R.Tensor((1280,), dtype="float32") = params[141]
            lv230_1 = R.call_tir(fused_group_norm8_silu6, (lv229_1, lv441, lv442), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv645 = R.call_tir(silu, (lv79,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv443: R.Tensor((1280, 1280), dtype="float32") = params[524]
            lv444_1: R.Tensor((1280,), dtype="float32") = params[143]
            lv231_1 = R.call_tir(fused_matmul6_add4_strided_slice5, (lv645, lv443, lv444_1), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv650 = R.call_tir(reshape32, (lv231_1,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv445: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[142]
            lv446: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[525]
            lv232_1 = R.call_tir(fused_conv2d14_add27_add28, (lv230_1, lv445, lv446, lv650), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv447: R.Tensor((1280,), dtype="float32") = params[144]
            lv448: R.Tensor((1280,), dtype="float32") = params[145]
            lv233_2 = R.call_tir(fused_group_norm8_silu6, (lv232_1, lv447, lv448), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv449: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[146]
            lv450: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[526]
            lv234_1 = R.call_tir(fused_conv2d14_add27_add29_divide3, (lv233_2, lv449, lv450, lv229_1), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv451: R.Tensor((1280,), dtype="float32") = params[147]
            lv452: R.Tensor((1280,), dtype="float32") = params[148]
            lv235_1 = R.call_tir(fused_group_norm8_silu6, (lv234_1, lv451, lv452), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv664 = R.call_tir(silu, (lv79,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv453: R.Tensor((1280, 1280), dtype="float32") = params[527]
            lv454: R.Tensor((1280,), dtype="float32") = params[150]
            lv236_1 = R.call_tir(fused_matmul6_add4_strided_slice5, (lv664, lv453, lv454), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv669 = R.call_tir(reshape32, (lv236_1,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv455: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[149]
            lv456_1: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[528]
            lv237_1 = R.call_tir(fused_conv2d14_add27_add28, (lv235_1, lv455, lv456_1, lv669), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv457: R.Tensor((1280,), dtype="float32") = params[151]
            lv458: R.Tensor((1280,), dtype="float32") = params[152]
            lv238_2 = R.call_tir(fused_group_norm8_silu6, (lv237_1, lv457, lv458), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv459: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[153]
            lv460: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[529]
            lv239_1 = R.call_tir(fused_conv2d14_add27_add29_divide3, (lv238_2, lv459, lv460, lv234_1), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv461: R.Tensor((1280,), dtype="float32") = params[154]
            lv462_1: R.Tensor((1280,), dtype="float32") = params[155]
            lv240_1 = R.call_tir(fused_group_norm8_silu6, (lv239_1, lv461, lv462_1), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv683 = R.call_tir(silu, (lv79,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv463: R.Tensor((1280, 1280), dtype="float32") = params[530]
            lv464_1: R.Tensor((1280,), dtype="float32") = params[157]
            lv241_1 = R.call_tir(fused_matmul6_add4_strided_slice5, (lv683, lv463, lv464_1), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv688 = R.call_tir(reshape32, (lv241_1,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv465: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[156]
            lv466_1: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[531]
            lv242_1 = R.call_tir(fused_conv2d14_add27_add28, (lv240_1, lv465, lv466_1, lv688), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv467: R.Tensor((1280,), dtype="float32") = params[158]
            lv468_1: R.Tensor((1280,), dtype="float32") = params[159]
            lv243_1 = R.call_tir(fused_group_norm8_silu6, (lv242_1, lv467, lv468_1), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv469: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[160]
            lv470: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[532]
            lv244_1 = R.call_tir(fused_conv2d14_add27_add29_divide3, (lv243_1, lv469, lv470, lv239_1), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv471: R.Tensor((1280,), dtype="float32") = params[168]
            lv472: R.Tensor((1280,), dtype="float32") = params[169]
            lv697 = R.call_tir(group_norm9, (lv244_1, lv471, lv472), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv473: R.Tensor((1280, 1280, 1, 1), dtype="float32") = params[170]
            lv474: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[533]
            lv245_1 = R.call_tir(fused_conv2d15_add27, (lv697, lv473, lv474), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv246_1 = R.call_tir(fused_transpose40_reshape41, (lv245_1,), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv475: R.Tensor((1280,), dtype="float32") = params[171]
            lv476: R.Tensor((1280,), dtype="float32") = params[172]
            lv703 = R.call_tir(layer_norm4, (lv246_1, lv475, lv476), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv477: R.Tensor((1280, 1280), dtype="float32") = params[534]
            lv705 = R.call_tir(matmul33, (lv703, lv477), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv478: R.Tensor((1280, 1280), dtype="float32") = params[535]
            lv707 = R.call_tir(matmul33, (lv703, lv478), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv479: R.Tensor((1280, 1280), dtype="float32") = params[536]
            lv709 = R.call_tir(matmul33, (lv703, lv479), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv247_1 = R.call_tir(fused_reshape42_transpose41_reshape43, (lv705,), out_sinfo=R.Tensor((16, 64, 160), dtype="float32"))
            lv248_1 = R.call_tir(fused_reshape42_transpose41_reshape43_transpose42, (lv707,), out_sinfo=R.Tensor((16, 160, 64), dtype="float32"))
            lv249_1 = R.call_tir(fused_reshape42_transpose41_reshape43, (lv709,), out_sinfo=R.Tensor((16, 64, 160), dtype="float32"))
            lv250_2 = R.call_tir(fused_matmul34_multiply14, (lv247_1, lv248_1), out_sinfo=R.Tensor((16, 64, 64), dtype="float32"))
            lv722 = R.call_tir(softmax7, (lv250_2,), out_sinfo=R.Tensor((16, 64, 64), dtype="float32"))
            lv723 = R.call_tir(matmul35, (lv722, lv249_1), out_sinfo=R.Tensor((16, 64, 160), dtype="float32"))
            lv251_1 = R.call_tir(fused_reshape44_transpose43_reshape45, (lv723,), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv480: R.Tensor((1280, 1280), dtype="float32") = params[537]
            lv481_1: R.Tensor((1280,), dtype="float32") = params[173]
            lv252_1 = R.call_tir(fused_matmul33_add30_add31, (lv251_1, lv480, lv481_1, lv246_1), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv482_1: R.Tensor((1280,), dtype="float32") = params[174]
            lv483: R.Tensor((1280,), dtype="float32") = params[175]
            lv731 = R.call_tir(layer_norm4, (lv252_1, lv482_1, lv483), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv484: R.Tensor((1280, 1280), dtype="float32") = params[538]
            lv733 = R.call_tir(matmul33, (lv731, lv484), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv485: R.Tensor((768, 1280), dtype="float32") = params[539]
            lv735 = R.call_tir(matmul28, (inp_2, lv485), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv486: R.Tensor((768, 1280), dtype="float32") = params[540]
            lv737 = R.call_tir(matmul28, (inp_2, lv486), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv253_1 = R.call_tir(fused_reshape42_transpose41_reshape43, (lv733,), out_sinfo=R.Tensor((16, 64, 160), dtype="float32"))
            lv254_1 = R.call_tir(fused_reshape38_transpose35_reshape39_transpose36, (lv735,), out_sinfo=R.Tensor((16, 160, 77), dtype="float32"))
            lv255_1 = R.call_tir(fused_reshape38_transpose35_reshape39, (lv737,), out_sinfo=R.Tensor((16, 77, 160), dtype="float32"))
            lv256_2 = R.call_tir(fused_matmul36_multiply15, (lv253_1, lv254_1), out_sinfo=R.Tensor((16, 64, 77), dtype="float32"))
            lv750 = R.call_tir(softmax8, (lv256_2,), out_sinfo=R.Tensor((16, 64, 77), dtype="float32"))
            lv751 = R.call_tir(matmul37, (lv750, lv255_1), out_sinfo=R.Tensor((16, 64, 160), dtype="float32"))
            lv257_1 = R.call_tir(fused_reshape44_transpose43_reshape45, (lv751,), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv487: R.Tensor((1280, 1280), dtype="float32") = params[541]
            lv488: R.Tensor((1280,), dtype="float32") = params[176]
            lv258_2 = R.call_tir(fused_matmul33_add30_add31, (lv257_1, lv487, lv488, lv252_1), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv489: R.Tensor((1280,), dtype="float32") = params[177]
            lv490_1: R.Tensor((1280,), dtype="float32") = params[178]
            lv759 = R.call_tir(layer_norm4, (lv258_2, lv489, lv490_1), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv491: R.Tensor((1280, 5120), dtype="float32") = params[543]
            lv492_1: R.Tensor((5120,), dtype="float32") = params[180]
            lv259_1 = R.call_tir(fused_matmul38_add32_gelu3, (lv759, lv491, lv492_1), out_sinfo=R.Tensor((2, 64, 5120), dtype="float32"))
            lv493: R.Tensor((1280, 5120), dtype="float32") = params[542]
            lv494_1: R.Tensor((5120,), dtype="float32") = params[179]
            lv260_2 = R.call_tir(fused_matmul38_add32_multiply16, (lv759, lv493, lv494_1, lv259_1), out_sinfo=R.Tensor((2, 64, 5120), dtype="float32"))
            lv495: R.Tensor((5120, 1280), dtype="float32") = params[544]
            lv496_1: R.Tensor((1280,), dtype="float32") = params[181]
            lv261_1 = R.call_tir(fused_matmul39_add30_add31, (lv260_2, lv495, lv496_1, lv258_2), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv262_2 = R.call_tir(fused_reshape46_transpose44, (lv261_1,), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv497: R.Tensor((1280, 1280, 1, 1), dtype="float32") = params[182]
            lv498: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[545]
            lv263_1 = R.call_tir(fused_conv2d15_add27_add29, (lv262_2, lv497, lv498, lv244_1), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv499: R.Tensor((1280,), dtype="float32") = params[161]
            lv500: R.Tensor((1280,), dtype="float32") = params[162]
            lv264_1 = R.call_tir(fused_group_norm8_silu6, (lv263_1, lv499, lv500), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv783 = R.call_tir(silu, (lv79,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv501: R.Tensor((1280, 1280), dtype="float32") = params[546]
            lv502: R.Tensor((1280,), dtype="float32") = params[164]
            lv265_1 = R.call_tir(fused_matmul6_add4_strided_slice5, (lv783, lv501, lv502), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv788 = R.call_tir(reshape32, (lv265_1,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv503: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[163]
            lv504: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[547]
            lv266_1 = R.call_tir(fused_conv2d14_add27_add28, (lv264_1, lv503, lv504, lv788), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv505: R.Tensor((1280,), dtype="float32") = params[165]
            lv506: R.Tensor((1280,), dtype="float32") = params[166]
            lv267_1 = R.call_tir(fused_group_norm8_silu6, (lv266_1, lv505, lv506), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv507: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[167]
            lv508: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[548]
            lv268_1 = R.call_tir(fused_conv2d14_add27_add29_divide3, (lv267_1, lv507, lv508, lv263_1), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv797 = R.call_tir(concatenate3, (lv268_1, lv239_1), out_sinfo=R.Tensor((2, 2560, 8, 8), dtype="float32"))
            lv509_1: R.Tensor((2560,), dtype="float32") = params[183]
            lv510_1: R.Tensor((2560,), dtype="float32") = params[184]
            lv269_1 = R.call_tir(fused_group_norm10_silu7, (lv797, lv509_1, lv510_1), out_sinfo=R.Tensor((2, 2560, 8, 8), dtype="float32"))
            lv803 = R.call_tir(silu, (lv79,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv511: R.Tensor((1280, 1280), dtype="float32") = params[549]
            lv512: R.Tensor((1280,), dtype="float32") = params[186]
            lv270_1 = R.call_tir(fused_matmul6_add4_strided_slice5, (lv803, lv511, lv512), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv808 = R.call_tir(reshape32, (lv270_1,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv513: R.Tensor((1280, 2560, 3, 3), dtype="float32") = params[185]
            lv514: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[550]
            lv271_1 = R.call_tir(fused_conv2d16_add27_add28, (lv269_1, lv513, lv514, lv808), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv515: R.Tensor((1280,), dtype="float32") = params[187]
            lv516: R.Tensor((1280,), dtype="float32") = params[188]
            lv272_1 = R.call_tir(fused_group_norm8_silu6, (lv271_1, lv515, lv516), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv517: R.Tensor((1280, 2560, 1, 1), dtype="float32") = params[190]
            lv518_1: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[552]
            lv273_1 = R.call_tir(fused_conv2d17_add27, (lv797, lv517, lv518_1), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv519: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[189]
            lv520: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[551]
            lv274_1 = R.call_tir(fused_conv2d14_add27_add29_divide3, (lv272_1, lv519, lv520, lv273_1), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv820 = R.call_tir(concatenate3, (lv274_1, lv234_1), out_sinfo=R.Tensor((2, 2560, 8, 8), dtype="float32"))
            lv521: R.Tensor((2560,), dtype="float32") = params[191]
            lv522: R.Tensor((2560,), dtype="float32") = params[192]
            lv275_2 = R.call_tir(fused_group_norm10_silu7, (lv820, lv521, lv522), out_sinfo=R.Tensor((2, 2560, 8, 8), dtype="float32"))
            lv826 = R.call_tir(silu, (lv79,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv523: R.Tensor((1280, 1280), dtype="float32") = params[553]
            lv524: R.Tensor((1280,), dtype="float32") = params[194]
            lv276_2 = R.call_tir(fused_matmul6_add4_strided_slice5, (lv826, lv523, lv524), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv831 = R.call_tir(reshape32, (lv276_2,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv525: R.Tensor((1280, 2560, 3, 3), dtype="float32") = params[193]
            lv526: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[554]
            lv277_1 = R.call_tir(fused_conv2d16_add27_add28, (lv275_2, lv525, lv526, lv831), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv527: R.Tensor((1280,), dtype="float32") = params[195]
            lv528: R.Tensor((1280,), dtype="float32") = params[196]
            lv278_1 = R.call_tir(fused_group_norm8_silu6, (lv277_1, lv527, lv528), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv529: R.Tensor((1280, 2560, 1, 1), dtype="float32") = params[198]
            lv530: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[555]
            lv279_1 = R.call_tir(fused_conv2d17_add27, (lv820, lv529, lv530), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv531: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[197]
            lv532: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[556]
            lv280_1 = R.call_tir(fused_conv2d14_add27_add29_divide3, (lv278_1, lv531, lv532, lv279_1), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv843 = R.call_tir(concatenate3, (lv280_1, lv229_1), out_sinfo=R.Tensor((2, 2560, 8, 8), dtype="float32"))
            lv533: R.Tensor((2560,), dtype="float32") = params[199]
            lv534: R.Tensor((2560,), dtype="float32") = params[200]
            lv281_1 = R.call_tir(fused_group_norm10_silu7, (lv843, lv533, lv534), out_sinfo=R.Tensor((2, 2560, 8, 8), dtype="float32"))
            lv849 = R.call_tir(silu, (lv79,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv535: R.Tensor((1280, 1280), dtype="float32") = params[557]
            lv536: R.Tensor((1280,), dtype="float32") = params[202]
            lv282_1 = R.call_tir(fused_matmul6_add4_strided_slice5, (lv849, lv535, lv536), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv854 = R.call_tir(reshape32, (lv282_1,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv537: R.Tensor((1280, 2560, 3, 3), dtype="float32") = params[201]
            lv538: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[558]
            lv283_1 = R.call_tir(fused_conv2d16_add27_add28, (lv281_1, lv537, lv538, lv854), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv539: R.Tensor((1280,), dtype="float32") = params[203]
            lv540: R.Tensor((1280,), dtype="float32") = params[204]
            lv284_2 = R.call_tir(fused_group_norm8_silu6, (lv283_1, lv539, lv540), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv541: R.Tensor((1280, 2560, 1, 1), dtype="float32") = params[206]
            lv542_1: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[559]
            lv285_1 = R.call_tir(fused_conv2d17_add27, (lv843, lv541, lv542_1), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv543: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[205]
            lv544: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[560]
            lv286_2 = R.call_tir(fused_conv2d14_add27_add29_divide3, (lv284_2, lv543, lv544, lv285_1), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv866 = R.call_tir(resize2d, (lv286_2,), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv545: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[207]
            lv546: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[561]
            lv287_1 = R.call_tir(fused_conv2d10_add21, (lv866, lv545, lv546), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv870 = R.call_tir(concatenate4, (lv287_1, lv228_1), out_sinfo=R.Tensor((2, 2560, 16, 16), dtype="float32"))
            lv547_1: R.Tensor((2560,), dtype="float32") = params[208]
            lv548: R.Tensor((2560,), dtype="float32") = params[209]
            lv288_2 = R.call_tir(fused_group_norm11_silu8, (lv870, lv547_1, lv548), out_sinfo=R.Tensor((2, 2560, 16, 16), dtype="float32"))
            lv876 = R.call_tir(silu, (lv79,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv549: R.Tensor((1280, 1280), dtype="float32") = params[563]
            lv550: R.Tensor((1280,), dtype="float32") = params[211]
            lv289_1 = R.call_tir(fused_matmul6_add4_strided_slice5, (lv876, lv549, lv550), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv881 = R.call_tir(reshape32, (lv289_1,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv551: R.Tensor((1280, 2560, 3, 3), dtype="float32") = params[210]
            lv552: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[562]
            lv290_2 = R.call_tir(fused_conv2d18_add21_add22, (lv288_2, lv551, lv552, lv881), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv553: R.Tensor((1280,), dtype="float32") = params[212]
            lv554: R.Tensor((1280,), dtype="float32") = params[213]
            lv291_1 = R.call_tir(fused_group_norm6_silu5, (lv290_2, lv553, lv554), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv555: R.Tensor((1280, 2560, 1, 1), dtype="float32") = params[215]
            lv556_1: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[564]
            lv292_1 = R.call_tir(fused_conv2d19_add21, (lv870, lv555, lv556_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv557: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[214]
            lv558: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[565]
            lv293_1 = R.call_tir(fused_conv2d10_add21_add23_divide2, (lv291_1, lv557, lv558, lv292_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv559: R.Tensor((1280,), dtype="float32") = params[232]
            lv560: R.Tensor((1280,), dtype="float32") = params[233]
            lv893 = R.call_tir(group_norm7, (lv293_1, lv559, lv560), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv561: R.Tensor((1280, 1280, 1, 1), dtype="float32") = params[234]
            lv562_1: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[566]
            lv294_1 = R.call_tir(fused_conv2d12_add21, (lv893, lv561, lv562_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv295_1 = R.call_tir(fused_transpose30_reshape33, (lv294_1,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv563: R.Tensor((1280,), dtype="float32") = params[235]
            lv564_1: R.Tensor((1280,), dtype="float32") = params[236]
            lv899 = R.call_tir(layer_norm3, (lv295_1, lv563, lv564_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv565: R.Tensor((1280, 1280), dtype="float32") = params[567]
            lv901 = R.call_tir(matmul25, (lv899, lv565), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv566_1: R.Tensor((1280, 1280), dtype="float32") = params[568]
            lv903 = R.call_tir(matmul25, (lv899, lv566_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv567: R.Tensor((1280, 1280), dtype="float32") = params[569]
            lv905 = R.call_tir(matmul25, (lv899, lv567), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv296_1 = R.call_tir(fused_reshape34_transpose31_reshape35, (lv901,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv297_1 = R.call_tir(fused_reshape34_transpose31_reshape35_transpose32, (lv903,), out_sinfo=R.Tensor((16, 160, 256), dtype="float32"))
            lv298_1 = R.call_tir(fused_reshape34_transpose31_reshape35, (lv905,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv299_1 = R.call_tir(fused_matmul26_multiply11, (lv296_1, lv297_1), out_sinfo=R.Tensor((16, 256, 256), dtype="float32"))
            lv918 = R.call_tir(softmax5, (lv299_1,), out_sinfo=R.Tensor((16, 256, 256), dtype="float32"))
            lv919 = R.call_tir(matmul27, (lv918, lv298_1), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv300_1 = R.call_tir(fused_reshape36_transpose33_reshape37, (lv919,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv568_1: R.Tensor((1280, 1280), dtype="float32") = params[570]
            lv569: R.Tensor((1280,), dtype="float32") = params[237]
            lv301_1 = R.call_tir(fused_matmul25_add24_add25, (lv300_1, lv568_1, lv569, lv295_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv570: R.Tensor((1280,), dtype="float32") = params[238]
            lv571: R.Tensor((1280,), dtype="float32") = params[239]
            lv927 = R.call_tir(layer_norm3, (lv301_1, lv570, lv571), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv572: R.Tensor((1280, 1280), dtype="float32") = params[571]
            lv929 = R.call_tir(matmul25, (lv927, lv572), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv573: R.Tensor((768, 1280), dtype="float32") = params[572]
            lv931 = R.call_tir(matmul28, (inp_2, lv573), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv574: R.Tensor((768, 1280), dtype="float32") = params[573]
            lv933 = R.call_tir(matmul28, (inp_2, lv574), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv302_1 = R.call_tir(fused_reshape34_transpose31_reshape35, (lv929,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv303_2 = R.call_tir(fused_reshape38_transpose35_reshape39_transpose36, (lv931,), out_sinfo=R.Tensor((16, 160, 77), dtype="float32"))
            lv304_2 = R.call_tir(fused_reshape38_transpose35_reshape39, (lv933,), out_sinfo=R.Tensor((16, 77, 160), dtype="float32"))
            lv305_1 = R.call_tir(fused_matmul29_multiply12, (lv302_1, lv303_2), out_sinfo=R.Tensor((16, 256, 77), dtype="float32"))
            lv946 = R.call_tir(softmax6, (lv305_1,), out_sinfo=R.Tensor((16, 256, 77), dtype="float32"))
            lv947 = R.call_tir(matmul30, (lv946, lv304_2), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv306_1 = R.call_tir(fused_reshape36_transpose33_reshape37, (lv947,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv575: R.Tensor((1280, 1280), dtype="float32") = params[574]
            lv576: R.Tensor((1280,), dtype="float32") = params[240]
            lv307_1 = R.call_tir(fused_matmul25_add24_add25, (lv306_1, lv575, lv576, lv301_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv577: R.Tensor((1280,), dtype="float32") = params[241]
            lv578: R.Tensor((1280,), dtype="float32") = params[242]
            lv955 = R.call_tir(layer_norm3, (lv307_1, lv577, lv578), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv579: R.Tensor((1280, 5120), dtype="float32") = params[576]
            lv580: R.Tensor((5120,), dtype="float32") = params[244]
            lv308_1 = R.call_tir(fused_matmul31_add26_gelu2, (lv955, lv579, lv580), out_sinfo=R.Tensor((2, 256, 5120), dtype="float32"))
            lv581_1: R.Tensor((1280, 5120), dtype="float32") = params[575]
            lv582_1: R.Tensor((5120,), dtype="float32") = params[243]
            lv309_1 = R.call_tir(fused_matmul31_add26_multiply13, (lv955, lv581_1, lv582_1, lv308_1), out_sinfo=R.Tensor((2, 256, 5120), dtype="float32"))
            lv583: R.Tensor((5120, 1280), dtype="float32") = params[577]
            lv584: R.Tensor((1280,), dtype="float32") = params[245]
            lv310_1 = R.call_tir(fused_matmul32_add24_add25, (lv309_1, lv583, lv584, lv307_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv311_1 = R.call_tir(fused_reshape40_transpose39, (lv310_1,), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv585: R.Tensor((1280, 1280, 1, 1), dtype="float32") = params[246]
            lv586: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[578]
            lv312_2 = R.call_tir(fused_conv2d12_add21_add23, (lv311_1, lv585, lv586, lv293_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv974 = R.call_tir(concatenate4, (lv312_2, lv204_1), out_sinfo=R.Tensor((2, 2560, 16, 16), dtype="float32"))
            lv587: R.Tensor((2560,), dtype="float32") = params[216]
            lv588: R.Tensor((2560,), dtype="float32") = params[217]
            lv313_1 = R.call_tir(fused_group_norm11_silu8, (lv974, lv587, lv588), out_sinfo=R.Tensor((2, 2560, 16, 16), dtype="float32"))
            lv980 = R.call_tir(silu, (lv79,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv589: R.Tensor((1280, 1280), dtype="float32") = params[579]
            lv590_1: R.Tensor((1280,), dtype="float32") = params[219]
            lv314_1 = R.call_tir(fused_matmul6_add4_strided_slice5, (lv980, lv589, lv590_1), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv985 = R.call_tir(reshape32, (lv314_1,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv591: R.Tensor((1280, 2560, 3, 3), dtype="float32") = params[218]
            lv592_1: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[580]
            lv315_1 = R.call_tir(fused_conv2d18_add21_add22, (lv313_1, lv591, lv592_1, lv985), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv593: R.Tensor((1280,), dtype="float32") = params[220]
            lv594_1: R.Tensor((1280,), dtype="float32") = params[221]
            lv316_1 = R.call_tir(fused_group_norm6_silu5, (lv315_1, lv593, lv594_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv595: R.Tensor((1280, 2560, 1, 1), dtype="float32") = params[223]
            lv596_1: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[581]
            lv317_1 = R.call_tir(fused_conv2d19_add21, (lv974, lv595, lv596_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv597: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[222]
            lv598: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[582]
            lv318_1 = R.call_tir(fused_conv2d10_add21_add23_divide2, (lv316_1, lv597, lv598, lv317_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv599: R.Tensor((1280,), dtype="float32") = params[247]
            lv600: R.Tensor((1280,), dtype="float32") = params[248]
            lv997 = R.call_tir(group_norm7, (lv318_1, lv599, lv600), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv601: R.Tensor((1280, 1280, 1, 1), dtype="float32") = params[249]
            lv602: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[583]
            lv319_1 = R.call_tir(fused_conv2d12_add21, (lv997, lv601, lv602), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv320_1 = R.call_tir(fused_transpose30_reshape33, (lv319_1,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv603: R.Tensor((1280,), dtype="float32") = params[250]
            lv604: R.Tensor((1280,), dtype="float32") = params[251]
            lv1003 = R.call_tir(layer_norm3, (lv320_1, lv603, lv604), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv605: R.Tensor((1280, 1280), dtype="float32") = params[584]
            lv1005 = R.call_tir(matmul25, (lv1003, lv605), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv606: R.Tensor((1280, 1280), dtype="float32") = params[585]
            lv1007 = R.call_tir(matmul25, (lv1003, lv606), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv607: R.Tensor((1280, 1280), dtype="float32") = params[586]
            lv1009 = R.call_tir(matmul25, (lv1003, lv607), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv321_1 = R.call_tir(fused_reshape34_transpose31_reshape35, (lv1005,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv322_1 = R.call_tir(fused_reshape34_transpose31_reshape35_transpose32, (lv1007,), out_sinfo=R.Tensor((16, 160, 256), dtype="float32"))
            lv323_1 = R.call_tir(fused_reshape34_transpose31_reshape35, (lv1009,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv324_1 = R.call_tir(fused_matmul26_multiply11, (lv321_1, lv322_1), out_sinfo=R.Tensor((16, 256, 256), dtype="float32"))
            lv1022 = R.call_tir(softmax5, (lv324_1,), out_sinfo=R.Tensor((16, 256, 256), dtype="float32"))
            lv1023 = R.call_tir(matmul27, (lv1022, lv323_1), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv325_1 = R.call_tir(fused_reshape36_transpose33_reshape37, (lv1023,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv608: R.Tensor((1280, 1280), dtype="float32") = params[587]
            lv609_1: R.Tensor((1280,), dtype="float32") = params[252]
            lv326_1 = R.call_tir(fused_matmul25_add24_add25, (lv325_1, lv608, lv609_1, lv320_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv610_1: R.Tensor((1280,), dtype="float32") = params[253]
            lv611: R.Tensor((1280,), dtype="float32") = params[254]
            lv1031 = R.call_tir(layer_norm3, (lv326_1, lv610_1, lv611), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv612: R.Tensor((1280, 1280), dtype="float32") = params[588]
            lv1033 = R.call_tir(matmul25, (lv1031, lv612), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv613: R.Tensor((768, 1280), dtype="float32") = params[589]
            lv1035 = R.call_tir(matmul28, (inp_2, lv613), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv614: R.Tensor((768, 1280), dtype="float32") = params[590]
            lv1037 = R.call_tir(matmul28, (inp_2, lv614), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv327_1 = R.call_tir(fused_reshape34_transpose31_reshape35, (lv1033,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv328_1 = R.call_tir(fused_reshape38_transpose35_reshape39_transpose36, (lv1035,), out_sinfo=R.Tensor((16, 160, 77), dtype="float32"))
            lv329_1 = R.call_tir(fused_reshape38_transpose35_reshape39, (lv1037,), out_sinfo=R.Tensor((16, 77, 160), dtype="float32"))
            lv330_1 = R.call_tir(fused_matmul29_multiply12, (lv327_1, lv328_1), out_sinfo=R.Tensor((16, 256, 77), dtype="float32"))
            lv1050 = R.call_tir(softmax6, (lv330_1,), out_sinfo=R.Tensor((16, 256, 77), dtype="float32"))
            lv1051 = R.call_tir(matmul30, (lv1050, lv329_1), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv331_1 = R.call_tir(fused_reshape36_transpose33_reshape37, (lv1051,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv615: R.Tensor((1280, 1280), dtype="float32") = params[591]
            lv616: R.Tensor((1280,), dtype="float32") = params[255]
            lv332_1 = R.call_tir(fused_matmul25_add24_add25, (lv331_1, lv615, lv616, lv326_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv617: R.Tensor((1280,), dtype="float32") = params[256]
            lv618_1: R.Tensor((1280,), dtype="float32") = params[257]
            lv1059 = R.call_tir(layer_norm3, (lv332_1, lv617, lv618_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv619: R.Tensor((1280, 5120), dtype="float32") = params[593]
            lv620: R.Tensor((5120,), dtype="float32") = params[259]
            lv333_1 = R.call_tir(fused_matmul31_add26_gelu2, (lv1059, lv619, lv620), out_sinfo=R.Tensor((2, 256, 5120), dtype="float32"))
            lv621: R.Tensor((1280, 5120), dtype="float32") = params[592]
            lv622: R.Tensor((5120,), dtype="float32") = params[258]
            lv334_1 = R.call_tir(fused_matmul31_add26_multiply13, (lv1059, lv621, lv622, lv333_1), out_sinfo=R.Tensor((2, 256, 5120), dtype="float32"))
            lv623: R.Tensor((5120, 1280), dtype="float32") = params[594]
            lv624: R.Tensor((1280,), dtype="float32") = params[260]
            lv335_1 = R.call_tir(fused_matmul32_add24_add25, (lv334_1, lv623, lv624, lv332_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv336_2 = R.call_tir(fused_reshape40_transpose39, (lv335_1,), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv625: R.Tensor((1280, 1280, 1, 1), dtype="float32") = params[261]
            lv626: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[595]
            lv337_1 = R.call_tir(fused_conv2d12_add21_add23, (lv336_2, lv625, lv626, lv318_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv1078 = R.call_tir(concatenate5, (lv337_1, lv179), out_sinfo=R.Tensor((2, 1920, 16, 16), dtype="float32"))
            lv627: R.Tensor((1920,), dtype="float32") = params[224]
            lv628: R.Tensor((1920,), dtype="float32") = params[225]
            lv338_1 = R.call_tir(fused_group_norm12_silu9, (lv1078, lv627, lv628), out_sinfo=R.Tensor((2, 1920, 16, 16), dtype="float32"))
            lv1084 = R.call_tir(silu, (lv79,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv629: R.Tensor((1280, 1280), dtype="float32") = params[596]
            lv630: R.Tensor((1280,), dtype="float32") = params[227]
            lv339_1 = R.call_tir(fused_matmul6_add4_strided_slice5, (lv1084, lv629, lv630), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv1089 = R.call_tir(reshape32, (lv339_1,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv631: R.Tensor((1280, 1920, 3, 3), dtype="float32") = params[226]
            lv632: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[597]
            lv340_1 = R.call_tir(fused_conv2d20_add21_add22, (lv338_1, lv631, lv632, lv1089), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv633: R.Tensor((1280,), dtype="float32") = params[228]
            lv634: R.Tensor((1280,), dtype="float32") = params[229]
            lv341_2 = R.call_tir(fused_group_norm6_silu5, (lv340_1, lv633, lv634), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv635: R.Tensor((1280, 1920, 1, 1), dtype="float32") = params[231]
            lv636: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[598]
            lv342_1 = R.call_tir(fused_conv2d21_add21, (lv1078, lv635, lv636), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv637: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[230]
            lv638: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[599]
            lv343_1 = R.call_tir(fused_conv2d10_add21_add23_divide2, (lv341_2, lv637, lv638, lv342_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv639: R.Tensor((1280,), dtype="float32") = params[262]
            lv640: R.Tensor((1280,), dtype="float32") = params[263]
            lv1101 = R.call_tir(group_norm7, (lv343_1, lv639, lv640), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv641: R.Tensor((1280, 1280, 1, 1), dtype="float32") = params[264]
            lv642: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[600]
            lv344_1 = R.call_tir(fused_conv2d12_add21, (lv1101, lv641, lv642), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv345_1 = R.call_tir(fused_transpose30_reshape33, (lv344_1,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv643: R.Tensor((1280,), dtype="float32") = params[265]
            lv644: R.Tensor((1280,), dtype="float32") = params[266]
            lv1107 = R.call_tir(layer_norm3, (lv345_1, lv643, lv644), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv645_1: R.Tensor((1280, 1280), dtype="float32") = params[601]
            lv1109 = R.call_tir(matmul25, (lv1107, lv645_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv646: R.Tensor((1280, 1280), dtype="float32") = params[602]
            lv1111 = R.call_tir(matmul25, (lv1107, lv646), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv647: R.Tensor((1280, 1280), dtype="float32") = params[603]
            lv1113 = R.call_tir(matmul25, (lv1107, lv647), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv346_1 = R.call_tir(fused_reshape34_transpose31_reshape35, (lv1109,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv347_1 = R.call_tir(fused_reshape34_transpose31_reshape35_transpose32, (lv1111,), out_sinfo=R.Tensor((16, 160, 256), dtype="float32"))
            lv348_1 = R.call_tir(fused_reshape34_transpose31_reshape35, (lv1113,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv349_1 = R.call_tir(fused_matmul26_multiply11, (lv346_1, lv347_1), out_sinfo=R.Tensor((16, 256, 256), dtype="float32"))
            lv1126 = R.call_tir(softmax5, (lv349_1,), out_sinfo=R.Tensor((16, 256, 256), dtype="float32"))
            lv1127 = R.call_tir(matmul27, (lv1126, lv348_1), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv350_2 = R.call_tir(fused_reshape36_transpose33_reshape37, (lv1127,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv648: R.Tensor((1280, 1280), dtype="float32") = params[604]
            lv649: R.Tensor((1280,), dtype="float32") = params[267]
            lv351_1 = R.call_tir(fused_matmul25_add24_add25, (lv350_2, lv648, lv649, lv345_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv650_1: R.Tensor((1280,), dtype="float32") = params[268]
            lv651: R.Tensor((1280,), dtype="float32") = params[269]
            lv1135 = R.call_tir(layer_norm3, (lv351_1, lv650_1, lv651), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv652: R.Tensor((1280, 1280), dtype="float32") = params[605]
            lv1137 = R.call_tir(matmul25, (lv1135, lv652), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv653: R.Tensor((768, 1280), dtype="float32") = params[606]
            lv1139 = R.call_tir(matmul28, (inp_2, lv653), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv654: R.Tensor((768, 1280), dtype="float32") = params[607]
            lv1141 = R.call_tir(matmul28, (inp_2, lv654), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv352_1 = R.call_tir(fused_reshape34_transpose31_reshape35, (lv1137,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv353_1 = R.call_tir(fused_reshape38_transpose35_reshape39_transpose36, (lv1139,), out_sinfo=R.Tensor((16, 160, 77), dtype="float32"))
            lv354_1 = R.call_tir(fused_reshape38_transpose35_reshape39, (lv1141,), out_sinfo=R.Tensor((16, 77, 160), dtype="float32"))
            lv355_1 = R.call_tir(fused_matmul29_multiply12, (lv352_1, lv353_1), out_sinfo=R.Tensor((16, 256, 77), dtype="float32"))
            lv1154 = R.call_tir(softmax6, (lv355_1,), out_sinfo=R.Tensor((16, 256, 77), dtype="float32"))
            lv1155 = R.call_tir(matmul30, (lv1154, lv354_1), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv356_2 = R.call_tir(fused_reshape36_transpose33_reshape37, (lv1155,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv655: R.Tensor((1280, 1280), dtype="float32") = params[608]
            lv656: R.Tensor((1280,), dtype="float32") = params[270]
            lv357_1 = R.call_tir(fused_matmul25_add24_add25, (lv356_2, lv655, lv656, lv351_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv657: R.Tensor((1280,), dtype="float32") = params[271]
            lv658: R.Tensor((1280,), dtype="float32") = params[272]
            lv1163 = R.call_tir(layer_norm3, (lv357_1, lv657, lv658), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv659: R.Tensor((1280, 5120), dtype="float32") = params[610]
            lv660: R.Tensor((5120,), dtype="float32") = params[274]
            lv358_2 = R.call_tir(fused_matmul31_add26_gelu2, (lv1163, lv659, lv660), out_sinfo=R.Tensor((2, 256, 5120), dtype="float32"))
            lv661: R.Tensor((1280, 5120), dtype="float32") = params[609]
            lv662: R.Tensor((5120,), dtype="float32") = params[273]
            lv359_1 = R.call_tir(fused_matmul31_add26_multiply13, (lv1163, lv661, lv662, lv358_2), out_sinfo=R.Tensor((2, 256, 5120), dtype="float32"))
            lv663: R.Tensor((5120, 1280), dtype="float32") = params[611]
            lv664_1: R.Tensor((1280,), dtype="float32") = params[275]
            lv360_2 = R.call_tir(fused_matmul32_add24_add25, (lv359_1, lv663, lv664_1, lv357_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv361_1 = R.call_tir(fused_reshape40_transpose39, (lv360_2,), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv665: R.Tensor((1280, 1280, 1, 1), dtype="float32") = params[276]
            lv666: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[612]
            lv362_2 = R.call_tir(fused_conv2d12_add21_add23, (lv361_1, lv665, lv666, lv343_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv1182 = R.call_tir(resize2d1, (lv362_2,), out_sinfo=R.Tensor((2, 1280, 32, 32), dtype="float32"))
            lv667: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[277]
            lv668: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[613]
            lv363_1 = R.call_tir(fused_conv2d22_add33, (lv1182, lv667, lv668), out_sinfo=R.Tensor((2, 1280, 32, 32), dtype="float32"))
            lv1186 = R.call_tir(concatenate6, (lv363_1, lv178_1), out_sinfo=R.Tensor((2, 1920, 32, 32), dtype="float32"))
            lv669_1: R.Tensor((1920,), dtype="float32") = params[278]
            lv670: R.Tensor((1920,), dtype="float32") = params[279]
            lv364_1 = R.call_tir(fused_group_norm13_silu10, (lv1186, lv669_1, lv670), out_sinfo=R.Tensor((2, 1920, 32, 32), dtype="float32"))
            lv1192 = R.call_tir(silu, (lv79,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv671: R.Tensor((1280, 640), dtype="float32") = params[615]
            lv672: R.Tensor((640,), dtype="float32") = params[281]
            lv365_1 = R.call_tir(fused_matmul16_add14_strided_slice4, (lv1192, lv671, lv672), out_sinfo=R.Tensor((2, 640), dtype="float32"))
            lv1197 = R.call_tir(reshape22, (lv365_1,), out_sinfo=R.Tensor((2, 640, 1, 1), dtype="float32"))
            lv673: R.Tensor((640, 1920, 3, 3), dtype="float32") = params[280]
            lv674: R.Tensor((1, 640, 1, 1), dtype="float32") = params[614]
            lv366_1 = R.call_tir(fused_conv2d23_add13_add15, (lv364_1, lv673, lv674, lv1197), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv675: R.Tensor((640,), dtype="float32") = params[282]
            lv676: R.Tensor((640,), dtype="float32") = params[283]
            lv367_1 = R.call_tir(fused_group_norm3_silu3, (lv366_1, lv675, lv676), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv677: R.Tensor((640, 1920, 1, 1), dtype="float32") = params[285]
            lv678: R.Tensor((1, 640, 1, 1), dtype="float32") = params[616]
            lv368_1 = R.call_tir(fused_conv2d24_add13, (lv1186, lv677, lv678), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv679: R.Tensor((640, 640, 3, 3), dtype="float32") = params[284]
            lv680: R.Tensor((1, 640, 1, 1), dtype="float32") = params[617]
            lv369_1 = R.call_tir(fused_conv2d5_add13_add16_divide1, (lv367_1, lv679, lv680, lv368_1), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv681: R.Tensor((640,), dtype="float32") = params[302]
            lv682: R.Tensor((640,), dtype="float32") = params[303]
            lv1209 = R.call_tir(group_norm4, (lv369_1, lv681, lv682), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv683_1: R.Tensor((640, 640, 1, 1), dtype="float32") = params[304]
            lv684: R.Tensor((1, 640, 1, 1), dtype="float32") = params[618]
            lv370_1 = R.call_tir(fused_conv2d7_add13, (lv1209, lv683_1, lv684), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv371_1 = R.call_tir(fused_transpose19_reshape23, (lv370_1,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv685: R.Tensor((640,), dtype="float32") = params[305]
            lv686: R.Tensor((640,), dtype="float32") = params[306]
            lv1215 = R.call_tir(layer_norm2, (lv371_1, lv685, lv686), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv687: R.Tensor((640, 640), dtype="float32") = params[619]
            lv1217 = R.call_tir(matmul17, (lv1215, lv687), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv688_1: R.Tensor((640, 640), dtype="float32") = params[620]
            lv1219 = R.call_tir(matmul17, (lv1215, lv688_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv689: R.Tensor((640, 640), dtype="float32") = params[621]
            lv1221 = R.call_tir(matmul17, (lv1215, lv689), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv372_1 = R.call_tir(fused_reshape24_transpose21_reshape25, (lv1217,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv373_1 = R.call_tir(fused_reshape24_transpose21_reshape25_transpose22, (lv1219,), out_sinfo=R.Tensor((16, 80, 1024), dtype="float32"))
            lv374_1 = R.call_tir(fused_reshape24_transpose21_reshape25, (lv1221,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv375_2 = R.call_tir(fused_matmul18_multiply8, (lv372_1, lv373_1), out_sinfo=R.Tensor((16, 1024, 1024), dtype="float32"))
            lv1234 = R.call_tir(softmax3, (lv375_2,), out_sinfo=R.Tensor((16, 1024, 1024), dtype="float32"))
            lv1235 = R.call_tir(matmul19, (lv1234, lv374_1), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv376_2 = R.call_tir(fused_reshape26_transpose23_reshape27, (lv1235,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv690: R.Tensor((640, 640), dtype="float32") = params[622]
            lv691: R.Tensor((640,), dtype="float32") = params[307]
            lv377_1 = R.call_tir(fused_matmul17_add17_add18, (lv376_2, lv690, lv691, lv371_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv692: R.Tensor((640,), dtype="float32") = params[308]
            lv693: R.Tensor((640,), dtype="float32") = params[309]
            lv1243 = R.call_tir(layer_norm2, (lv377_1, lv692, lv693), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv694: R.Tensor((640, 640), dtype="float32") = params[623]
            lv1245 = R.call_tir(matmul17, (lv1243, lv694), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv695: R.Tensor((768, 640), dtype="float32") = params[624]
            lv1247 = R.call_tir(matmul20, (inp_2, lv695), out_sinfo=R.Tensor((2, 77, 640), dtype="float32"))
            lv696: R.Tensor((768, 640), dtype="float32") = params[625]
            lv1249 = R.call_tir(matmul20, (inp_2, lv696), out_sinfo=R.Tensor((2, 77, 640), dtype="float32"))
            lv378_1 = R.call_tir(fused_reshape24_transpose21_reshape25, (lv1245,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv379_1 = R.call_tir(fused_reshape28_transpose25_reshape29_transpose26, (lv1247,), out_sinfo=R.Tensor((16, 80, 77), dtype="float32"))
            lv380_1 = R.call_tir(fused_reshape28_transpose25_reshape29, (lv1249,), out_sinfo=R.Tensor((16, 77, 80), dtype="float32"))
            lv381_1 = R.call_tir(fused_matmul21_multiply9, (lv378_1, lv379_1), out_sinfo=R.Tensor((16, 1024, 77), dtype="float32"))
            lv1262 = R.call_tir(softmax4, (lv381_1,), out_sinfo=R.Tensor((16, 1024, 77), dtype="float32"))
            lv1263 = R.call_tir(matmul22, (lv1262, lv380_1), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv382_1 = R.call_tir(fused_reshape26_transpose23_reshape27, (lv1263,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv697_1: R.Tensor((640, 640), dtype="float32") = params[626]
            lv698: R.Tensor((640,), dtype="float32") = params[310]
            lv383_1 = R.call_tir(fused_matmul17_add17_add18, (lv382_1, lv697_1, lv698, lv377_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv699: R.Tensor((640,), dtype="float32") = params[311]
            lv700: R.Tensor((640,), dtype="float32") = params[312]
            lv1271 = R.call_tir(layer_norm2, (lv383_1, lv699, lv700), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv701: R.Tensor((640, 2560), dtype="float32") = params[628]
            lv702: R.Tensor((2560,), dtype="float32") = params[314]
            lv384_2 = R.call_tir(fused_matmul23_add19_gelu1, (lv1271, lv701, lv702), out_sinfo=R.Tensor((2, 1024, 2560), dtype="float32"))
            lv703_1: R.Tensor((640, 2560), dtype="float32") = params[627]
            lv704: R.Tensor((2560,), dtype="float32") = params[313]
            lv385_1 = R.call_tir(fused_matmul23_add19_multiply10, (lv1271, lv703_1, lv704, lv384_2), out_sinfo=R.Tensor((2, 1024, 2560), dtype="float32"))
            lv705_1: R.Tensor((2560, 640), dtype="float32") = params[629]
            lv706: R.Tensor((640,), dtype="float32") = params[315]
            lv386_2 = R.call_tir(fused_matmul24_add17_add18, (lv385_1, lv705_1, lv706, lv383_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv387_1 = R.call_tir(fused_reshape30_transpose29, (lv386_2,), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv707_1: R.Tensor((640, 640, 1, 1), dtype="float32") = params[316]
            lv708: R.Tensor((1, 640, 1, 1), dtype="float32") = params[630]
            lv388_2 = R.call_tir(fused_conv2d7_add13_add16, (lv387_1, lv707_1, lv708, lv369_1), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv1290 = R.call_tir(concatenate7, (lv388_2, lv154_1), out_sinfo=R.Tensor((2, 1280, 32, 32), dtype="float32"))
            lv709_1: R.Tensor((1280,), dtype="float32") = params[286]
            lv710: R.Tensor((1280,), dtype="float32") = params[287]
            lv389_1 = R.call_tir(fused_group_norm14_silu11, (lv1290, lv709_1, lv710), out_sinfo=R.Tensor((2, 1280, 32, 32), dtype="float32"))
            lv1296 = R.call_tir(silu, (lv79,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv711: R.Tensor((1280, 640), dtype="float32") = params[632]
            lv712: R.Tensor((640,), dtype="float32") = params[289]
            lv390_2 = R.call_tir(fused_matmul16_add14_strided_slice4, (lv1296, lv711, lv712), out_sinfo=R.Tensor((2, 640), dtype="float32"))
            lv1301 = R.call_tir(reshape22, (lv390_2,), out_sinfo=R.Tensor((2, 640, 1, 1), dtype="float32"))
            lv713: R.Tensor((640, 1280, 3, 3), dtype="float32") = params[288]
            lv714: R.Tensor((1, 640, 1, 1), dtype="float32") = params[631]
            lv391_1 = R.call_tir(fused_conv2d25_add13_add15, (lv389_1, lv713, lv714, lv1301), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv715: R.Tensor((640,), dtype="float32") = params[290]
            lv716: R.Tensor((640,), dtype="float32") = params[291]
            lv392_1 = R.call_tir(fused_group_norm3_silu3, (lv391_1, lv715, lv716), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv717: R.Tensor((640, 1280, 1, 1), dtype="float32") = params[293]
            lv718: R.Tensor((1, 640, 1, 1), dtype="float32") = params[633]
            lv393_1 = R.call_tir(fused_conv2d26_add13, (lv1290, lv717, lv718), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv719: R.Tensor((640, 640, 3, 3), dtype="float32") = params[292]
            lv720: R.Tensor((1, 640, 1, 1), dtype="float32") = params[634]
            lv394_1 = R.call_tir(fused_conv2d5_add13_add16_divide1, (lv392_1, lv719, lv720, lv393_1), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv721: R.Tensor((640,), dtype="float32") = params[317]
            lv722_1: R.Tensor((640,), dtype="float32") = params[318]
            lv1313 = R.call_tir(group_norm4, (lv394_1, lv721, lv722_1), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv723_1: R.Tensor((640, 640, 1, 1), dtype="float32") = params[319]
            lv724: R.Tensor((1, 640, 1, 1), dtype="float32") = params[635]
            lv395_1 = R.call_tir(fused_conv2d7_add13, (lv1313, lv723_1, lv724), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv396_1 = R.call_tir(fused_transpose19_reshape23, (lv395_1,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv725: R.Tensor((640,), dtype="float32") = params[320]
            lv726: R.Tensor((640,), dtype="float32") = params[321]
            lv1319 = R.call_tir(layer_norm2, (lv396_1, lv725, lv726), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv727: R.Tensor((640, 640), dtype="float32") = params[636]
            lv1321 = R.call_tir(matmul17, (lv1319, lv727), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv728: R.Tensor((640, 640), dtype="float32") = params[637]
            lv1323 = R.call_tir(matmul17, (lv1319, lv728), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv729: R.Tensor((640, 640), dtype="float32") = params[638]
            lv1325 = R.call_tir(matmul17, (lv1319, lv729), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv397_1 = R.call_tir(fused_reshape24_transpose21_reshape25, (lv1321,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv398_1 = R.call_tir(fused_reshape24_transpose21_reshape25_transpose22, (lv1323,), out_sinfo=R.Tensor((16, 80, 1024), dtype="float32"))
            lv399_1 = R.call_tir(fused_reshape24_transpose21_reshape25, (lv1325,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv400_1 = R.call_tir(fused_matmul18_multiply8, (lv397_1, lv398_1), out_sinfo=R.Tensor((16, 1024, 1024), dtype="float32"))
            lv1338 = R.call_tir(softmax3, (lv400_1,), out_sinfo=R.Tensor((16, 1024, 1024), dtype="float32"))
            lv1339 = R.call_tir(matmul19, (lv1338, lv399_1), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv401_1 = R.call_tir(fused_reshape26_transpose23_reshape27, (lv1339,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv730: R.Tensor((640, 640), dtype="float32") = params[639]
            lv731_1: R.Tensor((640,), dtype="float32") = params[322]
            lv402_1 = R.call_tir(fused_matmul17_add17_add18, (lv401_1, lv730, lv731_1, lv396_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv732: R.Tensor((640,), dtype="float32") = params[323]
            lv733_1: R.Tensor((640,), dtype="float32") = params[324]
            lv1347 = R.call_tir(layer_norm2, (lv402_1, lv732, lv733_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv734: R.Tensor((640, 640), dtype="float32") = params[640]
            lv1349 = R.call_tir(matmul17, (lv1347, lv734), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv735_1: R.Tensor((768, 640), dtype="float32") = params[641]
            lv1351 = R.call_tir(matmul20, (inp_2, lv735_1), out_sinfo=R.Tensor((2, 77, 640), dtype="float32"))
            lv736: R.Tensor((768, 640), dtype="float32") = params[642]
            lv1353 = R.call_tir(matmul20, (inp_2, lv736), out_sinfo=R.Tensor((2, 77, 640), dtype="float32"))
            lv403_2 = R.call_tir(fused_reshape24_transpose21_reshape25, (lv1349,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv404_2 = R.call_tir(fused_reshape28_transpose25_reshape29_transpose26, (lv1351,), out_sinfo=R.Tensor((16, 80, 77), dtype="float32"))
            lv405_1 = R.call_tir(fused_reshape28_transpose25_reshape29, (lv1353,), out_sinfo=R.Tensor((16, 77, 80), dtype="float32"))
            lv406_1 = R.call_tir(fused_matmul21_multiply9, (lv403_2, lv404_2), out_sinfo=R.Tensor((16, 1024, 77), dtype="float32"))
            lv1366 = R.call_tir(softmax4, (lv406_1,), out_sinfo=R.Tensor((16, 1024, 77), dtype="float32"))
            lv1367 = R.call_tir(matmul22, (lv1366, lv405_1), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv407_1 = R.call_tir(fused_reshape26_transpose23_reshape27, (lv1367,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv737_1: R.Tensor((640, 640), dtype="float32") = params[643]
            lv738: R.Tensor((640,), dtype="float32") = params[325]
            lv408_1 = R.call_tir(fused_matmul17_add17_add18, (lv407_1, lv737_1, lv738, lv402_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv739: R.Tensor((640,), dtype="float32") = params[326]
            lv740: R.Tensor((640,), dtype="float32") = params[327]
            lv1375 = R.call_tir(layer_norm2, (lv408_1, lv739, lv740), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv741: R.Tensor((640, 2560), dtype="float32") = params[645]
            lv742: R.Tensor((2560,), dtype="float32") = params[329]
            lv409_1 = R.call_tir(fused_matmul23_add19_gelu1, (lv1375, lv741, lv742), out_sinfo=R.Tensor((2, 1024, 2560), dtype="float32"))
            lv743: R.Tensor((640, 2560), dtype="float32") = params[644]
            lv744: R.Tensor((2560,), dtype="float32") = params[328]
            lv410_1 = R.call_tir(fused_matmul23_add19_multiply10, (lv1375, lv743, lv744, lv409_1), out_sinfo=R.Tensor((2, 1024, 2560), dtype="float32"))
            lv745: R.Tensor((2560, 640), dtype="float32") = params[646]
            lv746: R.Tensor((640,), dtype="float32") = params[330]
            lv411_1 = R.call_tir(fused_matmul24_add17_add18, (lv410_1, lv745, lv746, lv408_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv412_2 = R.call_tir(fused_reshape30_transpose29, (lv411_1,), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv747: R.Tensor((640, 640, 1, 1), dtype="float32") = params[331]
            lv748: R.Tensor((1, 640, 1, 1), dtype="float32") = params[647]
            lv413_1 = R.call_tir(fused_conv2d7_add13_add16, (lv412_2, lv747, lv748, lv394_1), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv1394 = R.call_tir(concatenate8, (lv413_1, lv129), out_sinfo=R.Tensor((2, 960, 32, 32), dtype="float32"))
            lv749: R.Tensor((960,), dtype="float32") = params[294]
            lv750_1: R.Tensor((960,), dtype="float32") = params[295]
            lv414_1 = R.call_tir(fused_group_norm15_silu12, (lv1394, lv749, lv750_1), out_sinfo=R.Tensor((2, 960, 32, 32), dtype="float32"))
            lv1400 = R.call_tir(silu, (lv79,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv751_1: R.Tensor((1280, 640), dtype="float32") = params[648]
            lv752: R.Tensor((640,), dtype="float32") = params[297]
            lv415_1 = R.call_tir(fused_matmul16_add14_strided_slice4, (lv1400, lv751_1, lv752), out_sinfo=R.Tensor((2, 640), dtype="float32"))
            lv1405 = R.call_tir(reshape22, (lv415_1,), out_sinfo=R.Tensor((2, 640, 1, 1), dtype="float32"))
            lv753: R.Tensor((640, 960, 3, 3), dtype="float32") = params[296]
            lv754: R.Tensor((1, 640, 1, 1), dtype="float32") = params[649]
            lv416_1 = R.call_tir(fused_conv2d27_add13_add15, (lv414_1, lv753, lv754, lv1405), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv755: R.Tensor((640,), dtype="float32") = params[298]
            lv756: R.Tensor((640,), dtype="float32") = params[299]
            lv417_1 = R.call_tir(fused_group_norm3_silu3, (lv416_1, lv755, lv756), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv757: R.Tensor((640, 960, 1, 1), dtype="float32") = params[301]
            lv758: R.Tensor((1, 640, 1, 1), dtype="float32") = params[651]
            lv418_1 = R.call_tir(fused_conv2d28_add13, (lv1394, lv757, lv758), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv759_1: R.Tensor((640, 640, 3, 3), dtype="float32") = params[300]
            lv760: R.Tensor((1, 640, 1, 1), dtype="float32") = params[650]
            lv419_1 = R.call_tir(fused_conv2d5_add13_add16_divide1, (lv417_1, lv759_1, lv760, lv418_1), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv761: R.Tensor((640,), dtype="float32") = params[332]
            lv762: R.Tensor((640,), dtype="float32") = params[333]
            lv1417 = R.call_tir(group_norm4, (lv419_1, lv761, lv762), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv763: R.Tensor((640, 640, 1, 1), dtype="float32") = params[334]
            lv764: R.Tensor((1, 640, 1, 1), dtype="float32") = params[652]
            lv420_1 = R.call_tir(fused_conv2d7_add13, (lv1417, lv763, lv764), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv421_1 = R.call_tir(fused_transpose19_reshape23, (lv420_1,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv765: R.Tensor((640,), dtype="float32") = params[335]
            lv766: R.Tensor((640,), dtype="float32") = params[336]
            lv1423 = R.call_tir(layer_norm2, (lv421_1, lv765, lv766), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv767: R.Tensor((640, 640), dtype="float32") = params[653]
            lv1425 = R.call_tir(matmul17, (lv1423, lv767), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv768: R.Tensor((640, 640), dtype="float32") = params[654]
            lv1427 = R.call_tir(matmul17, (lv1423, lv768), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv769: R.Tensor((640, 640), dtype="float32") = params[655]
            lv1429 = R.call_tir(matmul17, (lv1423, lv769), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv422_1 = R.call_tir(fused_reshape24_transpose21_reshape25, (lv1425,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv423_1 = R.call_tir(fused_reshape24_transpose21_reshape25_transpose22, (lv1427,), out_sinfo=R.Tensor((16, 80, 1024), dtype="float32"))
            lv424_1 = R.call_tir(fused_reshape24_transpose21_reshape25, (lv1429,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv425_1 = R.call_tir(fused_matmul18_multiply8, (lv422_1, lv423_1), out_sinfo=R.Tensor((16, 1024, 1024), dtype="float32"))
            lv1442 = R.call_tir(softmax3, (lv425_1,), out_sinfo=R.Tensor((16, 1024, 1024), dtype="float32"))
            lv1443 = R.call_tir(matmul19, (lv1442, lv424_1), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv426_1 = R.call_tir(fused_reshape26_transpose23_reshape27, (lv1443,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv770: R.Tensor((640, 640), dtype="float32") = params[656]
            lv771: R.Tensor((640,), dtype="float32") = params[337]
            lv427_1 = R.call_tir(fused_matmul17_add17_add18, (lv426_1, lv770, lv771, lv421_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv772: R.Tensor((640,), dtype="float32") = params[338]
            lv773: R.Tensor((640,), dtype="float32") = params[339]
            lv1451 = R.call_tir(layer_norm2, (lv427_1, lv772, lv773), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv774: R.Tensor((640, 640), dtype="float32") = params[657]
            lv1453 = R.call_tir(matmul17, (lv1451, lv774), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv775: R.Tensor((768, 640), dtype="float32") = params[658]
            lv1455 = R.call_tir(matmul20, (inp_2, lv775), out_sinfo=R.Tensor((2, 77, 640), dtype="float32"))
            lv776: R.Tensor((768, 640), dtype="float32") = params[659]
            lv1457 = R.call_tir(matmul20, (inp_2, lv776), out_sinfo=R.Tensor((2, 77, 640), dtype="float32"))
            lv428_1 = R.call_tir(fused_reshape24_transpose21_reshape25, (lv1453,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv429_1 = R.call_tir(fused_reshape28_transpose25_reshape29_transpose26, (lv1455,), out_sinfo=R.Tensor((16, 80, 77), dtype="float32"))
            lv430_1 = R.call_tir(fused_reshape28_transpose25_reshape29, (lv1457,), out_sinfo=R.Tensor((16, 77, 80), dtype="float32"))
            lv431_1 = R.call_tir(fused_matmul21_multiply9, (lv428_1, lv429_1), out_sinfo=R.Tensor((16, 1024, 77), dtype="float32"))
            lv1470 = R.call_tir(softmax4, (lv431_1,), out_sinfo=R.Tensor((16, 1024, 77), dtype="float32"))
            lv1471 = R.call_tir(matmul22, (lv1470, lv430_1), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv432_1 = R.call_tir(fused_reshape26_transpose23_reshape27, (lv1471,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv777: R.Tensor((640, 640), dtype="float32") = params[660]
            lv778: R.Tensor((640,), dtype="float32") = params[340]
            lv433_1 = R.call_tir(fused_matmul17_add17_add18, (lv432_1, lv777, lv778, lv427_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv779: R.Tensor((640,), dtype="float32") = params[341]
            lv780: R.Tensor((640,), dtype="float32") = params[342]
            lv1479 = R.call_tir(layer_norm2, (lv433_1, lv779, lv780), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv781: R.Tensor((640, 2560), dtype="float32") = params[662]
            lv782: R.Tensor((2560,), dtype="float32") = params[344]
            lv434_1 = R.call_tir(fused_matmul23_add19_gelu1, (lv1479, lv781, lv782), out_sinfo=R.Tensor((2, 1024, 2560), dtype="float32"))
            lv783_1: R.Tensor((640, 2560), dtype="float32") = params[661]
            lv784: R.Tensor((2560,), dtype="float32") = params[343]
            lv435_1 = R.call_tir(fused_matmul23_add19_multiply10, (lv1479, lv783_1, lv784, lv434_1), out_sinfo=R.Tensor((2, 1024, 2560), dtype="float32"))
            lv785: R.Tensor((2560, 640), dtype="float32") = params[663]
            lv786: R.Tensor((640,), dtype="float32") = params[345]
            lv436_1 = R.call_tir(fused_matmul24_add17_add18, (lv435_1, lv785, lv786, lv433_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv437_1 = R.call_tir(fused_reshape30_transpose29, (lv436_1,), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv787: R.Tensor((640, 640, 1, 1), dtype="float32") = params[346]
            lv788_1: R.Tensor((1, 640, 1, 1), dtype="float32") = params[664]
            lv438_1 = R.call_tir(fused_conv2d7_add13_add16, (lv437_1, lv787, lv788_1, lv419_1), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv1498 = R.call_tir(resize2d2, (lv438_1,), out_sinfo=R.Tensor((2, 640, 64, 64), dtype="float32"))
            lv789: R.Tensor((640, 640, 3, 3), dtype="float32") = params[347]
            lv790: R.Tensor((1, 640, 1, 1), dtype="float32") = params[665]
            lv439_2 = R.call_tir(fused_conv2d29_add34, (lv1498, lv789, lv790), out_sinfo=R.Tensor((2, 640, 64, 64), dtype="float32"))
            lv1502 = R.call_tir(concatenate9, (lv439_2, lv128), out_sinfo=R.Tensor((2, 960, 64, 64), dtype="float32"))
            lv791: R.Tensor((960,), dtype="float32") = params[348]
            lv792: R.Tensor((960,), dtype="float32") = params[349]
            lv440_1 = R.call_tir(fused_group_norm16_silu13, (lv1502, lv791, lv792), out_sinfo=R.Tensor((2, 960, 64, 64), dtype="float32"))
            lv1508 = R.call_tir(silu, (lv79,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv793: R.Tensor((1280, 320), dtype="float32") = params[666]
            lv794: R.Tensor((320,), dtype="float32") = params[351]
            lv441_1 = R.call_tir(fused_matmul7_add6_strided_slice3, (lv1508, lv793, lv794), out_sinfo=R.Tensor((2, 320), dtype="float32"))
            lv1513 = R.call_tir(reshape12, (lv441_1,), out_sinfo=R.Tensor((2, 320, 1, 1), dtype="float32"))
            lv795: R.Tensor((320, 960, 3, 3), dtype="float32") = params[350]
            lv796: R.Tensor((1, 320, 1, 1), dtype="float32") = params[667]
            lv442_1 = R.call_tir(fused_conv2d30_add5_add7, (lv440_1, lv795, lv796, lv1513), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv797_1: R.Tensor((320,), dtype="float32") = params[352]
            lv798: R.Tensor((320,), dtype="float32") = params[353]
            lv443_1 = R.call_tir(fused_group_norm_silu1, (lv442_1, lv797_1, lv798), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv799: R.Tensor((320, 960, 1, 1), dtype="float32") = params[355]
            lv800: R.Tensor((1, 320, 1, 1), dtype="float32") = params[668]
            lv444_2 = R.call_tir(fused_conv2d31_add5, (lv1502, lv799, lv800), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv801: R.Tensor((320, 320, 3, 3), dtype="float32") = params[354]
            lv802: R.Tensor((1, 320, 1, 1), dtype="float32") = params[669]
            lv445_1 = R.call_tir(fused_conv2d1_add5_add8_divide, (lv443_1, lv801, lv802, lv444_2), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv803_1: R.Tensor((320,), dtype="float32") = params[372]
            lv804: R.Tensor((320,), dtype="float32") = params[373]
            lv1525 = R.call_tir(group_norm1, (lv445_1, lv803_1, lv804), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv805: R.Tensor((320, 320, 1, 1), dtype="float32") = params[374]
            lv806: R.Tensor((1, 320, 1, 1), dtype="float32") = params[670]
            lv446_1 = R.call_tir(fused_conv2d2_add5, (lv1525, lv805, lv806), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv447_1 = R.call_tir(fused_transpose9_reshape13, (lv446_1,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv807: R.Tensor((320,), dtype="float32") = params[375]
            lv808_1: R.Tensor((320,), dtype="float32") = params[376]
            lv1531 = R.call_tir(layer_norm1, (lv447_1, lv807, lv808_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv809: R.Tensor((320, 320), dtype="float32") = params[671]
            lv1533 = R.call_tir(matmul8, (lv1531, lv809), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv810: R.Tensor((320, 320), dtype="float32") = params[672]
            lv1535 = R.call_tir(matmul8, (lv1531, lv810), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv811: R.Tensor((320, 320), dtype="float32") = params[673]
            lv1537 = R.call_tir(matmul8, (lv1531, lv811), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv448_1 = R.call_tir(fused_reshape14_transpose11_reshape15, (lv1533,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv449_1 = R.call_tir(fused_reshape14_transpose11_reshape15_transpose12, (lv1535,), out_sinfo=R.Tensor((16, 40, 4096), dtype="float32"))
            lv450_1 = R.call_tir(fused_reshape14_transpose11_reshape15, (lv1537,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv451_1 = R.call_tir(fused_matmul9_multiply5, (lv448_1, lv449_1), out_sinfo=R.Tensor((16, 4096, 4096), dtype="float32"))
            lv1550 = R.call_tir(softmax1, (lv451_1,), out_sinfo=R.Tensor((16, 4096, 4096), dtype="float32"))
            lv1551 = R.call_tir(matmul10, (lv1550, lv450_1), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv452_1 = R.call_tir(fused_reshape16_transpose13_reshape17, (lv1551,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv812: R.Tensor((320, 320), dtype="float32") = params[674]
            lv813: R.Tensor((320,), dtype="float32") = params[377]
            lv453_1 = R.call_tir(fused_matmul8_add9_add10, (lv452_1, lv812, lv813, lv447_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv814: R.Tensor((320,), dtype="float32") = params[378]
            lv815: R.Tensor((320,), dtype="float32") = params[379]
            lv1559 = R.call_tir(layer_norm1, (lv453_1, lv814, lv815), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv816: R.Tensor((320, 320), dtype="float32") = params[675]
            lv1561 = R.call_tir(matmul8, (lv1559, lv816), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv817: R.Tensor((768, 320), dtype="float32") = params[676]
            lv1563 = R.call_tir(matmul11, (inp_2, lv817), out_sinfo=R.Tensor((2, 77, 320), dtype="float32"))
            lv818: R.Tensor((768, 320), dtype="float32") = params[677]
            lv1565 = R.call_tir(matmul11, (inp_2, lv818), out_sinfo=R.Tensor((2, 77, 320), dtype="float32"))
            lv454_1 = R.call_tir(fused_reshape14_transpose11_reshape15, (lv1561,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv455_1 = R.call_tir(fused_reshape18_transpose15_reshape19_transpose16, (lv1563,), out_sinfo=R.Tensor((16, 40, 77), dtype="float32"))
            lv456_2 = R.call_tir(fused_reshape18_transpose15_reshape19, (lv1565,), out_sinfo=R.Tensor((16, 77, 40), dtype="float32"))
            lv457_1 = R.call_tir(fused_matmul12_multiply6, (lv454_1, lv455_1), out_sinfo=R.Tensor((16, 4096, 77), dtype="float32"))
            lv1578 = R.call_tir(softmax2, (lv457_1,), out_sinfo=R.Tensor((16, 4096, 77), dtype="float32"))
            lv1579 = R.call_tir(matmul13, (lv1578, lv456_2), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv458_1 = R.call_tir(fused_reshape16_transpose13_reshape17, (lv1579,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv819: R.Tensor((320, 320), dtype="float32") = params[678]
            lv820_1: R.Tensor((320,), dtype="float32") = params[380]
            lv459_1 = R.call_tir(fused_matmul8_add9_add10, (lv458_1, lv819, lv820_1, lv453_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv821: R.Tensor((320,), dtype="float32") = params[381]
            lv822: R.Tensor((320,), dtype="float32") = params[382]
            lv1587 = R.call_tir(layer_norm1, (lv459_1, lv821, lv822), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv823: R.Tensor((320, 1280), dtype="float32") = params[680]
            lv824: R.Tensor((1280,), dtype="float32") = params[384]
            lv460_1 = R.call_tir(fused_matmul14_add11_gelu, (lv1587, lv823, lv824), out_sinfo=R.Tensor((2, 4096, 1280), dtype="float32"))
            lv825: R.Tensor((320, 1280), dtype="float32") = params[679]
            lv826_1: R.Tensor((1280,), dtype="float32") = params[383]
            lv461_1 = R.call_tir(fused_matmul14_add11_multiply7, (lv1587, lv825, lv826_1, lv460_1), out_sinfo=R.Tensor((2, 4096, 1280), dtype="float32"))
            lv827: R.Tensor((1280, 320), dtype="float32") = params[681]
            lv828: R.Tensor((320,), dtype="float32") = params[385]
            lv462_2 = R.call_tir(fused_matmul15_add9_add10, (lv461_1, lv827, lv828, lv459_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv463_1 = R.call_tir(fused_reshape20_transpose17, (lv462_2,), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv829: R.Tensor((320, 320, 1, 1), dtype="float32") = params[386]
            lv830: R.Tensor((1, 320, 1, 1), dtype="float32") = params[682]
            lv464_2 = R.call_tir(fused_conv2d2_add5_add8, (lv463_1, lv829, lv830, lv445_1), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv1606 = R.call_tir(concatenate10, (lv464_2, lv104), out_sinfo=R.Tensor((2, 640, 64, 64), dtype="float32"))
            lv831_1: R.Tensor((640,), dtype="float32") = params[356]
            lv832: R.Tensor((640,), dtype="float32") = params[357]
            lv465_1 = R.call_tir(fused_group_norm17_silu14, (lv1606, lv831_1, lv832), out_sinfo=R.Tensor((2, 640, 64, 64), dtype="float32"))
            lv1612 = R.call_tir(silu, (lv79,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv833: R.Tensor((1280, 320), dtype="float32") = params[683]
            lv834: R.Tensor((320,), dtype="float32") = params[359]
            lv466_2 = R.call_tir(fused_matmul7_add6_strided_slice3, (lv1612, lv833, lv834), out_sinfo=R.Tensor((2, 320), dtype="float32"))
            lv1617 = R.call_tir(reshape12, (lv466_2,), out_sinfo=R.Tensor((2, 320, 1, 1), dtype="float32"))
            lv835: R.Tensor((320, 640, 3, 3), dtype="float32") = params[358]
            lv836: R.Tensor((1, 320, 1, 1), dtype="float32") = params[684]
            lv467_1 = R.call_tir(fused_conv2d32_add5_add7, (lv465_1, lv835, lv836, lv1617), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv837: R.Tensor((320,), dtype="float32") = params[360]
            lv838: R.Tensor((320,), dtype="float32") = params[361]
            lv468_2 = R.call_tir(fused_group_norm_silu1, (lv467_1, lv837, lv838), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv839: R.Tensor((320, 640, 1, 1), dtype="float32") = params[363]
            lv840: R.Tensor((1, 320, 1, 1), dtype="float32") = params[686]
            lv469_1 = R.call_tir(fused_conv2d33_add5, (lv1606, lv839, lv840), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv841: R.Tensor((320, 320, 3, 3), dtype="float32") = params[362]
            lv842: R.Tensor((1, 320, 1, 1), dtype="float32") = params[685]
            lv470_1 = R.call_tir(fused_conv2d1_add5_add8_divide, (lv468_2, lv841, lv842, lv469_1), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv843_1: R.Tensor((320,), dtype="float32") = params[387]
            lv844: R.Tensor((320,), dtype="float32") = params[388]
            lv1629 = R.call_tir(group_norm1, (lv470_1, lv843_1, lv844), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv845: R.Tensor((320, 320, 1, 1), dtype="float32") = params[389]
            lv846: R.Tensor((1, 320, 1, 1), dtype="float32") = params[687]
            lv471_1 = R.call_tir(fused_conv2d2_add5, (lv1629, lv845, lv846), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv472_1 = R.call_tir(fused_transpose9_reshape13, (lv471_1,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv847: R.Tensor((320,), dtype="float32") = params[390]
            lv848: R.Tensor((320,), dtype="float32") = params[391]
            lv1635 = R.call_tir(layer_norm1, (lv472_1, lv847, lv848), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv849_1: R.Tensor((320, 320), dtype="float32") = params[688]
            lv1637 = R.call_tir(matmul8, (lv1635, lv849_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv850: R.Tensor((320, 320), dtype="float32") = params[689]
            lv1639 = R.call_tir(matmul8, (lv1635, lv850), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv851: R.Tensor((320, 320), dtype="float32") = params[690]
            lv1641 = R.call_tir(matmul8, (lv1635, lv851), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv473_1 = R.call_tir(fused_reshape14_transpose11_reshape15, (lv1637,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv474_1 = R.call_tir(fused_reshape14_transpose11_reshape15_transpose12, (lv1639,), out_sinfo=R.Tensor((16, 40, 4096), dtype="float32"))
            lv475_1 = R.call_tir(fused_reshape14_transpose11_reshape15, (lv1641,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv476_1 = R.call_tir(fused_matmul9_multiply5, (lv473_1, lv474_1), out_sinfo=R.Tensor((16, 4096, 4096), dtype="float32"))
            lv1654 = R.call_tir(softmax1, (lv476_1,), out_sinfo=R.Tensor((16, 4096, 4096), dtype="float32"))
            lv1655 = R.call_tir(matmul10, (lv1654, lv475_1), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv477_1 = R.call_tir(fused_reshape16_transpose13_reshape17, (lv1655,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv852: R.Tensor((320, 320), dtype="float32") = params[691]
            lv853: R.Tensor((320,), dtype="float32") = params[392]
            lv478_1 = R.call_tir(fused_matmul8_add9_add10, (lv477_1, lv852, lv853, lv472_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv854_1: R.Tensor((320,), dtype="float32") = params[393]
            lv855: R.Tensor((320,), dtype="float32") = params[394]
            lv1663 = R.call_tir(layer_norm1, (lv478_1, lv854_1, lv855), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv856: R.Tensor((320, 320), dtype="float32") = params[692]
            lv1665 = R.call_tir(matmul8, (lv1663, lv856), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv857: R.Tensor((768, 320), dtype="float32") = params[693]
            lv1667 = R.call_tir(matmul11, (inp_2, lv857), out_sinfo=R.Tensor((2, 77, 320), dtype="float32"))
            lv858: R.Tensor((768, 320), dtype="float32") = params[694]
            lv1669 = R.call_tir(matmul11, (inp_2, lv858), out_sinfo=R.Tensor((2, 77, 320), dtype="float32"))
            lv479_1 = R.call_tir(fused_reshape14_transpose11_reshape15, (lv1665,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv480_1 = R.call_tir(fused_reshape18_transpose15_reshape19_transpose16, (lv1667,), out_sinfo=R.Tensor((16, 40, 77), dtype="float32"))
            lv481_2 = R.call_tir(fused_reshape18_transpose15_reshape19, (lv1669,), out_sinfo=R.Tensor((16, 77, 40), dtype="float32"))
            lv482_2 = R.call_tir(fused_matmul12_multiply6, (lv479_1, lv480_1), out_sinfo=R.Tensor((16, 4096, 77), dtype="float32"))
            lv1682 = R.call_tir(softmax2, (lv482_2,), out_sinfo=R.Tensor((16, 4096, 77), dtype="float32"))
            lv1683 = R.call_tir(matmul13, (lv1682, lv481_2), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv483_1 = R.call_tir(fused_reshape16_transpose13_reshape17, (lv1683,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv859: R.Tensor((320, 320), dtype="float32") = params[695]
            lv860: R.Tensor((320,), dtype="float32") = params[395]
            lv484_1 = R.call_tir(fused_matmul8_add9_add10, (lv483_1, lv859, lv860, lv478_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv861: R.Tensor((320,), dtype="float32") = params[396]
            lv862: R.Tensor((320,), dtype="float32") = params[397]
            lv1691 = R.call_tir(layer_norm1, (lv484_1, lv861, lv862), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv863: R.Tensor((320, 1280), dtype="float32") = params[697]
            lv864: R.Tensor((1280,), dtype="float32") = params[399]
            lv485_1 = R.call_tir(fused_matmul14_add11_gelu, (lv1691, lv863, lv864), out_sinfo=R.Tensor((2, 4096, 1280), dtype="float32"))
            lv865: R.Tensor((320, 1280), dtype="float32") = params[696]
            lv866_1: R.Tensor((1280,), dtype="float32") = params[398]
            lv486_1 = R.call_tir(fused_matmul14_add11_multiply7, (lv1691, lv865, lv866_1, lv485_1), out_sinfo=R.Tensor((2, 4096, 1280), dtype="float32"))
            lv867: R.Tensor((1280, 320), dtype="float32") = params[698]
            lv868: R.Tensor((320,), dtype="float32") = params[400]
            lv487_1 = R.call_tir(fused_matmul15_add9_add10, (lv486_1, lv867, lv868, lv484_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv488_1 = R.call_tir(fused_reshape20_transpose17, (lv487_1,), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv869: R.Tensor((320, 320, 1, 1), dtype="float32") = params[401]
            lv870_1: R.Tensor((1, 320, 1, 1), dtype="float32") = params[699]
            lv489_1 = R.call_tir(fused_conv2d2_add5_add8, (lv488_1, lv869, lv870_1, lv470_1), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv1710 = R.call_tir(concatenate10, (lv489_1, lv80), out_sinfo=R.Tensor((2, 640, 64, 64), dtype="float32"))
            lv871: R.Tensor((640,), dtype="float32") = params[364]
            lv872: R.Tensor((640,), dtype="float32") = params[365]
            lv490_2 = R.call_tir(fused_group_norm17_silu14, (lv1710, lv871, lv872), out_sinfo=R.Tensor((2, 640, 64, 64), dtype="float32"))
            lv1716 = R.call_tir(silu, (lv79,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv873: R.Tensor((1280, 320), dtype="float32") = params[700]
            lv874: R.Tensor((320,), dtype="float32") = params[367]
            lv491_1 = R.call_tir(fused_matmul7_add6_strided_slice3, (lv1716, lv873, lv874), out_sinfo=R.Tensor((2, 320), dtype="float32"))
            lv1721 = R.call_tir(reshape12, (lv491_1,), out_sinfo=R.Tensor((2, 320, 1, 1), dtype="float32"))
            lv875: R.Tensor((320, 640, 3, 3), dtype="float32") = params[366]
            lv876_1: R.Tensor((1, 320, 1, 1), dtype="float32") = params[701]
            lv492_2 = R.call_tir(fused_conv2d32_add5_add7, (lv490_2, lv875, lv876_1, lv1721), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv877: R.Tensor((320,), dtype="float32") = params[368]
            lv878: R.Tensor((320,), dtype="float32") = params[369]
            lv493_1 = R.call_tir(fused_group_norm_silu1, (lv492_2, lv877, lv878), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv879: R.Tensor((320, 640, 1, 1), dtype="float32") = params[371]
            lv880: R.Tensor((1, 320, 1, 1), dtype="float32") = params[702]
            lv494_2 = R.call_tir(fused_conv2d33_add5, (lv1710, lv879, lv880), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv881_1: R.Tensor((320, 320, 3, 3), dtype="float32") = params[370]
            lv882: R.Tensor((1, 320, 1, 1), dtype="float32") = params[703]
            lv495_1 = R.call_tir(fused_conv2d1_add5_add8_divide, (lv493_1, lv881_1, lv882, lv494_2), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv883: R.Tensor((320,), dtype="float32") = params[402]
            lv884: R.Tensor((320,), dtype="float32") = params[403]
            lv1733 = R.call_tir(group_norm1, (lv495_1, lv883, lv884), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv885: R.Tensor((320, 320, 1, 1), dtype="float32") = params[404]
            lv886: R.Tensor((1, 320, 1, 1), dtype="float32") = params[704]
            lv496_2 = R.call_tir(fused_conv2d2_add5, (lv1733, lv885, lv886), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv497_1 = R.call_tir(fused_transpose9_reshape13, (lv496_2,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv887: R.Tensor((320,), dtype="float32") = params[405]
            lv888: R.Tensor((320,), dtype="float32") = params[406]
            lv1739 = R.call_tir(layer_norm1, (lv497_1, lv887, lv888), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv889: R.Tensor((320, 320), dtype="float32") = params[705]
            lv1741 = R.call_tir(matmul8, (lv1739, lv889), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv890: R.Tensor((320, 320), dtype="float32") = params[706]
            lv1743 = R.call_tir(matmul8, (lv1739, lv890), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv891: R.Tensor((320, 320), dtype="float32") = params[707]
            lv1745 = R.call_tir(matmul8, (lv1739, lv891), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv498_1 = R.call_tir(fused_reshape14_transpose11_reshape15, (lv1741,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv499_1 = R.call_tir(fused_reshape14_transpose11_reshape15_transpose12, (lv1743,), out_sinfo=R.Tensor((16, 40, 4096), dtype="float32"))
            lv500_1 = R.call_tir(fused_reshape14_transpose11_reshape15, (lv1745,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv501_1 = R.call_tir(fused_matmul9_multiply5, (lv498_1, lv499_1), out_sinfo=R.Tensor((16, 4096, 4096), dtype="float32"))
            lv1758 = R.call_tir(softmax1, (lv501_1,), out_sinfo=R.Tensor((16, 4096, 4096), dtype="float32"))
            lv1759 = R.call_tir(matmul10, (lv1758, lv500_1), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv502_1 = R.call_tir(fused_reshape16_transpose13_reshape17, (lv1759,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv892: R.Tensor((320, 320), dtype="float32") = params[708]
            lv893_1: R.Tensor((320,), dtype="float32") = params[407]
            lv503_1 = R.call_tir(fused_matmul8_add9_add10, (lv502_1, lv892, lv893_1, lv497_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv894: R.Tensor((320,), dtype="float32") = params[408]
            lv895: R.Tensor((320,), dtype="float32") = params[409]
            lv1767 = R.call_tir(layer_norm1, (lv503_1, lv894, lv895), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv896: R.Tensor((320, 320), dtype="float32") = params[709]
            lv1769 = R.call_tir(matmul8, (lv1767, lv896), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv897: R.Tensor((768, 320), dtype="float32") = params[710]
            lv1771 = R.call_tir(matmul11, (inp_2, lv897), out_sinfo=R.Tensor((2, 77, 320), dtype="float32"))
            lv898: R.Tensor((768, 320), dtype="float32") = params[711]
            lv1773 = R.call_tir(matmul11, (inp_2, lv898), out_sinfo=R.Tensor((2, 77, 320), dtype="float32"))
            lv504_1 = R.call_tir(fused_reshape14_transpose11_reshape15, (lv1769,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv505_1 = R.call_tir(fused_reshape18_transpose15_reshape19_transpose16, (lv1771,), out_sinfo=R.Tensor((16, 40, 77), dtype="float32"))
            lv506_1 = R.call_tir(fused_reshape18_transpose15_reshape19, (lv1773,), out_sinfo=R.Tensor((16, 77, 40), dtype="float32"))
            lv507_1 = R.call_tir(fused_matmul12_multiply6, (lv504_1, lv505_1), out_sinfo=R.Tensor((16, 4096, 77), dtype="float32"))
            lv1786 = R.call_tir(softmax2, (lv507_1,), out_sinfo=R.Tensor((16, 4096, 77), dtype="float32"))
            lv1787 = R.call_tir(matmul13, (lv1786, lv506_1), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv508_1 = R.call_tir(fused_reshape16_transpose13_reshape17, (lv1787,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv899_1: R.Tensor((320, 320), dtype="float32") = params[712]
            lv900: R.Tensor((320,), dtype="float32") = params[410]
            lv509_2 = R.call_tir(fused_matmul8_add9_add10, (lv508_1, lv899_1, lv900, lv503_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv901_1: R.Tensor((320,), dtype="float32") = params[411]
            lv902: R.Tensor((320,), dtype="float32") = params[412]
            lv1795 = R.call_tir(layer_norm1, (lv509_2, lv901_1, lv902), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv903_1: R.Tensor((320, 1280), dtype="float32") = params[714]
            lv904: R.Tensor((1280,), dtype="float32") = params[414]
            lv510_2 = R.call_tir(fused_matmul14_add11_gelu, (lv1795, lv903_1, lv904), out_sinfo=R.Tensor((2, 4096, 1280), dtype="float32"))
            lv905_1: R.Tensor((320, 1280), dtype="float32") = params[713]
            lv906: R.Tensor((1280,), dtype="float32") = params[413]
            lv511_1 = R.call_tir(fused_matmul14_add11_multiply7, (lv1795, lv905_1, lv906, lv510_2), out_sinfo=R.Tensor((2, 4096, 1280), dtype="float32"))
            lv907: R.Tensor((1280, 320), dtype="float32") = params[715]
            lv908: R.Tensor((320,), dtype="float32") = params[415]
            lv512_1 = R.call_tir(fused_matmul15_add9_add10, (lv511_1, lv907, lv908, lv509_2), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv513_1 = R.call_tir(fused_reshape20_transpose17, (lv512_1,), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv909: R.Tensor((320, 320, 1, 1), dtype="float32") = params[416]
            lv910: R.Tensor((1, 320, 1, 1), dtype="float32") = params[716]
            lv514_1 = R.call_tir(fused_conv2d2_add5_add8, (lv513_1, lv909, lv910, lv495_1), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv911: R.Tensor((320,), dtype="float32") = params[417]
            lv912: R.Tensor((320,), dtype="float32") = params[418]
            lv515_1 = R.call_tir(fused_group_norm_silu1, (lv514_1, lv911, lv912), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv913: R.Tensor((4, 320, 3, 3), dtype="float32") = params[419]
            lv914: R.Tensor((1, 4, 1, 1), dtype="float32") = params[717]
            lv516_1 = R.call_tir(fused_conv2d34_add35, (lv515_1, lv913, lv914), out_sinfo=R.Tensor((2, 4, 64, 64), dtype="float32"))
            lv517_1 = R.call_tir(fused_split_subtract_multiply17_add36, (lv516_1,), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
            gv: R.Tensor((1, 4, 64, 64), dtype="float32") = lv517_1
            R.output(gv)
        return gv

    @R.function
    def vae(inp_0: R.Tensor((1, 4, 64, 64), dtype="float32"), params: R.Tuple(R.Tensor((4, 4, 1, 1), dtype="float32"), R.Tensor((512, 4, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((256, 512, 3, 3), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256, 256, 3, 3), dtype="float32"), R.Tensor((256, 512, 1, 1), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256, 256, 3, 3), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256, 256, 3, 3), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256, 256, 3, 3), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256, 256, 3, 3), dtype="float32"), R.Tensor((256, 256, 3, 3), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((128, 256, 3, 3), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128, 128, 3, 3), dtype="float32"), R.Tensor((128, 256, 1, 1), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128, 128, 3, 3), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128, 128, 3, 3), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128, 128, 3, 3), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128, 128, 3, 3), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((3, 128, 3, 3), dtype="float32"), R.Tensor((1, 4, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((512, 512), dtype="float32"), R.Tensor((512, 512), dtype="float32"), R.Tensor((512, 512), dtype="float32"), R.Tensor((512, 512), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 256, 1, 1), dtype="float32"), R.Tensor((1, 256, 1, 1), dtype="float32"), R.Tensor((1, 256, 1, 1), dtype="float32"), R.Tensor((1, 256, 1, 1), dtype="float32"), R.Tensor((1, 256, 1, 1), dtype="float32"), R.Tensor((1, 256, 1, 1), dtype="float32"), R.Tensor((1, 256, 1, 1), dtype="float32"), R.Tensor((1, 256, 1, 1), dtype="float32"), R.Tensor((1, 128, 1, 1), dtype="float32"), R.Tensor((1, 128, 1, 1), dtype="float32"), R.Tensor((1, 128, 1, 1), dtype="float32"), R.Tensor((1, 128, 1, 1), dtype="float32"), R.Tensor((1, 128, 1, 1), dtype="float32"), R.Tensor((1, 128, 1, 1), dtype="float32"), R.Tensor((1, 128, 1, 1), dtype="float32"), R.Tensor((1, 3, 1, 1), dtype="float32"))) -> R.Tensor((1, 512, 512, 3), dtype="float32"):
        with R.dataflow():
            lv = R.call_tir(multiply18, (inp_0,), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
            lv915: R.Tensor((4, 4, 1, 1), dtype="float32") = params[0]
            lv916: R.Tensor((1, 4, 1, 1), dtype="float32") = params[100]
            lv_1 = R.call_tir(fused_conv2d35_add37, (lv, lv915, lv916), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
            lv917: R.Tensor((512, 4, 3, 3), dtype="float32") = params[1]
            lv918: R.Tensor((1, 512, 1, 1), dtype="float32") = params[101]
            lv1 = R.call_tir(fused_conv2d36_add38, (lv_1, lv917, lv918), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv919: R.Tensor((512,), dtype="float32") = params[2]
            lv920: R.Tensor((512,), dtype="float32") = params[3]
            lv2 = R.call_tir(fused_group_norm18_silu15, (lv1, lv919, lv920), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv921: R.Tensor((512, 512, 3, 3), dtype="float32") = params[4]
            lv922: R.Tensor((1, 512, 1, 1), dtype="float32") = params[102]
            lv3 = R.call_tir(fused_conv2d37_add38, (lv2, lv921, lv922), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv923: R.Tensor((512,), dtype="float32") = params[5]
            lv924: R.Tensor((512,), dtype="float32") = params[6]
            lv4 = R.call_tir(fused_group_norm18_silu15, (lv3, lv923, lv924), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv925: R.Tensor((512, 512, 3, 3), dtype="float32") = params[7]
            lv926: R.Tensor((1, 512, 1, 1), dtype="float32") = params[103]
            lv5 = R.call_tir(fused_conv2d37_add38_add39_divide4, (lv4, lv925, lv926, lv1), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv927: R.Tensor((512,), dtype="float32") = params[14]
            lv928: R.Tensor((512,), dtype="float32") = params[15]
            lv19 = R.call_tir(group_norm18, (lv5, lv927, lv928), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv6 = R.call_tir(fused_reshape49_transpose45, (lv19,), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv929: R.Tensor((512, 512), dtype="float32") = params[104]
            lv930: R.Tensor((512,), dtype="float32") = params[16]
            lv7 = R.call_tir(fused_matmul40_add40, (lv6, lv929, lv930), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv931: R.Tensor((512, 512), dtype="float32") = params[105]
            lv932: R.Tensor((512,), dtype="float32") = params[17]
            lv8 = R.call_tir(fused_matmul40_add40, (lv6, lv931, lv932), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv933: R.Tensor((512, 512), dtype="float32") = params[106]
            lv934: R.Tensor((512,), dtype="float32") = params[18]
            lv9 = R.call_tir(fused_matmul40_add40, (lv6, lv933, lv934), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv10 = R.call_tir(fused_reshape50_transpose47_reshape51, (lv7,), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv11 = R.call_tir(fused_reshape50_transpose47_reshape51_transpose48, (lv8,), out_sinfo=R.Tensor((1, 512, 4096), dtype="float32"))
            lv12 = R.call_tir(fused_reshape50_transpose47_reshape51, (lv9,), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv13 = R.call_tir(fused_matmul41_multiply19_cast3, (lv10, lv11), out_sinfo=R.Tensor((1, 4096, 4096), dtype="float32"))
            lv14 = R.call_tir(fused_softmax9_cast3, (lv13,), out_sinfo=R.Tensor((1, 4096, 4096), dtype="float32"))
            lv46 = R.call_tir(matmul42, (lv14, lv12), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv15 = R.call_tir(fused_reshape52_transpose49_reshape53, (lv46,), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv935: R.Tensor((512, 512), dtype="float32") = params[107]
            lv936: R.Tensor((512,), dtype="float32") = params[19]
            lv16 = R.call_tir(fused_matmul40_add40, (lv15, lv935, lv936), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv17 = R.call_tir(fused_transpose48_reshape54_add39_divide4, (lv16, lv5), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv937: R.Tensor((512,), dtype="float32") = params[8]
            lv938: R.Tensor((512,), dtype="float32") = params[9]
            lv18 = R.call_tir(fused_group_norm18_silu15, (lv17, lv937, lv938), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv939: R.Tensor((512, 512, 3, 3), dtype="float32") = params[10]
            lv940: R.Tensor((1, 512, 1, 1), dtype="float32") = params[108]
            lv19_1 = R.call_tir(fused_conv2d37_add38, (lv18, lv939, lv940), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv941: R.Tensor((512,), dtype="float32") = params[11]
            lv942: R.Tensor((512,), dtype="float32") = params[12]
            lv20 = R.call_tir(fused_group_norm18_silu15, (lv19_1, lv941, lv942), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv943: R.Tensor((512, 512, 3, 3), dtype="float32") = params[13]
            lv944: R.Tensor((1, 512, 1, 1), dtype="float32") = params[109]
            lv21 = R.call_tir(fused_conv2d37_add38_add39_divide4, (lv20, lv943, lv944, lv17), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv945: R.Tensor((512,), dtype="float32") = params[20]
            lv946: R.Tensor((512,), dtype="float32") = params[21]
            lv22 = R.call_tir(fused_group_norm18_silu15, (lv21, lv945, lv946), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv947: R.Tensor((512, 512, 3, 3), dtype="float32") = params[22]
            lv948: R.Tensor((1, 512, 1, 1), dtype="float32") = params[110]
            lv23 = R.call_tir(fused_conv2d37_add38, (lv22, lv947, lv948), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv949: R.Tensor((512,), dtype="float32") = params[23]
            lv950: R.Tensor((512,), dtype="float32") = params[24]
            lv24 = R.call_tir(fused_group_norm18_silu15, (lv23, lv949, lv950), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv951: R.Tensor((512, 512, 3, 3), dtype="float32") = params[25]
            lv952: R.Tensor((1, 512, 1, 1), dtype="float32") = params[111]
            lv25 = R.call_tir(fused_conv2d37_add38_add39_divide4, (lv24, lv951, lv952, lv21), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv953: R.Tensor((512,), dtype="float32") = params[26]
            lv954: R.Tensor((512,), dtype="float32") = params[27]
            lv26 = R.call_tir(fused_group_norm18_silu15, (lv25, lv953, lv954), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv955: R.Tensor((512, 512, 3, 3), dtype="float32") = params[28]
            lv956: R.Tensor((1, 512, 1, 1), dtype="float32") = params[112]
            lv27 = R.call_tir(fused_conv2d37_add38, (lv26, lv955, lv956), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv957: R.Tensor((512,), dtype="float32") = params[29]
            lv958: R.Tensor((512,), dtype="float32") = params[30]
            lv28 = R.call_tir(fused_group_norm18_silu15, (lv27, lv957, lv958), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv959: R.Tensor((512, 512, 3, 3), dtype="float32") = params[31]
            lv960: R.Tensor((1, 512, 1, 1), dtype="float32") = params[113]
            lv29 = R.call_tir(fused_conv2d37_add38_add39_divide4, (lv28, lv959, lv960, lv25), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv961: R.Tensor((512,), dtype="float32") = params[32]
            lv962: R.Tensor((512,), dtype="float32") = params[33]
            lv30 = R.call_tir(fused_group_norm18_silu15, (lv29, lv961, lv962), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv963: R.Tensor((512, 512, 3, 3), dtype="float32") = params[34]
            lv964: R.Tensor((1, 512, 1, 1), dtype="float32") = params[114]
            lv31 = R.call_tir(fused_conv2d37_add38, (lv30, lv963, lv964), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv965: R.Tensor((512,), dtype="float32") = params[35]
            lv966: R.Tensor((512,), dtype="float32") = params[36]
            lv32 = R.call_tir(fused_group_norm18_silu15, (lv31, lv965, lv966), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv967: R.Tensor((512, 512, 3, 3), dtype="float32") = params[37]
            lv968: R.Tensor((1, 512, 1, 1), dtype="float32") = params[115]
            lv33 = R.call_tir(fused_conv2d37_add38_add39_divide4, (lv32, lv967, lv968, lv29), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv105 = R.call_tir(resize2d3, (lv33,), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv969: R.Tensor((512, 512, 3, 3), dtype="float32") = params[38]
            lv970: R.Tensor((1, 512, 1, 1), dtype="float32") = params[116]
            lv34 = R.call_tir(fused_conv2d38_add41, (lv105, lv969, lv970), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv971: R.Tensor((512,), dtype="float32") = params[39]
            lv972: R.Tensor((512,), dtype="float32") = params[40]
            lv35 = R.call_tir(fused_group_norm19_silu16, (lv34, lv971, lv972), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv973: R.Tensor((512, 512, 3, 3), dtype="float32") = params[41]
            lv974: R.Tensor((1, 512, 1, 1), dtype="float32") = params[117]
            lv36 = R.call_tir(fused_conv2d38_add41, (lv35, lv973, lv974), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv975: R.Tensor((512,), dtype="float32") = params[42]
            lv976: R.Tensor((512,), dtype="float32") = params[43]
            lv37 = R.call_tir(fused_group_norm19_silu16, (lv36, lv975, lv976), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv977: R.Tensor((512, 512, 3, 3), dtype="float32") = params[44]
            lv978: R.Tensor((1, 512, 1, 1), dtype="float32") = params[118]
            lv38 = R.call_tir(fused_conv2d38_add41_add42_divide5, (lv37, lv977, lv978, lv34), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv979: R.Tensor((512,), dtype="float32") = params[45]
            lv980: R.Tensor((512,), dtype="float32") = params[46]
            lv39 = R.call_tir(fused_group_norm19_silu16, (lv38, lv979, lv980), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv981: R.Tensor((512, 512, 3, 3), dtype="float32") = params[47]
            lv982: R.Tensor((1, 512, 1, 1), dtype="float32") = params[119]
            lv40 = R.call_tir(fused_conv2d38_add41, (lv39, lv981, lv982), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv983: R.Tensor((512,), dtype="float32") = params[48]
            lv984: R.Tensor((512,), dtype="float32") = params[49]
            lv41 = R.call_tir(fused_group_norm19_silu16, (lv40, lv983, lv984), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv985: R.Tensor((512, 512, 3, 3), dtype="float32") = params[50]
            lv986: R.Tensor((1, 512, 1, 1), dtype="float32") = params[120]
            lv42 = R.call_tir(fused_conv2d38_add41_add42_divide5, (lv41, lv985, lv986, lv38), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv987: R.Tensor((512,), dtype="float32") = params[51]
            lv988: R.Tensor((512,), dtype="float32") = params[52]
            lv43 = R.call_tir(fused_group_norm19_silu16, (lv42, lv987, lv988), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv989: R.Tensor((512, 512, 3, 3), dtype="float32") = params[53]
            lv990: R.Tensor((1, 512, 1, 1), dtype="float32") = params[121]
            lv44 = R.call_tir(fused_conv2d38_add41, (lv43, lv989, lv990), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv991: R.Tensor((512,), dtype="float32") = params[54]
            lv992: R.Tensor((512,), dtype="float32") = params[55]
            lv45 = R.call_tir(fused_group_norm19_silu16, (lv44, lv991, lv992), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv993: R.Tensor((512, 512, 3, 3), dtype="float32") = params[56]
            lv994: R.Tensor((1, 512, 1, 1), dtype="float32") = params[122]
            lv46_1 = R.call_tir(fused_conv2d38_add41_add42_divide5, (lv45, lv993, lv994, lv42), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv145 = R.call_tir(resize2d4, (lv46_1,), out_sinfo=R.Tensor((1, 512, 256, 256), dtype="float32"))
            lv995: R.Tensor((512, 512, 3, 3), dtype="float32") = params[57]
            lv996: R.Tensor((1, 512, 1, 1), dtype="float32") = params[123]
            lv47 = R.call_tir(fused_conv2d39_add43, (lv145, lv995, lv996), out_sinfo=R.Tensor((1, 512, 256, 256), dtype="float32"))
            lv997: R.Tensor((512,), dtype="float32") = params[58]
            lv998: R.Tensor((512,), dtype="float32") = params[59]
            lv48 = R.call_tir(fused_group_norm20_silu17, (lv47, lv997, lv998), out_sinfo=R.Tensor((1, 512, 256, 256), dtype="float32"))
            lv999: R.Tensor((256, 512, 3, 3), dtype="float32") = params[60]
            lv1000: R.Tensor((1, 256, 1, 1), dtype="float32") = params[124]
            lv49 = R.call_tir(fused_conv2d40_add44, (lv48, lv999, lv1000), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv1001: R.Tensor((256,), dtype="float32") = params[61]
            lv1002: R.Tensor((256,), dtype="float32") = params[62]
            lv50 = R.call_tir(fused_group_norm21_silu18, (lv49, lv1001, lv1002), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv1003: R.Tensor((256, 512, 1, 1), dtype="float32") = params[64]
            lv1004: R.Tensor((1, 256, 1, 1), dtype="float32") = params[125]
            lv51 = R.call_tir(fused_conv2d42_add44, (lv47, lv1003, lv1004), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv1005: R.Tensor((256, 256, 3, 3), dtype="float32") = params[63]
            lv1006: R.Tensor((1, 256, 1, 1), dtype="float32") = params[126]
            lv52 = R.call_tir(fused_conv2d41_add44_add45_divide6, (lv50, lv1005, lv1006, lv51), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv1007: R.Tensor((256,), dtype="float32") = params[65]
            lv1008: R.Tensor((256,), dtype="float32") = params[66]
            lv53 = R.call_tir(fused_group_norm21_silu18, (lv52, lv1007, lv1008), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv1009: R.Tensor((256, 256, 3, 3), dtype="float32") = params[67]
            lv1010: R.Tensor((1, 256, 1, 1), dtype="float32") = params[127]
            lv54 = R.call_tir(fused_conv2d41_add44, (lv53, lv1009, lv1010), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv1011: R.Tensor((256,), dtype="float32") = params[68]
            lv1012: R.Tensor((256,), dtype="float32") = params[69]
            lv55 = R.call_tir(fused_group_norm21_silu18, (lv54, lv1011, lv1012), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv1013: R.Tensor((256, 256, 3, 3), dtype="float32") = params[70]
            lv1014: R.Tensor((1, 256, 1, 1), dtype="float32") = params[128]
            lv56 = R.call_tir(fused_conv2d41_add44_add45_divide6, (lv55, lv1013, lv1014, lv52), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv1015: R.Tensor((256,), dtype="float32") = params[71]
            lv1016: R.Tensor((256,), dtype="float32") = params[72]
            lv57 = R.call_tir(fused_group_norm21_silu18, (lv56, lv1015, lv1016), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv1017: R.Tensor((256, 256, 3, 3), dtype="float32") = params[73]
            lv1018: R.Tensor((1, 256, 1, 1), dtype="float32") = params[129]
            lv58 = R.call_tir(fused_conv2d41_add44, (lv57, lv1017, lv1018), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv1019: R.Tensor((256,), dtype="float32") = params[74]
            lv1020: R.Tensor((256,), dtype="float32") = params[75]
            lv59 = R.call_tir(fused_group_norm21_silu18, (lv58, lv1019, lv1020), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv1021: R.Tensor((256, 256, 3, 3), dtype="float32") = params[76]
            lv1022: R.Tensor((1, 256, 1, 1), dtype="float32") = params[130]
            lv60 = R.call_tir(fused_conv2d41_add44_add45_divide6, (lv59, lv1021, lv1022, lv56), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv188 = R.call_tir(resize2d5, (lv60,), out_sinfo=R.Tensor((1, 256, 512, 512), dtype="float32"))
            lv1023: R.Tensor((256, 256, 3, 3), dtype="float32") = params[77]
            lv1024: R.Tensor((1, 256, 1, 1), dtype="float32") = params[131]
            lv61 = R.call_tir(fused_conv2d43_add46, (lv188, lv1023, lv1024), out_sinfo=R.Tensor((1, 256, 512, 512), dtype="float32"))
            lv1025: R.Tensor((256,), dtype="float32") = params[78]
            lv1026: R.Tensor((256,), dtype="float32") = params[79]
            lv62 = R.call_tir(fused_group_norm22_silu19, (lv61, lv1025, lv1026), out_sinfo=R.Tensor((1, 256, 512, 512), dtype="float32"))
            lv1027: R.Tensor((128, 256, 3, 3), dtype="float32") = params[80]
            lv1028: R.Tensor((1, 128, 1, 1), dtype="float32") = params[132]
            lv63 = R.call_tir(fused_conv2d44_add47, (lv62, lv1027, lv1028), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1029: R.Tensor((128,), dtype="float32") = params[81]
            lv1030: R.Tensor((128,), dtype="float32") = params[82]
            lv64 = R.call_tir(fused_group_norm23_silu20, (lv63, lv1029, lv1030), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1031: R.Tensor((128, 256, 1, 1), dtype="float32") = params[84]
            lv1032: R.Tensor((1, 128, 1, 1), dtype="float32") = params[134]
            lv65 = R.call_tir(fused_conv2d46_add47, (lv61, lv1031, lv1032), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1033: R.Tensor((128, 128, 3, 3), dtype="float32") = params[83]
            lv1034: R.Tensor((1, 128, 1, 1), dtype="float32") = params[133]
            lv66 = R.call_tir(fused_conv2d45_add47_add48_divide7, (lv64, lv1033, lv1034, lv65), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1035: R.Tensor((128,), dtype="float32") = params[85]
            lv1036: R.Tensor((128,), dtype="float32") = params[86]
            lv67 = R.call_tir(fused_group_norm23_silu20, (lv66, lv1035, lv1036), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1037: R.Tensor((128, 128, 3, 3), dtype="float32") = params[87]
            lv1038: R.Tensor((1, 128, 1, 1), dtype="float32") = params[135]
            lv68 = R.call_tir(fused_conv2d45_add47, (lv67, lv1037, lv1038), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1039: R.Tensor((128,), dtype="float32") = params[88]
            lv1040: R.Tensor((128,), dtype="float32") = params[89]
            lv69 = R.call_tir(fused_group_norm23_silu20, (lv68, lv1039, lv1040), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1041: R.Tensor((128, 128, 3, 3), dtype="float32") = params[90]
            lv1042: R.Tensor((1, 128, 1, 1), dtype="float32") = params[136]
            lv70 = R.call_tir(fused_conv2d45_add47_add48_divide7, (lv69, lv1041, lv1042, lv66), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1043: R.Tensor((128,), dtype="float32") = params[91]
            lv1044: R.Tensor((128,), dtype="float32") = params[92]
            lv71 = R.call_tir(fused_group_norm23_silu20, (lv70, lv1043, lv1044), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1045: R.Tensor((128, 128, 3, 3), dtype="float32") = params[93]
            lv1046: R.Tensor((1, 128, 1, 1), dtype="float32") = params[137]
            lv72 = R.call_tir(fused_conv2d45_add47, (lv71, lv1045, lv1046), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1047: R.Tensor((128,), dtype="float32") = params[94]
            lv1048: R.Tensor((128,), dtype="float32") = params[95]
            lv73 = R.call_tir(fused_group_norm23_silu20, (lv72, lv1047, lv1048), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1049: R.Tensor((128, 128, 3, 3), dtype="float32") = params[96]
            lv1050: R.Tensor((1, 128, 1, 1), dtype="float32") = params[138]
            lv74 = R.call_tir(fused_conv2d45_add47_add48_divide7, (lv73, lv1049, lv1050, lv70), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1051: R.Tensor((128,), dtype="float32") = params[97]
            lv1052: R.Tensor((128,), dtype="float32") = params[98]
            lv75 = R.call_tir(fused_group_norm23_silu20, (lv74, lv1051, lv1052), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1053: R.Tensor((3, 128, 3, 3), dtype="float32") = params[99]
            lv1054: R.Tensor((1, 3, 1, 1), dtype="float32") = params[139]
            lv76 = R.call_tir(fused_conv2d47_add49_divide8_add50_tir_clip, (lv75, lv1053, lv1054), out_sinfo=R.Tensor((1, 3, 512, 512), dtype="float32"))
            lv239 = R.call_tir(transpose50, (lv76,), out_sinfo=R.Tensor((1, 512, 512, 3), dtype="float32"))
            gv: R.Tensor((1, 512, 512, 3), dtype="float32") = lv239
            R.output(gv)
        return gv
