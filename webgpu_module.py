import tvm
from tvm.script import ir as I
from tvm.script import tir as T
from tvm.script import relax as R

metadata = tvm.ir.load_json("{\n  \"root\": 1, \n  \"nodes\": [\n    {\n      \"type_key\": \"\"\n    }, \n    {\n      \"type_key\": \"Map\", \n      \"keys\": [\n        \"relax.expr.Constant\"\n      ], \n      \"data\": [2]\n    }, \n    {\n      \"type_key\": \"Array\", \n      \"data\": [\n        3, \n        3, \n        3, \n        3, \n        3, \n        3, \n        3, \n        3, \n        3, \n        3, \n        3, \n        3, \n        14\n      ]\n    }, \n    {\n      \"type_key\": \"relax.expr.Constant\", \n      \"attrs\": {\n        \"_checked_type_\": \"13\", \n        \"data\": \"0\", \n        \"span\": \"0\", \n        \"struct_info_\": \"4\"\n      }\n    }, \n    {\n      \"type_key\": \"relax.TensorStructInfo\", \n      \"attrs\": {\n        \"dtype\": \"float32\", \n        \"ndim\": \"4\", \n        \"shape\": \"5\", \n        \"span\": \"0\"\n      }\n    }, \n    {\n      \"type_key\": \"relax.expr.ShapeExpr\", \n      \"attrs\": {\n        \"_checked_type_\": \"12\", \n        \"span\": \"0\", \n        \"struct_info_\": \"11\", \n        \"values\": \"6\"\n      }\n    }, \n    {\n      \"type_key\": \"Array\", \n      \"data\": [7, 8, 9, 10]\n    }, \n    {\n      \"type_key\": \"IntImm\", \n      \"attrs\": {\n        \"dtype\": \"int64\", \n        \"span\": \"0\", \n        \"value\": \"1\"\n      }\n    }, \n    {\n      \"type_key\": \"IntImm\", \n      \"attrs\": {\n        \"dtype\": \"int64\", \n        \"span\": \"0\", \n        \"value\": \"1\"\n      }\n    }, \n    {\n      \"type_key\": \"IntImm\", \n      \"attrs\": {\n        \"dtype\": \"int64\", \n        \"span\": \"0\", \n        \"value\": \"77\"\n      }\n    }, \n    {\n      \"type_key\": \"IntImm\", \n      \"attrs\": {\n        \"dtype\": \"int64\", \n        \"span\": \"0\", \n        \"value\": \"77\"\n      }\n    }, \n    {\n      \"type_key\": \"relax.ShapeStructInfo\", \n      \"attrs\": {\n        \"ndim\": \"4\", \n        \"span\": \"0\", \n        \"values\": \"6\"\n      }\n    }, \n    {\n      \"type_key\": \"relax.ShapeType\", \n      \"attrs\": {\n        \"ndim\": \"4\", \n        \"span\": \"0\"\n      }\n    }, \n    {\n      \"type_key\": \"relax.DynTensorType\", \n      \"attrs\": {\n        \"dtype\": \"float32\", \n        \"ndim\": \"4\", \n        \"span\": \"0\"\n      }\n    }, \n    {\n      \"type_key\": \"relax.expr.Constant\", \n      \"attrs\": {\n        \"_checked_type_\": \"22\", \n        \"data\": \"1\", \n        \"span\": \"0\", \n        \"struct_info_\": \"15\"\n      }\n    }, \n    {\n      \"type_key\": \"relax.TensorStructInfo\", \n      \"attrs\": {\n        \"dtype\": \"float32\", \n        \"ndim\": \"2\", \n        \"shape\": \"16\", \n        \"span\": \"0\"\n      }\n    }, \n    {\n      \"type_key\": \"relax.expr.ShapeExpr\", \n      \"attrs\": {\n        \"_checked_type_\": \"21\", \n        \"span\": \"0\", \n        \"struct_info_\": \"20\", \n        \"values\": \"17\"\n      }\n    }, \n    {\n      \"type_key\": \"Array\", \n      \"data\": [18, 19]\n    }, \n    {\n      \"type_key\": \"IntImm\", \n      \"attrs\": {\n        \"dtype\": \"int64\", \n        \"span\": \"0\", \n        \"value\": \"1\"\n      }\n    }, \n    {\n      \"type_key\": \"IntImm\", \n      \"attrs\": {\n        \"dtype\": \"int64\", \n        \"span\": \"0\", \n        \"value\": \"160\"\n      }\n    }, \n    {\n      \"type_key\": \"relax.ShapeStructInfo\", \n      \"attrs\": {\n        \"ndim\": \"2\", \n        \"span\": \"0\", \n        \"values\": \"17\"\n      }\n    }, \n    {\n      \"type_key\": \"relax.ShapeType\", \n      \"attrs\": {\n        \"ndim\": \"2\", \n        \"span\": \"0\"\n      }\n    }, \n    {\n      \"type_key\": \"relax.DynTensorType\", \n      \"attrs\": {\n        \"dtype\": \"float32\", \n        \"ndim\": \"2\", \n        \"span\": \"0\"\n      }\n    }\n  ], \n  \"b64ndarrays\": [\n    \"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\", \n    \"P6G0lvBAXt0AAAAAAAAAAAEAAAAAAAAAAgAAAAIgAQABAAAAAAAAAKAAAAAAAAAAgAIAAAAAAAAAAIA/+a1xPwUpZD+rZVc/F1lLPxD5Pz/uOzU/lRgrP2uGIT9QfRg/mPUPPwroBz/OTQA/4EDyPrSz5D6a6Nc+s9TLPsFtwD4YqrU+loCrPpjooT4B2pg+G02QPqc6iD7Lm4A+INRyPrk+ZT7Ya1g+mFBMPrfiQD6HGDY+1egrPgVLIj7rNhk+0qQQPnaNCD746QA+vmfzPRLK5T1j79g9yMzMPflXwT03h7Y9VFGsPaytoj0MlJk9wPyQPXjgiD1VOIE9sPtzPb5VZj1Dc1k9R0lNPYHNQT0q9jY9E7osPY0QIz1o8Rk93VQRPaozCT3ihgE9AZD0PMPh5jxv99k8EsbNPE1DwjxjZbc8ECOtPK5zozz6Tpo8Oa2RPA+HiTyd1YE8pyR1PB5uZzzve1o8JUNOPGG5QjzY1Dc8VIwtPAPXIzzIrBo8xAUSPKzaCTyKJAI8rbn1O8j65zu7ANs7iMDOO8AvwzuYRLg7zPWtO6Q6pDvOCps7iV6SO3Yuijukc4I7Dk92O8mHaDvdhVs7Lz5PO2ymQzuctDg7kF8uO3KeJDsLaRs7g7cSO3OCCjvxwgI7vuT2OiMV6TpQC9w6KrzPOlwdxDrlJLk6icmuOn0CpTqFx5s6rRCTOqfWijpvEoM61Hp3OtSiaToUkVw6cjpQOoKURDpplTk6yjMvOtRmJTo4Jhw6FGoTOg0rCzodYgM6RhH4OdEw6jkgF905CLnQOQsMxTk7Bro5U56vOWDLpTkkhZw5q8OTOad/izn1sYM5E6h4OS+/ajl9nV056TdROdWDRTlQdzo5/ggwOSIwJjlD5Bw5fh0UOW7UCzn+AQQ5Oz/5OO1N6zg/JN44\"\n  ], \n  \"attrs\": {\"tvm_version\": \"0.11.dev0\"}\n}")


@I.ir_module
class Module:
    @T.prim_func
    def concatenate1(rxplaceholder: T.Buffer((T.int32(2), T.int32(1280), T.int32(8), T.int32(8)), "float32"), rxplaceholder_1: T.Buffer((T.int32(2), T.int32(1280), T.int32(8), T.int32(8)), "float32"), T_concat: T.Buffer((T.int32(2), T.int32(2560), T.int32(8), T.int32(8)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(5)):
                    with T.block("T_concat"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(163840))
                        v_ax1 = T.axis.spatial(T.int32(2560), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(163840) // T.int32(64))
                        v_ax2 = T.axis.spatial(T.int32(8), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(64) // T.int32(8))
                        v_ax3 = T.axis.spatial(T.int32(8), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(8))
                        T.reads(rxplaceholder_1[v_ax0, v_ax1 - T.int32(1280), v_ax2, v_ax3], rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int32(1280) <= v_ax1, rxplaceholder_1[v_ax0, v_ax1 - T.int32(1280), v_ax2, v_ax3], rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def concatenate2(rxplaceholder: T.Buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), "float32"), rxplaceholder_1: T.Buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), "float32"), T_concat: T.Buffer((T.int32(2), T.int32(2560), T.int32(16), T.int32(16)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(20)):
                    with T.block("T_concat"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(655360))
                        v_ax1 = T.axis.spatial(T.int32(2560), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(655360) // T.int32(256))
                        v_ax2 = T.axis.spatial(T.int32(16), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(256) // T.int32(16))
                        v_ax3 = T.axis.spatial(T.int32(16), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(16))
                        T.reads(rxplaceholder_1[v_ax0, v_ax1 - T.int32(1280), v_ax2, v_ax3], rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int32(1280) <= v_ax1, rxplaceholder_1[v_ax0, v_ax1 - T.int32(1280), v_ax2, v_ax3], rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def concatenate3(rxplaceholder: T.Buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), "float32"), rxplaceholder_1: T.Buffer((T.int32(2), T.int32(640), T.int32(16), T.int32(16)), "float32"), T_concat: T.Buffer((T.int32(2), T.int32(1920), T.int32(16), T.int32(16)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(15)):
                    with T.block("T_concat"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(491520))
                        v_ax1 = T.axis.spatial(T.int32(1920), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(491520) // T.int32(256))
                        v_ax2 = T.axis.spatial(T.int32(16), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(256) // T.int32(16))
                        v_ax3 = T.axis.spatial(T.int32(16), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(16))
                        T.reads(rxplaceholder_1[v_ax0, v_ax1 - T.int32(1280), v_ax2, v_ax3], rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int32(1280) <= v_ax1, rxplaceholder_1[v_ax0, v_ax1 - T.int32(1280), v_ax2, v_ax3], rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def concatenate4(rxplaceholder: T.Buffer((T.int32(2), T.int32(1280), T.int32(32), T.int32(32)), "float32"), rxplaceholder_1: T.Buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), "float32"), T_concat: T.Buffer((T.int32(2), T.int32(1920), T.int32(32), T.int32(32)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(60)):
                    with T.block("T_concat"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(1966080))
                        v_ax1 = T.axis.spatial(T.int32(1920), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(1966080) // T.int32(1024))
                        v_ax2 = T.axis.spatial(T.int32(32), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(1024) // T.int32(32))
                        v_ax3 = T.axis.spatial(T.int32(32), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(32))
                        T.reads(rxplaceholder_1[v_ax0, v_ax1 - T.int32(1280), v_ax2, v_ax3], rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int32(1280) <= v_ax1, rxplaceholder_1[v_ax0, v_ax1 - T.int32(1280), v_ax2, v_ax3], rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def concatenate5(rxplaceholder: T.Buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), "float32"), rxplaceholder_1: T.Buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), "float32"), T_concat: T.Buffer((T.int32(2), T.int32(1280), T.int32(32), T.int32(32)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(40)):
                    with T.block("T_concat"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(1310720))
                        v_ax1 = T.axis.spatial(T.int32(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(1310720) // T.int32(1024))
                        v_ax2 = T.axis.spatial(T.int32(32), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(1024) // T.int32(32))
                        v_ax3 = T.axis.spatial(T.int32(32), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(32))
                        T.reads(rxplaceholder_1[v_ax0, v_ax1 - T.int32(640), v_ax2, v_ax3], rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int32(640) <= v_ax1, rxplaceholder_1[v_ax0, v_ax1 - T.int32(640), v_ax2, v_ax3], rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def concatenate6(rxplaceholder: T.Buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), "float32"), rxplaceholder_1: T.Buffer((T.int32(2), T.int32(320), T.int32(32), T.int32(32)), "float32"), T_concat: T.Buffer((T.int32(2), T.int32(960), T.int32(32), T.int32(32)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(30)):
                    with T.block("T_concat"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(983040))
                        v_ax1 = T.axis.spatial(T.int32(960), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(983040) // T.int32(1024))
                        v_ax2 = T.axis.spatial(T.int32(32), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(1024) // T.int32(32))
                        v_ax3 = T.axis.spatial(T.int32(32), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(32))
                        T.reads(rxplaceholder_1[v_ax0, v_ax1 - T.int32(640), v_ax2, v_ax3], rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int32(640) <= v_ax1, rxplaceholder_1[v_ax0, v_ax1 - T.int32(640), v_ax2, v_ax3], rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def concatenate7(rxplaceholder: T.Buffer((T.int32(2), T.int32(640), T.int32(64), T.int32(64)), "float32"), rxplaceholder_1: T.Buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), "float32"), T_concat: T.Buffer((T.int32(2), T.int32(960), T.int32(64), T.int32(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(120)):
                    with T.block("T_concat"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(3932160))
                        v_ax1 = T.axis.spatial(T.int32(960), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(3932160) // T.int32(4096))
                        v_ax2 = T.axis.spatial(T.int32(64), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(4096) // T.int32(64))
                        v_ax3 = T.axis.spatial(T.int32(64), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(64))
                        T.reads(rxplaceholder_1[v_ax0, v_ax1 - T.int32(640), v_ax2, v_ax3], rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int32(640) <= v_ax1, rxplaceholder_1[v_ax0, v_ax1 - T.int32(640), v_ax2, v_ax3], rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def concatenate8(rxplaceholder: T.Buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), "float32"), rxplaceholder_1: T.Buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), "float32"), T_concat: T.Buffer((T.int32(2), T.int32(640), T.int32(64), T.int32(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(80)):
                    with T.block("T_concat"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(2621440))
                        v_ax1 = T.axis.spatial(T.int32(640), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(2621440) // T.int32(4096))
                        v_ax2 = T.axis.spatial(T.int32(64), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(4096) // T.int32(64))
                        v_ax3 = T.axis.spatial(T.int32(64), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(64))
                        T.reads(rxplaceholder_1[v_ax0, v_ax1 - T.int32(320), v_ax2, v_ax3], rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int32(320) <= v_ax1, rxplaceholder_1[v_ax0, v_ax1 - T.int32(320), v_ax2, v_ax3], rxplaceholder[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def fused_broadcast_to_strided_slice_reshape9_cast1_multiply3_multiply4_sin_cos_concatenate_strided_slice1_reshape10_strided_slice2_reshape10_concatenate(inp_1: T.Buffer((T.int32(1),), "int32"), param_0: T.Buffer((T.int32(1), T.int32(160)), "float32"), T_concat: T.Buffer((T.int32(2), T.int32(320)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int32(10), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                with T.block("T_concat_1"):
                    v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) // T.int32(320))
                    v_ax1 = T.axis.spatial(T.int32(320), (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) % T.int32(320))
                    T.reads(inp_1[T.int32(0)], param_0[T.int32(0), v_ax1 % T.int32(160) - T.int32(160):v_ax1 % T.int32(160) - T.int32(160) + T.int32(321)])
                    T.writes(T_concat[v_ax0, v_ax1])
                    T_concat[v_ax0, v_ax1] = T.if_then_else(T.int32(160) <= v_ax1, T.if_then_else(T.int32(160) <= (v_ax1 - T.int32(160)) % T.int32(160), T.cos(T.Cast("float32", inp_1[T.int32(0)]) * param_0[T.int32(0), (v_ax1 - T.int32(160)) % T.int32(160) - T.int32(160)]), T.sin(T.Cast("float32", inp_1[T.int32(0)]) * param_0[T.int32(0), (v_ax1 - T.int32(160)) % T.int32(160)])), T.if_then_else(T.int32(160) <= v_ax1 % T.int32(160) + T.int32(160), T.cos(T.Cast("float32", inp_1[T.int32(0)]) * param_0[T.int32(0), v_ax1 % T.int32(160) + T.int32(160) - T.int32(160)]), T.sin(T.Cast("float32", inp_1[T.int32(0)]) * param_0[T.int32(0), v_ax1 % T.int32(160) + T.int32(160)])))

    @T.prim_func
    def fused_conv2d10_add21(lv865: T.Buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), "float32"), param_0: T.Buffer((T.int32(1280), T.int32(1280), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(1280), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(1280), T.int32(18), T.int32(18)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(1280), T.int32(1280), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(160), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(2), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(80) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(1280), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(80) // T.int32(8) * T.int32(128) + nn_1_ff_1_yy_1_xx_1_fused // T.int32(2) * T.int32(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(2) + ff_3_init)
                            v_yy = T.axis.spatial(T.int32(16), nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) * T.int32(4) + yy_3_init * T.int32(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int32(16), xx_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int32(2) + xx_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(1280), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(80))
                                    v1 = T.axis.spatial(T.int32(1280), rc_0)
                                    v2 = T.axis.spatial(T.int32(18), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(4))
                                    v3 = T.axis.spatial(T.int32(18), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(4))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1 < T.int32(72))
                                    T.reads(lv865[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(17) and T.int32(1) <= v3 and v3 < T.int32(17), lv865[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int32(80) // T.int32(8) * T.int32(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(9))
                                    v1 = T.axis.spatial(T.int32(1280), rc_0)
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(2), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(2), T.int32(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(80) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(1280), ff_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(80) // T.int32(8) * T.int32(128) + nn_1_ff_1_yy_1_xx_1_fused // T.int32(2) * T.int32(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(2) + ff_3)
                                v_yy = T.axis.spatial(T.int32(16), nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) * T.int32(4) + yy_3 * T.int32(2) + yy_4)
                                v_xx = T.axis.spatial(T.int32(16), xx_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int32(2) + xx_3)
                                v_rc = T.axis.reduce(T.int32(1280), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_2 + ry_0 * T.int32(3) + ry_1)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 * T.int32(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(2), T.int32(4), T.int32(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(80) + ax0)
                            v1 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int32(80) // T.int32(8) * T.int32(128) + nn_1_ff_1_yy_1_xx_1_fused // T.int32(2) * T.int32(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(2) + ax1)
                            v2 = T.axis.spatial(T.int32(16), nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) * T.int32(4) + ax2)
                            v3 = T.axis.spatial(T.int32(16), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int32(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d10_add21_add22(lv537: T.Buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), "float32"), param_0: T.Buffer((T.int32(1280), T.int32(1280), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(1280), T.int32(1), T.int32(1)), "float32"), lv546: T.Buffer((T.int32(2), T.int32(1280), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(1280), T.int32(18), T.int32(18)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(1280), T.int32(1280), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(160), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(1), T.int32(2), T.int32(1), T.int32(4)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(80) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int32(80) // T.int32(4) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int32(2) * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(2) + ff_3_init * T.int32(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int32(16), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) + yy_3_init)
                            v_xx = T.axis.spatial(T.int32(16), nn_1_ff_1_yy_1_xx_1_fused % T.int32(2) * T.int32(8) + xx_3_init * T.int32(4) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(256), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(1280), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(80))
                                    v1 = T.axis.spatial(T.int32(1280), rc_0)
                                    v2 = T.axis.spatial(T.int32(18), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(18))
                                    v3 = T.axis.spatial(T.int32(18), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(18))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1 < T.int32(108))
                                    T.reads(lv537[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(17) and T.int32(1) <= v3 and v3 < T.int32(17), lv537[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int32(80) // T.int32(4) * T.int32(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(9))
                                    v1 = T.axis.spatial(T.int32(1280), rc_0)
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(3), T.int32(1), T.int32(2), T.int32(1), T.int32(4)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(80) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int32(80) // T.int32(4) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int32(2) * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(2) + ff_3 * T.int32(2) + ff_4)
                                v_yy = T.axis.spatial(T.int32(16), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) + yy_3)
                                v_xx = T.axis.spatial(T.int32(16), nn_1_ff_1_yy_1_xx_1_fused % T.int32(2) * T.int32(8) + xx_3 * T.int32(4) + xx_4)
                                v_rc = T.axis.reduce(T.int32(1280), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_2 + ry_0 * T.int32(3) + ry_1)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 * T.int32(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(256), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(2), T.int32(1), T.int32(8)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(80) + ax0)
                            v1 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int32(80) // T.int32(4) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int32(2) * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(2) + ax1)
                            v2 = T.axis.spatial(T.int32(16), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) + ax2)
                            v3 = T.axis.spatial(T.int32(16), nn_1_ff_1_yy_1_xx_1_fused % T.int32(2) * T.int32(8) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)], lv546[v0, v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)] + lv546[v0, v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d10_add21_add23_divide2(lv446: T.Buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), "float32"), param_0: T.Buffer((T.int32(1280), T.int32(1280), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(1280), T.int32(1), T.int32(1)), "float32"), lv452: T.Buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), "float32"), T_divide: T.Buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(1280), T.int32(18), T.int32(18)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(1280), T.int32(1280), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(160), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(2), T.int32(2), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(4)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_3_init + nn_4_init + nn_2_ff_2_yy_2_xx_2_fused // T.int32(80))
                            v_ff = T.axis.spatial(T.int32(1280), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused // T.int32(8) * T.int32(80) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(80) // T.int32(2) * T.int32(2) + ff_3_init)
                            v_yy = T.axis.spatial(T.int32(16), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) // T.int32(2) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int32(16), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(8) + xx_3_init * T.int32(4) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(160), T.int32(3), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(160), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(160) + ax0_ax1_ax2_ax3_fused_1) // T.int32(320))
                                    v1 = T.axis.spatial(T.int32(1280), rc_0 * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(160) + ax0_ax1_ax2_ax3_fused_1) % T.int32(320) // T.int32(40))
                                    v2 = T.axis.spatial(T.int32(18), ry_0 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) // T.int32(2) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(160) + ax0_ax1_ax2_ax3_fused_1) % T.int32(40) // T.int32(10))
                                    v3 = T.axis.spatial(T.int32(18), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(160) + ax0_ax1_ax2_ax3_fused_1) % T.int32(10))
                                    T.reads(lv446[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(17) and T.int32(1) <= v3 and v3 < T.int32(17), lv446[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(12)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(160), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int32(8) * T.int32(80) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(160) + ax0_ax1_ax2_ax3_fused_1) // T.int32(24))
                                    v1 = T.axis.spatial(T.int32(1280), rc_0 * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(160) + ax0_ax1_ax2_ax3_fused_1) % T.int32(24) // T.int32(3))
                                    v2 = T.axis.spatial(T.int32(3), ry_0)
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(160) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(8), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(2), T.int32(2), T.int32(1), T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(1), T.int32(4)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_3 + nn_4 + nn_2_ff_2_yy_2_xx_2_fused // T.int32(80))
                                v_ff = T.axis.spatial(T.int32(1280), ff_4 + nn_0_ff_0_yy_0_xx_0_fused // T.int32(8) * T.int32(80) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(80) // T.int32(2) * T.int32(2) + ff_3)
                                v_yy = T.axis.spatial(T.int32(16), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) // T.int32(2) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int32(16), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(8) + xx_3 * T.int32(4) + xx_4)
                                v_rc = T.axis.reduce(T.int32(1280), rc_0 * T.int32(8) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 * T.int32(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(2), T.int32(2), T.int32(8)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_2_ff_2_yy_2_xx_2_fused // T.int32(80) + ax0)
                            v1 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int32(8) * T.int32(80) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(80) // T.int32(2) * T.int32(2) + ax1)
                            v2 = T.axis.spatial(T.int32(16), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) // T.int32(2) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + ax2)
                            v3 = T.axis.spatial(T.int32(16), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(8) + ax3)
                            T.reads(lv452[v0, v1, v2, v3], conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)])
                            T.writes(T_divide[v0, v1, v2, v3])
                            T_divide[v0, v1, v2, v3] = lv452[v0, v1, v2, v3] + (conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)])

    @T.prim_func
    def fused_conv2d11_add21(lv432: T.Buffer((T.int32(2), T.int32(640), T.int32(16), T.int32(16)), "float32"), param_0: T.Buffer((T.int32(1280), T.int32(640), T.int32(1), T.int32(1)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(1280), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(640), T.int32(16), T.int32(16)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(1280), T.int32(640), T.int32(1), T.int32(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(8), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(2), T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(160) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(1280), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(160) // T.int32(8) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int32(2) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(2) + ff_3_init)
                            v_yy = T.axis.spatial(T.int32(16), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int32(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int32(16), xx_4_init + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) * T.int32(2) + xx_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(256), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(80), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(160))
                                    v1 = T.axis.spatial(T.int32(640), rc_0 * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(32))
                                    v2 = T.axis.spatial(T.int32(16), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(32) // T.int32(16))
                                    v3 = T.axis.spatial(T.int32(16), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(16))
                                    T.reads(lv432[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv432[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int32(160) // T.int32(8) * T.int32(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(8))
                                    v1 = T.axis.spatial(T.int32(640), rc_0 * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(8))
                                    v2 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v3 = T.axis.spatial(T.int32(1), T.int32(0))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(1), T.int32(2), T.int32(8), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(160) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(1280), ff_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(160) // T.int32(8) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int32(2) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(2) + ff_3)
                                v_yy = T.axis.spatial(T.int32(16), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int32(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int32(16), xx_4 + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) * T.int32(2) + xx_3)
                                v_rc = T.axis.reduce(T.int32(640), rc_0 * T.int32(8) + rc_1 * T.int32(8) + rc_2)
                                v_ry = T.axis.reduce(T.int32(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int32(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(256), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(2), T.int32(1), T.int32(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(160) + ax0)
                            v1 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int32(160) // T.int32(8) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int32(2) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(2) + ax1)
                            v2 = T.axis.spatial(T.int32(16), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int32(2) + ax2)
                            v3 = T.axis.spatial(T.int32(16), nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) * T.int32(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d12_add21(lv455: T.Buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), "float32"), param_0: T.Buffer((T.int32(1280), T.int32(1280), T.int32(1), T.int32(1)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(1280), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(1280), T.int32(1280), T.int32(1), T.int32(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(640), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(2), T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(320) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(1280), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(320) // T.int32(4) * T.int32(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(2) + ff_3_init)
                            v_yy = T.axis.spatial(T.int32(16), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(4) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int32(16), xx_3_init + xx_4_init + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16))
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(320), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(320))
                                    v1 = T.axis.spatial(T.int32(1280), rc_0 * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) // T.int32(64))
                                    v2 = T.axis.spatial(T.int32(16), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) % T.int32(64) // T.int32(16))
                                    v3 = T.axis.spatial(T.int32(16), (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) % T.int32(16))
                                    T.reads(lv455[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv455[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int32(320) // T.int32(4) * T.int32(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) // T.int32(4))
                                    v1 = T.axis.spatial(T.int32(1280), rc_0 * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) % T.int32(4))
                                    v2 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v3 = T.axis.spatial(T.int32(1), T.int32(0))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(4), T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(320) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(1280), ff_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(320) // T.int32(4) * T.int32(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(2) + ff_3)
                                v_yy = T.axis.spatial(T.int32(16), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(4) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int32(16), xx_3 + xx_4 + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16))
                                v_rc = T.axis.reduce(T.int32(1280), rc_0 * T.int32(4) + rc_1 * T.int32(4) + rc_2)
                                v_ry = T.axis.reduce(T.int32(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int32(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(2), T.int32(4), T.int32(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(320) + ax0)
                            v1 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int32(320) // T.int32(4) * T.int32(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(2) + ax1)
                            v2 = T.axis.spatial(T.int32(16), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(4) + ax2)
                            v3 = T.axis.spatial(T.int32(16), nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d12_add21_add23(lv531: T.Buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), "float32"), param_0: T.Buffer((T.int32(1280), T.int32(1280), T.int32(1), T.int32(1)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(1280), T.int32(1), T.int32(1)), "float32"), lv454: T.Buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(1280), T.int32(1280), T.int32(1), T.int32(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(10), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(128) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(1280), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(128) // T.int32(4) * T.int32(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(10) + ff_3_init)
                            v_yy = T.axis.spatial(T.int32(16), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) // T.int32(4) + yy_3_init)
                            v_xx = T.axis.spatial(T.int32(16), xx_3_init + xx_4_init + nn_1_ff_1_yy_1_xx_1_fused * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4))
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(320), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(128))
                                    v1 = T.axis.spatial(T.int32(1280), rc_0 * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(64))
                                    v2 = T.axis.spatial(T.int32(16), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(64) // T.int32(16))
                                    v3 = T.axis.spatial(T.int32(16), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(16))
                                    T.reads(lv531[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv531[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int32(128) // T.int32(4) * T.int32(40) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(4))
                                    v1 = T.axis.spatial(T.int32(1280), rc_0 * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(4))
                                    v2 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v3 = T.axis.spatial(T.int32(1), T.int32(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1 < T.int32(160))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(10), T.int32(1), T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(128) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(1280), ff_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(128) // T.int32(4) * T.int32(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(10) + ff_3)
                                v_yy = T.axis.spatial(T.int32(16), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) // T.int32(4) + yy_3)
                                v_xx = T.axis.spatial(T.int32(16), xx_3 + xx_4 + nn_1_ff_1_yy_1_xx_1_fused * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4))
                                v_rc = T.axis.reduce(T.int32(1280), rc_0 * T.int32(4) + rc_1 * T.int32(4) + rc_2)
                                v_ry = T.axis.reduce(T.int32(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int32(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(10), T.int32(1), T.int32(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(128) + ax0)
                            v1 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int32(128) // T.int32(4) * T.int32(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(10) + ax1)
                            v2 = T.axis.spatial(T.int32(16), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) // T.int32(4) + ax2)
                            v3 = T.axis.spatial(T.int32(16), nn_1_ff_1_yy_1_xx_1_fused * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)], lv454[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)] + lv454[v0, v1, v2, v3]

    @T.prim_func
    def fused_conv2d13_add27(lv635: T.Buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), "float32"), param_0: T.Buffer((T.int32(1280), T.int32(1280), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(1280), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(1280), T.int32(8), T.int32(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(1280), T.int32(8), T.int32(8)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(1280), T.int32(18), T.int32(18)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(1280), T.int32(1280), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(40), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(4), T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_4_init + nn_1_ff_1_yy_1_xx_1_fused + nn_3_init)
                            v_ff = T.axis.spatial(T.int32(1280), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused // T.int32(2) * T.int32(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(4) + ff_3_init)
                            v_yy = T.axis.spatial(T.int32(8), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) // T.int32(2) + yy_3_init)
                            v_xx = T.axis.spatial(T.int32(8), xx_4_init + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(4) + xx_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(640), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(306))
                                    v1 = T.axis.spatial(T.int32(1280), rc_0 * T.int32(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(306) // T.int32(153))
                                    v2 = T.axis.spatial(T.int32(18), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(153) // T.int32(17))
                                    v3 = T.axis.spatial(T.int32(18), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(17))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1 < T.int32(612))
                                    T.reads(lv635[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(17) and T.int32(1) <= v3 and v3 < T.int32(17), lv635[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int32(2) * T.int32(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(18))
                                    v1 = T.axis.spatial(T.int32(1280), rc_0 * T.int32(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(18) // T.int32(9))
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(2), T.int32(3), T.int32(1), T.int32(1), T.int32(4), T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_4 + nn_1_ff_1_yy_1_xx_1_fused + nn_3)
                                v_ff = T.axis.spatial(T.int32(1280), ff_4 + nn_0_ff_0_yy_0_xx_0_fused // T.int32(2) * T.int32(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(4) + ff_3)
                                v_yy = T.axis.spatial(T.int32(8), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) // T.int32(2) + yy_3)
                                v_xx = T.axis.spatial(T.int32(8), xx_4 + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(4) + xx_3)
                                v_rc = T.axis.reduce(T.int32(1280), rc_0 * T.int32(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_2 + ry_0 * T.int32(3) + ry_1)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 * T.int32(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy * T.int32(2) + v_ry, v_xx * T.int32(2) + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy * T.int32(2) + v_ry, v_xx * T.int32(2) + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(4), T.int32(1), T.int32(4)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_1_ff_1_yy_1_xx_1_fused + ax0)
                            v1 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int32(2) * T.int32(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(4) + ax1)
                            v2 = T.axis.spatial(T.int32(8), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) // T.int32(2) + ax2)
                            v3 = T.axis.spatial(T.int32(8), nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(4) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d14_add27_add28(lv640: T.Buffer((T.int32(2), T.int32(1280), T.int32(8), T.int32(8)), "float32"), param_0: T.Buffer((T.int32(1280), T.int32(1280), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(1280), T.int32(1), T.int32(1)), "float32"), lv649: T.Buffer((T.int32(2), T.int32(1280), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(1280), T.int32(8), T.int32(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(1280), T.int32(8), T.int32(8)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(1280), T.int32(10), T.int32(10)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(1280), T.int32(1280), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(40), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(4), T.int32(1), T.int32(2), T.int32(1), T.int32(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_3_init + nn_4_init + nn_2_ff_2_yy_2_xx_2_fused // T.int32(128))
                            v_ff = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(128) // T.int32(8) * T.int32(2) + ff_3_init * T.int32(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int32(8), yy_4_init + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) + yy_3_init)
                            v_xx = T.axis.spatial(T.int32(8), xx_3_init * T.int32(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(1280), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) // T.int32(100))
                                    v1 = T.axis.spatial(T.int32(1280), rc_0)
                                    v2 = T.axis.spatial(T.int32(10), (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) % T.int32(100) // T.int32(10))
                                    v3 = T.axis.spatial(T.int32(10), (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) % T.int32(10))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1 < T.int32(200))
                                    T.reads(lv640[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(9) and T.int32(1) <= v3 and v3 < T.int32(9), lv640[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused * T.int32(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) // T.int32(9))
                                    v1 = T.axis.spatial(T.int32(1280), rc_0)
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1 < T.int32(288))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(3), T.int32(1), T.int32(2), T.int32(1), T.int32(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_3 + nn_4 + nn_2_ff_2_yy_2_xx_2_fused // T.int32(128))
                                v_ff = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(128) // T.int32(8) * T.int32(2) + ff_3 * T.int32(2) + ff_4)
                                v_yy = T.axis.spatial(T.int32(8), yy_4 + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) + yy_3)
                                v_xx = T.axis.spatial(T.int32(8), xx_3 * T.int32(2) + xx_4)
                                v_rc = T.axis.reduce(T.int32(1280), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_2 + ry_0 * T.int32(3) + ry_1)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 * T.int32(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(2), T.int32(1), T.int32(8)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_2_ff_2_yy_2_xx_2_fused // T.int32(128) + ax0)
                            v1 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(128) // T.int32(8) * T.int32(2) + ax1)
                            v2 = T.axis.spatial(T.int32(8), nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) + ax2)
                            v3 = T.axis.spatial(T.int32(8), ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)], lv649[v0, v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)] + lv649[v0, v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d14_add27_add29_divide3(lv652: T.Buffer((T.int32(2), T.int32(1280), T.int32(8), T.int32(8)), "float32"), param_0: T.Buffer((T.int32(1280), T.int32(1280), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(1280), T.int32(1), T.int32(1)), "float32"), lv638: T.Buffer((T.int32(2), T.int32(1280), T.int32(8), T.int32(8)), "float32"), T_divide: T.Buffer((T.int32(2), T.int32(1280), T.int32(8), T.int32(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(1280), T.int32(8), T.int32(8)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(1280), T.int32(10), T.int32(10)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(1280), T.int32(1280), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(40), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(4)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(20) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(1280), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(20) * T.int32(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(4) + ff_3_init)
                            v_yy = T.axis.spatial(T.int32(8), yy_4_init + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) + yy_3_init)
                            v_xx = T.axis.spatial(T.int32(8), nn_1_ff_1_yy_1_xx_1_fused * T.int32(4) + xx_3_init * T.int32(4) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(640), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(20))
                                    v1 = T.axis.spatial(T.int32(1280), rc_0 * T.int32(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(100))
                                    v2 = T.axis.spatial(T.int32(10), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(100) // T.int32(10))
                                    v3 = T.axis.spatial(T.int32(10), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(10))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1 < T.int32(200))
                                    T.reads(lv652[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(9) and T.int32(1) <= v3 and v3 < T.int32(9), lv652[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int32(20) * T.int32(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(18))
                                    v1 = T.axis.spatial(T.int32(1280), rc_0 * T.int32(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(18) // T.int32(9))
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(2), T.int32(3), T.int32(1), T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(1), T.int32(4)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(20) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(1280), ff_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(20) * T.int32(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(4) + ff_3)
                                v_yy = T.axis.spatial(T.int32(8), yy_4 + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) + yy_3)
                                v_xx = T.axis.spatial(T.int32(8), nn_1_ff_1_yy_1_xx_1_fused * T.int32(4) + xx_3 * T.int32(4) + xx_4)
                                v_rc = T.axis.reduce(T.int32(1280), rc_0 * T.int32(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_2 + ry_0 * T.int32(3) + ry_1)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 * T.int32(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(4), T.int32(1), T.int32(4)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(20) + ax0)
                            v1 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int32(20) * T.int32(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(4) + ax1)
                            v2 = T.axis.spatial(T.int32(8), nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) + ax2)
                            v3 = T.axis.spatial(T.int32(8), nn_1_ff_1_yy_1_xx_1_fused * T.int32(4) + ax3)
                            T.reads(lv638[v0, v1, v2, v3], conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)])
                            T.writes(T_divide[v0, v1, v2, v3])
                            T_divide[v0, v1, v2, v3] = lv638[v0, v1, v2, v3] + (conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)])

    @T.prim_func
    def fused_conv2d15_add27(lv696: T.Buffer((T.int32(2), T.int32(1280), T.int32(8), T.int32(8)), "float32"), param_0: T.Buffer((T.int32(1280), T.int32(1280), T.int32(1), T.int32(1)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(1280), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(1280), T.int32(8), T.int32(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(1280), T.int32(8), T.int32(8)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(1280), T.int32(8), T.int32(8)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(1280), T.int32(1280), T.int32(1), T.int32(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(40), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(16), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(8), T.int32(2), T.int32(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_3_init + nn_4_init + nn_2_ff_2_yy_2_xx_2_fused // T.int32(128))
                            v_ff = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int32(2) * T.int32(64) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(128) // T.int32(16) * T.int32(8) + ff_3_init * T.int32(8) + ff_4_init)
                            v_yy = T.axis.spatial(T.int32(8), nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) // T.int32(4) * T.int32(2) + yy_3_init * T.int32(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int32(8), xx_3_init + xx_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4))
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(40), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) // T.int32(1024))
                                    v1 = T.axis.spatial(T.int32(1280), rc_0 * T.int32(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) % T.int32(1024) // T.int32(32))
                                    v2 = T.axis.spatial(T.int32(8), (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) % T.int32(32) // T.int32(4))
                                    v3 = T.axis.spatial(T.int32(8), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) % T.int32(4))
                                    T.reads(lv696[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv696[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int32(2) * T.int32(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) // T.int32(32))
                                    v1 = T.axis.spatial(T.int32(1280), rc_0 * T.int32(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) % T.int32(32))
                                    v2 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v3 = T.axis.spatial(T.int32(1), T.int32(0))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(32), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(8), T.int32(2), T.int32(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_3 + nn_4 + nn_2_ff_2_yy_2_xx_2_fused // T.int32(128))
                                v_ff = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int32(2) * T.int32(64) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(128) // T.int32(16) * T.int32(8) + ff_3 * T.int32(8) + ff_4)
                                v_yy = T.axis.spatial(T.int32(8), nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) // T.int32(4) * T.int32(2) + yy_3 * T.int32(2) + yy_4)
                                v_xx = T.axis.spatial(T.int32(8), xx_3 + xx_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4))
                                v_rc = T.axis.reduce(T.int32(1280), rc_0 * T.int32(32) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int32(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(8), T.int32(2), T.int32(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_2_ff_2_yy_2_xx_2_fused // T.int32(128) + ax0)
                            v1 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int32(2) * T.int32(64) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(128) // T.int32(16) * T.int32(8) + ax1)
                            v2 = T.axis.spatial(T.int32(8), nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) // T.int32(4) * T.int32(2) + ax2)
                            v3 = T.axis.spatial(T.int32(8), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d15_add27_add29(lv772: T.Buffer((T.int32(2), T.int32(1280), T.int32(8), T.int32(8)), "float32"), param_0: T.Buffer((T.int32(1280), T.int32(1280), T.int32(1), T.int32(1)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(1280), T.int32(1), T.int32(1)), "float32"), lv695: T.Buffer((T.int32(2), T.int32(1280), T.int32(8), T.int32(8)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(1280), T.int32(8), T.int32(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(1280), T.int32(8), T.int32(8)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(1280), T.int32(8), T.int32(8)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(1280), T.int32(1280), T.int32(1), T.int32(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(80), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(2), T.int32(2), T.int32(1), T.int32(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_3_init * T.int32(2) + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int32(4) * T.int32(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(8) + ff_3_init * T.int32(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int32(8), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) // T.int32(8) + yy_3_init)
                            v_xx = T.axis.spatial(T.int32(8), xx_3_init + xx_4_init + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8))
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(128), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(80), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(80) + ax0_ax1_ax2_ax3_fused_1) // T.int32(160))
                                    v1 = T.axis.spatial(T.int32(1280), rc_0 * T.int32(10) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(80) + ax0_ax1_ax2_ax3_fused_1) % T.int32(160) // T.int32(16))
                                    v2 = T.axis.spatial(T.int32(8), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(80) + ax0_ax1_ax2_ax3_fused_1) % T.int32(16) // T.int32(8))
                                    v3 = T.axis.spatial(T.int32(8), (ax0_ax1_ax2_ax3_fused_0 * T.int32(80) + ax0_ax1_ax2_ax3_fused_1) % T.int32(8))
                                    T.reads(lv772[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv772[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(80), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int32(4) * T.int32(40) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(80) + ax0_ax1_ax2_ax3_fused_1) // T.int32(10))
                                    v1 = T.axis.spatial(T.int32(1280), rc_0 * T.int32(10) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(80) + ax0_ax1_ax2_ax3_fused_1) % T.int32(10))
                                    v2 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v3 = T.axis.spatial(T.int32(1), T.int32(0))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(10), T.int32(1), T.int32(1), T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(2), T.int32(1), T.int32(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_3 * T.int32(2) + nn_4)
                                v_ff = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int32(4) * T.int32(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(8) + ff_3 * T.int32(2) + ff_4)
                                v_yy = T.axis.spatial(T.int32(8), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) // T.int32(8) + yy_3)
                                v_xx = T.axis.spatial(T.int32(8), xx_3 + xx_4 + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8))
                                v_rc = T.axis.reduce(T.int32(1280), rc_0 * T.int32(10) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int32(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(2), T.int32(8), T.int32(1), T.int32(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), ax0)
                            v1 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int32(4) * T.int32(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(8) + ax1)
                            v2 = T.axis.spatial(T.int32(8), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) // T.int32(8) + ax2)
                            v3 = T.axis.spatial(T.int32(8), nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)], lv695[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)] + lv695[v0, v1, v2, v3]

    @T.prim_func
    def fused_conv2d16_add27_add28(lv798: T.Buffer((T.int32(2), T.int32(2560), T.int32(8), T.int32(8)), "float32"), param_0: T.Buffer((T.int32(1280), T.int32(2560), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(1280), T.int32(1), T.int32(1)), "float32"), lv807: T.Buffer((T.int32(2), T.int32(1280), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(1280), T.int32(8), T.int32(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(1280), T.int32(8), T.int32(8)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(2560), T.int32(10), T.int32(10)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(1280), T.int32(2560), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(80), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(1), T.int32(4), T.int32(2), T.int32(1), T.int32(2), T.int32(1), T.int32(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_3_init + nn_4_init + nn_2_ff_2_yy_2_xx_2_fused // T.int32(64))
                            v_ff = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int32(2) * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(64) // T.int32(4) * T.int32(2) + ff_3_init * T.int32(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int32(8), nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) // T.int32(2) * T.int32(4) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int32(8), xx_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + xx_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(640), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(240))
                                    v1 = T.axis.spatial(T.int32(2560), rc_0 * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(240) // T.int32(60))
                                    v2 = T.axis.spatial(T.int32(10), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(60) // T.int32(6))
                                    v3 = T.axis.spatial(T.int32(10), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(6))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1 < T.int32(480))
                                    T.reads(lv798[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(9) and T.int32(1) <= v3 and v3 < T.int32(9), lv798[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int32(2) * T.int32(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(36))
                                    v1 = T.axis.spatial(T.int32(2560), rc_0 * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(36) // T.int32(9))
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(4), T.int32(3), T.int32(3), T.int32(1), T.int32(1), T.int32(4), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(1), T.int32(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_3 + nn_4 + nn_2_ff_2_yy_2_xx_2_fused // T.int32(64))
                                v_ff = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int32(2) * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(64) // T.int32(4) * T.int32(2) + ff_3 * T.int32(2) + ff_4)
                                v_yy = T.axis.spatial(T.int32(8), nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) // T.int32(2) * T.int32(4) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int32(8), xx_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + xx_3)
                                v_rc = T.axis.reduce(T.int32(2560), rc_0 * T.int32(4) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_2 + ry_0 * T.int32(3) + ry_1)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(2), T.int32(4), T.int32(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_2_ff_2_yy_2_xx_2_fused // T.int32(64) + ax0)
                            v1 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int32(2) * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(64) // T.int32(4) * T.int32(2) + ax1)
                            v2 = T.axis.spatial(T.int32(8), nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) // T.int32(2) * T.int32(4) + ax2)
                            v3 = T.axis.spatial(T.int32(8), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)], lv807[v0, v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)] + lv807[v0, v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d17_add27(lv796: T.Buffer((T.int32(2), T.int32(2560), T.int32(8), T.int32(8)), "float32"), param_0: T.Buffer((T.int32(1280), T.int32(2560), T.int32(1), T.int32(1)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(1280), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(1280), T.int32(8), T.int32(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(1280), T.int32(8), T.int32(8)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(2560), T.int32(8), T.int32(8)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(1280), T.int32(2560), T.int32(1), T.int32(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(160), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(2), T.int32(2), T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(2), T.int32(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(1280), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused // T.int32(4) * T.int32(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(2) + ff_3_init)
                            v_yy = T.axis.spatial(T.int32(8), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(2) + yy_3_init * T.int32(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int32(8), xx_4_init + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) * T.int32(2) + xx_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(640), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) // T.int32(64))
                                    v1 = T.axis.spatial(T.int32(2560), rc_0 * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) % T.int32(64) // T.int32(16))
                                    v2 = T.axis.spatial(T.int32(8), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) % T.int32(16) // T.int32(8))
                                    v3 = T.axis.spatial(T.int32(8), (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) % T.int32(8))
                                    T.reads(lv796[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv796[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int32(4) * T.int32(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) // T.int32(4))
                                    v1 = T.axis.spatial(T.int32(2560), rc_0 * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) % T.int32(4))
                                    v2 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v3 = T.axis.spatial(T.int32(1), T.int32(0))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(2), T.int32(1), T.int32(2), T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(1280), ff_4 + nn_0_ff_0_yy_0_xx_0_fused // T.int32(4) * T.int32(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(2) + ff_3)
                                v_yy = T.axis.spatial(T.int32(8), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(2) + yy_3 * T.int32(2) + yy_4)
                                v_xx = T.axis.spatial(T.int32(8), xx_4 + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) * T.int32(2) + xx_3)
                                v_rc = T.axis.reduce(T.int32(2560), rc_0 * T.int32(4) + rc_1 * T.int32(4) + rc_2)
                                v_ry = T.axis.reduce(T.int32(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int32(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(2), T.int32(2), T.int32(2), T.int32(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), ax0)
                            v1 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int32(4) * T.int32(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(2) + ax1)
                            v2 = T.axis.spatial(T.int32(8), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(2) + ax2)
                            v3 = T.axis.spatial(T.int32(8), nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) * T.int32(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d18_add21_add22(lv871: T.Buffer((T.int32(2), T.int32(2560), T.int32(16), T.int32(16)), "float32"), param_0: T.Buffer((T.int32(1280), T.int32(2560), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(1280), T.int32(1), T.int32(1)), "float32"), lv880: T.Buffer((T.int32(2), T.int32(1280), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(2560), T.int32(18), T.int32(18)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(1280), T.int32(2560), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(160), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(1), T.int32(2), T.int32(1), T.int32(4)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_3_init + nn_4_init + nn_2_ff_2_yy_2_xx_2_fused // T.int32(80))
                            v_ff = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int32(8) * T.int32(40) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(80) // T.int32(4) * T.int32(2) + ff_3_init * T.int32(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int32(16), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) // T.int32(2) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) + yy_3_init)
                            v_xx = T.axis.spatial(T.int32(16), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(8) + xx_3_init * T.int32(4) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(320), T.int32(3), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(160), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(160) + ax0_ax1_ax2_ax3_fused_1) // T.int32(320))
                                    v1 = T.axis.spatial(T.int32(2560), rc_0 * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(160) + ax0_ax1_ax2_ax3_fused_1) % T.int32(320) // T.int32(40))
                                    v2 = T.axis.spatial(T.int32(18), ry_0 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) // T.int32(2) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(160) + ax0_ax1_ax2_ax3_fused_1) % T.int32(40) // T.int32(10))
                                    v3 = T.axis.spatial(T.int32(18), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(160) + ax0_ax1_ax2_ax3_fused_1) % T.int32(10))
                                    T.reads(lv871[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(17) and T.int32(1) <= v3 and v3 < T.int32(17), lv871[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(6)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(160), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int32(8) * T.int32(40) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(160) + ax0_ax1_ax2_ax3_fused_1) // T.int32(24))
                                    v1 = T.axis.spatial(T.int32(2560), rc_0 * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(160) + ax0_ax1_ax2_ax3_fused_1) % T.int32(24) // T.int32(3))
                                    v2 = T.axis.spatial(T.int32(3), ry_0)
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(160) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(8), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(3), T.int32(1), T.int32(2), T.int32(1), T.int32(4)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_3 + nn_4 + nn_2_ff_2_yy_2_xx_2_fused // T.int32(80))
                                v_ff = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int32(8) * T.int32(40) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(80) // T.int32(4) * T.int32(2) + ff_3 * T.int32(2) + ff_4)
                                v_yy = T.axis.spatial(T.int32(16), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) // T.int32(2) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) + yy_3)
                                v_xx = T.axis.spatial(T.int32(16), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(8) + xx_3 * T.int32(4) + xx_4)
                                v_rc = T.axis.reduce(T.int32(2560), rc_0 * T.int32(8) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 * T.int32(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(2), T.int32(1), T.int32(8)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_2_ff_2_yy_2_xx_2_fused // T.int32(80) + ax0)
                            v1 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int32(8) * T.int32(40) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(80) // T.int32(4) * T.int32(2) + ax1)
                            v2 = T.axis.spatial(T.int32(16), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) // T.int32(2) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) + ax2)
                            v3 = T.axis.spatial(T.int32(16), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(8) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)], lv880[v0, v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)] + lv880[v0, v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d19_add21(lv869: T.Buffer((T.int32(2), T.int32(2560), T.int32(16), T.int32(16)), "float32"), param_0: T.Buffer((T.int32(1280), T.int32(2560), T.int32(1), T.int32(1)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(1280), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(2560), T.int32(16), T.int32(16)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(1280), T.int32(2560), T.int32(1), T.int32(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(160), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(2), T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(1280), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused // T.int32(8) * T.int32(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(4) + ff_3_init)
                            v_yy = T.axis.spatial(T.int32(16), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) // T.int32(8) + yy_3_init)
                            v_xx = T.axis.spatial(T.int32(16), nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) * T.int32(2) + xx_3_init * T.int32(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(80), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(13)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(160), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(160) + ax0_ax1_ax2_ax3_fused_1) // T.int32(1024))
                                    v1 = T.axis.spatial(T.int32(2560), rc_0 * T.int32(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(160) + ax0_ax1_ax2_ax3_fused_1) % T.int32(1024) // T.int32(32))
                                    v2 = T.axis.spatial(T.int32(16), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(160) + ax0_ax1_ax2_ax3_fused_1) % T.int32(32) // T.int32(16))
                                    v3 = T.axis.spatial(T.int32(16), (ax0_ax1_ax2_ax3_fused_0 * T.int32(160) + ax0_ax1_ax2_ax3_fused_1) % T.int32(16))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(160) + ax0_ax1_ax2_ax3_fused_1 < T.int32(2048))
                                    T.reads(lv869[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv869[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(160), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int32(8) * T.int32(40) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(160) + ax0_ax1_ax2_ax3_fused_1) // T.int32(32))
                                    v1 = T.axis.spatial(T.int32(2560), rc_0 * T.int32(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(160) + ax0_ax1_ax2_ax3_fused_1) % T.int32(32))
                                    v2 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v3 = T.axis.spatial(T.int32(1), T.int32(0))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(8), T.int32(1), T.int32(1), T.int32(2), T.int32(4), T.int32(1), T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(1280), ff_4 + nn_0_ff_0_yy_0_xx_0_fused // T.int32(8) * T.int32(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(4) + ff_3)
                                v_yy = T.axis.spatial(T.int32(16), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) // T.int32(8) + yy_3)
                                v_xx = T.axis.spatial(T.int32(16), nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) * T.int32(2) + xx_3 * T.int32(2) + xx_4)
                                v_rc = T.axis.reduce(T.int32(2560), rc_0 * T.int32(32) + rc_1 * T.int32(4) + rc_2)
                                v_ry = T.axis.reduce(T.int32(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int32(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(2), T.int32(4), T.int32(1), T.int32(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), ax0)
                            v1 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int32(8) * T.int32(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(4) + ax1)
                            v2 = T.axis.spatial(T.int32(16), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) // T.int32(8) + ax2)
                            v3 = T.axis.spatial(T.int32(16), nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) * T.int32(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d1_add5_add7(lv25: T.Buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), "float32"), param_0: T.Buffer((T.int32(320), T.int32(320), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(320), T.int32(1), T.int32(1)), "float32"), lv34: T.Buffer((T.int32(2), T.int32(320), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(320), T.int32(66), T.int32(66)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(320), T.int32(320), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(5), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(256) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(320), nn_0_ff_0_yy_0_xx_0_fused % T.int32(256) // T.int32(16) * T.int32(20) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(64) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(16) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(64) // T.int32(32) * T.int32(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int32(64), nn_2_ff_2_yy_2_xx_2_fused % T.int32(32) * T.int32(2) + xx_3_init * T.int32(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(320), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(256))
                                    v1 = T.axis.spatial(T.int32(320), rc_0)
                                    v2 = T.axis.spatial(T.int32(66), nn_0_ff_0_yy_0_xx_0_fused % T.int32(16) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) // T.int32(66))
                                    v3 = T.axis.spatial(T.int32(66), (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) % T.int32(66))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1 < T.int32(396))
                                    T.reads(lv25[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(65) and T.int32(1) <= v3 and v3 < T.int32(65), lv25[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(320), nn_0_ff_0_yy_0_xx_0_fused % T.int32(256) // T.int32(16) * T.int32(20) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) // T.int32(9))
                                    v1 = T.axis.spatial(T.int32(320), rc_0)
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1 < T.int32(180))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(1), T.int32(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(256) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(320), nn_0_ff_0_yy_0_xx_0_fused % T.int32(256) // T.int32(16) * T.int32(20) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(64) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(16) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(64) // T.int32(32) * T.int32(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int32(64), nn_2_ff_2_yy_2_xx_2_fused % T.int32(32) * T.int32(2) + xx_3 * T.int32(2) + xx_4)
                                v_rc = T.axis.reduce(T.int32(320), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_2 + ry_0 * T.int32(3) + ry_1)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 * T.int32(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(1), T.int32(2), T.int32(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(256) + ax0)
                            v1 = T.axis.spatial(T.int32(320), nn_0_ff_0_yy_0_xx_0_fused % T.int32(256) // T.int32(16) * T.int32(20) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(64) + ax1)
                            v2 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(16) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(64) // T.int32(32) * T.int32(2) + ax2)
                            v3 = T.axis.spatial(T.int32(64), nn_2_ff_2_yy_2_xx_2_fused % T.int32(32) * T.int32(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)], lv34[v0, v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)] + lv34[v0, v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d1_add5_add8_divide(lv37: T.Buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), "float32"), param_0: T.Buffer((T.int32(320), T.int32(320), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(320), T.int32(1), T.int32(1)), "float32"), lv23: T.Buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), "float32"), T_divide: T.Buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(320), T.int32(66), T.int32(66)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(320), T.int32(320), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(640), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(2), T.int32(1), T.int32(2), T.int32(1), T.int32(2), T.int32(2), T.int32(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(320) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(320), nn_0_ff_0_yy_0_xx_0_fused % T.int32(320) // T.int32(64) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(4) + ff_3_init * T.int32(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) // T.int32(4) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) // T.int32(4) * T.int32(2) + yy_3_init * T.int32(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) * T.int32(4) + xx_3_init * T.int32(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(320), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(320))
                                    v1 = T.axis.spatial(T.int32(320), rc_0)
                                    v2 = T.axis.spatial(T.int32(66), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) // T.int32(4) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(18))
                                    v3 = T.axis.spatial(T.int32(66), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(18))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1 < T.int32(108))
                                    T.reads(lv37[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(65) and T.int32(1) <= v3 and v3 < T.int32(65), lv37[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(320), nn_0_ff_0_yy_0_xx_0_fused % T.int32(320) // T.int32(64) * T.int32(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(9))
                                    v1 = T.axis.spatial(T.int32(320), rc_0)
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(3), T.int32(3), T.int32(1), T.int32(2), T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(2), T.int32(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(320) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(320), nn_0_ff_0_yy_0_xx_0_fused % T.int32(320) // T.int32(64) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(4) + ff_3 * T.int32(2) + ff_4)
                                v_yy = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) // T.int32(4) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) // T.int32(4) * T.int32(2) + yy_3 * T.int32(2) + yy_4)
                                v_xx = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) * T.int32(4) + xx_3 * T.int32(2) + xx_4)
                                v_rc = T.axis.reduce(T.int32(320), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_2 + ry_0 * T.int32(3) + ry_1)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(4), T.int32(2), T.int32(4)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(320) + ax0)
                            v1 = T.axis.spatial(T.int32(320), nn_0_ff_0_yy_0_xx_0_fused % T.int32(320) // T.int32(64) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(4) + ax1)
                            v2 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) // T.int32(4) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) // T.int32(4) * T.int32(2) + ax2)
                            v3 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) * T.int32(4) + ax3)
                            T.reads(lv23[v0, v1, v2, v3], conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)])
                            T.writes(T_divide[v0, v1, v2, v3])
                            T_divide[v0, v1, v2, v3] = lv23[v0, v1, v2, v3] + (conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)])

    @T.prim_func
    def fused_conv2d20_add21_add22(lv1079: T.Buffer((T.int32(2), T.int32(1920), T.int32(16), T.int32(16)), "float32"), param_0: T.Buffer((T.int32(1280), T.int32(1920), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(1280), T.int32(1), T.int32(1)), "float32"), lv1088: T.Buffer((T.int32(2), T.int32(1280), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(1920), T.int32(18), T.int32(18)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(1280), T.int32(1920), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(80), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(4), T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_4_init + nn_1_ff_1_yy_1_xx_1_fused + nn_3_init)
                            v_ff = T.axis.spatial(T.int32(1280), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(64) * T.int32(4) + ff_3_init)
                            v_yy = T.axis.spatial(T.int32(16), yy_4_init + nn_2_ff_2_yy_2_xx_2_fused % T.int32(64) // T.int32(4) + yy_3_init)
                            v_xx = T.axis.spatial(T.int32(16), nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) * T.int32(4) + xx_3_init * T.int32(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(1920), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) // T.int32(324))
                                    v1 = T.axis.spatial(T.int32(1920), rc_0)
                                    v2 = T.axis.spatial(T.int32(18), (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) % T.int32(324) // T.int32(18))
                                    v3 = T.axis.spatial(T.int32(18), (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) % T.int32(18))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1 < T.int32(648))
                                    T.reads(lv1079[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(17) and T.int32(1) <= v3 and v3 < T.int32(17), lv1079[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused * T.int32(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) // T.int32(9))
                                    v1 = T.axis.spatial(T.int32(1920), rc_0)
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1 < T.int32(144))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(4), T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(1), T.int32(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_4 + nn_1_ff_1_yy_1_xx_1_fused + nn_3)
                                v_ff = T.axis.spatial(T.int32(1280), ff_4 + nn_0_ff_0_yy_0_xx_0_fused * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(64) * T.int32(4) + ff_3)
                                v_yy = T.axis.spatial(T.int32(16), yy_4 + nn_2_ff_2_yy_2_xx_2_fused % T.int32(64) // T.int32(4) + yy_3)
                                v_xx = T.axis.spatial(T.int32(16), nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) * T.int32(4) + xx_3 * T.int32(2) + xx_4)
                                v_rc = T.axis.reduce(T.int32(1920), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_2 + ry_0 * T.int32(3) + ry_1)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 * T.int32(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(4), T.int32(1), T.int32(4)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_1_ff_1_yy_1_xx_1_fused + ax0)
                            v1 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(64) * T.int32(4) + ax1)
                            v2 = T.axis.spatial(T.int32(16), nn_2_ff_2_yy_2_xx_2_fused % T.int32(64) // T.int32(4) + ax2)
                            v3 = T.axis.spatial(T.int32(16), nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) * T.int32(4) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)], lv1088[v0, v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)] + lv1088[v0, v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d21_add21(lv1077: T.Buffer((T.int32(2), T.int32(1920), T.int32(16), T.int32(16)), "float32"), param_0: T.Buffer((T.int32(1280), T.int32(1920), T.int32(1), T.int32(1)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(1280), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(1920), T.int32(16), T.int32(16)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(1280), T.int32(1920), T.int32(1), T.int32(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(160), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(2), T.int32(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_3_init + nn_4_init + nn_2_ff_2_yy_2_xx_2_fused // T.int32(64))
                            v_ff = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int32(8) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int32(2) * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(64) // T.int32(8) * T.int32(4) + ff_3_init * T.int32(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int32(16), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(2) + yy_3_init * T.int32(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int32(16), xx_3_init + xx_4_init + nn_1_ff_1_yy_1_xx_1_fused % T.int32(2) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8))
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(160), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(6)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(384))
                                    v1 = T.axis.spatial(T.int32(1920), rc_0 * T.int32(12) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(384) // T.int32(32))
                                    v2 = T.axis.spatial(T.int32(16), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(32) // T.int32(16))
                                    v3 = T.axis.spatial(T.int32(16), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(16))
                                    T.reads(lv1077[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv1077[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(6)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int32(8) * T.int32(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(12))
                                    v1 = T.axis.spatial(T.int32(1920), rc_0 * T.int32(12) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(12))
                                    v2 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v3 = T.axis.spatial(T.int32(1), T.int32(0))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(2), T.int32(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_3 + nn_4 + nn_2_ff_2_yy_2_xx_2_fused // T.int32(64))
                                v_ff = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int32(8) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int32(2) * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(64) // T.int32(8) * T.int32(4) + ff_3 * T.int32(2) + ff_4)
                                v_yy = T.axis.spatial(T.int32(16), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(2) + yy_3 * T.int32(2) + yy_4)
                                v_xx = T.axis.spatial(T.int32(16), xx_3 + xx_4 + nn_1_ff_1_yy_1_xx_1_fused % T.int32(2) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8))
                                v_rc = T.axis.reduce(T.int32(1920), rc_0 * T.int32(12) + rc_1 * T.int32(3) + rc_2)
                                v_ry = T.axis.reduce(T.int32(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int32(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(4), T.int32(2), T.int32(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_2_ff_2_yy_2_xx_2_fused // T.int32(64) + ax0)
                            v1 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int32(8) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int32(2) * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(64) // T.int32(8) * T.int32(4) + ax1)
                            v2 = T.axis.spatial(T.int32(16), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(2) + ax2)
                            v3 = T.axis.spatial(T.int32(16), nn_1_ff_1_yy_1_xx_1_fused % T.int32(2) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d22_add33(lv1181: T.Buffer((T.int32(2), T.int32(1280), T.int32(32), T.int32(32)), "float32"), param_0: T.Buffer((T.int32(1280), T.int32(1280), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(1280), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(1280), T.int32(32), T.int32(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(1280), T.int32(32), T.int32(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(1280), T.int32(34), T.int32(34)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(1280), T.int32(1280), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(1280), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(2), T.int32(2), T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(640) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(1280), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(640) // T.int32(32) * T.int32(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(2) * T.int32(2) + ff_3_init)
                            v_yy = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) // T.int32(4) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int32(32), xx_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(4) + xx_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(1280), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(640))
                                    v1 = T.axis.spatial(T.int32(1280), rc_0)
                                    v2 = T.axis.spatial(T.int32(34), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) // T.int32(4) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(10))
                                    v3 = T.axis.spatial(T.int32(34), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(10))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1 < T.int32(60))
                                    T.reads(lv1181[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(33) and T.int32(1) <= v3 and v3 < T.int32(33), lv1181[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int32(640) // T.int32(32) * T.int32(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(9))
                                    v1 = T.axis.spatial(T.int32(1280), rc_0)
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(2), T.int32(2), T.int32(4), T.int32(1), T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(640) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(1280), ff_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(640) // T.int32(32) * T.int32(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(2) * T.int32(2) + ff_3)
                                v_yy = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) // T.int32(4) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int32(32), xx_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(4) + xx_3)
                                v_rc = T.axis.reduce(T.int32(1280), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_2 + ry_0 * T.int32(3) + ry_1)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 * T.int32(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(2), T.int32(2), T.int32(4)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(640) + ax0)
                            v1 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int32(640) // T.int32(32) * T.int32(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(2) * T.int32(2) + ax1)
                            v2 = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) // T.int32(4) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + ax2)
                            v3 = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(4) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d23_add13_add15(lv1187: T.Buffer((T.int32(2), T.int32(1920), T.int32(32), T.int32(32)), "float32"), param_0: T.Buffer((T.int32(640), T.int32(1920), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(640), T.int32(1), T.int32(1)), "float32"), lv1196: T.Buffer((T.int32(2), T.int32(640), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(1920), T.int32(34), T.int32(34)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(640), T.int32(1920), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(160), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(2), T.int32(2), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(80) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(640), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(80) // T.int32(8) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(2) + ff_3_init)
                            v_yy = T.axis.spatial(T.int32(32), nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) * T.int32(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(4) + xx_3_init * T.int32(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(960), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(80))
                                    v1 = T.axis.spatial(T.int32(1920), rc_0 * T.int32(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(204))
                                    v2 = T.axis.spatial(T.int32(34), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(204) // T.int32(6))
                                    v3 = T.axis.spatial(T.int32(34), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(6))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1 < T.int32(408))
                                    T.reads(lv1187[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(33) and T.int32(1) <= v3 and v3 < T.int32(33), lv1187[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused % T.int32(80) // T.int32(8) * T.int32(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(18))
                                    v1 = T.axis.spatial(T.int32(1920), rc_0 * T.int32(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(18) // T.int32(9))
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(2), T.int32(3), T.int32(1), T.int32(1), T.int32(2), T.int32(2), T.int32(2), T.int32(1), T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(1), T.int32(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(80) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(640), ff_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(80) // T.int32(8) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(2) + ff_3)
                                v_yy = T.axis.spatial(T.int32(32), nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) * T.int32(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(4) + xx_3 * T.int32(2) + xx_4)
                                v_rc = T.axis.reduce(T.int32(1920), rc_0 * T.int32(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_2 + ry_0 * T.int32(3) + ry_1)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 * T.int32(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(2), T.int32(2), T.int32(4)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(80) + ax0)
                            v1 = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused % T.int32(80) // T.int32(8) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(2) + ax1)
                            v2 = T.axis.spatial(T.int32(32), nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) * T.int32(2) + ax2)
                            v3 = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(4) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)], lv1196[v0, v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)] + lv1196[v0, v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d24_add13(lv1185: T.Buffer((T.int32(2), T.int32(1920), T.int32(32), T.int32(32)), "float32"), param_0: T.Buffer((T.int32(640), T.int32(1920), T.int32(1), T.int32(1)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(640), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(1920), T.int32(32), T.int32(32)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(640), T.int32(1920), T.int32(1), T.int32(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(5), T.int32(2), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_4_init + nn_1_ff_1_yy_1_xx_1_fused + nn_3_init)
                            v_ff = T.axis.spatial(T.int32(640), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused // T.int32(32) * T.int32(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(5) + ff_3_init)
                            v_yy = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) // T.int32(2) * T.int32(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int32(32), xx_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) * T.int32(2) + xx_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(96), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(640))
                                    v1 = T.axis.spatial(T.int32(1920), rc_0 * T.int32(20) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(640) // T.int32(32))
                                    v2 = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) // T.int32(2) * T.int32(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(32) // T.int32(16))
                                    v3 = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(16))
                                    T.reads(lv1185[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv1185[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(13)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused // T.int32(32) * T.int32(80) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(20))
                                    v1 = T.axis.spatial(T.int32(1920), rc_0 * T.int32(20) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(20))
                                    v2 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v3 = T.axis.spatial(T.int32(1), T.int32(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1 < T.int32(1600))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(5), T.int32(1), T.int32(1), T.int32(1), T.int32(5), T.int32(2), T.int32(2), T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_4 + nn_1_ff_1_yy_1_xx_1_fused + nn_3)
                                v_ff = T.axis.spatial(T.int32(640), ff_4 + nn_0_ff_0_yy_0_xx_0_fused // T.int32(32) * T.int32(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(5) + ff_3)
                                v_yy = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) // T.int32(2) * T.int32(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int32(32), xx_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) * T.int32(2) + xx_3)
                                v_rc = T.axis.reduce(T.int32(1920), rc_0 * T.int32(20) + rc_1 * T.int32(4) + rc_2)
                                v_ry = T.axis.reduce(T.int32(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int32(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(5), T.int32(2), T.int32(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_1_ff_1_yy_1_xx_1_fused + ax0)
                            v1 = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused // T.int32(32) * T.int32(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(5) + ax1)
                            v2 = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) // T.int32(2) * T.int32(2) + ax2)
                            v3 = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) * T.int32(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d25_add13_add15(lv1291: T.Buffer((T.int32(2), T.int32(1280), T.int32(32), T.int32(32)), "float32"), param_0: T.Buffer((T.int32(640), T.int32(1280), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(640), T.int32(1), T.int32(1)), "float32"), lv1300: T.Buffer((T.int32(2), T.int32(640), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(1280), T.int32(34), T.int32(34)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(640), T.int32(1280), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(4), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(160) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(640), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(160) // T.int32(8) * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(4) + ff_3_init)
                            v_yy = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(4) + yy_3_init * T.int32(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int32(32), nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) * T.int32(2) + xx_3_init * T.int32(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(640), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(160))
                                    v1 = T.axis.spatial(T.int32(1280), rc_0 * T.int32(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(204))
                                    v2 = T.axis.spatial(T.int32(34), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(204) // T.int32(34))
                                    v3 = T.axis.spatial(T.int32(34), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(34))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1 < T.int32(408))
                                    T.reads(lv1291[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(33) and T.int32(1) <= v3 and v3 < T.int32(33), lv1291[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused % T.int32(160) // T.int32(8) * T.int32(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(18))
                                    v1 = T.axis.spatial(T.int32(1280), rc_0 * T.int32(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(18) // T.int32(9))
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1 < T.int32(576))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(4), T.int32(2), T.int32(1), T.int32(1), T.int32(3), T.int32(3), T.int32(1), T.int32(1), T.int32(2), T.int32(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(160) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(640), ff_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(160) // T.int32(8) * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(4) + ff_3)
                                v_yy = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(4) + yy_3 * T.int32(2) + yy_4)
                                v_xx = T.axis.spatial(T.int32(32), nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) * T.int32(2) + xx_3 * T.int32(2) + xx_4)
                                v_rc = T.axis.reduce(T.int32(1280), rc_0 * T.int32(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_0 * T.int32(3) + ry_1 * T.int32(3) + ry_2)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 * T.int32(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(4), T.int32(4), T.int32(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(160) + ax0)
                            v1 = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused % T.int32(160) // T.int32(8) * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(4) + ax1)
                            v2 = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(4) + ax2)
                            v3 = T.axis.spatial(T.int32(32), nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) * T.int32(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)], lv1300[v0, v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)] + lv1300[v0, v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d26_add13(lv1289: T.Buffer((T.int32(2), T.int32(1280), T.int32(32), T.int32(32)), "float32"), param_0: T.Buffer((T.int32(640), T.int32(1280), T.int32(1), T.int32(1)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(640), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(1280), T.int32(32), T.int32(32)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(640), T.int32(1280), T.int32(1), T.int32(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(1280), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(2), T.int32(2), T.int32(1), T.int32(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_3_init * T.int32(2) + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused // T.int32(32) * T.int32(16) + nn_1_ff_1_yy_1_xx_1_fused // T.int32(2) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(4) + ff_3_init * T.int32(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int32(32), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) // T.int32(2) * T.int32(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) // T.int32(8) + yy_3_init)
                            v_xx = T.axis.spatial(T.int32(32), xx_3_init + xx_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int32(2) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8))
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(160), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) // T.int32(256))
                                    v1 = T.axis.spatial(T.int32(1280), rc_0 * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) % T.int32(256) // T.int32(32))
                                    v2 = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) // T.int32(2) * T.int32(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) % T.int32(32) // T.int32(16))
                                    v3 = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) % T.int32(16))
                                    T.reads(lv1289[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv1289[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused // T.int32(32) * T.int32(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) // T.int32(8))
                                    v1 = T.axis.spatial(T.int32(1280), rc_0 * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) % T.int32(8))
                                    v2 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v3 = T.axis.spatial(T.int32(1), T.int32(0))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(2), T.int32(2), T.int32(1), T.int32(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_3 * T.int32(2) + nn_4)
                                v_ff = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused // T.int32(32) * T.int32(16) + nn_1_ff_1_yy_1_xx_1_fused // T.int32(2) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(4) + ff_3 * T.int32(2) + ff_4)
                                v_yy = T.axis.spatial(T.int32(32), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) // T.int32(2) * T.int32(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) // T.int32(8) + yy_3)
                                v_xx = T.axis.spatial(T.int32(32), xx_3 + xx_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int32(2) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8))
                                v_rc = T.axis.reduce(T.int32(1280), rc_0 * T.int32(8) + rc_1 * T.int32(4) + rc_2)
                                v_ry = T.axis.reduce(T.int32(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int32(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(2), T.int32(4), T.int32(1), T.int32(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), ax0)
                            v1 = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused // T.int32(32) * T.int32(16) + nn_1_ff_1_yy_1_xx_1_fused // T.int32(2) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(4) + ax1)
                            v2 = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) // T.int32(2) * T.int32(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) // T.int32(8) + ax2)
                            v3 = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int32(2) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d27_add13_add15(lv1395: T.Buffer((T.int32(2), T.int32(960), T.int32(32), T.int32(32)), "float32"), param_0: T.Buffer((T.int32(640), T.int32(960), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(640), T.int32(1), T.int32(1)), "float32"), lv1404: T.Buffer((T.int32(2), T.int32(640), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(960), T.int32(34), T.int32(34)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(640), T.int32(960), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(160), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(2), T.int32(4)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(80) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused % T.int32(80) // T.int32(16) * T.int32(128) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(2) + ff_3_init * T.int32(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(16) // T.int32(2) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) // T.int32(4) * T.int32(2) + yy_3_init * T.int32(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) * T.int32(4) + xx_3_init * T.int32(4) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(960), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(80))
                                    v1 = T.axis.spatial(T.int32(960), rc_0)
                                    v2 = T.axis.spatial(T.int32(34), nn_0_ff_0_yy_0_xx_0_fused % T.int32(16) // T.int32(2) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(18))
                                    v3 = T.axis.spatial(T.int32(34), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(18))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1 < T.int32(108))
                                    T.reads(lv1395[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(33) and T.int32(1) <= v3 and v3 < T.int32(33), lv1395[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused % T.int32(80) // T.int32(16) * T.int32(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(9))
                                    v1 = T.axis.spatial(T.int32(960), rc_0)
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(3), T.int32(3), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(2), T.int32(4)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(80) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused % T.int32(80) // T.int32(16) * T.int32(128) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(2) + ff_3 * T.int32(2) + ff_4)
                                v_yy = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(16) // T.int32(2) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) // T.int32(4) * T.int32(2) + yy_3 * T.int32(2) + yy_4)
                                v_xx = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) * T.int32(4) + xx_3 * T.int32(4) + xx_4)
                                v_rc = T.axis.reduce(T.int32(960), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_2 + ry_0 * T.int32(3) + ry_1)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(2), T.int32(2), T.int32(4)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(80) + ax0)
                            v1 = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused % T.int32(80) // T.int32(16) * T.int32(128) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(2) + ax1)
                            v2 = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(16) // T.int32(2) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) // T.int32(4) * T.int32(2) + ax2)
                            v3 = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) * T.int32(4) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)], lv1404[v0, v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)] + lv1404[v0, v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d28_add13(lv1393: T.Buffer((T.int32(2), T.int32(960), T.int32(32), T.int32(32)), "float32"), param_0: T.Buffer((T.int32(640), T.int32(960), T.int32(1), T.int32(1)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(640), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(960), T.int32(32), T.int32(32)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(640), T.int32(960), T.int32(1), T.int32(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(640), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(8), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(1), T.int32(2), T.int32(1), T.int32(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(320) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused % T.int32(320) // T.int32(32) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int32(2) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(2) + ff_3_init * T.int32(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int32(32), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) + yy_3_init)
                            v_xx = T.axis.spatial(T.int32(32), xx_4_init + nn_1_ff_1_yy_1_xx_1_fused % T.int32(2) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) * T.int32(2) + xx_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(120), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(320))
                                    v1 = T.axis.spatial(T.int32(960), rc_0 * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(32))
                                    v2 = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32))
                                    v3 = T.axis.spatial(T.int32(32), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(32))
                                    T.reads(lv1393[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv1393[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused % T.int32(320) // T.int32(32) * T.int32(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(8))
                                    v1 = T.axis.spatial(T.int32(960), rc_0 * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(8))
                                    v2 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v3 = T.axis.spatial(T.int32(1), T.int32(0))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(1), T.int32(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(320) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused % T.int32(320) // T.int32(32) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int32(2) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(2) + ff_3 * T.int32(2) + ff_4)
                                v_yy = T.axis.spatial(T.int32(32), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) + yy_3)
                                v_xx = T.axis.spatial(T.int32(32), xx_4 + nn_1_ff_1_yy_1_xx_1_fused % T.int32(2) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) * T.int32(2) + xx_3)
                                v_rc = T.axis.reduce(T.int32(960), rc_0 * T.int32(8) + rc_1 * T.int32(2) + rc_2)
                                v_ry = T.axis.reduce(T.int32(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int32(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(2), T.int32(1), T.int32(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(320) + ax0)
                            v1 = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused % T.int32(320) // T.int32(32) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int32(2) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(2) + ax1)
                            v2 = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) + ax2)
                            v3 = T.axis.spatial(T.int32(32), nn_1_ff_1_yy_1_xx_1_fused % T.int32(2) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) * T.int32(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d29_add34(lv1497: T.Buffer((T.int32(2), T.int32(640), T.int32(64), T.int32(64)), "float32"), param_0: T.Buffer((T.int32(640), T.int32(640), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(640), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(640), T.int32(64), T.int32(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(640), T.int32(64), T.int32(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(640), T.int32(66), T.int32(66)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(640), T.int32(640), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(5), T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(512) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(640), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(512) // T.int32(64) * T.int32(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(5) + ff_3_init)
                            v_yy = T.axis.spatial(T.int32(64), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) // T.int32(8) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) + yy_3_init)
                            v_xx = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(8) + xx_3_init * T.int32(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(320), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(512))
                                    v1 = T.axis.spatial(T.int32(640), rc_0 * T.int32(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(100))
                                    v2 = T.axis.spatial(T.int32(66), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) // T.int32(8) * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(100) // T.int32(10))
                                    v3 = T.axis.spatial(T.int32(66), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(10))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1 < T.int32(200))
                                    T.reads(lv1497[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(65) and T.int32(1) <= v3 and v3 < T.int32(65), lv1497[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(12)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused % T.int32(512) // T.int32(64) * T.int32(80) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(18))
                                    v1 = T.axis.spatial(T.int32(640), rc_0 * T.int32(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(18) // T.int32(9))
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1 < T.int32(1440))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(2), T.int32(3), T.int32(1), T.int32(1), T.int32(5), T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(1), T.int32(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(512) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(640), ff_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(512) // T.int32(64) * T.int32(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(5) + ff_3)
                                v_yy = T.axis.spatial(T.int32(64), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) // T.int32(8) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) + yy_3)
                                v_xx = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(8) + xx_3 * T.int32(2) + xx_4)
                                v_rc = T.axis.reduce(T.int32(640), rc_0 * T.int32(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_2 + ry_0 * T.int32(3) + ry_1)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 * T.int32(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(5), T.int32(1), T.int32(8)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(512) + ax0)
                            v1 = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused % T.int32(512) // T.int32(64) * T.int32(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(5) + ax1)
                            v2 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) // T.int32(8) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) + ax2)
                            v3 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(8) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d2_add5(lv43: T.Buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), "float32"), param_0: T.Buffer((T.int32(320), T.int32(320), T.int32(1), T.int32(1)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(320), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(320), T.int32(320), T.int32(1), T.int32(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(10), T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_3_init + nn_4_init + nn_2_ff_2_yy_2_xx_2_fused // T.int32(32))
                            v_ff = T.axis.spatial(T.int32(320), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused // T.int32(256) * T.int32(80) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(32) // T.int32(4) * T.int32(10) + ff_3_init)
                            v_yy = T.axis.spatial(T.int32(64), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(256) // T.int32(8) * T.int32(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) // T.int32(2) + yy_3_init)
                            v_xx = T.axis.spatial(T.int32(64), xx_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + xx_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(40), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(128))
                                    v1 = T.axis.spatial(T.int32(320), rc_0 * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(128) // T.int32(16))
                                    v2 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(256) // T.int32(8) * T.int32(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(16) // T.int32(8))
                                    v3 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(8))
                                    T.reads(lv43[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv43[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(320), nn_0_ff_0_yy_0_xx_0_fused // T.int32(256) * T.int32(80) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(8))
                                    v1 = T.axis.spatial(T.int32(320), rc_0 * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(8))
                                    v2 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v3 = T.axis.spatial(T.int32(1), T.int32(0))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(10), T.int32(1), T.int32(2), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_3 + nn_4 + nn_2_ff_2_yy_2_xx_2_fused // T.int32(32))
                                v_ff = T.axis.spatial(T.int32(320), ff_4 + nn_0_ff_0_yy_0_xx_0_fused // T.int32(256) * T.int32(80) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(32) // T.int32(4) * T.int32(10) + ff_3)
                                v_yy = T.axis.spatial(T.int32(64), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(256) // T.int32(8) * T.int32(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) // T.int32(2) + yy_3)
                                v_xx = T.axis.spatial(T.int32(64), xx_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + xx_3)
                                v_rc = T.axis.reduce(T.int32(320), rc_0 * T.int32(8) + rc_1 * T.int32(2) + rc_2)
                                v_ry = T.axis.reduce(T.int32(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int32(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(10), T.int32(1), T.int32(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_2_ff_2_yy_2_xx_2_fused // T.int32(32) + ax0)
                            v1 = T.axis.spatial(T.int32(320), nn_0_ff_0_yy_0_xx_0_fused // T.int32(256) * T.int32(80) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(32) // T.int32(4) * T.int32(10) + ax1)
                            v2 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(256) // T.int32(8) * T.int32(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) // T.int32(2) + ax2)
                            v3 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d2_add5_add8(lv119: T.Buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), "float32"), param_0: T.Buffer((T.int32(320), T.int32(320), T.int32(1), T.int32(1)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(320), T.int32(1), T.int32(1)), "float32"), lv42: T.Buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(320), T.int32(320), T.int32(1), T.int32(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(5), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(4)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_4_init + nn_1_ff_1_yy_1_xx_1_fused + nn_3_init)
                            v_ff = T.axis.spatial(T.int32(320), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused // T.int32(256) * T.int32(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(5) + ff_3_init)
                            v_yy = T.axis.spatial(T.int32(64), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(256) // T.int32(4) + yy_3_init)
                            v_xx = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) * T.int32(4) + xx_3_init * T.int32(4) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(20), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(256))
                                    v1 = T.axis.spatial(T.int32(320), rc_0 * T.int32(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(256) // T.int32(16))
                                    v2 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(256) // T.int32(4))
                                    v3 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(16))
                                    T.reads(lv119[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv119[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(20)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(320), nn_0_ff_0_yy_0_xx_0_fused // T.int32(256) * T.int32(80) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(16))
                                    v1 = T.axis.spatial(T.int32(320), rc_0 * T.int32(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(16))
                                    v2 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v3 = T.axis.spatial(T.int32(1), T.int32(0))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(5), T.int32(1), T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(4)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_4 + nn_1_ff_1_yy_1_xx_1_fused + nn_3)
                                v_ff = T.axis.spatial(T.int32(320), ff_4 + nn_0_ff_0_yy_0_xx_0_fused // T.int32(256) * T.int32(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(5) + ff_3)
                                v_yy = T.axis.spatial(T.int32(64), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(256) // T.int32(4) + yy_3)
                                v_xx = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) * T.int32(4) + xx_3 * T.int32(4) + xx_4)
                                v_rc = T.axis.reduce(T.int32(320), rc_0 * T.int32(16) + rc_1 * T.int32(4) + rc_2)
                                v_ry = T.axis.reduce(T.int32(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int32(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(5), T.int32(1), T.int32(4)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_1_ff_1_yy_1_xx_1_fused + ax0)
                            v1 = T.axis.spatial(T.int32(320), nn_0_ff_0_yy_0_xx_0_fused // T.int32(256) * T.int32(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(5) + ax1)
                            v2 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(256) // T.int32(4) + ax2)
                            v3 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) * T.int32(4) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)], lv42[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)] + lv42[v0, v1, v2, v3]

    @T.prim_func
    def fused_conv2d30_add5_add7(lv1503: T.Buffer((T.int32(2), T.int32(960), T.int32(64), T.int32(64)), "float32"), param_0: T.Buffer((T.int32(320), T.int32(960), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(320), T.int32(1), T.int32(1)), "float32"), lv1512: T.Buffer((T.int32(2), T.int32(320), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(960), T.int32(66), T.int32(66)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(320), T.int32(960), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(4), T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(160) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(320), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(160) // T.int32(32) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int32(2) * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(4) + ff_3_init)
                            v_yy = T.axis.spatial(T.int32(64), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) // T.int32(8) * T.int32(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int32(2) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) // T.int32(2) + yy_3_init)
                            v_xx = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(4) + xx_3_init * T.int32(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(480), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(160))
                                    v1 = T.axis.spatial(T.int32(960), rc_0 * T.int32(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(180))
                                    v2 = T.axis.spatial(T.int32(66), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) // T.int32(8) * T.int32(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(180) // T.int32(10))
                                    v3 = T.axis.spatial(T.int32(66), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(10))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1 < T.int32(360))
                                    T.reads(lv1503[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(65) and T.int32(1) <= v3 and v3 < T.int32(65), lv1503[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(320), nn_0_ff_0_yy_0_xx_0_fused % T.int32(160) // T.int32(32) * T.int32(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(18))
                                    v1 = T.axis.spatial(T.int32(960), rc_0 * T.int32(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(18) // T.int32(9))
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(2), T.int32(3), T.int32(1), T.int32(1), T.int32(4), T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(1), T.int32(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(160) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(320), ff_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(160) // T.int32(32) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int32(2) * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(4) + ff_3)
                                v_yy = T.axis.spatial(T.int32(64), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) // T.int32(8) * T.int32(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int32(2) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) // T.int32(2) + yy_3)
                                v_xx = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(4) + xx_3 * T.int32(2) + xx_4)
                                v_rc = T.axis.reduce(T.int32(960), rc_0 * T.int32(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_2 + ry_0 * T.int32(3) + ry_1)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 * T.int32(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(4), T.int32(1), T.int32(4)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(160) + ax0)
                            v1 = T.axis.spatial(T.int32(320), nn_0_ff_0_yy_0_xx_0_fused % T.int32(160) // T.int32(32) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int32(2) * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(4) + ax1)
                            v2 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) // T.int32(8) * T.int32(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int32(2) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) // T.int32(2) + ax2)
                            v3 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(4) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)], lv1512[v0, v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)] + lv1512[v0, v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d31_add5(lv1501: T.Buffer((T.int32(2), T.int32(960), T.int32(64), T.int32(64)), "float32"), param_0: T.Buffer((T.int32(320), T.int32(960), T.int32(1), T.int32(1)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(320), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(960), T.int32(64), T.int32(64)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(320), T.int32(960), T.int32(1), T.int32(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(640), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(4), T.int32(1), T.int32(4), T.int32(1), T.int32(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_3_init + nn_4_init + nn_2_ff_2_yy_2_xx_2_fused // T.int32(64))
                            v_ff = T.axis.spatial(T.int32(320), nn_0_ff_0_yy_0_xx_0_fused // T.int32(128) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(64) // T.int32(8) * T.int32(4) + ff_3_init * T.int32(4) + ff_4_init)
                            v_yy = T.axis.spatial(T.int32(64), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(128) // T.int32(8) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) // T.int32(2) + yy_3_init)
                            v_xx = T.axis.spatial(T.int32(64), xx_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(4) + xx_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(60), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(512))
                                    v1 = T.axis.spatial(T.int32(960), rc_0 * T.int32(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(512) // T.int32(32))
                                    v2 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(128) // T.int32(8) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(32) // T.int32(8))
                                    v3 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(8))
                                    T.reads(lv1501[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv1501[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(320), nn_0_ff_0_yy_0_xx_0_fused // T.int32(128) * T.int32(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(16))
                                    v1 = T.axis.spatial(T.int32(960), rc_0 * T.int32(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(16))
                                    v2 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v3 = T.axis.spatial(T.int32(1), T.int32(0))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(4), T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(4), T.int32(1), T.int32(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_3 + nn_4 + nn_2_ff_2_yy_2_xx_2_fused // T.int32(64))
                                v_ff = T.axis.spatial(T.int32(320), nn_0_ff_0_yy_0_xx_0_fused // T.int32(128) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(64) // T.int32(8) * T.int32(4) + ff_3 * T.int32(4) + ff_4)
                                v_yy = T.axis.spatial(T.int32(64), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(128) // T.int32(8) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) // T.int32(2) + yy_3)
                                v_xx = T.axis.spatial(T.int32(64), xx_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(4) + xx_3)
                                v_rc = T.axis.reduce(T.int32(960), rc_0 * T.int32(16) + rc_1 * T.int32(4) + rc_2)
                                v_ry = T.axis.reduce(T.int32(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int32(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(4), T.int32(1), T.int32(4)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_2_ff_2_yy_2_xx_2_fused // T.int32(64) + ax0)
                            v1 = T.axis.spatial(T.int32(320), nn_0_ff_0_yy_0_xx_0_fused // T.int32(128) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(64) // T.int32(8) * T.int32(4) + ax1)
                            v2 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(128) // T.int32(8) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) // T.int32(2) + ax2)
                            v3 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(4) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d32_add5_add7(lv1607: T.Buffer((T.int32(2), T.int32(640), T.int32(64), T.int32(64)), "float32"), param_0: T.Buffer((T.int32(320), T.int32(640), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(320), T.int32(1), T.int32(1)), "float32"), lv1616: T.Buffer((T.int32(2), T.int32(320), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(640), T.int32(66), T.int32(66)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(320), T.int32(640), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(160), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(2), T.int32(2), T.int32(1), T.int32(1), T.int32(2), T.int32(2), T.int32(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(256) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(320), nn_0_ff_0_yy_0_xx_0_fused % T.int32(256) // T.int32(64) * T.int32(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(4) + ff_3_init * T.int32(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) // T.int32(32) * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) * T.int32(4) + yy_3_init * T.int32(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) * T.int32(2) + xx_3_init * T.int32(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(640), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(160), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(256))
                                    v1 = T.axis.spatial(T.int32(640), rc_0)
                                    v2 = T.axis.spatial(T.int32(66), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) // T.int32(32) * T.int32(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(160) + ax0_ax1_ax2_ax3_fused_1) // T.int32(4))
                                    v3 = T.axis.spatial(T.int32(66), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) * T.int32(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(160) + ax0_ax1_ax2_ax3_fused_1) % T.int32(4))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(160) + ax0_ax1_ax2_ax3_fused_1 < T.int32(136))
                                    T.reads(lv1607[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(65) and T.int32(1) <= v3 and v3 < T.int32(65), lv1607[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(160), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(320), nn_0_ff_0_yy_0_xx_0_fused % T.int32(256) // T.int32(64) * T.int32(80) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(160) + ax0_ax1_ax2_ax3_fused_1) // T.int32(9))
                                    v1 = T.axis.spatial(T.int32(640), rc_0)
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(160) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(160) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(160) + ax0_ax1_ax2_ax3_fused_1 < T.int32(720))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(3), T.int32(3), T.int32(1), T.int32(2), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(2), T.int32(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(256) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(320), nn_0_ff_0_yy_0_xx_0_fused % T.int32(256) // T.int32(64) * T.int32(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(4) + ff_3 * T.int32(2) + ff_4)
                                v_yy = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) // T.int32(32) * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) * T.int32(4) + yy_3 * T.int32(2) + yy_4)
                                v_xx = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) * T.int32(2) + xx_3 * T.int32(2) + xx_4)
                                v_rc = T.axis.reduce(T.int32(640), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_2 + ry_0 * T.int32(3) + ry_1)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(4), T.int32(4), T.int32(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(256) + ax0)
                            v1 = T.axis.spatial(T.int32(320), nn_0_ff_0_yy_0_xx_0_fused % T.int32(256) // T.int32(64) * T.int32(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(4) + ax1)
                            v2 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) // T.int32(32) * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) * T.int32(4) + ax2)
                            v3 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) * T.int32(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)], lv1616[v0, v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)] + lv1616[v0, v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d33_add5(lv1605: T.Buffer((T.int32(2), T.int32(640), T.int32(64), T.int32(64)), "float32"), param_0: T.Buffer((T.int32(320), T.int32(640), T.int32(1), T.int32(1)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(320), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(640), T.int32(64), T.int32(64)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(320), T.int32(640), T.int32(1), T.int32(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(640), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(16), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(2), T.int32(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(320) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(320), nn_0_ff_0_yy_0_xx_0_fused % T.int32(320) // T.int32(64) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(32) * T.int32(8) + ff_3_init * T.int32(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) // T.int32(4) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(32) // T.int32(16) * T.int32(2) + yy_3_init * T.int32(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int32(64), xx_3_init + xx_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16))
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(40), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(320))
                                    v1 = T.axis.spatial(T.int32(640), rc_0 * T.int32(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(64))
                                    v2 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) // T.int32(4) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(64) // T.int32(16))
                                    v3 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(16))
                                    T.reads(lv1605[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv1605[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(320), nn_0_ff_0_yy_0_xx_0_fused % T.int32(320) // T.int32(64) * T.int32(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(16))
                                    v1 = T.axis.spatial(T.int32(640), rc_0 * T.int32(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(16))
                                    v2 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v3 = T.axis.spatial(T.int32(1), T.int32(0))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(2), T.int32(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(320) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(320), nn_0_ff_0_yy_0_xx_0_fused % T.int32(320) // T.int32(64) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(32) * T.int32(8) + ff_3 * T.int32(2) + ff_4)
                                v_yy = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) // T.int32(4) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(32) // T.int32(16) * T.int32(2) + yy_3 * T.int32(2) + yy_4)
                                v_xx = T.axis.spatial(T.int32(64), xx_3 + xx_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16))
                                v_rc = T.axis.reduce(T.int32(640), rc_0 * T.int32(16) + rc_1 * T.int32(4) + rc_2)
                                v_ry = T.axis.reduce(T.int32(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int32(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(8), T.int32(2), T.int32(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(320) + ax0)
                            v1 = T.axis.spatial(T.int32(320), nn_0_ff_0_yy_0_xx_0_fused % T.int32(320) // T.int32(64) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(32) * T.int32(8) + ax1)
                            v2 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) // T.int32(4) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(32) // T.int32(16) * T.int32(2) + ax2)
                            v3 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d34_add35(lv1814: T.Buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), "float32"), param_0: T.Buffer((T.int32(4), T.int32(320), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(4), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(4), T.int32(64), T.int32(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(4), T.int32(64), T.int32(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(320), T.int32(66), T.int32(66)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(4), T.int32(320), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(2), T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(64) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(4), ff_4_init + nn_2_ff_2_yy_2_xx_2_fused // T.int32(32) * T.int32(2) + ff_3_init)
                            v_yy = T.axis.spatial(T.int32(64), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) + yy_3_init)
                            v_xx = T.axis.spatial(T.int32(64), xx_4_init + nn_2_ff_2_yy_2_xx_2_fused % T.int32(32) * T.int32(2) + xx_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(80), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(13)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(64))
                                    v1 = T.axis.spatial(T.int32(320), rc_0 * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(198))
                                    v2 = T.axis.spatial(T.int32(66), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(198) // T.int32(66))
                                    v3 = T.axis.spatial(T.int32(66), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(66))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1 < T.int32(792))
                                    T.reads(lv1814[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(65) and T.int32(1) <= v3 and v3 < T.int32(65), lv1814[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(4), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(36))
                                    v1 = T.axis.spatial(T.int32(320), rc_0 * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(36) // T.int32(9))
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1 < T.int32(144))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(3), T.int32(3), T.int32(1), T.int32(2), T.int32(1), T.int32(2), T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(64) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(4), ff_4 + nn_2_ff_2_yy_2_xx_2_fused // T.int32(32) * T.int32(2) + ff_3)
                                v_yy = T.axis.spatial(T.int32(64), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) + yy_3)
                                v_xx = T.axis.spatial(T.int32(64), xx_4 + nn_2_ff_2_yy_2_xx_2_fused % T.int32(32) * T.int32(2) + xx_3)
                                v_rc = T.axis.reduce(T.int32(320), rc_0 * T.int32(4) + rc_1 * T.int32(4) + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_2 + ry_0 * T.int32(3) + ry_1)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(2), T.int32(1), T.int32(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(64) + ax0)
                            v1 = T.axis.spatial(T.int32(4), nn_2_ff_2_yy_2_xx_2_fused // T.int32(32) * T.int32(2) + ax1)
                            v2 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) + ax2)
                            v3 = T.axis.spatial(T.int32(64), nn_2_ff_2_yy_2_xx_2_fused % T.int32(32) * T.int32(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d35_add36(inp_0: T.Buffer((T.int32(1), T.int32(4), T.int32(64), T.int32(64)), "float32"), param_0: T.Buffer((T.int32(4), T.int32(4), T.int32(1), T.int32(1)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(4), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(1), T.int32(4), T.int32(64), T.int32(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(1), T.int32(4), T.int32(64), T.int32(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(1), T.int32(4), T.int32(64), T.int32(64)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(4), T.int32(4), T.int32(1), T.int32(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(1), T.int32(2), T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(4), nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused // T.int32(16) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) * T.int32(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int32(64), xx_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(16) * T.int32(4) + xx_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(4), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v1 = T.axis.spatial(T.int32(4), rc_0)
                                    v2 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused // T.int32(16) * T.int32(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) // T.int32(4))
                                    v3 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(16) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) % T.int32(4))
                                    T.reads(inp_0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = inp_0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(4), ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1)
                                    v1 = T.axis.spatial(T.int32(4), rc_0)
                                    v2 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v3 = T.axis.spatial(T.int32(1), T.int32(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1 < T.int32(4))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(4), nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused // T.int32(16) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) * T.int32(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int32(64), xx_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(16) * T.int32(4) + xx_3)
                                v_rc = T.axis.reduce(T.int32(4), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int32(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(1), T.int32(2), T.int32(4)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(1), ax0)
                            v1 = T.axis.spatial(T.int32(4), nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) + ax1)
                            v2 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused // T.int32(16) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) * T.int32(2) + ax2)
                            v3 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(16) * T.int32(4) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[v0, v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[v0, v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d36_add37(lv2: T.Buffer((T.int32(1), T.int32(4), T.int32(64), T.int32(64)), "float32"), param_0: T.Buffer((T.int32(512), T.int32(4), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(512), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(1), T.int32(512), T.int32(64), T.int32(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(1), T.int32(512), T.int32(64), T.int32(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(1), T.int32(4), T.int32(66), T.int32(66)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(512), T.int32(4), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(1), T.int32(4), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused // T.int32(32) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) // T.int32(8) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) // T.int32(2) * T.int32(4) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int32(64), xx_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + xx_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(4), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v1 = T.axis.spatial(T.int32(4), rc_0)
                                    v2 = T.axis.spatial(T.int32(66), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) // T.int32(8) * T.int32(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(10))
                                    v3 = T.axis.spatial(T.int32(66), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(10))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1 < T.int32(180))
                                    T.reads(lv2[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(65) and T.int32(1) <= v3 and v3 < T.int32(65), lv2[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused // T.int32(32) * T.int32(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(9))
                                    v1 = T.axis.spatial(T.int32(4), rc_0)
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1 < T.int32(144))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(4), T.int32(2), T.int32(1), T.int32(3), T.int32(3), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused // T.int32(32) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) // T.int32(8) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) // T.int32(2) * T.int32(4) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int32(64), xx_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + xx_3)
                                v_rc = T.axis.reduce(T.int32(4), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_0 * T.int32(3) + ry_1 * T.int32(3) + ry_2)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 * T.int32(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(1), T.int32(4), T.int32(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(1), ax0)
                            v1 = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused // T.int32(32) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) + ax1)
                            v2 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) // T.int32(8) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) // T.int32(2) * T.int32(4) + ax2)
                            v3 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[v0, v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[v0, v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d37_add37(lv7: T.Buffer((T.int32(1), T.int32(512), T.int32(64), T.int32(64)), "float32"), param_0: T.Buffer((T.int32(512), T.int32(512), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(512), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(1), T.int32(512), T.int32(64), T.int32(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(1), T.int32(512), T.int32(64), T.int32(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(1), T.int32(512), T.int32(66), T.int32(66)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(512), T.int32(512), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(1), T.int32(2), T.int32(2), T.int32(1), T.int32(2), T.int32(1), T.int32(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused // T.int32(64) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(2) + ff_3_init * T.int32(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) // T.int32(4) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) // T.int32(4) * T.int32(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) * T.int32(4) + xx_3_init * T.int32(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(512), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v1 = T.axis.spatial(T.int32(512), rc_0)
                                    v2 = T.axis.spatial(T.int32(66), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) // T.int32(4) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(18))
                                    v3 = T.axis.spatial(T.int32(66), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(18))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1 < T.int32(108))
                                    T.reads(lv7[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(65) and T.int32(1) <= v3 and v3 < T.int32(65), lv7[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused // T.int32(64) * T.int32(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(9))
                                    v1 = T.axis.spatial(T.int32(512), rc_0)
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(3), T.int32(3), T.int32(1), T.int32(1), T.int32(2), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(1), T.int32(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused // T.int32(64) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(2) + ff_3 * T.int32(2) + ff_4)
                                v_yy = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) // T.int32(4) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) // T.int32(4) * T.int32(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) * T.int32(4) + xx_3 * T.int32(2) + xx_4)
                                v_rc = T.axis.reduce(T.int32(512), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_2 + ry_0 * T.int32(3) + ry_1)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(2), T.int32(2), T.int32(4)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(1), ax0)
                            v1 = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused // T.int32(64) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(2) + ax1)
                            v2 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) // T.int32(4) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) // T.int32(4) * T.int32(2) + ax2)
                            v3 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) * T.int32(4) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[v0, v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[v0, v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d37_add37_add38_divide4(lv12: T.Buffer((T.int32(1), T.int32(512), T.int32(64), T.int32(64)), "float32"), param_0: T.Buffer((T.int32(512), T.int32(512), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(512), T.int32(1), T.int32(1)), "float32"), lv5: T.Buffer((T.int32(1), T.int32(512), T.int32(64), T.int32(64)), "float32"), T_divide: T.Buffer((T.int32(1), T.int32(512), T.int32(64), T.int32(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(1), T.int32(512), T.int32(64), T.int32(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(1), T.int32(512), T.int32(66), T.int32(66)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(512), T.int32(512), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(4), T.int32(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused // T.int32(64) * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(4) + ff_3_init * T.int32(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) // T.int32(16) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) // T.int32(2) * T.int32(4) + yy_3_init * T.int32(4) + yy_4_init)
                            v_xx = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(16) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + xx_3_init * T.int32(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(512), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v1 = T.axis.spatial(T.int32(512), rc_0)
                                    v2 = T.axis.spatial(T.int32(66), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) // T.int32(16) * T.int32(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(6))
                                    v3 = T.axis.spatial(T.int32(66), nn_0_ff_0_yy_0_xx_0_fused % T.int32(16) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(6))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1 < T.int32(108))
                                    T.reads(lv12[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(65) and T.int32(1) <= v3 and v3 < T.int32(65), lv12[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused // T.int32(64) * T.int32(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(9))
                                    v1 = T.axis.spatial(T.int32(512), rc_0)
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1 < T.int32(288))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(3), T.int32(3), T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(4), T.int32(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused // T.int32(64) * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(4) + ff_3 * T.int32(2) + ff_4)
                                v_yy = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) // T.int32(16) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) // T.int32(2) * T.int32(4) + yy_3 * T.int32(4) + yy_4)
                                v_xx = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(16) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + xx_3 * T.int32(2) + xx_4)
                                v_rc = T.axis.reduce(T.int32(512), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_2 + ry_0 * T.int32(3) + ry_1)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(4), T.int32(4), T.int32(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(1), ax0)
                            v1 = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused // T.int32(64) * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(4) + ax1)
                            v2 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) // T.int32(16) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) // T.int32(2) * T.int32(4) + ax2)
                            v3 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(16) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + ax3)
                            T.reads(lv5[v0, v1, v2, v3], conv2d_nchw_local[v0, v1, v2, v3], param_1[v0, v1, T.int32(0), T.int32(0)])
                            T.writes(T_divide[v0, v1, v2, v3])
                            T_divide[v0, v1, v2, v3] = lv5[v0, v1, v2, v3] + (conv2d_nchw_local[v0, v1, v2, v3] + param_1[v0, v1, T.int32(0), T.int32(0)])

    @T.prim_func
    def fused_conv2d38_add40(lv104: T.Buffer((T.int32(1), T.int32(512), T.int32(128), T.int32(128)), "float32"), param_0: T.Buffer((T.int32(512), T.int32(512), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(512), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(1), T.int32(512), T.int32(128), T.int32(128)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(1), T.int32(512), T.int32(128), T.int32(128)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(1), T.int32(512), T.int32(130), T.int32(130)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(512), T.int32(512), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(2048), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(8), T.int32(1), T.int32(2), T.int32(1), T.int32(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused // T.int32(256) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int32(2) * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(2) + ff_3_init * T.int32(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int32(128), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(256) // T.int32(8) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) + yy_3_init)
                            v_xx = T.axis.spatial(T.int32(128), xx_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int32(2) * T.int32(8) + xx_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(512), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v1 = T.axis.spatial(T.int32(512), rc_0)
                                    v2 = T.axis.spatial(T.int32(130), nn_0_ff_0_yy_0_xx_0_fused % T.int32(256) // T.int32(8) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(18))
                                    v3 = T.axis.spatial(T.int32(130), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(18))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1 < T.int32(108))
                                    T.reads(lv104[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(129) and T.int32(1) <= v3 and v3 < T.int32(129), lv104[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused // T.int32(256) * T.int32(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(9))
                                    v1 = T.axis.spatial(T.int32(512), rc_0)
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(8), T.int32(1), T.int32(1), T.int32(3), T.int32(1), T.int32(2), T.int32(1), T.int32(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused // T.int32(256) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int32(2) * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(2) + ff_3 * T.int32(2) + ff_4)
                                v_yy = T.axis.spatial(T.int32(128), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(256) // T.int32(8) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) + yy_3)
                                v_xx = T.axis.spatial(T.int32(128), xx_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int32(2) * T.int32(8) + xx_3)
                                v_rc = T.axis.reduce(T.int32(512), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_2 + ry_0 * T.int32(3) + ry_1)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 * T.int32(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(2), T.int32(1), T.int32(8)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(1), ax0)
                            v1 = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused // T.int32(256) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int32(2) * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(2) + ax1)
                            v2 = T.axis.spatial(T.int32(128), nn_0_ff_0_yy_0_xx_0_fused % T.int32(256) // T.int32(8) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) + ax2)
                            v3 = T.axis.spatial(T.int32(128), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int32(2) * T.int32(8) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[v0, v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[v0, v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d38_add40_add41_divide5(lv114: T.Buffer((T.int32(1), T.int32(512), T.int32(128), T.int32(128)), "float32"), param_0: T.Buffer((T.int32(512), T.int32(512), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(512), T.int32(1), T.int32(1)), "float32"), lv107: T.Buffer((T.int32(1), T.int32(512), T.int32(128), T.int32(128)), "float32"), T_divide: T.Buffer((T.int32(1), T.int32(512), T.int32(128), T.int32(128)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(1), T.int32(512), T.int32(128), T.int32(128)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(1), T.int32(512), T.int32(130), T.int32(130)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(512), T.int32(512), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(4096), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(2), T.int32(4), T.int32(1), T.int32(1), T.int32(2), T.int32(1), T.int32(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused // T.int32(512) * T.int32(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(4) + ff_3_init * T.int32(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int32(128), nn_0_ff_0_yy_0_xx_0_fused % T.int32(512) // T.int32(32) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) // T.int32(2) * T.int32(4) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int32(128), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + xx_3_init * T.int32(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(512), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v1 = T.axis.spatial(T.int32(512), rc_0)
                                    v2 = T.axis.spatial(T.int32(130), nn_0_ff_0_yy_0_xx_0_fused % T.int32(512) // T.int32(32) * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(6))
                                    v3 = T.axis.spatial(T.int32(130), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(6))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1 < T.int32(60))
                                    T.reads(lv114[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(129) and T.int32(1) <= v3 and v3 < T.int32(129), lv114[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused // T.int32(512) * T.int32(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(9))
                                    v1 = T.axis.spatial(T.int32(512), rc_0)
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(3), T.int32(3), T.int32(1), T.int32(2), T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(1), T.int32(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused // T.int32(512) * T.int32(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(4) + ff_3 * T.int32(2) + ff_4)
                                v_yy = T.axis.spatial(T.int32(128), nn_0_ff_0_yy_0_xx_0_fused % T.int32(512) // T.int32(32) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) // T.int32(2) * T.int32(4) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int32(128), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + xx_3 * T.int32(2) + xx_4)
                                v_rc = T.axis.reduce(T.int32(512), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_2 + ry_0 * T.int32(3) + ry_1)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(4), T.int32(4), T.int32(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(1), ax0)
                            v1 = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused // T.int32(512) * T.int32(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(4) + ax1)
                            v2 = T.axis.spatial(T.int32(128), nn_0_ff_0_yy_0_xx_0_fused % T.int32(512) // T.int32(32) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) // T.int32(2) * T.int32(4) + ax2)
                            v3 = T.axis.spatial(T.int32(128), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + ax3)
                            T.reads(lv107[v0, v1, v2, v3], conv2d_nchw_local[v0, v1, v2, v3], param_1[v0, v1, T.int32(0), T.int32(0)])
                            T.writes(T_divide[v0, v1, v2, v3])
                            T_divide[v0, v1, v2, v3] = lv107[v0, v1, v2, v3] + (conv2d_nchw_local[v0, v1, v2, v3] + param_1[v0, v1, T.int32(0), T.int32(0)])

    @T.prim_func
    def fused_conv2d39_add42(lv144: T.Buffer((T.int32(1), T.int32(512), T.int32(256), T.int32(256)), "float32"), param_0: T.Buffer((T.int32(512), T.int32(512), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(512), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(1), T.int32(512), T.int32(256), T.int32(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(1), T.int32(512), T.int32(256), T.int32(256)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(1), T.int32(512), T.int32(258), T.int32(258)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(512), T.int32(512), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(8192), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(4), T.int32(1), T.int32(2), T.int32(1), T.int32(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused // T.int32(1024) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int32(2) * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(2) + ff_3_init * T.int32(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int32(256), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(1024) // T.int32(16) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) + yy_3_init)
                            v_xx = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused % T.int32(16) * T.int32(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int32(2) * T.int32(8) + xx_3_init * T.int32(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(512), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v1 = T.axis.spatial(T.int32(512), rc_0)
                                    v2 = T.axis.spatial(T.int32(258), nn_0_ff_0_yy_0_xx_0_fused % T.int32(1024) // T.int32(16) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(18))
                                    v3 = T.axis.spatial(T.int32(258), nn_0_ff_0_yy_0_xx_0_fused % T.int32(16) * T.int32(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(18))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1 < T.int32(108))
                                    T.reads(lv144[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(257) and T.int32(1) <= v3 and v3 < T.int32(257), lv144[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused // T.int32(1024) * T.int32(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(9))
                                    v1 = T.axis.spatial(T.int32(512), rc_0)
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(3), T.int32(1), T.int32(2), T.int32(1), T.int32(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused // T.int32(1024) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int32(2) * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(2) + ff_3 * T.int32(2) + ff_4)
                                v_yy = T.axis.spatial(T.int32(256), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(1024) // T.int32(16) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) + yy_3)
                                v_xx = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused % T.int32(16) * T.int32(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int32(2) * T.int32(8) + xx_3 * T.int32(2) + xx_4)
                                v_rc = T.axis.reduce(T.int32(512), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_2 + ry_0 * T.int32(3) + ry_1)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 * T.int32(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(2), T.int32(1), T.int32(8)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(1), ax0)
                            v1 = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused // T.int32(1024) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int32(2) * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(2) + ax1)
                            v2 = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused % T.int32(1024) // T.int32(16) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) + ax2)
                            v3 = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused % T.int32(16) * T.int32(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int32(2) * T.int32(8) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[v0, v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[v0, v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d3_add12(lv223: T.Buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), "float32"), param_0: T.Buffer((T.int32(320), T.int32(320), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(320), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(320), T.int32(32), T.int32(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(320), T.int32(32), T.int32(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(320), T.int32(66), T.int32(66)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(320), T.int32(320), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(160), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(8), T.int32(1), T.int32(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(128) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(320), nn_0_ff_0_yy_0_xx_0_fused % T.int32(128) // T.int32(32) * T.int32(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(8) + ff_3_init * T.int32(8) + ff_4_init)
                            v_yy = T.axis.spatial(T.int32(32), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) // T.int32(4) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) // T.int32(4) + yy_3_init)
                            v_xx = T.axis.spatial(T.int32(32), xx_3_init + xx_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4))
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(320), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(160), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(128))
                                    v1 = T.axis.spatial(T.int32(320), rc_0)
                                    v2 = T.axis.spatial(T.int32(66), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) // T.int32(4) * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(160) + ax0_ax1_ax2_ax3_fused_1) // T.int32(17))
                                    v3 = T.axis.spatial(T.int32(66), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(160) + ax0_ax1_ax2_ax3_fused_1) % T.int32(17))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(160) + ax0_ax1_ax2_ax3_fused_1 < T.int32(153))
                                    T.reads(lv223[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(65) and T.int32(1) <= v3 and v3 < T.int32(65), lv223[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(160), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(320), nn_0_ff_0_yy_0_xx_0_fused % T.int32(128) // T.int32(32) * T.int32(80) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(160) + ax0_ax1_ax2_ax3_fused_1) // T.int32(9))
                                    v1 = T.axis.spatial(T.int32(320), rc_0)
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(160) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(160) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(160) + ax0_ax1_ax2_ax3_fused_1 < T.int32(720))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(3), T.int32(3), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(8), T.int32(1), T.int32(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(128) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(320), nn_0_ff_0_yy_0_xx_0_fused % T.int32(128) // T.int32(32) * T.int32(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(8) + ff_3 * T.int32(8) + ff_4)
                                v_yy = T.axis.spatial(T.int32(32), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) // T.int32(4) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) // T.int32(4) + yy_3)
                                v_xx = T.axis.spatial(T.int32(32), xx_3 + xx_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4))
                                v_rc = T.axis.reduce(T.int32(320), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_2 + ry_0 * T.int32(3) + ry_1)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy * T.int32(2) + v_ry, v_xx * T.int32(2) + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy * T.int32(2) + v_ry, v_xx * T.int32(2) + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(8), T.int32(1), T.int32(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(128) + ax0)
                            v1 = T.axis.spatial(T.int32(320), nn_0_ff_0_yy_0_xx_0_fused % T.int32(128) // T.int32(32) * T.int32(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(8) + ax1)
                            v2 = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) // T.int32(4) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) // T.int32(4) + ax2)
                            v3 = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d40_add43(lv149: T.Buffer((T.int32(1), T.int32(512), T.int32(256), T.int32(256)), "float32"), param_0: T.Buffer((T.int32(256), T.int32(512), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(256), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(1), T.int32(256), T.int32(256), T.int32(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(1), T.int32(256), T.int32(256), T.int32(256)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(1), T.int32(512), T.int32(258), T.int32(258)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(256), T.int32(512), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(8192), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(2), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(256), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused // T.int32(2048) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(2) + ff_3_init)
                            v_yy = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2048) // T.int32(64) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) // T.int32(2) * T.int32(4) + yy_3_init * T.int32(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + xx_3_init * T.int32(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(512), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v1 = T.axis.spatial(T.int32(512), rc_0)
                                    v2 = T.axis.spatial(T.int32(258), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2048) // T.int32(64) * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(6))
                                    v3 = T.axis.spatial(T.int32(258), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(6))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1 < T.int32(60))
                                    T.reads(lv149[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(257) and T.int32(1) <= v3 and v3 < T.int32(257), lv149[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused // T.int32(2048) * T.int32(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(9))
                                    v1 = T.axis.spatial(T.int32(512), rc_0)
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(2), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(2), T.int32(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(256), ff_4 + nn_0_ff_0_yy_0_xx_0_fused // T.int32(2048) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(2) + ff_3)
                                v_yy = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2048) // T.int32(64) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) // T.int32(2) * T.int32(4) + yy_3 * T.int32(2) + yy_4)
                                v_xx = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + xx_3 * T.int32(2) + xx_4)
                                v_rc = T.axis.reduce(T.int32(512), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_2 + ry_0 * T.int32(3) + ry_1)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 * T.int32(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(2), T.int32(4), T.int32(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(1), ax0)
                            v1 = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused // T.int32(2048) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(2) + ax1)
                            v2 = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2048) // T.int32(64) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) // T.int32(2) * T.int32(4) + ax2)
                            v3 = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[v0, v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[v0, v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d41_add43(lv164: T.Buffer((T.int32(1), T.int32(256), T.int32(256), T.int32(256)), "float32"), param_0: T.Buffer((T.int32(256), T.int32(256), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(256), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(1), T.int32(256), T.int32(256), T.int32(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(1), T.int32(256), T.int32(256), T.int32(256)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(1), T.int32(256), T.int32(258), T.int32(258)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(256), T.int32(256), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(8192), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(2), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(256), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused // T.int32(2048) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(2) + ff_3_init)
                            v_yy = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2048) // T.int32(64) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) // T.int32(2) * T.int32(4) + yy_3_init * T.int32(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + xx_3_init * T.int32(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(256), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v1 = T.axis.spatial(T.int32(256), rc_0)
                                    v2 = T.axis.spatial(T.int32(258), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2048) // T.int32(64) * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(6))
                                    v3 = T.axis.spatial(T.int32(258), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(6))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1 < T.int32(60))
                                    T.reads(lv164[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(257) and T.int32(1) <= v3 and v3 < T.int32(257), lv164[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused // T.int32(2048) * T.int32(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(9))
                                    v1 = T.axis.spatial(T.int32(256), rc_0)
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(2), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(2), T.int32(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(256), ff_4 + nn_0_ff_0_yy_0_xx_0_fused // T.int32(2048) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(2) + ff_3)
                                v_yy = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2048) // T.int32(64) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) // T.int32(2) * T.int32(4) + yy_3 * T.int32(2) + yy_4)
                                v_xx = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + xx_3 * T.int32(2) + xx_4)
                                v_rc = T.axis.reduce(T.int32(256), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_2 + ry_0 * T.int32(3) + ry_1)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 * T.int32(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(2), T.int32(4), T.int32(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(1), ax0)
                            v1 = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused // T.int32(2048) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(2) + ax1)
                            v2 = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2048) // T.int32(64) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) // T.int32(2) * T.int32(4) + ax2)
                            v3 = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[v0, v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[v0, v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d41_add43_add44_divide6(lv154: T.Buffer((T.int32(1), T.int32(256), T.int32(256), T.int32(256)), "float32"), param_0: T.Buffer((T.int32(256), T.int32(256), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(256), T.int32(1), T.int32(1)), "float32"), lv160: T.Buffer((T.int32(1), T.int32(256), T.int32(256), T.int32(256)), "float32"), T_divide: T.Buffer((T.int32(1), T.int32(256), T.int32(256), T.int32(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(1), T.int32(256), T.int32(256), T.int32(256)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(1), T.int32(256), T.int32(258), T.int32(258)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(256), T.int32(256), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(8192), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(2), T.int32(2), T.int32(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused // T.int32(2048) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(2) + ff_3_init * T.int32(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2048) // T.int32(64) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) // T.int32(2) * T.int32(4) + yy_3_init * T.int32(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + xx_3_init * T.int32(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(256), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v1 = T.axis.spatial(T.int32(256), rc_0)
                                    v2 = T.axis.spatial(T.int32(258), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2048) // T.int32(64) * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(6))
                                    v3 = T.axis.spatial(T.int32(258), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(6))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1 < T.int32(60))
                                    T.reads(lv154[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(257) and T.int32(1) <= v3 and v3 < T.int32(257), lv154[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused // T.int32(2048) * T.int32(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(9))
                                    v1 = T.axis.spatial(T.int32(256), rc_0)
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(3), T.int32(3), T.int32(1), T.int32(2), T.int32(2), T.int32(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused // T.int32(2048) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(2) + ff_3 * T.int32(2) + ff_4)
                                v_yy = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2048) // T.int32(64) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) // T.int32(2) * T.int32(4) + yy_3 * T.int32(2) + yy_4)
                                v_xx = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + xx_3 * T.int32(2) + xx_4)
                                v_rc = T.axis.reduce(T.int32(256), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_0 * T.int32(3) + ry_1 * T.int32(3) + ry_2)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 * T.int32(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(2), T.int32(4), T.int32(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(1), ax0)
                            v1 = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused // T.int32(2048) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(2) + ax1)
                            v2 = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2048) // T.int32(64) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) // T.int32(2) * T.int32(4) + ax2)
                            v3 = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + ax3)
                            T.reads(lv160[v0, v1, v2, v3], conv2d_nchw_local[v0, v1, v2, v3], param_1[v0, v1, T.int32(0), T.int32(0)])
                            T.writes(T_divide[v0, v1, v2, v3])
                            T_divide[v0, v1, v2, v3] = lv160[v0, v1, v2, v3] + (conv2d_nchw_local[v0, v1, v2, v3] + param_1[v0, v1, T.int32(0), T.int32(0)])

    @T.prim_func
    def fused_conv2d42_add43(lv147: T.Buffer((T.int32(1), T.int32(512), T.int32(256), T.int32(256)), "float32"), param_0: T.Buffer((T.int32(256), T.int32(512), T.int32(1), T.int32(1)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(256), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(1), T.int32(256), T.int32(256), T.int32(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(1), T.int32(256), T.int32(256), T.int32(256)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(1), T.int32(512), T.int32(256), T.int32(256)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(256), T.int32(512), T.int32(1), T.int32(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(4096), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(16), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(8), T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(256), ff_4_init + nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(8) + ff_3_init)
                            v_yy = T.axis.spatial(T.int32(256), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused // T.int32(32) * T.int32(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) // T.int32(2) + yy_3_init)
                            v_xx = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(4) + xx_3_init * T.int32(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(64), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v1 = T.axis.spatial(T.int32(512), rc_0 * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(16))
                                    v2 = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused // T.int32(32) * T.int32(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(16) // T.int32(8))
                                    v3 = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(8))
                                    T.reads(lv147[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv147[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(256), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(8))
                                    v1 = T.axis.spatial(T.int32(512), rc_0 * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(8))
                                    v2 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v3 = T.axis.spatial(T.int32(1), T.int32(0))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(8), T.int32(1), T.int32(2), T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(256), ff_4 + nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(8) + ff_3)
                                v_yy = T.axis.spatial(T.int32(256), yy_4 + nn_0_ff_0_yy_0_xx_0_fused // T.int32(32) * T.int32(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) // T.int32(2) + yy_3)
                                v_xx = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(4) + xx_3 * T.int32(2) + xx_4)
                                v_rc = T.axis.reduce(T.int32(512), rc_0 * T.int32(8) + rc_1 * T.int32(4) + rc_2)
                                v_ry = T.axis.reduce(T.int32(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int32(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(8), T.int32(1), T.int32(4)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(1), ax0)
                            v1 = T.axis.spatial(T.int32(256), nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(8) + ax1)
                            v2 = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused // T.int32(32) * T.int32(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) // T.int32(2) + ax2)
                            v3 = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(4) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[v0, v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[v0, v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d43_add45(lv187: T.Buffer((T.int32(1), T.int32(256), T.int32(512), T.int32(512)), "float32"), param_0: T.Buffer((T.int32(256), T.int32(256), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(256), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(1), T.int32(256), T.int32(512), T.int32(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(1), T.int32(256), T.int32(512), T.int32(512)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(1), T.int32(256), T.int32(514), T.int32(514)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(256), T.int32(256), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(32768), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(2), T.int32(1), T.int32(8), T.int32(1), T.int32(1), T.int32(2), T.int32(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(256), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused // T.int32(8192) * T.int32(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(2) * T.int32(2) + ff_3_init)
                            v_yy = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8192) // T.int32(64) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + yy_3_init * T.int32(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int32(512), xx_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) * T.int32(8) + xx_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(256), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v1 = T.axis.spatial(T.int32(256), rc_0)
                                    v2 = T.axis.spatial(T.int32(514), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8192) // T.int32(64) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(10))
                                    v3 = T.axis.spatial(T.int32(514), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(10))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1 < T.int32(60))
                                    T.reads(lv187[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(513) and T.int32(1) <= v3 and v3 < T.int32(513), lv187[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused // T.int32(8192) * T.int32(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(9))
                                    v1 = T.axis.spatial(T.int32(256), rc_0)
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(2), T.int32(1), T.int32(8), T.int32(1), T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(2), T.int32(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(256), ff_4 + nn_0_ff_0_yy_0_xx_0_fused // T.int32(8192) * T.int32(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(2) * T.int32(2) + ff_3)
                                v_yy = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8192) // T.int32(64) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + yy_3 * T.int32(2) + yy_4)
                                v_xx = T.axis.spatial(T.int32(512), xx_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) * T.int32(8) + xx_3)
                                v_rc = T.axis.reduce(T.int32(256), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_2 + ry_0 * T.int32(3) + ry_1)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 * T.int32(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(2), T.int32(2), T.int32(8)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(1), ax0)
                            v1 = T.axis.spatial(T.int32(256), nn_0_ff_0_yy_0_xx_0_fused // T.int32(8192) * T.int32(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(2) * T.int32(2) + ax1)
                            v2 = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8192) // T.int32(64) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + ax2)
                            v3 = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused % T.int32(64) * T.int32(8) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[v0, v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[v0, v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d44_add46(lv192: T.Buffer((T.int32(1), T.int32(256), T.int32(512), T.int32(512)), "float32"), param_0: T.Buffer((T.int32(128), T.int32(256), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(128), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(1), T.int32(128), T.int32(512), T.int32(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(1), T.int32(128), T.int32(512), T.int32(512)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(1), T.int32(256), T.int32(514), T.int32(514)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(128), T.int32(256), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(4096), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(8), T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(2), T.int32(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(128), ff_4_init + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(8) + ff_3_init)
                            v_yy = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused // T.int32(32) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) // T.int32(4) * T.int32(2) + yy_3_init * T.int32(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) * T.int32(4) + xx_3_init * T.int32(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(256), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v1 = T.axis.spatial(T.int32(256), rc_0)
                                    v2 = T.axis.spatial(T.int32(514), nn_0_ff_0_yy_0_xx_0_fused // T.int32(32) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(18))
                                    v3 = T.axis.spatial(T.int32(514), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) * T.int32(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(18))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1 < T.int32(108))
                                    T.reads(lv192[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(513) and T.int32(1) <= v3 and v3 < T.int32(513), lv192[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(128), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(9))
                                    v1 = T.axis.spatial(T.int32(256), rc_0)
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(8), T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(2), T.int32(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(128), ff_4 + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(8) + ff_3)
                                v_yy = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused // T.int32(32) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) // T.int32(4) * T.int32(2) + yy_3 * T.int32(2) + yy_4)
                                v_xx = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) * T.int32(4) + xx_3 * T.int32(2) + xx_4)
                                v_rc = T.axis.reduce(T.int32(256), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_2 + ry_0 * T.int32(3) + ry_1)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 * T.int32(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(8), T.int32(2), T.int32(4)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(1), ax0)
                            v1 = T.axis.spatial(T.int32(128), nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(8) + ax1)
                            v2 = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused // T.int32(32) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) // T.int32(4) * T.int32(2) + ax2)
                            v3 = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) * T.int32(4) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[v0, v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[v0, v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d45_add46(lv207: T.Buffer((T.int32(1), T.int32(128), T.int32(512), T.int32(512)), "float32"), param_0: T.Buffer((T.int32(128), T.int32(128), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(128), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(1), T.int32(128), T.int32(512), T.int32(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(1), T.int32(128), T.int32(512), T.int32(512)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(1), T.int32(128), T.int32(514), T.int32(514)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(128), T.int32(128), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(8192), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(8)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(128), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused // T.int32(2048) * T.int32(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(2) + ff_3_init)
                            v_yy = T.axis.spatial(T.int32(512), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(2048) // T.int32(32) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) // T.int32(2) + yy_3_init)
                            v_xx = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(8) + xx_3_init * T.int32(8) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(128), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v1 = T.axis.spatial(T.int32(128), rc_0)
                                    v2 = T.axis.spatial(T.int32(514), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2048) // T.int32(32) * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(18))
                                    v3 = T.axis.spatial(T.int32(514), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) * T.int32(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(18))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1 < T.int32(180))
                                    T.reads(lv207[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(513) and T.int32(1) <= v3 and v3 < T.int32(513), lv207[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(128), nn_0_ff_0_yy_0_xx_0_fused // T.int32(2048) * T.int32(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(9))
                                    v1 = T.axis.spatial(T.int32(128), rc_0)
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1 < T.int32(288))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(1), T.int32(8)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(128), ff_4 + nn_0_ff_0_yy_0_xx_0_fused // T.int32(2048) * T.int32(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(2) + ff_3)
                                v_yy = T.axis.spatial(T.int32(512), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(2048) // T.int32(32) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) // T.int32(2) + yy_3)
                                v_xx = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(8) + xx_3 * T.int32(8) + xx_4)
                                v_rc = T.axis.reduce(T.int32(128), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_2 + ry_0 * T.int32(3) + ry_1)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 * T.int32(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(2), T.int32(1), T.int32(8)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(1), ax0)
                            v1 = T.axis.spatial(T.int32(128), nn_0_ff_0_yy_0_xx_0_fused // T.int32(2048) * T.int32(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(2) + ax1)
                            v2 = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2048) // T.int32(32) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) // T.int32(2) + ax2)
                            v3 = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(8) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[v0, v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[v0, v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d45_add46_add47_divide7(lv197: T.Buffer((T.int32(1), T.int32(128), T.int32(512), T.int32(512)), "float32"), param_0: T.Buffer((T.int32(128), T.int32(128), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(128), T.int32(1), T.int32(1)), "float32"), lv203: T.Buffer((T.int32(1), T.int32(128), T.int32(512), T.int32(512)), "float32"), T_divide: T.Buffer((T.int32(1), T.int32(128), T.int32(512), T.int32(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(1), T.int32(128), T.int32(512), T.int32(512)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(1), T.int32(128), T.int32(514), T.int32(514)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(128), T.int32(128), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(8192), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(2), T.int32(4), T.int32(1), T.int32(1), T.int32(2), T.int32(1), T.int32(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(128), nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(4) + ff_3_init * T.int32(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused // T.int32(128) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) // T.int32(2) * T.int32(4) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused % T.int32(128) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + xx_3_init * T.int32(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(128), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v1 = T.axis.spatial(T.int32(128), rc_0)
                                    v2 = T.axis.spatial(T.int32(514), nn_0_ff_0_yy_0_xx_0_fused // T.int32(128) * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(6))
                                    v3 = T.axis.spatial(T.int32(514), nn_0_ff_0_yy_0_xx_0_fused % T.int32(128) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(6))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1 < T.int32(60))
                                    T.reads(lv197[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(513) and T.int32(1) <= v3 and v3 < T.int32(513), lv197[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(128), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(9))
                                    v1 = T.axis.spatial(T.int32(128), rc_0)
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(4), T.int32(1), T.int32(1), T.int32(3), T.int32(3), T.int32(1), T.int32(2), T.int32(1), T.int32(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(128), nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(4) + ff_3 * T.int32(2) + ff_4)
                                v_yy = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused // T.int32(128) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) // T.int32(2) * T.int32(4) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused % T.int32(128) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + xx_3 * T.int32(2) + xx_4)
                                v_rc = T.axis.reduce(T.int32(128), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_0 * T.int32(3) + ry_1 * T.int32(3) + ry_2)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 * T.int32(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(4), T.int32(4), T.int32(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(1), ax0)
                            v1 = T.axis.spatial(T.int32(128), nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(4) + ax1)
                            v2 = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused // T.int32(128) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) // T.int32(2) * T.int32(4) + ax2)
                            v3 = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused % T.int32(128) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + ax3)
                            T.reads(lv203[v0, v1, v2, v3], conv2d_nchw_local[v0, v1, v2, v3], param_1[v0, v1, T.int32(0), T.int32(0)])
                            T.writes(T_divide[v0, v1, v2, v3])
                            T_divide[v0, v1, v2, v3] = lv203[v0, v1, v2, v3] + (conv2d_nchw_local[v0, v1, v2, v3] + param_1[v0, v1, T.int32(0), T.int32(0)])

    @T.prim_func
    def fused_conv2d46_add46(lv190: T.Buffer((T.int32(1), T.int32(256), T.int32(512), T.int32(512)), "float32"), param_0: T.Buffer((T.int32(128), T.int32(256), T.int32(1), T.int32(1)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(128), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(1), T.int32(128), T.int32(512), T.int32(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(1), T.int32(128), T.int32(512), T.int32(512)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(1), T.int32(256), T.int32(512), T.int32(512)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(128), T.int32(256), T.int32(1), T.int32(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(8192), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(8), T.int32(1), T.int32(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(128), nn_0_ff_0_yy_0_xx_0_fused // T.int32(4096) * T.int32(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(8) + ff_3_init * T.int32(8) + ff_4_init)
                            v_yy = T.axis.spatial(T.int32(512), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(4096) // T.int32(128) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) + yy_3_init)
                            v_xx = T.axis.spatial(T.int32(512), xx_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(128) * T.int32(4) + nn_1_ff_1_yy_1_xx_1_fused + xx_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(64), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v1 = T.axis.spatial(T.int32(256), rc_0 * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(64))
                                    v2 = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4096) // T.int32(128) * T.int32(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(64) // T.int32(4))
                                    v3 = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused % T.int32(128) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(4))
                                    T.reads(lv190[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv190[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(128), nn_0_ff_0_yy_0_xx_0_fused // T.int32(4096) * T.int32(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(4))
                                    v1 = T.axis.spatial(T.int32(256), rc_0 * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(4))
                                    v2 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v3 = T.axis.spatial(T.int32(1), T.int32(0))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(8), T.int32(1), T.int32(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(128), nn_0_ff_0_yy_0_xx_0_fused // T.int32(4096) * T.int32(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(8) + ff_3 * T.int32(8) + ff_4)
                                v_yy = T.axis.spatial(T.int32(512), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(4096) // T.int32(128) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) + yy_3)
                                v_xx = T.axis.spatial(T.int32(512), xx_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(128) * T.int32(4) + nn_1_ff_1_yy_1_xx_1_fused + xx_3)
                                v_rc = T.axis.reduce(T.int32(256), rc_0 * T.int32(4) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int32(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(8), T.int32(1), T.int32(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(1), ax0)
                            v1 = T.axis.spatial(T.int32(128), nn_0_ff_0_yy_0_xx_0_fused // T.int32(4096) * T.int32(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(8) + ax1)
                            v2 = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4096) // T.int32(128) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) + ax2)
                            v3 = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused % T.int32(128) * T.int32(4) + nn_1_ff_1_yy_1_xx_1_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[v0, v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[v0, v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d47_add48(lv231: T.Buffer((T.int32(1), T.int32(128), T.int32(512), T.int32(512)), "float32"), param_0: T.Buffer((T.int32(3), T.int32(128), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(3), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(1), T.int32(3), T.int32(512), T.int32(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(1), T.int32(3), T.int32(512), T.int32(512)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(1), T.int32(128), T.int32(514), T.int32(514)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(3), T.int32(128), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(16), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(3), T.int32(2), T.int32(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(3), ff_3_init * T.int32(3) + ff_4_init)
                            v_yy = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused // T.int32(4) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(128) * T.int32(2) + yy_3_init * T.int32(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int32(512), xx_3_init + xx_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(128) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(128))
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(128), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v1 = T.axis.spatial(T.int32(128), rc_0)
                                    v2 = T.axis.spatial(T.int32(514), nn_0_ff_0_yy_0_xx_0_fused // T.int32(4) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) // T.int32(130))
                                    v3 = T.axis.spatial(T.int32(514), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) % T.int32(130))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1 < T.int32(780))
                                    T.reads(lv231[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(513) and T.int32(1) <= v3 and v3 < T.int32(513), lv231[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) // T.int32(9))
                                    v1 = T.axis.spatial(T.int32(128), rc_0)
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1 < T.int32(27))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(3), T.int32(2), T.int32(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(3), ff_3 * T.int32(3) + ff_4)
                                v_yy = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused // T.int32(4) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(128) * T.int32(2) + yy_3 * T.int32(2) + yy_4)
                                v_xx = T.axis.spatial(T.int32(512), xx_3 + xx_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(128) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(128))
                                v_rc = T.axis.reduce(T.int32(128), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_0 * T.int32(3) + ry_1 * T.int32(3) + ry_2)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(3), T.int32(2), T.int32(1)):
                        with T.block("conv2d_nchw_local"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused // T.int32(4) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(128) * T.int32(2) + ax2)
                            v3 = T.axis.spatial(T.int32(512), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(128) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(128) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[v0, v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[v0, v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d4_add13_add15(lv228: T.Buffer((T.int32(2), T.int32(320), T.int32(32), T.int32(32)), "float32"), param_0: T.Buffer((T.int32(640), T.int32(320), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(640), T.int32(1), T.int32(1)), "float32"), lv237: T.Buffer((T.int32(2), T.int32(640), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(320), T.int32(34), T.int32(34)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(640), T.int32(320), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(2), T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(160) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(640), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(160) // T.int32(16) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(2) + ff_3_init)
                            v_yy = T.axis.spatial(T.int32(32), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(16) // T.int32(2) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) // T.int32(4) + yy_3_init)
                            v_xx = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) * T.int32(4) + xx_3_init * T.int32(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(320), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(160))
                                    v1 = T.axis.spatial(T.int32(320), rc_0)
                                    v2 = T.axis.spatial(T.int32(34), nn_0_ff_0_yy_0_xx_0_fused % T.int32(16) // T.int32(2) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(18))
                                    v3 = T.axis.spatial(T.int32(34), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(18))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1 < T.int32(108))
                                    T.reads(lv228[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(33) and T.int32(1) <= v3 and v3 < T.int32(33), lv228[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused % T.int32(160) // T.int32(16) * T.int32(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(9))
                                    v1 = T.axis.spatial(T.int32(320), rc_0)
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1 < T.int32(576))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(2), T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(1), T.int32(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(160) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(640), ff_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(160) // T.int32(16) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(2) + ff_3)
                                v_yy = T.axis.spatial(T.int32(32), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(16) // T.int32(2) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) // T.int32(4) + yy_3)
                                v_xx = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) * T.int32(4) + xx_3 * T.int32(2) + xx_4)
                                v_rc = T.axis.reduce(T.int32(320), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_2 + ry_0 * T.int32(3) + ry_1)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 * T.int32(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(2), T.int32(1), T.int32(4)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(160) + ax0)
                            v1 = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused % T.int32(160) // T.int32(16) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(2) + ax1)
                            v2 = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(16) // T.int32(2) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) // T.int32(4) + ax2)
                            v3 = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) * T.int32(4) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)], lv237[v0, v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)] + lv237[v0, v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d5_add13_add15(lv331: T.Buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), "float32"), param_0: T.Buffer((T.int32(640), T.int32(640), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(640), T.int32(1), T.int32(1)), "float32"), lv340: T.Buffer((T.int32(2), T.int32(640), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(640), T.int32(34), T.int32(34)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(640), T.int32(640), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(5), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(4), T.int32(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(128) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(640), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(128) // T.int32(8) * T.int32(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(5) + ff_3_init)
                            v_yy = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(4) + yy_3_init * T.int32(4) + yy_4_init)
                            v_xx = T.axis.spatial(T.int32(32), nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) * T.int32(2) + xx_3_init * T.int32(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(640), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(128))
                                    v1 = T.axis.spatial(T.int32(640), rc_0)
                                    v2 = T.axis.spatial(T.int32(34), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(34))
                                    v3 = T.axis.spatial(T.int32(34), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(34))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1 < T.int32(204))
                                    T.reads(lv331[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(33) and T.int32(1) <= v3 and v3 < T.int32(33), lv331[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused % T.int32(128) // T.int32(8) * T.int32(40) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(9))
                                    v1 = T.axis.spatial(T.int32(640), rc_0)
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1 < T.int32(360))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(5), T.int32(1), T.int32(1), T.int32(1), T.int32(3), T.int32(3), T.int32(1), T.int32(1), T.int32(4), T.int32(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(128) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(640), ff_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(128) // T.int32(8) * T.int32(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(5) + ff_3)
                                v_yy = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(4) + yy_3 * T.int32(4) + yy_4)
                                v_xx = T.axis.spatial(T.int32(32), nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) * T.int32(2) + xx_3 * T.int32(2) + xx_4)
                                v_rc = T.axis.reduce(T.int32(640), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_0 * T.int32(3) + ry_1 * T.int32(3) + ry_2)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 * T.int32(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(5), T.int32(4), T.int32(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(128) + ax0)
                            v1 = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused % T.int32(128) // T.int32(8) * T.int32(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(5) + ax1)
                            v2 = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(4) + ax2)
                            v3 = T.axis.spatial(T.int32(32), nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) * T.int32(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)], lv340[v0, v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)] + lv340[v0, v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d5_add13_add16_divide1(lv240: T.Buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), "float32"), param_0: T.Buffer((T.int32(640), T.int32(640), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(640), T.int32(1), T.int32(1)), "float32"), lv246: T.Buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), "float32"), T_divide: T.Buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(640), T.int32(34), T.int32(34)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(640), T.int32(640), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(640), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(4), T.int32(2), T.int32(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(320) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused % T.int32(320) // T.int32(32) * T.int32(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(4) + ff_3_init * T.int32(4) + ff_4_init)
                            v_yy = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) // T.int32(8) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) // T.int32(2) * T.int32(4) + yy_3_init * T.int32(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + xx_3_init * T.int32(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(640), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(320))
                                    v1 = T.axis.spatial(T.int32(640), rc_0)
                                    v2 = T.axis.spatial(T.int32(34), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) // T.int32(8) * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(6))
                                    v3 = T.axis.spatial(T.int32(34), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(6))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1 < T.int32(60))
                                    T.reads(lv240[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(33) and T.int32(1) <= v3 and v3 < T.int32(33), lv240[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused % T.int32(320) // T.int32(32) * T.int32(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(9))
                                    v1 = T.axis.spatial(T.int32(640), rc_0)
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(3), T.int32(1), T.int32(4), T.int32(2), T.int32(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(320) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused % T.int32(320) // T.int32(32) * T.int32(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(4) + ff_3 * T.int32(4) + ff_4)
                                v_yy = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) // T.int32(8) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) // T.int32(2) * T.int32(4) + yy_3 * T.int32(2) + yy_4)
                                v_xx = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + xx_3 * T.int32(2) + xx_4)
                                v_rc = T.axis.reduce(T.int32(640), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_2 + ry_0 * T.int32(3) + ry_1)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 * T.int32(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(4), T.int32(4), T.int32(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(320) + ax0)
                            v1 = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused % T.int32(320) // T.int32(32) * T.int32(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(4) * T.int32(4) + ax1)
                            v2 = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) // T.int32(8) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) // T.int32(2) * T.int32(4) + ax2)
                            v3 = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + ax3)
                            T.reads(lv246[v0, v1, v2, v3], conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)])
                            T.writes(T_divide[v0, v1, v2, v3])
                            T_divide[v0, v1, v2, v3] = lv246[v0, v1, v2, v3] + (conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)])

    @T.prim_func
    def fused_conv2d6_add13(lv226: T.Buffer((T.int32(2), T.int32(320), T.int32(32), T.int32(32)), "float32"), param_0: T.Buffer((T.int32(640), T.int32(320), T.int32(1), T.int32(1)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(640), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(320), T.int32(32), T.int32(32)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(640), T.int32(320), T.int32(1), T.int32(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(32), T.int32(1), T.int32(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_3_init + nn_4_init + nn_2_ff_2_yy_2_xx_2_fused // T.int32(32))
                            v_ff = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused // T.int32(16) * T.int32(32) + ff_3_init * T.int32(32) + ff_4_init)
                            v_yy = T.axis.spatial(T.int32(32), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(16) // T.int32(2) * T.int32(4) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(32) // T.int32(16) + yy_3_init)
                            v_xx = T.axis.spatial(T.int32(32), xx_3_init + xx_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16))
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(320), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(64))
                                    v1 = T.axis.spatial(T.int32(320), rc_0)
                                    v2 = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(16) // T.int32(2) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(64) // T.int32(16))
                                    v3 = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(16))
                                    T.reads(lv226[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv226[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused // T.int32(16) * T.int32(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1))
                                    v1 = T.axis.spatial(T.int32(320), rc_0)
                                    v2 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v3 = T.axis.spatial(T.int32(1), T.int32(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1 < T.int32(32))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(32), T.int32(1), T.int32(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_3 + nn_4 + nn_2_ff_2_yy_2_xx_2_fused // T.int32(32))
                                v_ff = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused // T.int32(16) * T.int32(32) + ff_3 * T.int32(32) + ff_4)
                                v_yy = T.axis.spatial(T.int32(32), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(16) // T.int32(2) * T.int32(4) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(32) // T.int32(16) + yy_3)
                                v_xx = T.axis.spatial(T.int32(32), xx_3 + xx_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16))
                                v_rc = T.axis.reduce(T.int32(320), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int32(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(32), T.int32(1), T.int32(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_2_ff_2_yy_2_xx_2_fused // T.int32(32) + ax0)
                            v1 = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused // T.int32(16) * T.int32(32) + ax1)
                            v2 = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(16) // T.int32(2) * T.int32(4) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(32) // T.int32(16) + ax2)
                            v3 = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(2) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d7_add13(lv249: T.Buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), "float32"), param_0: T.Buffer((T.int32(640), T.int32(640), T.int32(1), T.int32(1)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(640), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(640), T.int32(640), T.int32(1), T.int32(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(8), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(5), T.int32(1), T.int32(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_4_init + nn_1_ff_1_yy_1_xx_1_fused // T.int32(4) + nn_3_init)
                            v_ff = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused // T.int32(32) * T.int32(40) + nn_1_ff_1_yy_1_xx_1_fused % T.int32(4) // T.int32(2) * T.int32(20) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(5) + ff_3_init * T.int32(5) + ff_4_init)
                            v_yy = T.axis.spatial(T.int32(32), yy_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) + yy_3_init)
                            v_xx = T.axis.spatial(T.int32(32), xx_3_init + xx_4_init + nn_1_ff_1_yy_1_xx_1_fused % T.int32(2) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16))
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(80), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(256))
                                    v1 = T.axis.spatial(T.int32(640), rc_0 * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(256) // T.int32(32))
                                    v2 = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32))
                                    v3 = T.axis.spatial(T.int32(32), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(32))
                                    T.reads(lv249[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv249[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused // T.int32(32) * T.int32(40) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(8))
                                    v1 = T.axis.spatial(T.int32(640), rc_0 * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(8))
                                    v2 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v3 = T.axis.spatial(T.int32(1), T.int32(0))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(5), T.int32(1), T.int32(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_4 + nn_1_ff_1_yy_1_xx_1_fused // T.int32(4) + nn_3)
                                v_ff = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused // T.int32(32) * T.int32(40) + nn_1_ff_1_yy_1_xx_1_fused % T.int32(4) // T.int32(2) * T.int32(20) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(5) + ff_3 * T.int32(5) + ff_4)
                                v_yy = T.axis.spatial(T.int32(32), yy_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) + yy_3)
                                v_xx = T.axis.spatial(T.int32(32), xx_3 + xx_4 + nn_1_ff_1_yy_1_xx_1_fused % T.int32(2) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16))
                                v_rc = T.axis.reduce(T.int32(640), rc_0 * T.int32(8) + rc_1 * T.int32(2) + rc_2)
                                v_ry = T.axis.reduce(T.int32(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int32(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(5), T.int32(1), T.int32(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_1_ff_1_yy_1_xx_1_fused // T.int32(4) + ax0)
                            v1 = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused // T.int32(32) * T.int32(40) + nn_1_ff_1_yy_1_xx_1_fused % T.int32(4) // T.int32(2) * T.int32(20) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(5) + ax1)
                            v2 = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) + ax2)
                            v3 = T.axis.spatial(T.int32(32), nn_1_ff_1_yy_1_xx_1_fused % T.int32(2) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d7_add13_add16(lv325: T.Buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), "float32"), param_0: T.Buffer((T.int32(640), T.int32(640), T.int32(1), T.int32(1)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(640), T.int32(1), T.int32(1)), "float32"), lv248: T.Buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(640), T.int32(640), T.int32(1), T.int32(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(4), T.int32(2), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(160) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(640), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(160) // T.int32(32) * T.int32(128) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(4) + ff_3_init)
                            v_yy = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) // T.int32(4) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) // T.int32(4) * T.int32(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int32(32), xx_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) * T.int32(2) + xx_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(80), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(160))
                                    v1 = T.axis.spatial(T.int32(640), rc_0 * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(32))
                                    v2 = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) // T.int32(4) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(32) // T.int32(8))
                                    v3 = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(8))
                                    T.reads(lv325[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv325[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused % T.int32(160) // T.int32(32) * T.int32(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(8))
                                    v1 = T.axis.spatial(T.int32(640), rc_0 * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(8))
                                    v2 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v3 = T.axis.spatial(T.int32(1), T.int32(0))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(4), T.int32(2), T.int32(2), T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(160) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(640), ff_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(160) // T.int32(32) * T.int32(128) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(4) + ff_3)
                                v_yy = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) // T.int32(4) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) // T.int32(4) * T.int32(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int32(32), xx_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) * T.int32(2) + xx_3)
                                v_rc = T.axis.reduce(T.int32(640), rc_0 * T.int32(8) + rc_1 * T.int32(4) + rc_2)
                                v_ry = T.axis.reduce(T.int32(1), ry_2 + ry_0 + ry_1)
                                v_rx = T.axis.reduce(T.int32(1), rx_1 + rx_2 + rx_0)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(4), T.int32(2), T.int32(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(160) + ax0)
                            v1 = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused % T.int32(160) // T.int32(32) * T.int32(128) + nn_1_ff_1_yy_1_xx_1_fused * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(4) + ax1)
                            v2 = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(32) // T.int32(4) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) // T.int32(4) * T.int32(2) + ax2)
                            v3 = T.axis.spatial(T.int32(32), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(4) * T.int32(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)], lv248[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)] + lv248[v0, v1, v2, v3]

    @T.prim_func
    def fused_conv2d8_add20(lv429: T.Buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), "float32"), param_0: T.Buffer((T.int32(640), T.int32(640), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(640), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(640), T.int32(16), T.int32(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(640), T.int32(16), T.int32(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(640), T.int32(34), T.int32(34)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(640), T.int32(640), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(80), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(2), T.int32(1), T.int32(2), T.int32(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_3_init * T.int32(2) + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(640), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused // T.int32(8) * T.int32(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(4) + ff_3_init)
                            v_yy = T.axis.spatial(T.int32(16), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(2) + yy_3_init * T.int32(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int32(16), nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) * T.int32(2) + xx_3_init * T.int32(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(640), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(165))
                                    v1 = T.axis.spatial(T.int32(640), rc_0)
                                    v2 = T.axis.spatial(T.int32(34), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(165) // T.int32(33))
                                    v3 = T.axis.spatial(T.int32(34), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(33))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1 < T.int32(330))
                                    T.reads(lv429[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(33) and T.int32(1) <= v3 and v3 < T.int32(33), lv429[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused // T.int32(8) * T.int32(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(9))
                                    v1 = T.axis.spatial(T.int32(640), rc_0)
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1 < T.int32(576))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(3), T.int32(2), T.int32(1), T.int32(2), T.int32(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_3 * T.int32(2) + nn_4)
                                v_ff = T.axis.spatial(T.int32(640), ff_4 + nn_0_ff_0_yy_0_xx_0_fused // T.int32(8) * T.int32(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(4) + ff_3)
                                v_yy = T.axis.spatial(T.int32(16), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(2) + yy_3 * T.int32(2) + yy_4)
                                v_xx = T.axis.spatial(T.int32(16), nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) * T.int32(2) + xx_3 * T.int32(2) + xx_4)
                                v_rc = T.axis.reduce(T.int32(640), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_2 + ry_0 * T.int32(3) + ry_1)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 * T.int32(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy * T.int32(2) + v_ry, v_xx * T.int32(2) + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy * T.int32(2) + v_ry, v_xx * T.int32(2) + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(2), T.int32(4), T.int32(2), T.int32(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), ax0)
                            v1 = T.axis.spatial(T.int32(640), nn_0_ff_0_yy_0_xx_0_fused // T.int32(8) * T.int32(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(8) * T.int32(4) + ax1)
                            v2 = T.axis.spatial(T.int32(16), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(2) + ax2)
                            v3 = T.axis.spatial(T.int32(16), nn_2_ff_2_yy_2_xx_2_fused % T.int32(8) * T.int32(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d9_add21_add22(lv434: T.Buffer((T.int32(2), T.int32(640), T.int32(16), T.int32(16)), "float32"), param_0: T.Buffer((T.int32(1280), T.int32(640), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(1280), T.int32(1), T.int32(1)), "float32"), lv443: T.Buffer((T.int32(2), T.int32(1280), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(640), T.int32(18), T.int32(18)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(1280), T.int32(640), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(5), T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(128) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(1280), ff_4_init + nn_0_ff_0_yy_0_xx_0_fused % T.int32(128) // T.int32(4) * T.int32(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(5) + ff_3_init)
                            v_yy = T.axis.spatial(T.int32(16), yy_4_init + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) + yy_3_init)
                            v_xx = T.axis.spatial(T.int32(16), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(4) + xx_3_init * T.int32(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(640), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(128))
                                    v1 = T.axis.spatial(T.int32(640), rc_0)
                                    v2 = T.axis.spatial(T.int32(18), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(6))
                                    v3 = T.axis.spatial(T.int32(18), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(6))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1 < T.int32(108))
                                    T.reads(lv434[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(17) and T.int32(1) <= v3 and v3 < T.int32(17), lv434[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int32(128) // T.int32(4) * T.int32(40) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(9))
                                    v1 = T.axis.spatial(T.int32(640), rc_0)
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1 < T.int32(360))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(1), T.int32(3), T.int32(3), T.int32(1), T.int32(5), T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(128) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(1280), ff_4 + nn_0_ff_0_yy_0_xx_0_fused % T.int32(128) // T.int32(4) * T.int32(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(5) + ff_3)
                                v_yy = T.axis.spatial(T.int32(16), yy_4 + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) + yy_3)
                                v_xx = T.axis.spatial(T.int32(16), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(4) + xx_3 * T.int32(2) + xx_4)
                                v_rc = T.axis.reduce(T.int32(640), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_2 + ry_0 * T.int32(3) + ry_1)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(5), T.int32(1), T.int32(4)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(128) + ax0)
                            v1 = T.axis.spatial(T.int32(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int32(128) // T.int32(4) * T.int32(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) * T.int32(5) + ax1)
                            v2 = T.axis.spatial(T.int32(16), nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) + ax2)
                            v3 = T.axis.spatial(T.int32(16), nn_0_ff_0_yy_0_xx_0_fused % T.int32(4) * T.int32(4) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)], lv443[v0, v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)] + lv443[v0, v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_conv2d_add5(inp_0: T.Buffer((T.int32(2), T.int32(4), T.int32(64), T.int32(64)), "float32"), param_0: T.Buffer((T.int32(320), T.int32(4), T.int32(3), T.int32(3)), "float32"), param_1: T.Buffer((T.int32(1), T.int32(320), T.int32(1), T.int32(1)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int32(2), T.int32(4), T.int32(66), T.int32(66)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(320), T.int32(4), T.int32(3), T.int32(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int32(160), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int32(8), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int32(1), T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(80) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int32(320), nn_0_ff_0_yy_0_xx_0_fused % T.int32(80) // T.int32(16) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int32(2) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(16) // T.int32(8) * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) // T.int32(2) * T.int32(4) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(8) + nn_1_ff_1_yy_1_xx_1_fused % T.int32(2) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + xx_3_init * T.int32(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int32(2), T.int32(1), T.int32(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(80))
                                    v1 = T.axis.spatial(T.int32(4), rc_0 * T.int32(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) // T.int32(340))
                                    v2 = T.axis.spatial(T.int32(66), nn_0_ff_0_yy_0_xx_0_fused % T.int32(16) // T.int32(8) * T.int32(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) % T.int32(340) // T.int32(10))
                                    v3 = T.axis.spatial(T.int32(66), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) % T.int32(10))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1 < T.int32(680))
                                    T.reads(inp_0[v0, v1, v2 - T.int32(1), v3 - T.int32(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int32(1) <= v2 and v2 < T.int32(65) and T.int32(1) <= v3 and v3 < T.int32(65), inp_0[v0, v1, v2 - T.int32(1), v3 - T.int32(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(320), nn_0_ff_0_yy_0_xx_0_fused % T.int32(80) // T.int32(16) * T.int32(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) // T.int32(18))
                                    v1 = T.axis.spatial(T.int32(4), rc_0 * T.int32(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) % T.int32(18) // T.int32(9))
                                    v2 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) % T.int32(9) // T.int32(3))
                                    v3 = T.axis.spatial(T.int32(3), (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) % T.int32(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1 < T.int32(1152))
                                    T.reads(param_0[v0, v1, v2, v3])
                                    T.writes(param_0_shared[v0, v1, v2, v3])
                                    param_0_shared[v0, v1, v2, v3] = param_0[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(3), T.int32(3), T.int32(1), T.int32(1), T.int32(1), T.int32(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(80) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int32(320), nn_0_ff_0_yy_0_xx_0_fused % T.int32(80) // T.int32(16) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int32(2) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(16) // T.int32(8) * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) // T.int32(2) * T.int32(4) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(8) + nn_1_ff_1_yy_1_xx_1_fused % T.int32(2) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + xx_3 * T.int32(2) + xx_4)
                                v_rc = T.axis.reduce(T.int32(4), rc_0 * T.int32(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int32(3), ry_0 * T.int32(3) + ry_1 * T.int32(3) + ry_2)
                                v_rx = T.axis.reduce(T.int32(3), rx_0 * T.int32(3) + rx_1 * T.int32(3) + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], param_0_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * param_0_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int32(1), T.int32(1), T.int32(4), T.int32(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int32(2), nn_0_ff_0_yy_0_xx_0_fused // T.int32(80) + ax0)
                            v1 = T.axis.spatial(T.int32(320), nn_0_ff_0_yy_0_xx_0_fused % T.int32(80) // T.int32(16) * T.int32(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int32(2) * T.int32(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int32(16) + ax1)
                            v2 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(16) // T.int32(8) * T.int32(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(16) // T.int32(2) * T.int32(4) + ax2)
                            v3 = T.axis.spatial(T.int32(64), nn_0_ff_0_yy_0_xx_0_fused % T.int32(8) * T.int32(8) + nn_1_ff_1_yy_1_xx_1_fused % T.int32(2) * T.int32(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int32(2) * T.int32(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], param_1[T.int32(0), v1, T.int32(0), T.int32(0)])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + param_1[T.int32(0), v1, T.int32(0), T.int32(0)]

    @T.prim_func
    def fused_group_norm10_silu7(lv796: T.Buffer((T.int32(2), T.int32(2560), T.int32(8), T.int32(8)), "float32"), param_0: T.Buffer((T.int32(2560),), "float32"), param_1: T.Buffer((T.int32(2560),), "float32"), T_multiply: T.Buffer((T.int32(2), T.int32(2560), T.int32(8), T.int32(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int32(2), T.int32(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int32(2), T.int32(32)))
        for ax0_ax1_fused in T.thread_binding(T.int32(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(16), "pragma_unroll_explicit": T.int32(1)}):
            for k2_k3_k4_fused_0 in range(T.int32(40)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int32(2), ax0_ax1_fused // T.int32(32))
                        v_ax1 = T.axis.spatial(T.int32(32), ax0_ax1_fused % T.int32(32))
                        v_k2 = T.axis.reduce(T.int32(80), (k2_k3_k4_fused_0 * T.int32(128) + k2_k3_k4_fused_1) // T.int32(64))
                        v_k3 = T.axis.reduce(T.int32(8), (k2_k3_k4_fused_0 * T.int32(128) + k2_k3_k4_fused_1) % T.int32(64) // T.int32(8))
                        v_k4 = T.axis.reduce(T.int32(8), (k2_k3_k4_fused_0 * T.int32(128) + k2_k3_k4_fused_1) % T.int32(8))
                        T.reads(lv796[((v_ax1 * T.int32(80) + (v_k4 // T.int32(8) + v_k3) // T.int32(8) + v_k2) // T.int32(2560) + v_ax0) % T.int32(2), (v_ax1 * T.int32(80) + (v_k4 // T.int32(8) + v_k3) // T.int32(8) + v_k2) % T.int32(2560), (v_k4 // T.int32(8) + v_k3) % T.int32(8), v_k4 % T.int32(8)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv796[((v_ax1 * T.int32(80) + (v_k4 // T.int32(8) + v_k3) // T.int32(8) + v_k2) // T.int32(2560) + v_ax0) % T.int32(2), (v_ax1 * T.int32(80) + (v_k4 // T.int32(8) + v_k3) // T.int32(8) + v_k2) % T.int32(2560), (v_k4 // T.int32(8) + v_k3) % T.int32(8), v_k4 % T.int32(8)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv796[((v_ax1 * T.int32(80) + (v_k4 // T.int32(8) + v_k3) // T.int32(8) + v_k2) // T.int32(2560) + v_ax0) % T.int32(2), (v_ax1 * T.int32(80) + (v_k4 // T.int32(8) + v_k3) // T.int32(8) + v_k2) % T.int32(2560), (v_k4 // T.int32(8) + v_k3) % T.int32(8), v_k4 % T.int32(8)] * lv796[((v_ax1 * T.int32(80) + (v_k4 // T.int32(8) + v_k3) // T.int32(8) + v_k2) // T.int32(2560) + v_ax0) % T.int32(2), (v_ax1 * T.int32(80) + (v_k4 // T.int32(8) + v_k3) // T.int32(8) + v_k2) % T.int32(2560), (v_k4 // T.int32(8) + v_k3) % T.int32(8), v_k4 % T.int32(8)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(16), "pragma_unroll_explicit": T.int32(1)}):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(5)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(163840))
                        v_ax1 = T.axis.spatial(T.int32(2560), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(163840) // T.int32(64))
                        v_ax2 = T.axis.spatial(T.int32(8), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(64) // T.int32(8))
                        v_ax3 = T.axis.spatial(T.int32(8), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(8))
                        T.reads(lv796[(((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) // T.int32(2560) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(2560), (v_ax3 // T.int32(8) + v_ax2) % T.int32(8), v_ax3 % T.int32(8)], rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) // T.int32(2560) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(2560) // T.int32(80)], rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) // T.int32(2560) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(2560) // T.int32(80)], param_0[((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(2560)], param_1[((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(2560)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv796[((((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(2560) // T.int32(80) * T.int32(80) + (v_ax3 % T.int32(8) // T.int32(8) + (v_ax3 // T.int32(8) + v_ax2) % T.int32(8)) // T.int32(8) + ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(80)) // T.int32(2560) + (((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) // T.int32(2560) + v_ax0) % T.int32(2)) % T.int32(2), (((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(2560) // T.int32(80) * T.int32(80) + (v_ax3 % T.int32(8) // T.int32(8) + (v_ax3 // T.int32(8) + v_ax2) % T.int32(8)) // T.int32(8) + ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(80)) % T.int32(2560), (v_ax3 % T.int32(8) // T.int32(8) + (v_ax3 // T.int32(8) + v_ax2) % T.int32(8)) % T.int32(8), v_ax3 % T.int32(8) % T.int32(8)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) // T.int32(2560) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(2560) // T.int32(80)] * T.float32(0.00019531250000000001)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) // T.int32(2560) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(2560) // T.int32(80)] * T.float32(0.00019531250000000001) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) // T.int32(2560) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(2560) // T.int32(80)] * T.float32(0.00019531250000000001) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) // T.int32(2560) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(2560) // T.int32(80)] * T.float32(0.00019531250000000001)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(2560) // T.int32(80) * T.int32(80) + ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(80)) % T.int32(2560)] + param_1[(((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(2560) // T.int32(80) * T.int32(80) + ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(80)) % T.int32(2560)]) * T.sigmoid((lv796[((((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(2560) // T.int32(80) * T.int32(80) + (v_ax3 % T.int32(8) // T.int32(8) + (v_ax3 // T.int32(8) + v_ax2) % T.int32(8)) // T.int32(8) + ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(80)) // T.int32(2560) + (((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) // T.int32(2560) + v_ax0) % T.int32(2)) % T.int32(2), (((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(2560) // T.int32(80) * T.int32(80) + (v_ax3 % T.int32(8) // T.int32(8) + (v_ax3 // T.int32(8) + v_ax2) % T.int32(8)) // T.int32(8) + ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(80)) % T.int32(2560), (v_ax3 % T.int32(8) // T.int32(8) + (v_ax3 // T.int32(8) + v_ax2) % T.int32(8)) % T.int32(8), v_ax3 % T.int32(8) % T.int32(8)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) // T.int32(2560) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(2560) // T.int32(80)] * T.float32(0.00019531250000000001)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) // T.int32(2560) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(2560) // T.int32(80)] * T.float32(0.00019531250000000001) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) // T.int32(2560) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(2560) // T.int32(80)] * T.float32(0.00019531250000000001) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) // T.int32(2560) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(2560) // T.int32(80)] * T.float32(0.00019531250000000001)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(2560) // T.int32(80) * T.int32(80) + ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(80)) % T.int32(2560)] + param_1[(((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(2560) // T.int32(80) * T.int32(80) + ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(80)) % T.int32(2560)])

    @T.prim_func
    def fused_group_norm11_silu8(lv869: T.Buffer((T.int32(2), T.int32(2560), T.int32(16), T.int32(16)), "float32"), param_0: T.Buffer((T.int32(2560),), "float32"), param_1: T.Buffer((T.int32(2560),), "float32"), T_multiply: T.Buffer((T.int32(2), T.int32(2560), T.int32(16), T.int32(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int32(2), T.int32(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int32(2), T.int32(32)))
        for ax0_ax1_fused in T.thread_binding(T.int32(64), thread="blockIdx.x"):
            for k2_k3_k4_fused_0 in range(T.int32(320)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int32(2), ax0_ax1_fused // T.int32(32))
                        v_ax1 = T.axis.spatial(T.int32(32), ax0_ax1_fused % T.int32(32))
                        v_k2 = T.axis.reduce(T.int32(80), (k2_k3_k4_fused_0 * T.int32(64) + k2_k3_k4_fused_1) // T.int32(256))
                        v_k3 = T.axis.reduce(T.int32(16), (k2_k3_k4_fused_0 * T.int32(64) + k2_k3_k4_fused_1) % T.int32(256) // T.int32(16))
                        v_k4 = T.axis.reduce(T.int32(16), (k2_k3_k4_fused_0 * T.int32(64) + k2_k3_k4_fused_1) % T.int32(16))
                        T.reads(lv869[((v_ax1 * T.int32(80) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) // T.int32(2560) + v_ax0) % T.int32(2), (v_ax1 * T.int32(80) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) % T.int32(2560), (v_k4 // T.int32(16) + v_k3) % T.int32(16), v_k4 % T.int32(16)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv869[((v_ax1 * T.int32(80) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) // T.int32(2560) + v_ax0) % T.int32(2), (v_ax1 * T.int32(80) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) % T.int32(2560), (v_k4 // T.int32(16) + v_k3) % T.int32(16), v_k4 % T.int32(16)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv869[((v_ax1 * T.int32(80) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) // T.int32(2560) + v_ax0) % T.int32(2), (v_ax1 * T.int32(80) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) % T.int32(2560), (v_k4 // T.int32(16) + v_k3) % T.int32(16), v_k4 % T.int32(16)] * lv869[((v_ax1 * T.int32(80) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) // T.int32(2560) + v_ax0) % T.int32(2), (v_ax1 * T.int32(80) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) % T.int32(2560), (v_k4 // T.int32(16) + v_k3) % T.int32(16), v_k4 % T.int32(16)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(20)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(655360))
                        v_ax1 = T.axis.spatial(T.int32(2560), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(655360) // T.int32(256))
                        v_ax2 = T.axis.spatial(T.int32(16), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(256) // T.int32(16))
                        v_ax3 = T.axis.spatial(T.int32(16), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(16))
                        T.reads(lv869[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(2560) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(2560), (v_ax3 // T.int32(16) + v_ax2) % T.int32(16), v_ax3 % T.int32(16)], rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(2560) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(2560) // T.int32(80)], rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(2560) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(2560) // T.int32(80)], param_0[((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(2560)], param_1[((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(2560)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv869[((((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(2560) // T.int32(80) * T.int32(80) + (v_ax3 % T.int32(16) // T.int32(16) + (v_ax3 // T.int32(16) + v_ax2) % T.int32(16)) // T.int32(16) + ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(80)) // T.int32(2560) + (((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(2560) + v_ax0) % T.int32(2)) % T.int32(2), (((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(2560) // T.int32(80) * T.int32(80) + (v_ax3 % T.int32(16) // T.int32(16) + (v_ax3 // T.int32(16) + v_ax2) % T.int32(16)) // T.int32(16) + ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(80)) % T.int32(2560), (v_ax3 % T.int32(16) // T.int32(16) + (v_ax3 // T.int32(16) + v_ax2) % T.int32(16)) % T.int32(16), v_ax3 % T.int32(16) % T.int32(16)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(2560) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(2560) // T.int32(80)] * T.float32(4.8828125000000003e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(2560) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(2560) // T.int32(80)] * T.float32(4.8828125000000003e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(2560) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(2560) // T.int32(80)] * T.float32(4.8828125000000003e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(2560) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(2560) // T.int32(80)] * T.float32(4.8828125000000003e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(2560) // T.int32(80) * T.int32(80) + ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(80)) % T.int32(2560)] + param_1[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(2560) // T.int32(80) * T.int32(80) + ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(80)) % T.int32(2560)]) * T.sigmoid((lv869[((((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(2560) // T.int32(80) * T.int32(80) + (v_ax3 % T.int32(16) // T.int32(16) + (v_ax3 // T.int32(16) + v_ax2) % T.int32(16)) // T.int32(16) + ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(80)) // T.int32(2560) + (((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(2560) + v_ax0) % T.int32(2)) % T.int32(2), (((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(2560) // T.int32(80) * T.int32(80) + (v_ax3 % T.int32(16) // T.int32(16) + (v_ax3 // T.int32(16) + v_ax2) % T.int32(16)) // T.int32(16) + ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(80)) % T.int32(2560), (v_ax3 % T.int32(16) // T.int32(16) + (v_ax3 // T.int32(16) + v_ax2) % T.int32(16)) % T.int32(16), v_ax3 % T.int32(16) % T.int32(16)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(2560) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(2560) // T.int32(80)] * T.float32(4.8828125000000003e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(2560) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(2560) // T.int32(80)] * T.float32(4.8828125000000003e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(2560) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(2560) // T.int32(80)] * T.float32(4.8828125000000003e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(2560) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(2560) // T.int32(80)] * T.float32(4.8828125000000003e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(2560) // T.int32(80) * T.int32(80) + ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(80)) % T.int32(2560)] + param_1[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(2560) // T.int32(80) * T.int32(80) + ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(80)) % T.int32(2560)])

    @T.prim_func
    def fused_group_norm12_silu9(lv1077: T.Buffer((T.int32(2), T.int32(1920), T.int32(16), T.int32(16)), "float32"), param_0: T.Buffer((T.int32(1920),), "float32"), param_1: T.Buffer((T.int32(1920),), "float32"), T_multiply: T.Buffer((T.int32(2), T.int32(1920), T.int32(16), T.int32(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int32(2), T.int32(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int32(2), T.int32(32)))
        for ax0_ax1_fused in T.thread_binding(T.int32(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(16), "pragma_unroll_explicit": T.int32(1)}):
            for k2_k3_k4_fused_0 in range(T.int32(60)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int32(2), ax0_ax1_fused // T.int32(32))
                        v_ax1 = T.axis.spatial(T.int32(32), ax0_ax1_fused % T.int32(32))
                        v_k2 = T.axis.reduce(T.int32(60), (k2_k3_k4_fused_0 * T.int32(256) + k2_k3_k4_fused_1) // T.int32(256))
                        v_k3 = T.axis.reduce(T.int32(16), (k2_k3_k4_fused_0 * T.int32(256) + k2_k3_k4_fused_1) % T.int32(256) // T.int32(16))
                        v_k4 = T.axis.reduce(T.int32(16), (k2_k3_k4_fused_0 * T.int32(256) + k2_k3_k4_fused_1) % T.int32(16))
                        T.reads(lv1077[((v_ax1 * T.int32(60) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) // T.int32(1920) + v_ax0) % T.int32(2), (v_ax1 * T.int32(60) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) % T.int32(1920), (v_k4 // T.int32(16) + v_k3) % T.int32(16), v_k4 % T.int32(16)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv1077[((v_ax1 * T.int32(60) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) // T.int32(1920) + v_ax0) % T.int32(2), (v_ax1 * T.int32(60) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) % T.int32(1920), (v_k4 // T.int32(16) + v_k3) % T.int32(16), v_k4 % T.int32(16)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv1077[((v_ax1 * T.int32(60) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) // T.int32(1920) + v_ax0) % T.int32(2), (v_ax1 * T.int32(60) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) % T.int32(1920), (v_k4 // T.int32(16) + v_k3) % T.int32(16), v_k4 % T.int32(16)] * lv1077[((v_ax1 * T.int32(60) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) // T.int32(1920) + v_ax0) % T.int32(2), (v_ax1 * T.int32(60) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) % T.int32(1920), (v_k4 // T.int32(16) + v_k3) % T.int32(16), v_k4 % T.int32(16)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(16), "pragma_unroll_explicit": T.int32(1)}):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(15)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(491520))
                        v_ax1 = T.axis.spatial(T.int32(1920), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(491520) // T.int32(256))
                        v_ax2 = T.axis.spatial(T.int32(16), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(256) // T.int32(16))
                        v_ax3 = T.axis.spatial(T.int32(16), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(16))
                        T.reads(lv1077[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(1920) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1920), (v_ax3 // T.int32(16) + v_ax2) % T.int32(16), v_ax3 % T.int32(16)], rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(1920) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1920) // T.int32(60)], rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(1920) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1920) // T.int32(60)], param_0[((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1920)], param_1[((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1920)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv1077[((((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1920) // T.int32(60) * T.int32(60) + (v_ax3 % T.int32(16) // T.int32(16) + (v_ax3 // T.int32(16) + v_ax2) % T.int32(16)) // T.int32(16) + ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(60)) // T.int32(1920) + (((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(1920) + v_ax0) % T.int32(2)) % T.int32(2), (((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1920) // T.int32(60) * T.int32(60) + (v_ax3 % T.int32(16) // T.int32(16) + (v_ax3 // T.int32(16) + v_ax2) % T.int32(16)) // T.int32(16) + ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(60)) % T.int32(1920), (v_ax3 % T.int32(16) // T.int32(16) + (v_ax3 // T.int32(16) + v_ax2) % T.int32(16)) % T.int32(16), v_ax3 % T.int32(16) % T.int32(16)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(1920) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1920) // T.int32(60)] * T.float32(6.5104166666666666e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(1920) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1920) // T.int32(60)] * T.float32(6.5104166666666666e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(1920) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1920) // T.int32(60)] * T.float32(6.5104166666666666e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(1920) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1920) // T.int32(60)] * T.float32(6.5104166666666666e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1920) // T.int32(60) * T.int32(60) + ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(60)) % T.int32(1920)] + param_1[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1920) // T.int32(60) * T.int32(60) + ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(60)) % T.int32(1920)]) * T.sigmoid((lv1077[((((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1920) // T.int32(60) * T.int32(60) + (v_ax3 % T.int32(16) // T.int32(16) + (v_ax3 // T.int32(16) + v_ax2) % T.int32(16)) // T.int32(16) + ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(60)) // T.int32(1920) + (((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(1920) + v_ax0) % T.int32(2)) % T.int32(2), (((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1920) // T.int32(60) * T.int32(60) + (v_ax3 % T.int32(16) // T.int32(16) + (v_ax3 // T.int32(16) + v_ax2) % T.int32(16)) // T.int32(16) + ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(60)) % T.int32(1920), (v_ax3 % T.int32(16) // T.int32(16) + (v_ax3 // T.int32(16) + v_ax2) % T.int32(16)) % T.int32(16), v_ax3 % T.int32(16) % T.int32(16)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(1920) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1920) // T.int32(60)] * T.float32(6.5104166666666666e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(1920) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1920) // T.int32(60)] * T.float32(6.5104166666666666e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(1920) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1920) // T.int32(60)] * T.float32(6.5104166666666666e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(1920) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1920) // T.int32(60)] * T.float32(6.5104166666666666e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1920) // T.int32(60) * T.int32(60) + ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(60)) % T.int32(1920)] + param_1[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1920) // T.int32(60) * T.int32(60) + ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(60)) % T.int32(1920)])

    @T.prim_func
    def fused_group_norm13_silu10(lv1185: T.Buffer((T.int32(2), T.int32(1920), T.int32(32), T.int32(32)), "float32"), param_0: T.Buffer((T.int32(1920),), "float32"), param_1: T.Buffer((T.int32(1920),), "float32"), T_multiply: T.Buffer((T.int32(2), T.int32(1920), T.int32(32), T.int32(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int32(2), T.int32(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int32(2), T.int32(32)))
        for ax0_ax1_fused in T.thread_binding(T.int32(64), thread="blockIdx.x"):
            for k2_k3_k4_fused_0 in range(T.int32(480)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int32(2), ax0_ax1_fused // T.int32(32))
                        v_ax1 = T.axis.spatial(T.int32(32), ax0_ax1_fused % T.int32(32))
                        v_k2 = T.axis.reduce(T.int32(60), (k2_k3_k4_fused_0 * T.int32(128) + k2_k3_k4_fused_1) // T.int32(1024))
                        v_k3 = T.axis.reduce(T.int32(32), (k2_k3_k4_fused_0 * T.int32(128) + k2_k3_k4_fused_1) % T.int32(1024) // T.int32(32))
                        v_k4 = T.axis.reduce(T.int32(32), (k2_k3_k4_fused_0 * T.int32(128) + k2_k3_k4_fused_1) % T.int32(32))
                        T.reads(lv1185[((v_ax1 * T.int32(60) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) // T.int32(1920) + v_ax0) % T.int32(2), (v_ax1 * T.int32(60) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) % T.int32(1920), (v_k4 // T.int32(32) + v_k3) % T.int32(32), v_k4 % T.int32(32)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv1185[((v_ax1 * T.int32(60) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) // T.int32(1920) + v_ax0) % T.int32(2), (v_ax1 * T.int32(60) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) % T.int32(1920), (v_k4 // T.int32(32) + v_k3) % T.int32(32), v_k4 % T.int32(32)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv1185[((v_ax1 * T.int32(60) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) // T.int32(1920) + v_ax0) % T.int32(2), (v_ax1 * T.int32(60) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) % T.int32(1920), (v_k4 // T.int32(32) + v_k3) % T.int32(32), v_k4 % T.int32(32)] * lv1185[((v_ax1 * T.int32(60) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) // T.int32(1920) + v_ax0) % T.int32(2), (v_ax1 * T.int32(60) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) % T.int32(1920), (v_k4 // T.int32(32) + v_k3) % T.int32(32), v_k4 % T.int32(32)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(60)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(1966080))
                        v_ax1 = T.axis.spatial(T.int32(1920), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(1966080) // T.int32(1024))
                        v_ax2 = T.axis.spatial(T.int32(32), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(1024) // T.int32(32))
                        v_ax3 = T.axis.spatial(T.int32(32), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(32))
                        T.reads(lv1185[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(1920) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1920), (v_ax3 // T.int32(32) + v_ax2) % T.int32(32), v_ax3 % T.int32(32)], rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(1920) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1920) // T.int32(60)], rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(1920) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1920) // T.int32(60)], param_0[((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1920)], param_1[((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1920)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv1185[((((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1920) // T.int32(60) * T.int32(60) + (v_ax3 % T.int32(32) // T.int32(32) + (v_ax3 // T.int32(32) + v_ax2) % T.int32(32)) // T.int32(32) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(60)) // T.int32(1920) + (((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(1920) + v_ax0) % T.int32(2)) % T.int32(2), (((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1920) // T.int32(60) * T.int32(60) + (v_ax3 % T.int32(32) // T.int32(32) + (v_ax3 // T.int32(32) + v_ax2) % T.int32(32)) // T.int32(32) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(60)) % T.int32(1920), (v_ax3 % T.int32(32) // T.int32(32) + (v_ax3 // T.int32(32) + v_ax2) % T.int32(32)) % T.int32(32), v_ax3 % T.int32(32) % T.int32(32)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(1920) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1920) // T.int32(60)] * T.float32(1.6276041666666666e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(1920) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1920) // T.int32(60)] * T.float32(1.6276041666666666e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(1920) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1920) // T.int32(60)] * T.float32(1.6276041666666666e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(1920) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1920) // T.int32(60)] * T.float32(1.6276041666666666e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1920) // T.int32(60) * T.int32(60) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(60)) % T.int32(1920)] + param_1[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1920) // T.int32(60) * T.int32(60) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(60)) % T.int32(1920)]) * T.sigmoid((lv1185[((((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1920) // T.int32(60) * T.int32(60) + (v_ax3 % T.int32(32) // T.int32(32) + (v_ax3 // T.int32(32) + v_ax2) % T.int32(32)) // T.int32(32) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(60)) // T.int32(1920) + (((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(1920) + v_ax0) % T.int32(2)) % T.int32(2), (((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1920) // T.int32(60) * T.int32(60) + (v_ax3 % T.int32(32) // T.int32(32) + (v_ax3 // T.int32(32) + v_ax2) % T.int32(32)) // T.int32(32) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(60)) % T.int32(1920), (v_ax3 % T.int32(32) // T.int32(32) + (v_ax3 // T.int32(32) + v_ax2) % T.int32(32)) % T.int32(32), v_ax3 % T.int32(32) % T.int32(32)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(1920) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1920) // T.int32(60)] * T.float32(1.6276041666666666e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(1920) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1920) // T.int32(60)] * T.float32(1.6276041666666666e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(1920) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1920) // T.int32(60)] * T.float32(1.6276041666666666e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(1920) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1920) // T.int32(60)] * T.float32(1.6276041666666666e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1920) // T.int32(60) * T.int32(60) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(60)) % T.int32(1920)] + param_1[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1920) // T.int32(60) * T.int32(60) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(60)) % T.int32(1920)])

    @T.prim_func
    def fused_group_norm14_silu11(lv1289: T.Buffer((T.int32(2), T.int32(1280), T.int32(32), T.int32(32)), "float32"), param_0: T.Buffer((T.int32(1280),), "float32"), param_1: T.Buffer((T.int32(1280),), "float32"), T_multiply: T.Buffer((T.int32(2), T.int32(1280), T.int32(32), T.int32(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int32(2), T.int32(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int32(2), T.int32(32)))
        for ax0_ax1_fused in T.thread_binding(T.int32(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(16), "pragma_unroll_explicit": T.int32(1)}):
            for k2_k3_k4_fused_0 in range(T.int32(320)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int32(2), ax0_ax1_fused // T.int32(32))
                        v_ax1 = T.axis.spatial(T.int32(32), ax0_ax1_fused % T.int32(32))
                        v_k2 = T.axis.reduce(T.int32(40), (k2_k3_k4_fused_0 * T.int32(128) + k2_k3_k4_fused_1) // T.int32(1024))
                        v_k3 = T.axis.reduce(T.int32(32), (k2_k3_k4_fused_0 * T.int32(128) + k2_k3_k4_fused_1) % T.int32(1024) // T.int32(32))
                        v_k4 = T.axis.reduce(T.int32(32), (k2_k3_k4_fused_0 * T.int32(128) + k2_k3_k4_fused_1) % T.int32(32))
                        T.reads(lv1289[((v_ax1 * T.int32(40) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) // T.int32(1280) + v_ax0) % T.int32(2), (v_ax1 * T.int32(40) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) % T.int32(1280), (v_k4 // T.int32(32) + v_k3) % T.int32(32), v_k4 % T.int32(32)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv1289[((v_ax1 * T.int32(40) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) // T.int32(1280) + v_ax0) % T.int32(2), (v_ax1 * T.int32(40) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) % T.int32(1280), (v_k4 // T.int32(32) + v_k3) % T.int32(32), v_k4 % T.int32(32)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv1289[((v_ax1 * T.int32(40) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) // T.int32(1280) + v_ax0) % T.int32(2), (v_ax1 * T.int32(40) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) % T.int32(1280), (v_k4 // T.int32(32) + v_k3) % T.int32(32), v_k4 % T.int32(32)] * lv1289[((v_ax1 * T.int32(40) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) // T.int32(1280) + v_ax0) % T.int32(2), (v_ax1 * T.int32(40) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) % T.int32(1280), (v_k4 // T.int32(32) + v_k3) % T.int32(32), v_k4 % T.int32(32)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(16), "pragma_unroll_explicit": T.int32(1)}):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(40)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(1310720))
                        v_ax1 = T.axis.spatial(T.int32(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(1310720) // T.int32(1024))
                        v_ax2 = T.axis.spatial(T.int32(32), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(1024) // T.int32(32))
                        v_ax3 = T.axis.spatial(T.int32(32), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(32))
                        T.reads(lv1289[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1280), (v_ax3 // T.int32(32) + v_ax2) % T.int32(32), v_ax3 % T.int32(32)], rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1280) // T.int32(40)], rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1280) // T.int32(40)], param_0[((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1280)], param_1[((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1280)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv1289[((((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1280) // T.int32(40) * T.int32(40) + (v_ax3 % T.int32(32) // T.int32(32) + (v_ax3 // T.int32(32) + v_ax2) % T.int32(32)) // T.int32(32) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(40)) // T.int32(1280) + (((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2)) % T.int32(2), (((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1280) // T.int32(40) * T.int32(40) + (v_ax3 % T.int32(32) // T.int32(32) + (v_ax3 // T.int32(32) + v_ax2) % T.int32(32)) // T.int32(32) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(40)) % T.int32(1280), (v_ax3 % T.int32(32) // T.int32(32) + (v_ax3 // T.int32(32) + v_ax2) % T.int32(32)) % T.int32(32), v_ax3 % T.int32(32) % T.int32(32)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1280) // T.int32(40)] * T.float32(2.4414062500000001e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1280) // T.int32(40)] * T.float32(2.4414062500000001e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1280) // T.int32(40)] * T.float32(2.4414062500000001e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1280) // T.int32(40)] * T.float32(2.4414062500000001e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1280) // T.int32(40) * T.int32(40) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(40)) % T.int32(1280)] + param_1[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1280) // T.int32(40) * T.int32(40) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(40)) % T.int32(1280)]) * T.sigmoid((lv1289[((((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1280) // T.int32(40) * T.int32(40) + (v_ax3 % T.int32(32) // T.int32(32) + (v_ax3 // T.int32(32) + v_ax2) % T.int32(32)) // T.int32(32) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(40)) // T.int32(1280) + (((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2)) % T.int32(2), (((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1280) // T.int32(40) * T.int32(40) + (v_ax3 % T.int32(32) // T.int32(32) + (v_ax3 // T.int32(32) + v_ax2) % T.int32(32)) // T.int32(32) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(40)) % T.int32(1280), (v_ax3 % T.int32(32) // T.int32(32) + (v_ax3 // T.int32(32) + v_ax2) % T.int32(32)) % T.int32(32), v_ax3 % T.int32(32) % T.int32(32)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1280) // T.int32(40)] * T.float32(2.4414062500000001e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1280) // T.int32(40)] * T.float32(2.4414062500000001e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1280) // T.int32(40)] * T.float32(2.4414062500000001e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1280) // T.int32(40)] * T.float32(2.4414062500000001e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1280) // T.int32(40) * T.int32(40) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(40)) % T.int32(1280)] + param_1[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(1280) // T.int32(40) * T.int32(40) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(40)) % T.int32(1280)])

    @T.prim_func
    def fused_group_norm15_silu12(lv1393: T.Buffer((T.int32(2), T.int32(960), T.int32(32), T.int32(32)), "float32"), param_0: T.Buffer((T.int32(960),), "float32"), param_1: T.Buffer((T.int32(960),), "float32"), T_multiply: T.Buffer((T.int32(2), T.int32(960), T.int32(32), T.int32(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int32(2), T.int32(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int32(2), T.int32(32)))
        for ax0_ax1_fused in T.thread_binding(T.int32(64), thread="blockIdx.x"):
            for k2_k3_k4_fused_0 in range(T.int32(480)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int32(2), ax0_ax1_fused // T.int32(32))
                        v_ax1 = T.axis.spatial(T.int32(32), ax0_ax1_fused % T.int32(32))
                        v_k2 = T.axis.reduce(T.int32(30), (k2_k3_k4_fused_0 * T.int32(64) + k2_k3_k4_fused_1) // T.int32(1024))
                        v_k3 = T.axis.reduce(T.int32(32), (k2_k3_k4_fused_0 * T.int32(64) + k2_k3_k4_fused_1) % T.int32(1024) // T.int32(32))
                        v_k4 = T.axis.reduce(T.int32(32), (k2_k3_k4_fused_0 * T.int32(64) + k2_k3_k4_fused_1) % T.int32(32))
                        T.reads(lv1393[((v_ax1 * T.int32(30) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) // T.int32(960) + v_ax0) % T.int32(2), (v_ax1 * T.int32(30) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) % T.int32(960), (v_k4 // T.int32(32) + v_k3) % T.int32(32), v_k4 % T.int32(32)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv1393[((v_ax1 * T.int32(30) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) // T.int32(960) + v_ax0) % T.int32(2), (v_ax1 * T.int32(30) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) % T.int32(960), (v_k4 // T.int32(32) + v_k3) % T.int32(32), v_k4 % T.int32(32)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv1393[((v_ax1 * T.int32(30) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) // T.int32(960) + v_ax0) % T.int32(2), (v_ax1 * T.int32(30) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) % T.int32(960), (v_k4 // T.int32(32) + v_k3) % T.int32(32), v_k4 % T.int32(32)] * lv1393[((v_ax1 * T.int32(30) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) // T.int32(960) + v_ax0) % T.int32(2), (v_ax1 * T.int32(30) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) % T.int32(960), (v_k4 // T.int32(32) + v_k3) % T.int32(32), v_k4 % T.int32(32)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(30)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(983040))
                        v_ax1 = T.axis.spatial(T.int32(960), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(983040) // T.int32(1024))
                        v_ax2 = T.axis.spatial(T.int32(32), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(1024) // T.int32(32))
                        v_ax3 = T.axis.spatial(T.int32(32), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(32))
                        T.reads(lv1393[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(960) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(960), (v_ax3 // T.int32(32) + v_ax2) % T.int32(32), v_ax3 % T.int32(32)], rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(960) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(960) // T.int32(30)], rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(960) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(960) // T.int32(30)], param_0[((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(960)], param_1[((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(960)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv1393[((((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(960) // T.int32(30) * T.int32(30) + (v_ax3 % T.int32(32) // T.int32(32) + (v_ax3 // T.int32(32) + v_ax2) % T.int32(32)) // T.int32(32) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(30)) // T.int32(960) + (((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(960) + v_ax0) % T.int32(2)) % T.int32(2), (((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(960) // T.int32(30) * T.int32(30) + (v_ax3 % T.int32(32) // T.int32(32) + (v_ax3 // T.int32(32) + v_ax2) % T.int32(32)) // T.int32(32) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(30)) % T.int32(960), (v_ax3 % T.int32(32) // T.int32(32) + (v_ax3 // T.int32(32) + v_ax2) % T.int32(32)) % T.int32(32), v_ax3 % T.int32(32) % T.int32(32)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(960) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(960) // T.int32(30)] * T.float32(3.2552083333333333e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(960) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(960) // T.int32(30)] * T.float32(3.2552083333333333e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(960) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(960) // T.int32(30)] * T.float32(3.2552083333333333e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(960) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(960) // T.int32(30)] * T.float32(3.2552083333333333e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(960) // T.int32(30) * T.int32(30) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(30)) % T.int32(960)] + param_1[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(960) // T.int32(30) * T.int32(30) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(30)) % T.int32(960)]) * T.sigmoid((lv1393[((((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(960) // T.int32(30) * T.int32(30) + (v_ax3 % T.int32(32) // T.int32(32) + (v_ax3 // T.int32(32) + v_ax2) % T.int32(32)) // T.int32(32) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(30)) // T.int32(960) + (((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(960) + v_ax0) % T.int32(2)) % T.int32(2), (((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(960) // T.int32(30) * T.int32(30) + (v_ax3 % T.int32(32) // T.int32(32) + (v_ax3 // T.int32(32) + v_ax2) % T.int32(32)) // T.int32(32) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(30)) % T.int32(960), (v_ax3 % T.int32(32) // T.int32(32) + (v_ax3 // T.int32(32) + v_ax2) % T.int32(32)) % T.int32(32), v_ax3 % T.int32(32) % T.int32(32)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(960) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(960) // T.int32(30)] * T.float32(3.2552083333333333e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(960) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(960) // T.int32(30)] * T.float32(3.2552083333333333e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(960) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(960) // T.int32(30)] * T.float32(3.2552083333333333e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(960) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(960) // T.int32(30)] * T.float32(3.2552083333333333e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(960) // T.int32(30) * T.int32(30) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(30)) % T.int32(960)] + param_1[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(960) // T.int32(30) * T.int32(30) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(30)) % T.int32(960)])

    @T.prim_func
    def fused_group_norm16_silu13(lv1501: T.Buffer((T.int32(2), T.int32(960), T.int32(64), T.int32(64)), "float32"), param_0: T.Buffer((T.int32(960),), "float32"), param_1: T.Buffer((T.int32(960),), "float32"), T_multiply: T.Buffer((T.int32(2), T.int32(960), T.int32(64), T.int32(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int32(2), T.int32(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int32(2), T.int32(32)))
        for ax0_ax1_fused in T.thread_binding(T.int32(64), thread="blockIdx.x"):
            for k2_k3_k4_fused_0 in range(T.int32(1920)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int32(2), ax0_ax1_fused // T.int32(32))
                        v_ax1 = T.axis.spatial(T.int32(32), ax0_ax1_fused % T.int32(32))
                        v_k2 = T.axis.reduce(T.int32(30), (k2_k3_k4_fused_0 * T.int32(64) + k2_k3_k4_fused_1) // T.int32(4096))
                        v_k3 = T.axis.reduce(T.int32(64), (k2_k3_k4_fused_0 * T.int32(64) + k2_k3_k4_fused_1) % T.int32(4096) // T.int32(64))
                        v_k4 = T.axis.reduce(T.int32(64), (k2_k3_k4_fused_0 * T.int32(64) + k2_k3_k4_fused_1) % T.int32(64))
                        T.reads(lv1501[((v_ax1 * T.int32(30) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) // T.int32(960) + v_ax0) % T.int32(2), (v_ax1 * T.int32(30) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) % T.int32(960), (v_k4 // T.int32(64) + v_k3) % T.int32(64), v_k4 % T.int32(64)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv1501[((v_ax1 * T.int32(30) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) // T.int32(960) + v_ax0) % T.int32(2), (v_ax1 * T.int32(30) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) % T.int32(960), (v_k4 // T.int32(64) + v_k3) % T.int32(64), v_k4 % T.int32(64)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv1501[((v_ax1 * T.int32(30) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) // T.int32(960) + v_ax0) % T.int32(2), (v_ax1 * T.int32(30) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) % T.int32(960), (v_k4 // T.int32(64) + v_k3) % T.int32(64), v_k4 % T.int32(64)] * lv1501[((v_ax1 * T.int32(30) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) // T.int32(960) + v_ax0) % T.int32(2), (v_ax1 * T.int32(30) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) % T.int32(960), (v_k4 // T.int32(64) + v_k3) % T.int32(64), v_k4 % T.int32(64)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(120)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(3932160))
                        v_ax1 = T.axis.spatial(T.int32(960), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(3932160) // T.int32(4096))
                        v_ax2 = T.axis.spatial(T.int32(64), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(4096) // T.int32(64))
                        v_ax3 = T.axis.spatial(T.int32(64), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(64))
                        T.reads(lv1501[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(960) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(960), (v_ax3 // T.int32(64) + v_ax2) % T.int32(64), v_ax3 % T.int32(64)], rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(960) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(960) // T.int32(30)], rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(960) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(960) // T.int32(30)], param_0[((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(960)], param_1[((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(960)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv1501[((((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(960) // T.int32(30) * T.int32(30) + (v_ax3 % T.int32(64) // T.int32(64) + (v_ax3 // T.int32(64) + v_ax2) % T.int32(64)) // T.int32(64) + ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(30)) // T.int32(960) + (((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(960) + v_ax0) % T.int32(2)) % T.int32(2), (((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(960) // T.int32(30) * T.int32(30) + (v_ax3 % T.int32(64) // T.int32(64) + (v_ax3 // T.int32(64) + v_ax2) % T.int32(64)) // T.int32(64) + ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(30)) % T.int32(960), (v_ax3 % T.int32(64) // T.int32(64) + (v_ax3 // T.int32(64) + v_ax2) % T.int32(64)) % T.int32(64), v_ax3 % T.int32(64) % T.int32(64)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(960) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(960) // T.int32(30)] * T.float32(8.1380208333333332e-06)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(960) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(960) // T.int32(30)] * T.float32(8.1380208333333332e-06) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(960) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(960) // T.int32(30)] * T.float32(8.1380208333333332e-06) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(960) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(960) // T.int32(30)] * T.float32(8.1380208333333332e-06)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(960) // T.int32(30) * T.int32(30) + ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(30)) % T.int32(960)] + param_1[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(960) // T.int32(30) * T.int32(30) + ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(30)) % T.int32(960)]) * T.sigmoid((lv1501[((((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(960) // T.int32(30) * T.int32(30) + (v_ax3 % T.int32(64) // T.int32(64) + (v_ax3 // T.int32(64) + v_ax2) % T.int32(64)) // T.int32(64) + ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(30)) // T.int32(960) + (((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(960) + v_ax0) % T.int32(2)) % T.int32(2), (((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(960) // T.int32(30) * T.int32(30) + (v_ax3 % T.int32(64) // T.int32(64) + (v_ax3 // T.int32(64) + v_ax2) % T.int32(64)) // T.int32(64) + ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(30)) % T.int32(960), (v_ax3 % T.int32(64) // T.int32(64) + (v_ax3 // T.int32(64) + v_ax2) % T.int32(64)) % T.int32(64), v_ax3 % T.int32(64) % T.int32(64)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(960) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(960) // T.int32(30)] * T.float32(8.1380208333333332e-06)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(960) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(960) // T.int32(30)] * T.float32(8.1380208333333332e-06) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(960) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(960) // T.int32(30)] * T.float32(8.1380208333333332e-06) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(960) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(960) // T.int32(30)] * T.float32(8.1380208333333332e-06)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(960) // T.int32(30) * T.int32(30) + ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(30)) % T.int32(960)] + param_1[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(960) // T.int32(30) * T.int32(30) + ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(30)) % T.int32(960)])

    @T.prim_func
    def fused_group_norm17_silu14(lv1605: T.Buffer((T.int32(2), T.int32(640), T.int32(64), T.int32(64)), "float32"), param_0: T.Buffer((T.int32(640),), "float32"), param_1: T.Buffer((T.int32(640),), "float32"), T_multiply: T.Buffer((T.int32(2), T.int32(640), T.int32(64), T.int32(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int32(2), T.int32(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int32(2), T.int32(32)))
        for ax0_ax1_fused in T.thread_binding(T.int32(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for k2_k3_k4_fused_0 in range(T.int32(1280)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int32(2), ax0_ax1_fused // T.int32(32))
                        v_ax1 = T.axis.spatial(T.int32(32), ax0_ax1_fused % T.int32(32))
                        v_k2 = T.axis.reduce(T.int32(20), (k2_k3_k4_fused_0 * T.int32(64) + k2_k3_k4_fused_1) // T.int32(4096))
                        v_k3 = T.axis.reduce(T.int32(64), (k2_k3_k4_fused_0 * T.int32(64) + k2_k3_k4_fused_1) % T.int32(4096) // T.int32(64))
                        v_k4 = T.axis.reduce(T.int32(64), (k2_k3_k4_fused_0 * T.int32(64) + k2_k3_k4_fused_1) % T.int32(64))
                        T.reads(lv1605[((v_ax1 * T.int32(20) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) // T.int32(640) + v_ax0) % T.int32(2), (v_ax1 * T.int32(20) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) % T.int32(640), (v_k4 // T.int32(64) + v_k3) % T.int32(64), v_k4 % T.int32(64)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv1605[((v_ax1 * T.int32(20) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) // T.int32(640) + v_ax0) % T.int32(2), (v_ax1 * T.int32(20) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) % T.int32(640), (v_k4 // T.int32(64) + v_k3) % T.int32(64), v_k4 % T.int32(64)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv1605[((v_ax1 * T.int32(20) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) // T.int32(640) + v_ax0) % T.int32(2), (v_ax1 * T.int32(20) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) % T.int32(640), (v_k4 // T.int32(64) + v_k3) % T.int32(64), v_k4 % T.int32(64)] * lv1605[((v_ax1 * T.int32(20) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) // T.int32(640) + v_ax0) % T.int32(2), (v_ax1 * T.int32(20) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) % T.int32(640), (v_k4 // T.int32(64) + v_k3) % T.int32(64), v_k4 % T.int32(64)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(80)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(2621440))
                        v_ax1 = T.axis.spatial(T.int32(640), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(2621440) // T.int32(4096))
                        v_ax2 = T.axis.spatial(T.int32(64), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(4096) // T.int32(64))
                        v_ax3 = T.axis.spatial(T.int32(64), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(64))
                        T.reads(lv1605[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(640), (v_ax3 // T.int32(64) + v_ax2) % T.int32(64), v_ax3 % T.int32(64)], rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(640) // T.int32(20)], rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(640) // T.int32(20)], param_0[((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(640)], param_1[((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(640)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv1605[((((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(640) // T.int32(20) * T.int32(20) + (v_ax3 % T.int32(64) // T.int32(64) + (v_ax3 // T.int32(64) + v_ax2) % T.int32(64)) // T.int32(64) + ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(20)) // T.int32(640) + (((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2)) % T.int32(2), (((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(640) // T.int32(20) * T.int32(20) + (v_ax3 % T.int32(64) // T.int32(64) + (v_ax3 // T.int32(64) + v_ax2) % T.int32(64)) // T.int32(64) + ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(20)) % T.int32(640), (v_ax3 % T.int32(64) // T.int32(64) + (v_ax3 // T.int32(64) + v_ax2) % T.int32(64)) % T.int32(64), v_ax3 % T.int32(64) % T.int32(64)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(640) // T.int32(20)] * T.float32(1.2207031250000001e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(640) // T.int32(20)] * T.float32(1.2207031250000001e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(640) // T.int32(20)] * T.float32(1.2207031250000001e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(640) // T.int32(20)] * T.float32(1.2207031250000001e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(640) // T.int32(20) * T.int32(20) + ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(20)) % T.int32(640)] + param_1[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(640) // T.int32(20) * T.int32(20) + ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(20)) % T.int32(640)]) * T.sigmoid((lv1605[((((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(640) // T.int32(20) * T.int32(20) + (v_ax3 % T.int32(64) // T.int32(64) + (v_ax3 // T.int32(64) + v_ax2) % T.int32(64)) // T.int32(64) + ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(20)) // T.int32(640) + (((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2)) % T.int32(2), (((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(640) // T.int32(20) * T.int32(20) + (v_ax3 % T.int32(64) // T.int32(64) + (v_ax3 // T.int32(64) + v_ax2) % T.int32(64)) // T.int32(64) + ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(20)) % T.int32(640), (v_ax3 % T.int32(64) // T.int32(64) + (v_ax3 // T.int32(64) + v_ax2) % T.int32(64)) % T.int32(64), v_ax3 % T.int32(64) % T.int32(64)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(640) // T.int32(20)] * T.float32(1.2207031250000001e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(640) // T.int32(20)] * T.float32(1.2207031250000001e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(640) // T.int32(20)] * T.float32(1.2207031250000001e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(640) // T.int32(20)] * T.float32(1.2207031250000001e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(640) // T.int32(20) * T.int32(20) + ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(20)) % T.int32(640)] + param_1[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(640) // T.int32(20) * T.int32(20) + ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(20)) % T.int32(640)])

    @T.prim_func
    def fused_group_norm18_silu15(lv5: T.Buffer((T.int32(1), T.int32(512), T.int32(64), T.int32(64)), "float32"), param_0: T.Buffer((T.int32(512),), "float32"), param_1: T.Buffer((T.int32(512),), "float32"), T_multiply: T.Buffer((T.int32(1), T.int32(512), T.int32(64), T.int32(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int32(1), T.int32(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int32(1), T.int32(32)))
        for ax0_ax1_fused in T.thread_binding(T.int32(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for k2_k3_k4_fused_0 in range(T.int32(512)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int32(1), T.int32(0))
                        v_ax1 = T.axis.spatial(T.int32(32), ax0_ax1_fused)
                        v_k2 = T.axis.reduce(T.int32(16), (k2_k3_k4_fused_0 * T.int32(128) + k2_k3_k4_fused_1) // T.int32(4096))
                        v_k3 = T.axis.reduce(T.int32(64), (k2_k3_k4_fused_0 * T.int32(128) + k2_k3_k4_fused_1) % T.int32(4096) // T.int32(64))
                        v_k4 = T.axis.reduce(T.int32(64), (k2_k3_k4_fused_0 * T.int32(128) + k2_k3_k4_fused_1) % T.int32(64))
                        T.reads(lv5[T.int32(0), (v_ax1 * T.int32(16) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) % T.int32(512), (v_k4 // T.int32(64) + v_k3) % T.int32(64), v_k4 % T.int32(64)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv5[T.int32(0), (v_ax1 * T.int32(16) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) % T.int32(512), (v_k4 // T.int32(64) + v_k3) % T.int32(64), v_k4 % T.int32(64)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv5[T.int32(0), (v_ax1 * T.int32(16) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) % T.int32(512), (v_k4 // T.int32(64) + v_k3) % T.int32(64), v_k4 % T.int32(64)] * lv5[T.int32(0), (v_ax1 * T.int32(16) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) % T.int32(512), (v_k4 // T.int32(64) + v_k3) % T.int32(64), v_k4 % T.int32(64)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(32)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int32(1), T.int32(0))
                        v_ax1 = T.axis.spatial(T.int32(512), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(4096))
                        v_ax2 = T.axis.spatial(T.int32(64), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(4096) // T.int32(64))
                        v_ax3 = T.axis.spatial(T.int32(64), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(64))
                        T.reads(lv5[T.int32(0), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(512), (v_ax3 // T.int32(64) + v_ax2) % T.int32(64), v_ax3 % T.int32(64)], rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(512) // T.int32(16)], rxplaceholder_red_temp_v1[T.int32(0), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(512) // T.int32(16)], param_0[((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(512)], param_1[((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(512)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv5[T.int32(0), (((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(512) // T.int32(16) * T.int32(16) + (v_ax3 % T.int32(64) // T.int32(64) + (v_ax3 // T.int32(64) + v_ax2) % T.int32(64)) // T.int32(64) + ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(16)) % T.int32(512), (v_ax3 % T.int32(64) // T.int32(64) + (v_ax3 // T.int32(64) + v_ax2) % T.int32(64)) % T.int32(64), v_ax3 % T.int32(64) % T.int32(64)] - rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(512) // T.int32(16)] * T.float32(1.52587890625e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[T.int32(0), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(512) // T.int32(16)] * T.float32(1.52587890625e-05) - rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(512) // T.int32(16)] * T.float32(1.52587890625e-05) * (rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(512) // T.int32(16)] * T.float32(1.52587890625e-05)) + T.float32(9.9999999999999995e-07)) * param_0[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(512) // T.int32(16) * T.int32(16) + ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(16)) % T.int32(512)] + param_1[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(512) // T.int32(16) * T.int32(16) + ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(16)) % T.int32(512)]) * T.sigmoid((lv5[T.int32(0), (((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(512) // T.int32(16) * T.int32(16) + (v_ax3 % T.int32(64) // T.int32(64) + (v_ax3 // T.int32(64) + v_ax2) % T.int32(64)) // T.int32(64) + ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(16)) % T.int32(512), (v_ax3 % T.int32(64) // T.int32(64) + (v_ax3 // T.int32(64) + v_ax2) % T.int32(64)) % T.int32(64), v_ax3 % T.int32(64) % T.int32(64)] - rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(512) // T.int32(16)] * T.float32(1.52587890625e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[T.int32(0), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(512) // T.int32(16)] * T.float32(1.52587890625e-05) - rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(512) // T.int32(16)] * T.float32(1.52587890625e-05) * (rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(512) // T.int32(16)] * T.float32(1.52587890625e-05)) + T.float32(9.9999999999999995e-07)) * param_0[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(512) // T.int32(16) * T.int32(16) + ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(16)) % T.int32(512)] + param_1[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(512) // T.int32(16) * T.int32(16) + ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(16)) % T.int32(512)])

    @T.prim_func
    def fused_group_norm19_silu16(lv107: T.Buffer((T.int32(1), T.int32(512), T.int32(128), T.int32(128)), "float32"), param_0: T.Buffer((T.int32(512),), "float32"), param_1: T.Buffer((T.int32(512),), "float32"), T_multiply: T.Buffer((T.int32(1), T.int32(512), T.int32(128), T.int32(128)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int32(1), T.int32(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int32(1), T.int32(32)))
        for ax0_ax1_fused in T.thread_binding(T.int32(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for k2_k3_k4_fused_0 in range(T.int32(8192)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int32(1), T.int32(0))
                        v_ax1 = T.axis.spatial(T.int32(32), ax0_ax1_fused)
                        v_k2 = T.axis.reduce(T.int32(16), (k2_k3_k4_fused_0 * T.int32(32) + k2_k3_k4_fused_1) // T.int32(16384))
                        v_k3 = T.axis.reduce(T.int32(128), (k2_k3_k4_fused_0 * T.int32(32) + k2_k3_k4_fused_1) % T.int32(16384) // T.int32(128))
                        v_k4 = T.axis.reduce(T.int32(128), (k2_k3_k4_fused_0 * T.int32(32) + k2_k3_k4_fused_1) % T.int32(128))
                        T.reads(lv107[T.int32(0), (v_ax1 * T.int32(16) + (v_k4 // T.int32(128) + v_k3) // T.int32(128) + v_k2) % T.int32(512), (v_k4 // T.int32(128) + v_k3) % T.int32(128), v_k4 % T.int32(128)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv107[T.int32(0), (v_ax1 * T.int32(16) + (v_k4 // T.int32(128) + v_k3) // T.int32(128) + v_k2) % T.int32(512), (v_k4 // T.int32(128) + v_k3) % T.int32(128), v_k4 % T.int32(128)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv107[T.int32(0), (v_ax1 * T.int32(16) + (v_k4 // T.int32(128) + v_k3) // T.int32(128) + v_k2) % T.int32(512), (v_k4 // T.int32(128) + v_k3) % T.int32(128), v_k4 % T.int32(128)] * lv107[T.int32(0), (v_ax1 * T.int32(16) + (v_k4 // T.int32(128) + v_k3) // T.int32(128) + v_k2) % T.int32(512), (v_k4 // T.int32(128) + v_k3) % T.int32(128), v_k4 % T.int32(128)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(128)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int32(1), T.int32(0))
                        v_ax1 = T.axis.spatial(T.int32(512), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(16384))
                        v_ax2 = T.axis.spatial(T.int32(128), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(16384) // T.int32(128))
                        v_ax3 = T.axis.spatial(T.int32(128), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(128))
                        T.reads(lv107[T.int32(0), ((v_ax3 // T.int32(128) + v_ax2) // T.int32(128) + v_ax1) % T.int32(512), (v_ax3 // T.int32(128) + v_ax2) % T.int32(128), v_ax3 % T.int32(128)], rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(128) + v_ax2) // T.int32(128) + v_ax1) % T.int32(512) // T.int32(16)], rxplaceholder_red_temp_v1[T.int32(0), ((v_ax3 // T.int32(128) + v_ax2) // T.int32(128) + v_ax1) % T.int32(512) // T.int32(16)], param_0[((v_ax3 // T.int32(128) + v_ax2) // T.int32(128) + v_ax1) % T.int32(512)], param_1[((v_ax3 // T.int32(128) + v_ax2) // T.int32(128) + v_ax1) % T.int32(512)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv107[T.int32(0), (((v_ax3 // T.int32(128) + v_ax2) // T.int32(128) + v_ax1) % T.int32(512) // T.int32(16) * T.int32(16) + (v_ax3 % T.int32(128) // T.int32(128) + (v_ax3 // T.int32(128) + v_ax2) % T.int32(128)) // T.int32(128) + ((v_ax3 // T.int32(128) + v_ax2) // T.int32(128) + v_ax1) % T.int32(16)) % T.int32(512), (v_ax3 % T.int32(128) // T.int32(128) + (v_ax3 // T.int32(128) + v_ax2) % T.int32(128)) % T.int32(128), v_ax3 % T.int32(128) % T.int32(128)] - rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(128) + v_ax2) // T.int32(128) + v_ax1) % T.int32(512) // T.int32(16)] * T.float32(3.814697265625e-06)) * T.rsqrt(rxplaceholder_red_temp_v1[T.int32(0), ((v_ax3 // T.int32(128) + v_ax2) // T.int32(128) + v_ax1) % T.int32(512) // T.int32(16)] * T.float32(3.814697265625e-06) - rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(128) + v_ax2) // T.int32(128) + v_ax1) % T.int32(512) // T.int32(16)] * T.float32(3.814697265625e-06) * (rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(128) + v_ax2) // T.int32(128) + v_ax1) % T.int32(512) // T.int32(16)] * T.float32(3.814697265625e-06)) + T.float32(9.9999999999999995e-07)) * param_0[(((v_ax3 // T.int32(128) + v_ax2) // T.int32(128) + v_ax1) % T.int32(512) // T.int32(16) * T.int32(16) + ((v_ax3 // T.int32(128) + v_ax2) // T.int32(128) + v_ax1) % T.int32(16)) % T.int32(512)] + param_1[(((v_ax3 // T.int32(128) + v_ax2) // T.int32(128) + v_ax1) % T.int32(512) // T.int32(16) * T.int32(16) + ((v_ax3 // T.int32(128) + v_ax2) // T.int32(128) + v_ax1) % T.int32(16)) % T.int32(512)]) * T.sigmoid((lv107[T.int32(0), (((v_ax3 // T.int32(128) + v_ax2) // T.int32(128) + v_ax1) % T.int32(512) // T.int32(16) * T.int32(16) + (v_ax3 % T.int32(128) // T.int32(128) + (v_ax3 // T.int32(128) + v_ax2) % T.int32(128)) // T.int32(128) + ((v_ax3 // T.int32(128) + v_ax2) // T.int32(128) + v_ax1) % T.int32(16)) % T.int32(512), (v_ax3 % T.int32(128) // T.int32(128) + (v_ax3 // T.int32(128) + v_ax2) % T.int32(128)) % T.int32(128), v_ax3 % T.int32(128) % T.int32(128)] - rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(128) + v_ax2) // T.int32(128) + v_ax1) % T.int32(512) // T.int32(16)] * T.float32(3.814697265625e-06)) * T.rsqrt(rxplaceholder_red_temp_v1[T.int32(0), ((v_ax3 // T.int32(128) + v_ax2) // T.int32(128) + v_ax1) % T.int32(512) // T.int32(16)] * T.float32(3.814697265625e-06) - rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(128) + v_ax2) // T.int32(128) + v_ax1) % T.int32(512) // T.int32(16)] * T.float32(3.814697265625e-06) * (rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(128) + v_ax2) // T.int32(128) + v_ax1) % T.int32(512) // T.int32(16)] * T.float32(3.814697265625e-06)) + T.float32(9.9999999999999995e-07)) * param_0[(((v_ax3 // T.int32(128) + v_ax2) // T.int32(128) + v_ax1) % T.int32(512) // T.int32(16) * T.int32(16) + ((v_ax3 // T.int32(128) + v_ax2) // T.int32(128) + v_ax1) % T.int32(16)) % T.int32(512)] + param_1[(((v_ax3 // T.int32(128) + v_ax2) // T.int32(128) + v_ax1) % T.int32(512) // T.int32(16) * T.int32(16) + ((v_ax3 // T.int32(128) + v_ax2) // T.int32(128) + v_ax1) % T.int32(16)) % T.int32(512)])

    @T.prim_func
    def fused_group_norm20_silu17(lv147: T.Buffer((T.int32(1), T.int32(512), T.int32(256), T.int32(256)), "float32"), param_0: T.Buffer((T.int32(512),), "float32"), param_1: T.Buffer((T.int32(512),), "float32"), T_multiply: T.Buffer((T.int32(1), T.int32(512), T.int32(256), T.int32(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int32(1), T.int32(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int32(1), T.int32(32)))
        for ax0_ax1_fused in T.thread_binding(T.int32(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for k2_k3_k4_fused_0 in range(T.int32(32768)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int32(1), T.int32(0))
                        v_ax1 = T.axis.spatial(T.int32(32), ax0_ax1_fused)
                        v_k2 = T.axis.reduce(T.int32(16), (k2_k3_k4_fused_0 * T.int32(32) + k2_k3_k4_fused_1) // T.int32(65536))
                        v_k3 = T.axis.reduce(T.int32(256), (k2_k3_k4_fused_0 * T.int32(32) + k2_k3_k4_fused_1) % T.int32(65536) // T.int32(256))
                        v_k4 = T.axis.reduce(T.int32(256), (k2_k3_k4_fused_0 * T.int32(32) + k2_k3_k4_fused_1) % T.int32(256))
                        T.reads(lv147[T.int32(0), (v_ax1 * T.int32(16) + (v_k4 // T.int32(256) + v_k3) // T.int32(256) + v_k2) % T.int32(512), (v_k4 // T.int32(256) + v_k3) % T.int32(256), v_k4 % T.int32(256)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv147[T.int32(0), (v_ax1 * T.int32(16) + (v_k4 // T.int32(256) + v_k3) // T.int32(256) + v_k2) % T.int32(512), (v_k4 // T.int32(256) + v_k3) % T.int32(256), v_k4 % T.int32(256)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv147[T.int32(0), (v_ax1 * T.int32(16) + (v_k4 // T.int32(256) + v_k3) // T.int32(256) + v_k2) % T.int32(512), (v_k4 // T.int32(256) + v_k3) % T.int32(256), v_k4 % T.int32(256)] * lv147[T.int32(0), (v_ax1 * T.int32(16) + (v_k4 // T.int32(256) + v_k3) // T.int32(256) + v_k2) % T.int32(512), (v_k4 // T.int32(256) + v_k3) % T.int32(256), v_k4 % T.int32(256)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(512)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int32(1), T.int32(0))
                        v_ax1 = T.axis.spatial(T.int32(512), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(65536))
                        v_ax2 = T.axis.spatial(T.int32(256), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(65536) // T.int32(256))
                        v_ax3 = T.axis.spatial(T.int32(256), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(256))
                        T.reads(lv147[T.int32(0), ((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(512), (v_ax3 // T.int32(256) + v_ax2) % T.int32(256), v_ax3 % T.int32(256)], rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(512) // T.int32(16)], rxplaceholder_red_temp_v1[T.int32(0), ((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(512) // T.int32(16)], param_0[((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(512)], param_1[((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(512)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv147[T.int32(0), (((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(512) // T.int32(16) * T.int32(16) + (v_ax3 % T.int32(256) // T.int32(256) + (v_ax3 // T.int32(256) + v_ax2) % T.int32(256)) // T.int32(256) + ((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(16)) % T.int32(512), (v_ax3 % T.int32(256) // T.int32(256) + (v_ax3 // T.int32(256) + v_ax2) % T.int32(256)) % T.int32(256), v_ax3 % T.int32(256) % T.int32(256)] - rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(512) // T.int32(16)] * T.float32(9.5367431640625e-07)) * T.rsqrt(rxplaceholder_red_temp_v1[T.int32(0), ((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(512) // T.int32(16)] * T.float32(9.5367431640625e-07) - rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(512) // T.int32(16)] * T.float32(9.5367431640625e-07) * (rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(512) // T.int32(16)] * T.float32(9.5367431640625e-07)) + T.float32(9.9999999999999995e-07)) * param_0[(((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(512) // T.int32(16) * T.int32(16) + ((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(16)) % T.int32(512)] + param_1[(((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(512) // T.int32(16) * T.int32(16) + ((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(16)) % T.int32(512)]) * T.sigmoid((lv147[T.int32(0), (((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(512) // T.int32(16) * T.int32(16) + (v_ax3 % T.int32(256) // T.int32(256) + (v_ax3 // T.int32(256) + v_ax2) % T.int32(256)) // T.int32(256) + ((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(16)) % T.int32(512), (v_ax3 % T.int32(256) // T.int32(256) + (v_ax3 // T.int32(256) + v_ax2) % T.int32(256)) % T.int32(256), v_ax3 % T.int32(256) % T.int32(256)] - rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(512) // T.int32(16)] * T.float32(9.5367431640625e-07)) * T.rsqrt(rxplaceholder_red_temp_v1[T.int32(0), ((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(512) // T.int32(16)] * T.float32(9.5367431640625e-07) - rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(512) // T.int32(16)] * T.float32(9.5367431640625e-07) * (rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(512) // T.int32(16)] * T.float32(9.5367431640625e-07)) + T.float32(9.9999999999999995e-07)) * param_0[(((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(512) // T.int32(16) * T.int32(16) + ((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(16)) % T.int32(512)] + param_1[(((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(512) // T.int32(16) * T.int32(16) + ((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(16)) % T.int32(512)])

    @T.prim_func
    def fused_group_norm21_silu18(lv152: T.Buffer((T.int32(1), T.int32(256), T.int32(256), T.int32(256)), "float32"), param_0: T.Buffer((T.int32(256),), "float32"), param_1: T.Buffer((T.int32(256),), "float32"), T_multiply: T.Buffer((T.int32(1), T.int32(256), T.int32(256), T.int32(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int32(1), T.int32(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int32(1), T.int32(32)))
        for ax0_ax1_fused in T.thread_binding(T.int32(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for k2_k3_k4_fused_0 in range(T.int32(16384)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int32(1), T.int32(0))
                        v_ax1 = T.axis.spatial(T.int32(32), ax0_ax1_fused)
                        v_k2 = T.axis.reduce(T.int32(8), (k2_k3_k4_fused_0 * T.int32(32) + k2_k3_k4_fused_1) // T.int32(65536))
                        v_k3 = T.axis.reduce(T.int32(256), (k2_k3_k4_fused_0 * T.int32(32) + k2_k3_k4_fused_1) % T.int32(65536) // T.int32(256))
                        v_k4 = T.axis.reduce(T.int32(256), (k2_k3_k4_fused_0 * T.int32(32) + k2_k3_k4_fused_1) % T.int32(256))
                        T.reads(lv152[T.int32(0), (v_ax1 * T.int32(8) + (v_k4 // T.int32(256) + v_k3) // T.int32(256) + v_k2) % T.int32(256), (v_k4 // T.int32(256) + v_k3) % T.int32(256), v_k4 % T.int32(256)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv152[T.int32(0), (v_ax1 * T.int32(8) + (v_k4 // T.int32(256) + v_k3) // T.int32(256) + v_k2) % T.int32(256), (v_k4 // T.int32(256) + v_k3) % T.int32(256), v_k4 % T.int32(256)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv152[T.int32(0), (v_ax1 * T.int32(8) + (v_k4 // T.int32(256) + v_k3) // T.int32(256) + v_k2) % T.int32(256), (v_k4 // T.int32(256) + v_k3) % T.int32(256), v_k4 % T.int32(256)] * lv152[T.int32(0), (v_ax1 * T.int32(8) + (v_k4 // T.int32(256) + v_k3) // T.int32(256) + v_k2) % T.int32(256), (v_k4 // T.int32(256) + v_k3) % T.int32(256), v_k4 % T.int32(256)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(256)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int32(1), T.int32(0))
                        v_ax1 = T.axis.spatial(T.int32(256), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(65536))
                        v_ax2 = T.axis.spatial(T.int32(256), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(65536) // T.int32(256))
                        v_ax3 = T.axis.spatial(T.int32(256), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(256))
                        T.reads(lv152[T.int32(0), ((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(256), (v_ax3 // T.int32(256) + v_ax2) % T.int32(256), v_ax3 % T.int32(256)], rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(256) // T.int32(8)], rxplaceholder_red_temp_v1[T.int32(0), ((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(256) // T.int32(8)], param_0[((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(256)], param_1[((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(256)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv152[T.int32(0), (((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(256) // T.int32(8) * T.int32(8) + (v_ax3 % T.int32(256) // T.int32(256) + (v_ax3 // T.int32(256) + v_ax2) % T.int32(256)) // T.int32(256) + ((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(8)) % T.int32(256), (v_ax3 % T.int32(256) // T.int32(256) + (v_ax3 // T.int32(256) + v_ax2) % T.int32(256)) % T.int32(256), v_ax3 % T.int32(256) % T.int32(256)] - rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(256) // T.int32(8)] * T.float32(1.9073486328125e-06)) * T.rsqrt(rxplaceholder_red_temp_v1[T.int32(0), ((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(256) // T.int32(8)] * T.float32(1.9073486328125e-06) - rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(256) // T.int32(8)] * T.float32(1.9073486328125e-06) * (rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(256) // T.int32(8)] * T.float32(1.9073486328125e-06)) + T.float32(9.9999999999999995e-07)) * param_0[(((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(256) // T.int32(8) * T.int32(8) + ((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(8)) % T.int32(256)] + param_1[(((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(256) // T.int32(8) * T.int32(8) + ((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(8)) % T.int32(256)]) * T.sigmoid((lv152[T.int32(0), (((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(256) // T.int32(8) * T.int32(8) + (v_ax3 % T.int32(256) // T.int32(256) + (v_ax3 // T.int32(256) + v_ax2) % T.int32(256)) // T.int32(256) + ((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(8)) % T.int32(256), (v_ax3 % T.int32(256) // T.int32(256) + (v_ax3 // T.int32(256) + v_ax2) % T.int32(256)) % T.int32(256), v_ax3 % T.int32(256) % T.int32(256)] - rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(256) // T.int32(8)] * T.float32(1.9073486328125e-06)) * T.rsqrt(rxplaceholder_red_temp_v1[T.int32(0), ((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(256) // T.int32(8)] * T.float32(1.9073486328125e-06) - rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(256) // T.int32(8)] * T.float32(1.9073486328125e-06) * (rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(256) // T.int32(8)] * T.float32(1.9073486328125e-06)) + T.float32(9.9999999999999995e-07)) * param_0[(((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(256) // T.int32(8) * T.int32(8) + ((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(8)) % T.int32(256)] + param_1[(((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(256) // T.int32(8) * T.int32(8) + ((v_ax3 // T.int32(256) + v_ax2) // T.int32(256) + v_ax1) % T.int32(8)) % T.int32(256)])

    @T.prim_func
    def fused_group_norm22_silu19(lv190: T.Buffer((T.int32(1), T.int32(256), T.int32(512), T.int32(512)), "float32"), param_0: T.Buffer((T.int32(256),), "float32"), param_1: T.Buffer((T.int32(256),), "float32"), T_multiply: T.Buffer((T.int32(1), T.int32(256), T.int32(512), T.int32(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int32(1), T.int32(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int32(1), T.int32(32)))
        for ax0_ax1_fused in T.thread_binding(T.int32(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for k2_k3_k4_fused_0 in range(T.int32(16384)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int32(1), T.int32(0))
                        v_ax1 = T.axis.spatial(T.int32(32), ax0_ax1_fused)
                        v_k2 = T.axis.reduce(T.int32(8), (k2_k3_k4_fused_0 * T.int32(128) + k2_k3_k4_fused_1) // T.int32(262144))
                        v_k3 = T.axis.reduce(T.int32(512), (k2_k3_k4_fused_0 * T.int32(128) + k2_k3_k4_fused_1) % T.int32(262144) // T.int32(512))
                        v_k4 = T.axis.reduce(T.int32(512), (k2_k3_k4_fused_0 * T.int32(128) + k2_k3_k4_fused_1) % T.int32(512))
                        T.reads(lv190[T.int32(0), (v_ax1 * T.int32(8) + (v_k4 // T.int32(512) + v_k3) // T.int32(512) + v_k2) % T.int32(256), (v_k4 // T.int32(512) + v_k3) % T.int32(512), v_k4 % T.int32(512)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv190[T.int32(0), (v_ax1 * T.int32(8) + (v_k4 // T.int32(512) + v_k3) // T.int32(512) + v_k2) % T.int32(256), (v_k4 // T.int32(512) + v_k3) % T.int32(512), v_k4 % T.int32(512)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv190[T.int32(0), (v_ax1 * T.int32(8) + (v_k4 // T.int32(512) + v_k3) // T.int32(512) + v_k2) % T.int32(256), (v_k4 // T.int32(512) + v_k3) % T.int32(512), v_k4 % T.int32(512)] * lv190[T.int32(0), (v_ax1 * T.int32(8) + (v_k4 // T.int32(512) + v_k3) // T.int32(512) + v_k2) % T.int32(256), (v_k4 // T.int32(512) + v_k3) % T.int32(512), v_k4 % T.int32(512)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(1024)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int32(1), T.int32(0))
                        v_ax1 = T.axis.spatial(T.int32(256), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(262144))
                        v_ax2 = T.axis.spatial(T.int32(512), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(262144) // T.int32(512))
                        v_ax3 = T.axis.spatial(T.int32(512), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(512))
                        T.reads(lv190[T.int32(0), ((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(256), (v_ax3 // T.int32(512) + v_ax2) % T.int32(512), v_ax3 % T.int32(512)], rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(256) // T.int32(8)], rxplaceholder_red_temp_v1[T.int32(0), ((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(256) // T.int32(8)], param_0[((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(256)], param_1[((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(256)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv190[T.int32(0), (((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(256) // T.int32(8) * T.int32(8) + (v_ax3 % T.int32(512) // T.int32(512) + (v_ax3 // T.int32(512) + v_ax2) % T.int32(512)) // T.int32(512) + ((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(8)) % T.int32(256), (v_ax3 % T.int32(512) // T.int32(512) + (v_ax3 // T.int32(512) + v_ax2) % T.int32(512)) % T.int32(512), v_ax3 % T.int32(512) % T.int32(512)] - rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(256) // T.int32(8)] * T.float32(4.76837158203125e-07)) * T.rsqrt(rxplaceholder_red_temp_v1[T.int32(0), ((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(256) // T.int32(8)] * T.float32(4.76837158203125e-07) - rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(256) // T.int32(8)] * T.float32(4.76837158203125e-07) * (rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(256) // T.int32(8)] * T.float32(4.76837158203125e-07)) + T.float32(9.9999999999999995e-07)) * param_0[(((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(256) // T.int32(8) * T.int32(8) + ((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(8)) % T.int32(256)] + param_1[(((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(256) // T.int32(8) * T.int32(8) + ((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(8)) % T.int32(256)]) * T.sigmoid((lv190[T.int32(0), (((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(256) // T.int32(8) * T.int32(8) + (v_ax3 % T.int32(512) // T.int32(512) + (v_ax3 // T.int32(512) + v_ax2) % T.int32(512)) // T.int32(512) + ((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(8)) % T.int32(256), (v_ax3 % T.int32(512) // T.int32(512) + (v_ax3 // T.int32(512) + v_ax2) % T.int32(512)) % T.int32(512), v_ax3 % T.int32(512) % T.int32(512)] - rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(256) // T.int32(8)] * T.float32(4.76837158203125e-07)) * T.rsqrt(rxplaceholder_red_temp_v1[T.int32(0), ((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(256) // T.int32(8)] * T.float32(4.76837158203125e-07) - rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(256) // T.int32(8)] * T.float32(4.76837158203125e-07) * (rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(256) // T.int32(8)] * T.float32(4.76837158203125e-07)) + T.float32(9.9999999999999995e-07)) * param_0[(((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(256) // T.int32(8) * T.int32(8) + ((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(8)) % T.int32(256)] + param_1[(((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(256) // T.int32(8) * T.int32(8) + ((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(8)) % T.int32(256)])

    @T.prim_func
    def fused_group_norm23_silu20(lv195: T.Buffer((T.int32(1), T.int32(128), T.int32(512), T.int32(512)), "float32"), param_0: T.Buffer((T.int32(128),), "float32"), param_1: T.Buffer((T.int32(128),), "float32"), T_multiply: T.Buffer((T.int32(1), T.int32(128), T.int32(512), T.int32(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int32(1), T.int32(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int32(1), T.int32(32)))
        for ax0_ax1_fused in T.thread_binding(T.int32(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for k2_k3_k4_fused_0 in range(T.int32(32768)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int32(1), T.int32(0))
                        v_ax1 = T.axis.spatial(T.int32(32), ax0_ax1_fused)
                        v_k2 = T.axis.reduce(T.int32(4), (k2_k3_k4_fused_0 * T.int32(32) + k2_k3_k4_fused_1) // T.int32(262144))
                        v_k3 = T.axis.reduce(T.int32(512), (k2_k3_k4_fused_0 * T.int32(32) + k2_k3_k4_fused_1) % T.int32(262144) // T.int32(512))
                        v_k4 = T.axis.reduce(T.int32(512), (k2_k3_k4_fused_0 * T.int32(32) + k2_k3_k4_fused_1) % T.int32(512))
                        T.reads(lv195[T.int32(0), (v_ax1 * T.int32(4) + (v_k4 // T.int32(512) + v_k3) // T.int32(512) + v_k2) % T.int32(128), (v_k4 // T.int32(512) + v_k3) % T.int32(512), v_k4 % T.int32(512)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv195[T.int32(0), (v_ax1 * T.int32(4) + (v_k4 // T.int32(512) + v_k3) // T.int32(512) + v_k2) % T.int32(128), (v_k4 // T.int32(512) + v_k3) % T.int32(512), v_k4 % T.int32(512)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv195[T.int32(0), (v_ax1 * T.int32(4) + (v_k4 // T.int32(512) + v_k3) // T.int32(512) + v_k2) % T.int32(128), (v_k4 // T.int32(512) + v_k3) % T.int32(512), v_k4 % T.int32(512)] * lv195[T.int32(0), (v_ax1 * T.int32(4) + (v_k4 // T.int32(512) + v_k3) // T.int32(512) + v_k2) % T.int32(128), (v_k4 // T.int32(512) + v_k3) % T.int32(512), v_k4 % T.int32(512)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(512)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int32(1), T.int32(0))
                        v_ax1 = T.axis.spatial(T.int32(128), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(262144))
                        v_ax2 = T.axis.spatial(T.int32(512), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(262144) // T.int32(512))
                        v_ax3 = T.axis.spatial(T.int32(512), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(512))
                        T.reads(lv195[T.int32(0), ((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(128), (v_ax3 // T.int32(512) + v_ax2) % T.int32(512), v_ax3 % T.int32(512)], rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(128) // T.int32(4)], rxplaceholder_red_temp_v1[T.int32(0), ((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(128) // T.int32(4)], param_0[((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(128)], param_1[((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(128)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv195[T.int32(0), (((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(128) // T.int32(4) * T.int32(4) + (v_ax3 % T.int32(512) // T.int32(512) + (v_ax3 // T.int32(512) + v_ax2) % T.int32(512)) // T.int32(512) + ((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(4)) % T.int32(128), (v_ax3 % T.int32(512) // T.int32(512) + (v_ax3 // T.int32(512) + v_ax2) % T.int32(512)) % T.int32(512), v_ax3 % T.int32(512) % T.int32(512)] - rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(128) // T.int32(4)] * T.float32(9.5367431640625e-07)) * T.rsqrt(rxplaceholder_red_temp_v1[T.int32(0), ((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(128) // T.int32(4)] * T.float32(9.5367431640625e-07) - rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(128) // T.int32(4)] * T.float32(9.5367431640625e-07) * (rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(128) // T.int32(4)] * T.float32(9.5367431640625e-07)) + T.float32(9.9999999999999995e-07)) * param_0[(((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(128) // T.int32(4) * T.int32(4) + ((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(4)) % T.int32(128)] + param_1[(((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(128) // T.int32(4) * T.int32(4) + ((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(4)) % T.int32(128)]) * T.sigmoid((lv195[T.int32(0), (((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(128) // T.int32(4) * T.int32(4) + (v_ax3 % T.int32(512) // T.int32(512) + (v_ax3 // T.int32(512) + v_ax2) % T.int32(512)) // T.int32(512) + ((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(4)) % T.int32(128), (v_ax3 % T.int32(512) // T.int32(512) + (v_ax3 // T.int32(512) + v_ax2) % T.int32(512)) % T.int32(512), v_ax3 % T.int32(512) % T.int32(512)] - rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(128) // T.int32(4)] * T.float32(9.5367431640625e-07)) * T.rsqrt(rxplaceholder_red_temp_v1[T.int32(0), ((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(128) // T.int32(4)] * T.float32(9.5367431640625e-07) - rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(128) // T.int32(4)] * T.float32(9.5367431640625e-07) * (rxplaceholder_red_temp_v0[T.int32(0), ((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(128) // T.int32(4)] * T.float32(9.5367431640625e-07)) + T.float32(9.9999999999999995e-07)) * param_0[(((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(128) // T.int32(4) * T.int32(4) + ((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(4)) % T.int32(128)] + param_1[(((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(128) // T.int32(4) * T.int32(4) + ((v_ax3 // T.int32(512) + v_ax2) // T.int32(512) + v_ax1) % T.int32(4)) % T.int32(128)])

    @T.prim_func
    def fused_group_norm2_silu2(lv226: T.Buffer((T.int32(2), T.int32(320), T.int32(32), T.int32(32)), "float32"), param_0: T.Buffer((T.int32(320),), "float32"), param_1: T.Buffer((T.int32(320),), "float32"), T_multiply: T.Buffer((T.int32(2), T.int32(320), T.int32(32), T.int32(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int32(2), T.int32(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int32(2), T.int32(32)))
        for ax0_ax1_fused in T.thread_binding(T.int32(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for k2_k3_k4_fused_0 in range(T.int32(80)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int32(2), ax0_ax1_fused // T.int32(32))
                        v_ax1 = T.axis.spatial(T.int32(32), ax0_ax1_fused % T.int32(32))
                        v_k2 = T.axis.reduce(T.int32(10), (k2_k3_k4_fused_0 * T.int32(128) + k2_k3_k4_fused_1) // T.int32(1024))
                        v_k3 = T.axis.reduce(T.int32(32), (k2_k3_k4_fused_0 * T.int32(128) + k2_k3_k4_fused_1) % T.int32(1024) // T.int32(32))
                        v_k4 = T.axis.reduce(T.int32(32), (k2_k3_k4_fused_0 * T.int32(128) + k2_k3_k4_fused_1) % T.int32(32))
                        T.reads(lv226[((v_ax1 * T.int32(10) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) // T.int32(320) + v_ax0) % T.int32(2), (v_ax1 * T.int32(10) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) % T.int32(320), (v_k4 // T.int32(32) + v_k3) % T.int32(32), v_k4 % T.int32(32)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv226[((v_ax1 * T.int32(10) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) // T.int32(320) + v_ax0) % T.int32(2), (v_ax1 * T.int32(10) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) % T.int32(320), (v_k4 // T.int32(32) + v_k3) % T.int32(32), v_k4 % T.int32(32)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv226[((v_ax1 * T.int32(10) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) // T.int32(320) + v_ax0) % T.int32(2), (v_ax1 * T.int32(10) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) % T.int32(320), (v_k4 // T.int32(32) + v_k3) % T.int32(32), v_k4 % T.int32(32)] * lv226[((v_ax1 * T.int32(10) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) // T.int32(320) + v_ax0) % T.int32(2), (v_ax1 * T.int32(10) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) % T.int32(320), (v_k4 // T.int32(32) + v_k3) % T.int32(32), v_k4 % T.int32(32)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(10)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(327680))
                        v_ax1 = T.axis.spatial(T.int32(320), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(327680) // T.int32(1024))
                        v_ax2 = T.axis.spatial(T.int32(32), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(1024) // T.int32(32))
                        v_ax3 = T.axis.spatial(T.int32(32), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(32))
                        T.reads(lv226[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(320) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(320), (v_ax3 // T.int32(32) + v_ax2) % T.int32(32), v_ax3 % T.int32(32)], rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(320) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(320) // T.int32(10)], rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(320) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(320) // T.int32(10)], param_0[((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(320)], param_1[((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(320)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv226[((((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(320) // T.int32(10) * T.int32(10) + (v_ax3 % T.int32(32) // T.int32(32) + (v_ax3 // T.int32(32) + v_ax2) % T.int32(32)) // T.int32(32) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(10)) // T.int32(320) + (((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(320) + v_ax0) % T.int32(2)) % T.int32(2), (((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(320) // T.int32(10) * T.int32(10) + (v_ax3 % T.int32(32) // T.int32(32) + (v_ax3 // T.int32(32) + v_ax2) % T.int32(32)) // T.int32(32) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(10)) % T.int32(320), (v_ax3 % T.int32(32) // T.int32(32) + (v_ax3 // T.int32(32) + v_ax2) % T.int32(32)) % T.int32(32), v_ax3 % T.int32(32) % T.int32(32)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(320) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(320) // T.int32(10)] * T.float32(9.7656250000000005e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(320) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(320) // T.int32(10)] * T.float32(9.7656250000000005e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(320) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(320) // T.int32(10)] * T.float32(9.7656250000000005e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(320) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(320) // T.int32(10)] * T.float32(9.7656250000000005e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(320) // T.int32(10) * T.int32(10) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(10)) % T.int32(320)] + param_1[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(320) // T.int32(10) * T.int32(10) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(10)) % T.int32(320)]) * T.sigmoid((lv226[((((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(320) // T.int32(10) * T.int32(10) + (v_ax3 % T.int32(32) // T.int32(32) + (v_ax3 // T.int32(32) + v_ax2) % T.int32(32)) // T.int32(32) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(10)) // T.int32(320) + (((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(320) + v_ax0) % T.int32(2)) % T.int32(2), (((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(320) // T.int32(10) * T.int32(10) + (v_ax3 % T.int32(32) // T.int32(32) + (v_ax3 // T.int32(32) + v_ax2) % T.int32(32)) // T.int32(32) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(10)) % T.int32(320), (v_ax3 % T.int32(32) // T.int32(32) + (v_ax3 // T.int32(32) + v_ax2) % T.int32(32)) % T.int32(32), v_ax3 % T.int32(32) % T.int32(32)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(320) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(320) // T.int32(10)] * T.float32(9.7656250000000005e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(320) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(320) // T.int32(10)] * T.float32(9.7656250000000005e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(320) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(320) // T.int32(10)] * T.float32(9.7656250000000005e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(320) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(320) // T.int32(10)] * T.float32(9.7656250000000005e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(320) // T.int32(10) * T.int32(10) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(10)) % T.int32(320)] + param_1[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(320) // T.int32(10) * T.int32(10) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(10)) % T.int32(320)])

    @T.prim_func
    def fused_group_norm3_silu3(lv238: T.Buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), "float32"), param_0: T.Buffer((T.int32(640),), "float32"), param_1: T.Buffer((T.int32(640),), "float32"), T_multiply: T.Buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int32(2), T.int32(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int32(2), T.int32(32)))
        for ax0_ax1_fused in T.thread_binding(T.int32(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for k2_k3_k4_fused_0 in range(T.int32(320)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int32(2), ax0_ax1_fused // T.int32(32))
                        v_ax1 = T.axis.spatial(T.int32(32), ax0_ax1_fused % T.int32(32))
                        v_k2 = T.axis.reduce(T.int32(20), (k2_k3_k4_fused_0 * T.int32(64) + k2_k3_k4_fused_1) // T.int32(1024))
                        v_k3 = T.axis.reduce(T.int32(32), (k2_k3_k4_fused_0 * T.int32(64) + k2_k3_k4_fused_1) % T.int32(1024) // T.int32(32))
                        v_k4 = T.axis.reduce(T.int32(32), (k2_k3_k4_fused_0 * T.int32(64) + k2_k3_k4_fused_1) % T.int32(32))
                        T.reads(lv238[((v_ax1 * T.int32(20) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) // T.int32(640) + v_ax0) % T.int32(2), (v_ax1 * T.int32(20) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) % T.int32(640), (v_k4 // T.int32(32) + v_k3) % T.int32(32), v_k4 % T.int32(32)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv238[((v_ax1 * T.int32(20) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) // T.int32(640) + v_ax0) % T.int32(2), (v_ax1 * T.int32(20) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) % T.int32(640), (v_k4 // T.int32(32) + v_k3) % T.int32(32), v_k4 % T.int32(32)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv238[((v_ax1 * T.int32(20) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) // T.int32(640) + v_ax0) % T.int32(2), (v_ax1 * T.int32(20) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) % T.int32(640), (v_k4 // T.int32(32) + v_k3) % T.int32(32), v_k4 % T.int32(32)] * lv238[((v_ax1 * T.int32(20) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) // T.int32(640) + v_ax0) % T.int32(2), (v_ax1 * T.int32(20) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) % T.int32(640), (v_k4 // T.int32(32) + v_k3) % T.int32(32), v_k4 % T.int32(32)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(20)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(655360))
                        v_ax1 = T.axis.spatial(T.int32(640), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(655360) // T.int32(1024))
                        v_ax2 = T.axis.spatial(T.int32(32), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(1024) // T.int32(32))
                        v_ax3 = T.axis.spatial(T.int32(32), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(32))
                        T.reads(lv238[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(640), (v_ax3 // T.int32(32) + v_ax2) % T.int32(32), v_ax3 % T.int32(32)], rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(640) // T.int32(20)], rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(640) // T.int32(20)], param_0[((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(640)], param_1[((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(640)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv238[((((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(640) // T.int32(20) * T.int32(20) + (v_ax3 % T.int32(32) // T.int32(32) + (v_ax3 // T.int32(32) + v_ax2) % T.int32(32)) // T.int32(32) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(20)) // T.int32(640) + (((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2)) % T.int32(2), (((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(640) // T.int32(20) * T.int32(20) + (v_ax3 % T.int32(32) // T.int32(32) + (v_ax3 // T.int32(32) + v_ax2) % T.int32(32)) // T.int32(32) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(20)) % T.int32(640), (v_ax3 % T.int32(32) // T.int32(32) + (v_ax3 // T.int32(32) + v_ax2) % T.int32(32)) % T.int32(32), v_ax3 % T.int32(32) % T.int32(32)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(640) // T.int32(20)] * T.float32(4.8828125000000003e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(640) // T.int32(20)] * T.float32(4.8828125000000003e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(640) // T.int32(20)] * T.float32(4.8828125000000003e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(640) // T.int32(20)] * T.float32(4.8828125000000003e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(640) // T.int32(20) * T.int32(20) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(20)) % T.int32(640)] + param_1[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(640) // T.int32(20) * T.int32(20) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(20)) % T.int32(640)]) * T.sigmoid((lv238[((((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(640) // T.int32(20) * T.int32(20) + (v_ax3 % T.int32(32) // T.int32(32) + (v_ax3 // T.int32(32) + v_ax2) % T.int32(32)) // T.int32(32) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(20)) // T.int32(640) + (((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2)) % T.int32(2), (((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(640) // T.int32(20) * T.int32(20) + (v_ax3 % T.int32(32) // T.int32(32) + (v_ax3 // T.int32(32) + v_ax2) % T.int32(32)) // T.int32(32) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(20)) % T.int32(640), (v_ax3 % T.int32(32) // T.int32(32) + (v_ax3 // T.int32(32) + v_ax2) % T.int32(32)) % T.int32(32), v_ax3 % T.int32(32) % T.int32(32)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(640) // T.int32(20)] * T.float32(4.8828125000000003e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(640) // T.int32(20)] * T.float32(4.8828125000000003e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(640) // T.int32(20)] * T.float32(4.8828125000000003e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(640) // T.int32(20)] * T.float32(4.8828125000000003e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(640) // T.int32(20) * T.int32(20) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(20)) % T.int32(640)] + param_1[(((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(640) // T.int32(20) * T.int32(20) + ((v_ax3 // T.int32(32) + v_ax2) // T.int32(32) + v_ax1) % T.int32(20)) % T.int32(640)])

    @T.prim_func
    def fused_group_norm5_silu4(lv432: T.Buffer((T.int32(2), T.int32(640), T.int32(16), T.int32(16)), "float32"), param_0: T.Buffer((T.int32(640),), "float32"), param_1: T.Buffer((T.int32(640),), "float32"), T_multiply: T.Buffer((T.int32(2), T.int32(640), T.int32(16), T.int32(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int32(2), T.int32(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int32(2), T.int32(32)))
        for ax0_ax1_fused in T.thread_binding(T.int32(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for k2_k3_k4_fused_0 in range(T.int32(20)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int32(2), ax0_ax1_fused // T.int32(32))
                        v_ax1 = T.axis.spatial(T.int32(32), ax0_ax1_fused % T.int32(32))
                        v_k2 = T.axis.reduce(T.int32(20), (k2_k3_k4_fused_0 * T.int32(256) + k2_k3_k4_fused_1) // T.int32(256))
                        v_k3 = T.axis.reduce(T.int32(16), (k2_k3_k4_fused_0 * T.int32(256) + k2_k3_k4_fused_1) % T.int32(256) // T.int32(16))
                        v_k4 = T.axis.reduce(T.int32(16), (k2_k3_k4_fused_0 * T.int32(256) + k2_k3_k4_fused_1) % T.int32(16))
                        T.reads(lv432[((v_ax1 * T.int32(20) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) // T.int32(640) + v_ax0) % T.int32(2), (v_ax1 * T.int32(20) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) % T.int32(640), (v_k4 // T.int32(16) + v_k3) % T.int32(16), v_k4 % T.int32(16)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv432[((v_ax1 * T.int32(20) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) // T.int32(640) + v_ax0) % T.int32(2), (v_ax1 * T.int32(20) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) % T.int32(640), (v_k4 // T.int32(16) + v_k3) % T.int32(16), v_k4 % T.int32(16)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv432[((v_ax1 * T.int32(20) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) // T.int32(640) + v_ax0) % T.int32(2), (v_ax1 * T.int32(20) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) % T.int32(640), (v_k4 // T.int32(16) + v_k3) % T.int32(16), v_k4 % T.int32(16)] * lv432[((v_ax1 * T.int32(20) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) // T.int32(640) + v_ax0) % T.int32(2), (v_ax1 * T.int32(20) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) % T.int32(640), (v_k4 // T.int32(16) + v_k3) % T.int32(16), v_k4 % T.int32(16)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(5)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(163840))
                        v_ax1 = T.axis.spatial(T.int32(640), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(163840) // T.int32(256))
                        v_ax2 = T.axis.spatial(T.int32(16), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(256) // T.int32(16))
                        v_ax3 = T.axis.spatial(T.int32(16), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(16))
                        T.reads(lv432[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(640), (v_ax3 // T.int32(16) + v_ax2) % T.int32(16), v_ax3 % T.int32(16)], rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(640) // T.int32(20)], rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(640) // T.int32(20)], param_0[((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(640)], param_1[((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(640)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv432[((((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(640) // T.int32(20) * T.int32(20) + (v_ax3 % T.int32(16) // T.int32(16) + (v_ax3 // T.int32(16) + v_ax2) % T.int32(16)) // T.int32(16) + ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(20)) // T.int32(640) + (((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2)) % T.int32(2), (((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(640) // T.int32(20) * T.int32(20) + (v_ax3 % T.int32(16) // T.int32(16) + (v_ax3 // T.int32(16) + v_ax2) % T.int32(16)) // T.int32(16) + ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(20)) % T.int32(640), (v_ax3 % T.int32(16) // T.int32(16) + (v_ax3 // T.int32(16) + v_ax2) % T.int32(16)) % T.int32(16), v_ax3 % T.int32(16) % T.int32(16)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(640) // T.int32(20)] * T.float32(0.00019531250000000001)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(640) // T.int32(20)] * T.float32(0.00019531250000000001) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(640) // T.int32(20)] * T.float32(0.00019531250000000001) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(640) // T.int32(20)] * T.float32(0.00019531250000000001)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(640) // T.int32(20) * T.int32(20) + ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(20)) % T.int32(640)] + param_1[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(640) // T.int32(20) * T.int32(20) + ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(20)) % T.int32(640)]) * T.sigmoid((lv432[((((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(640) // T.int32(20) * T.int32(20) + (v_ax3 % T.int32(16) // T.int32(16) + (v_ax3 // T.int32(16) + v_ax2) % T.int32(16)) // T.int32(16) + ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(20)) // T.int32(640) + (((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2)) % T.int32(2), (((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(640) // T.int32(20) * T.int32(20) + (v_ax3 % T.int32(16) // T.int32(16) + (v_ax3 // T.int32(16) + v_ax2) % T.int32(16)) // T.int32(16) + ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(20)) % T.int32(640), (v_ax3 % T.int32(16) // T.int32(16) + (v_ax3 // T.int32(16) + v_ax2) % T.int32(16)) % T.int32(16), v_ax3 % T.int32(16) % T.int32(16)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(640) // T.int32(20)] * T.float32(0.00019531250000000001)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(640) // T.int32(20)] * T.float32(0.00019531250000000001) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(640) // T.int32(20)] * T.float32(0.00019531250000000001) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(640) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(640) // T.int32(20)] * T.float32(0.00019531250000000001)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(640) // T.int32(20) * T.int32(20) + ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(20)) % T.int32(640)] + param_1[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(640) // T.int32(20) * T.int32(20) + ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(20)) % T.int32(640)])

    @T.prim_func
    def fused_group_norm6_silu5(lv444: T.Buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), "float32"), param_0: T.Buffer((T.int32(1280),), "float32"), param_1: T.Buffer((T.int32(1280),), "float32"), T_multiply: T.Buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int32(2), T.int32(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int32(2), T.int32(32)))
        for ax0_ax1_fused in T.thread_binding(T.int32(64), thread="blockIdx.x"):
            for k2_k3_k4_fused_0 in range(T.int32(160)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int32(2), ax0_ax1_fused // T.int32(32))
                        v_ax1 = T.axis.spatial(T.int32(32), ax0_ax1_fused % T.int32(32))
                        v_k2 = T.axis.reduce(T.int32(40), (k2_k3_k4_fused_0 * T.int32(64) + k2_k3_k4_fused_1) // T.int32(256))
                        v_k3 = T.axis.reduce(T.int32(16), (k2_k3_k4_fused_0 * T.int32(64) + k2_k3_k4_fused_1) % T.int32(256) // T.int32(16))
                        v_k4 = T.axis.reduce(T.int32(16), (k2_k3_k4_fused_0 * T.int32(64) + k2_k3_k4_fused_1) % T.int32(16))
                        T.reads(lv444[((v_ax1 * T.int32(40) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) // T.int32(1280) + v_ax0) % T.int32(2), (v_ax1 * T.int32(40) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) % T.int32(1280), (v_k4 // T.int32(16) + v_k3) % T.int32(16), v_k4 % T.int32(16)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv444[((v_ax1 * T.int32(40) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) // T.int32(1280) + v_ax0) % T.int32(2), (v_ax1 * T.int32(40) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) % T.int32(1280), (v_k4 // T.int32(16) + v_k3) % T.int32(16), v_k4 % T.int32(16)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv444[((v_ax1 * T.int32(40) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) // T.int32(1280) + v_ax0) % T.int32(2), (v_ax1 * T.int32(40) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) % T.int32(1280), (v_k4 // T.int32(16) + v_k3) % T.int32(16), v_k4 % T.int32(16)] * lv444[((v_ax1 * T.int32(40) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) // T.int32(1280) + v_ax0) % T.int32(2), (v_ax1 * T.int32(40) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) % T.int32(1280), (v_k4 // T.int32(16) + v_k3) % T.int32(16), v_k4 % T.int32(16)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(10)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(327680))
                        v_ax1 = T.axis.spatial(T.int32(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(327680) // T.int32(256))
                        v_ax2 = T.axis.spatial(T.int32(16), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(256) // T.int32(16))
                        v_ax3 = T.axis.spatial(T.int32(16), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(16))
                        T.reads(lv444[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1280), (v_ax3 // T.int32(16) + v_ax2) % T.int32(16), v_ax3 % T.int32(16)], rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1280) // T.int32(40)], rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1280) // T.int32(40)], param_0[((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1280)], param_1[((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1280)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv444[((((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1280) // T.int32(40) * T.int32(40) + (v_ax3 % T.int32(16) // T.int32(16) + (v_ax3 // T.int32(16) + v_ax2) % T.int32(16)) // T.int32(16) + ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(40)) // T.int32(1280) + (((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2)) % T.int32(2), (((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1280) // T.int32(40) * T.int32(40) + (v_ax3 % T.int32(16) // T.int32(16) + (v_ax3 // T.int32(16) + v_ax2) % T.int32(16)) // T.int32(16) + ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(40)) % T.int32(1280), (v_ax3 % T.int32(16) // T.int32(16) + (v_ax3 // T.int32(16) + v_ax2) % T.int32(16)) % T.int32(16), v_ax3 % T.int32(16) % T.int32(16)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1280) // T.int32(40)] * T.float32(9.7656250000000005e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1280) // T.int32(40)] * T.float32(9.7656250000000005e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1280) // T.int32(40)] * T.float32(9.7656250000000005e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1280) // T.int32(40)] * T.float32(9.7656250000000005e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1280) // T.int32(40) * T.int32(40) + ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(40)) % T.int32(1280)] + param_1[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1280) // T.int32(40) * T.int32(40) + ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(40)) % T.int32(1280)]) * T.sigmoid((lv444[((((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1280) // T.int32(40) * T.int32(40) + (v_ax3 % T.int32(16) // T.int32(16) + (v_ax3 // T.int32(16) + v_ax2) % T.int32(16)) // T.int32(16) + ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(40)) // T.int32(1280) + (((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2)) % T.int32(2), (((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1280) // T.int32(40) * T.int32(40) + (v_ax3 % T.int32(16) // T.int32(16) + (v_ax3 // T.int32(16) + v_ax2) % T.int32(16)) // T.int32(16) + ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(40)) % T.int32(1280), (v_ax3 % T.int32(16) // T.int32(16) + (v_ax3 // T.int32(16) + v_ax2) % T.int32(16)) % T.int32(16), v_ax3 % T.int32(16) % T.int32(16)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1280) // T.int32(40)] * T.float32(9.7656250000000005e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1280) // T.int32(40)] * T.float32(9.7656250000000005e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1280) // T.int32(40)] * T.float32(9.7656250000000005e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1280) // T.int32(40)] * T.float32(9.7656250000000005e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1280) // T.int32(40) * T.int32(40) + ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(40)) % T.int32(1280)] + param_1[(((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(1280) // T.int32(40) * T.int32(40) + ((v_ax3 // T.int32(16) + v_ax2) // T.int32(16) + v_ax1) % T.int32(40)) % T.int32(1280)])

    @T.prim_func
    def fused_group_norm8_silu6(lv638: T.Buffer((T.int32(2), T.int32(1280), T.int32(8), T.int32(8)), "float32"), param_0: T.Buffer((T.int32(1280),), "float32"), param_1: T.Buffer((T.int32(1280),), "float32"), T_multiply: T.Buffer((T.int32(2), T.int32(1280), T.int32(8), T.int32(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int32(2), T.int32(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int32(2), T.int32(32)))
        for ax0_ax1_fused in T.thread_binding(T.int32(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(16), "pragma_unroll_explicit": T.int32(1)}):
            for k2_k3_k4_fused_0 in range(T.int32(160)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int32(16), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int32(2), ax0_ax1_fused // T.int32(32))
                        v_ax1 = T.axis.spatial(T.int32(32), ax0_ax1_fused % T.int32(32))
                        v_k2 = T.axis.reduce(T.int32(40), (k2_k3_k4_fused_0 * T.int32(16) + k2_k3_k4_fused_1) // T.int32(64))
                        v_k3 = T.axis.reduce(T.int32(8), (k2_k3_k4_fused_0 * T.int32(16) + k2_k3_k4_fused_1) % T.int32(64) // T.int32(8))
                        v_k4 = T.axis.reduce(T.int32(8), (k2_k3_k4_fused_0 * T.int32(16) + k2_k3_k4_fused_1) % T.int32(8))
                        T.reads(lv638[((v_ax1 * T.int32(40) + (v_k4 // T.int32(8) + v_k3) // T.int32(8) + v_k2) // T.int32(1280) + v_ax0) % T.int32(2), (v_ax1 * T.int32(40) + (v_k4 // T.int32(8) + v_k3) // T.int32(8) + v_k2) % T.int32(1280), (v_k4 // T.int32(8) + v_k3) % T.int32(8), v_k4 % T.int32(8)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv638[((v_ax1 * T.int32(40) + (v_k4 // T.int32(8) + v_k3) // T.int32(8) + v_k2) // T.int32(1280) + v_ax0) % T.int32(2), (v_ax1 * T.int32(40) + (v_k4 // T.int32(8) + v_k3) // T.int32(8) + v_k2) % T.int32(1280), (v_k4 // T.int32(8) + v_k3) % T.int32(8), v_k4 % T.int32(8)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv638[((v_ax1 * T.int32(40) + (v_k4 // T.int32(8) + v_k3) // T.int32(8) + v_k2) // T.int32(1280) + v_ax0) % T.int32(2), (v_ax1 * T.int32(40) + (v_k4 // T.int32(8) + v_k3) // T.int32(8) + v_k2) % T.int32(1280), (v_k4 // T.int32(8) + v_k3) % T.int32(8), v_k4 % T.int32(8)] * lv638[((v_ax1 * T.int32(40) + (v_k4 // T.int32(8) + v_k3) // T.int32(8) + v_k2) // T.int32(1280) + v_ax0) % T.int32(2), (v_ax1 * T.int32(40) + (v_k4 // T.int32(8) + v_k3) // T.int32(8) + v_k2) % T.int32(1280), (v_k4 // T.int32(8) + v_k3) % T.int32(8), v_k4 % T.int32(8)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int32(640), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(16), "pragma_unroll_explicit": T.int32(1)}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) // T.int32(81920))
                    v_ax1 = T.axis.spatial(T.int32(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) % T.int32(81920) // T.int32(64))
                    v_ax2 = T.axis.spatial(T.int32(8), (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) % T.int32(64) // T.int32(8))
                    v_ax3 = T.axis.spatial(T.int32(8), (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) % T.int32(8))
                    T.reads(lv638[(((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(1280), (v_ax3 // T.int32(8) + v_ax2) % T.int32(8), v_ax3 % T.int32(8)], rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(1280) // T.int32(40)], rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(1280) // T.int32(40)], param_0[((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(1280)], param_1[((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(1280)])
                    T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv638[((((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(1280) // T.int32(40) * T.int32(40) + (v_ax3 % T.int32(8) // T.int32(8) + (v_ax3 // T.int32(8) + v_ax2) % T.int32(8)) // T.int32(8) + ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(40)) // T.int32(1280) + (((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2)) % T.int32(2), (((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(1280) // T.int32(40) * T.int32(40) + (v_ax3 % T.int32(8) // T.int32(8) + (v_ax3 // T.int32(8) + v_ax2) % T.int32(8)) // T.int32(8) + ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(40)) % T.int32(1280), (v_ax3 % T.int32(8) // T.int32(8) + (v_ax3 // T.int32(8) + v_ax2) % T.int32(8)) % T.int32(8), v_ax3 % T.int32(8) % T.int32(8)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(1280) // T.int32(40)] * T.float32(0.00039062500000000002)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(1280) // T.int32(40)] * T.float32(0.00039062500000000002) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(1280) // T.int32(40)] * T.float32(0.00039062500000000002) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(1280) // T.int32(40)] * T.float32(0.00039062500000000002)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(1280) // T.int32(40) * T.int32(40) + ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(40)) % T.int32(1280)] + param_1[(((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(1280) // T.int32(40) * T.int32(40) + ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(40)) % T.int32(1280)]) * T.sigmoid((lv638[((((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(1280) // T.int32(40) * T.int32(40) + (v_ax3 % T.int32(8) // T.int32(8) + (v_ax3 // T.int32(8) + v_ax2) % T.int32(8)) // T.int32(8) + ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(40)) // T.int32(1280) + (((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2)) % T.int32(2), (((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(1280) // T.int32(40) * T.int32(40) + (v_ax3 % T.int32(8) // T.int32(8) + (v_ax3 // T.int32(8) + v_ax2) % T.int32(8)) // T.int32(8) + ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(40)) % T.int32(1280), (v_ax3 % T.int32(8) // T.int32(8) + (v_ax3 // T.int32(8) + v_ax2) % T.int32(8)) % T.int32(8), v_ax3 % T.int32(8) % T.int32(8)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(1280) // T.int32(40)] * T.float32(0.00039062500000000002)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(1280) // T.int32(40)] * T.float32(0.00039062500000000002) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(1280) // T.int32(40)] * T.float32(0.00039062500000000002) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) // T.int32(1280) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(1280) // T.int32(40)] * T.float32(0.00039062500000000002)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(1280) // T.int32(40) * T.int32(40) + ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(40)) % T.int32(1280)] + param_1[(((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(1280) // T.int32(40) * T.int32(40) + ((v_ax3 // T.int32(8) + v_ax2) // T.int32(8) + v_ax1) % T.int32(40)) % T.int32(1280)])

    @T.prim_func
    def fused_group_norm_silu1(lv23: T.Buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), "float32"), param_0: T.Buffer((T.int32(320),), "float32"), param_1: T.Buffer((T.int32(320),), "float32"), T_multiply: T.Buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int32(2), T.int32(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int32(2), T.int32(32)))
        for ax0_ax1_fused in T.thread_binding(T.int32(64), thread="blockIdx.x"):
            for k2_k3_k4_fused_0 in range(T.int32(1280)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int32(2), ax0_ax1_fused // T.int32(32))
                        v_ax1 = T.axis.spatial(T.int32(32), ax0_ax1_fused % T.int32(32))
                        v_k2 = T.axis.reduce(T.int32(10), (k2_k3_k4_fused_0 * T.int32(32) + k2_k3_k4_fused_1) // T.int32(4096))
                        v_k3 = T.axis.reduce(T.int32(64), (k2_k3_k4_fused_0 * T.int32(32) + k2_k3_k4_fused_1) % T.int32(4096) // T.int32(64))
                        v_k4 = T.axis.reduce(T.int32(64), (k2_k3_k4_fused_0 * T.int32(32) + k2_k3_k4_fused_1) % T.int32(64))
                        T.reads(lv23[((v_ax1 * T.int32(10) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) // T.int32(320) + v_ax0) % T.int32(2), (v_ax1 * T.int32(10) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) % T.int32(320), (v_k4 // T.int32(64) + v_k3) % T.int32(64), v_k4 % T.int32(64)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + lv23[((v_ax1 * T.int32(10) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) // T.int32(320) + v_ax0) % T.int32(2), (v_ax1 * T.int32(10) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) % T.int32(320), (v_k4 // T.int32(64) + v_k3) % T.int32(64), v_k4 % T.int32(64)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + lv23[((v_ax1 * T.int32(10) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) // T.int32(320) + v_ax0) % T.int32(2), (v_ax1 * T.int32(10) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) % T.int32(320), (v_k4 // T.int32(64) + v_k3) % T.int32(64), v_k4 % T.int32(64)] * lv23[((v_ax1 * T.int32(10) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) // T.int32(320) + v_ax0) % T.int32(2), (v_ax1 * T.int32(10) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) % T.int32(320), (v_k4 // T.int32(64) + v_k3) % T.int32(64), v_k4 % T.int32(64)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(40)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(1310720))
                        v_ax1 = T.axis.spatial(T.int32(320), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(1310720) // T.int32(4096))
                        v_ax2 = T.axis.spatial(T.int32(64), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(4096) // T.int32(64))
                        v_ax3 = T.axis.spatial(T.int32(64), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(64))
                        T.reads(lv23[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(320) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(320), (v_ax3 // T.int32(64) + v_ax2) % T.int32(64), v_ax3 % T.int32(64)], rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(320) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(320) // T.int32(10)], rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(320) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(320) // T.int32(10)], param_0[((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(320)], param_1[((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(320)])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv23[((((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(320) // T.int32(10) * T.int32(10) + (v_ax3 % T.int32(64) // T.int32(64) + (v_ax3 // T.int32(64) + v_ax2) % T.int32(64)) // T.int32(64) + ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(10)) // T.int32(320) + (((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(320) + v_ax0) % T.int32(2)) % T.int32(2), (((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(320) // T.int32(10) * T.int32(10) + (v_ax3 % T.int32(64) // T.int32(64) + (v_ax3 // T.int32(64) + v_ax2) % T.int32(64)) // T.int32(64) + ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(10)) % T.int32(320), (v_ax3 % T.int32(64) // T.int32(64) + (v_ax3 // T.int32(64) + v_ax2) % T.int32(64)) % T.int32(64), v_ax3 % T.int32(64) % T.int32(64)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(320) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(320) // T.int32(10)] * T.float32(2.4414062500000001e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(320) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(320) // T.int32(10)] * T.float32(2.4414062500000001e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(320) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(320) // T.int32(10)] * T.float32(2.4414062500000001e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(320) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(320) // T.int32(10)] * T.float32(2.4414062500000001e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(320) // T.int32(10) * T.int32(10) + ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(10)) % T.int32(320)] + param_1[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(320) // T.int32(10) * T.int32(10) + ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(10)) % T.int32(320)]) * T.sigmoid((lv23[((((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(320) // T.int32(10) * T.int32(10) + (v_ax3 % T.int32(64) // T.int32(64) + (v_ax3 // T.int32(64) + v_ax2) % T.int32(64)) // T.int32(64) + ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(10)) // T.int32(320) + (((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(320) + v_ax0) % T.int32(2)) % T.int32(2), (((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(320) // T.int32(10) * T.int32(10) + (v_ax3 % T.int32(64) // T.int32(64) + (v_ax3 // T.int32(64) + v_ax2) % T.int32(64)) // T.int32(64) + ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(10)) % T.int32(320), (v_ax3 % T.int32(64) // T.int32(64) + (v_ax3 // T.int32(64) + v_ax2) % T.int32(64)) % T.int32(64), v_ax3 % T.int32(64) % T.int32(64)] - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(320) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(320) // T.int32(10)] * T.float32(2.4414062500000001e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(320) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(320) // T.int32(10)] * T.float32(2.4414062500000001e-05) - rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(320) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(320) // T.int32(10)] * T.float32(2.4414062500000001e-05) * (rxplaceholder_red_temp_v0[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) // T.int32(320) + v_ax0) % T.int32(2), ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(320) // T.int32(10)] * T.float32(2.4414062500000001e-05)) + T.float32(1.0000000000000001e-05)) * param_0[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(320) // T.int32(10) * T.int32(10) + ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(10)) % T.int32(320)] + param_1[(((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(320) // T.int32(10) * T.int32(10) + ((v_ax3 // T.int32(64) + v_ax2) // T.int32(64) + v_ax1) % T.int32(10)) % T.int32(320)])

    @T.prim_func
    def fused_matmul12_multiply6(lv86: T.Buffer((T.int32(16), T.int32(4096), T.int32(40)), "float32"), lv93: T.Buffer((T.int32(16), T.int32(40), T.int32(77)), "float32"), T_multiply: T.Buffer((T.int32(16), T.int32(4096), T.int32(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(16), T.int32(4096), T.int32(77)), scope="local")
        lv86_shared = T.alloc_buffer((T.int32(16), T.int32(4096), T.int32(40)), scope="shared")
        lv93_shared = T.alloc_buffer((T.int32(16), T.int32(40), T.int32(77)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(2048), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(176), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(7)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(16), i0_3_init + i0_4_init + i0_0_i1_0_i2_0_fused // T.int32(256) * T.int32(2) + i0_2_i1_2_i2_2_fused // T.int32(88))
                            v_i1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused % T.int32(256) * T.int32(16) + i0_1_i1_1_i2_1_fused * T.int32(8) + i0_2_i1_2_i2_2_fused % T.int32(88) // T.int32(11) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(77), i0_2_i1_2_i2_2_fused % T.int32(11) * T.int32(7) + i2_3_init * T.int32(7) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(2)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(176), thread="threadIdx.x"):
                                with T.block("lv86_shared"):
                                    v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(256) * T.int32(2) + (ax0_ax1_ax2_fused_0 * T.int32(176) + ax0_ax1_ax2_fused_1) // T.int32(320))
                                    v1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused % T.int32(256) * T.int32(16) + (ax0_ax1_ax2_fused_0 * T.int32(176) + ax0_ax1_ax2_fused_1) % T.int32(320) // T.int32(20))
                                    v2 = T.axis.spatial(T.int32(40), k_0 * T.int32(20) + (ax0_ax1_ax2_fused_0 * T.int32(176) + ax0_ax1_ax2_fused_1) % T.int32(20))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int32(176) + ax0_ax1_ax2_fused_1 < T.int32(640))
                                    T.reads(lv86[v0, v1, v2])
                                    T.writes(lv86_shared[v0, v1, v2])
                                    lv86_shared[v0, v1, v2] = lv86[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int32(18)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(176), thread="threadIdx.x"):
                                with T.block("lv93_shared"):
                                    v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(256) * T.int32(2) + (ax0_ax1_ax2_fused_0 * T.int32(176) + ax0_ax1_ax2_fused_1) // T.int32(1540))
                                    v1 = T.axis.spatial(T.int32(40), k_0 * T.int32(20) + (ax0_ax1_ax2_fused_0 * T.int32(176) + ax0_ax1_ax2_fused_1) % T.int32(1540) // T.int32(77))
                                    v2 = T.axis.spatial(T.int32(77), (ax0_ax1_ax2_fused_0 * T.int32(176) + ax0_ax1_ax2_fused_1) % T.int32(77))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int32(176) + ax0_ax1_ax2_fused_1 < T.int32(3080))
                                    T.reads(lv93[v0, v1, v2])
                                    T.writes(lv93_shared[v0, v1, v2])
                                    lv93_shared[v0, v1, v2] = lv93[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(5), T.int32(1), T.int32(1), T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(7)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(16), i0_3 + i0_4 + i0_0_i1_0_i2_0_fused // T.int32(256) * T.int32(2) + i0_2_i1_2_i2_2_fused // T.int32(88))
                                v_i1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused % T.int32(256) * T.int32(16) + i0_1_i1_1_i2_1_fused * T.int32(8) + i0_2_i1_2_i2_2_fused % T.int32(88) // T.int32(11) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int32(77), i0_2_i1_2_i2_2_fused % T.int32(11) * T.int32(7) + i2_3 * T.int32(7) + i2_4)
                                v_k = T.axis.reduce(T.int32(40), k_0 * T.int32(20) + k_1 * T.int32(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv86_shared[v_i0, v_i1, v_k], lv93_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv86_shared[v_i0, v_i1, v_k] * lv93_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(1), T.int32(7)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(256) * T.int32(2) + i0_2_i1_2_i2_2_fused // T.int32(88) + ax0)
                            v1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused % T.int32(256) * T.int32(16) + i0_1_i1_1_i2_1_fused * T.int32(8) + i0_2_i1_2_i2_2_fused % T.int32(88) // T.int32(11) + ax1)
                            v2 = T.axis.spatial(T.int32(77), i0_2_i1_2_i2_2_fused % T.int32(11) * T.int32(7) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(T_multiply[v0, v1, v2])
                            T_multiply[v0, v1, v2] = matmul_local[v0, v1, v2] * T.float32(0.15811388194561005)

    @T.prim_func
    def fused_matmul14_add11_gelu(lv105: T.Buffer((T.int32(2), T.int32(4096), T.int32(320)), "float32"), param_0: T.Buffer((T.int32(320), T.int32(1280)), "float32"), param_1: T.Buffer((T.int32(1280),), "float32"), T_multiply: T.Buffer((T.int32(2), T.int32(4096), T.int32(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul = T.alloc_buffer((T.int32(2), T.int32(4096), T.int32(1280)))
        matmul_local = T.alloc_buffer((T.int32(2), T.int32(4096), T.int32(1280)), scope="local")
        lv105_shared = T.alloc_buffer((T.int32(2), T.int32(4096), T.int32(320)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(320), T.int32(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(2560), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(64), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(2), i0_4_init + i0_0_i1_0_i2_0_fused // T.int32(1280) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused % T.int32(1280) // T.int32(20) * T.int32(64) + i0_1_i1_1_i2_1_fused // T.int32(4) * T.int32(4) + i0_2_i1_2_i2_2_fused // T.int32(16) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(1280), i2_4_init + i0_0_i1_0_i2_0_fused % T.int32(20) * T.int32(64) + i0_1_i1_1_i2_1_fused % T.int32(4) * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(16) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(256), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(80)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("lv105_shared"):
                                    v0 = T.axis.spatial(T.int32(2), i0_0_i1_0_i2_0_fused // T.int32(1280))
                                    v1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused % T.int32(1280) // T.int32(20) * T.int32(64) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) // T.int32(4))
                                    v2 = T.axis.spatial(T.int32(320), k_0 * T.int32(4) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(4))
                                    T.reads(lv105[v0, v1, v2])
                                    T.writes(lv105_shared[v0, v1, v2])
                                    lv105_shared[v0, v1, v2] = lv105[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int32(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(320), k_0 * T.int32(4) + (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) // T.int32(64))
                                    v1 = T.axis.spatial(T.int32(1280), i0_0_i1_0_i2_0_fused % T.int32(20) * T.int32(64) + (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) % T.int32(64))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(2), i0_4 + i0_0_i1_0_i2_0_fused // T.int32(1280) + i0_3)
                                v_i1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused % T.int32(1280) // T.int32(20) * T.int32(64) + i0_1_i1_1_i2_1_fused // T.int32(4) * T.int32(4) + i0_2_i1_2_i2_2_fused // T.int32(16) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int32(1280), i2_4 + i0_0_i1_0_i2_0_fused % T.int32(20) * T.int32(64) + i0_1_i1_1_i2_1_fused % T.int32(4) * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(16) + i2_3)
                                v_k = T.axis.reduce(T.int32(320), k_0 * T.int32(4) + k_1 * T.int32(2) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv105_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(256), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv105_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(2), i0_0_i1_0_i2_0_fused // T.int32(1280) + ax0)
                            v1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused % T.int32(1280) // T.int32(20) * T.int32(64) + i0_1_i1_1_i2_1_fused // T.int32(4) * T.int32(4) + i0_2_i1_2_i2_2_fused // T.int32(16) + ax1)
                            v2 = T.axis.spatial(T.int32(1280), i0_0_i1_0_i2_0_fused % T.int32(20) * T.int32(64) + i0_1_i1_1_i2_1_fused % T.int32(4) * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(16) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int32(160)):
                    with T.block("T_multiply_2"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_fused_0 * T.int32(65536) + ax0_ax1_ax2_fused_1 * T.int32(256) + ax0_ax1_ax2_fused_2) // T.int32(5242880))
                        v_ax1 = T.axis.spatial(T.int32(4096), (ax0_ax1_ax2_fused_0 * T.int32(65536) + ax0_ax1_ax2_fused_1 * T.int32(256) + ax0_ax1_ax2_fused_2) % T.int32(5242880) // T.int32(1280))
                        v_ax2 = T.axis.spatial(T.int32(1280), (ax0_ax1_ax2_fused_0 * T.int32(65536) + ax0_ax1_ax2_fused_1 * T.int32(256) + ax0_ax1_ax2_fused_2) % T.int32(1280))
                        T.reads(matmul[v_ax0, v_ax1, v_ax2], param_1[v_ax2])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2])
                        T_multiply[v_ax0, v_ax1, v_ax2] = (matmul[v_ax0, v_ax1, v_ax2] + param_1[v_ax2]) * (T.float32(0.5) + T.erf((matmul[v_ax0, v_ax1, v_ax2] + param_1[v_ax2]) * T.float32(0.70710678118654757)) * T.float32(0.5))

    @T.prim_func
    def fused_matmul14_add11_multiply7(lv105: T.Buffer((T.int32(2), T.int32(4096), T.int32(320)), "float32"), param_0: T.Buffer((T.int32(320), T.int32(1280)), "float32"), param_1: T.Buffer((T.int32(1280),), "float32"), lv112: T.Buffer((T.int32(2), T.int32(4096), T.int32(1280)), "float32"), T_multiply: T.Buffer((T.int32(2), T.int32(4096), T.int32(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(2), T.int32(4096), T.int32(1280)), scope="local")
        lv105_shared = T.alloc_buffer((T.int32(2), T.int32(4096), T.int32(320)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(320), T.int32(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(5120), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(1), T.int32(2), T.int32(1), T.int32(4), T.int32(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(2), i0_4_init + i0_1_i1_1_i2_1_fused // T.int32(2) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused // T.int32(20) * T.int32(16) + i0_2_i1_2_i2_2_fused // T.int32(16) * T.int32(4) + i1_3_init * T.int32(4) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(1280), i2_4_init + i0_0_i1_0_i2_0_fused % T.int32(20) * T.int32(64) + i0_1_i1_1_i2_1_fused % T.int32(2) * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(16) * T.int32(2) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(40)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("lv105_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) // T.int32(128))
                                    v1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused // T.int32(20) * T.int32(16) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(128) // T.int32(8))
                                    v2 = T.axis.spatial(T.int32(320), k_0 * T.int32(8) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(8))
                                    T.reads(lv105[v0, v1, v2])
                                    T.writes(lv105_shared[v0, v1, v2])
                                    lv105_shared[v0, v1, v2] = lv105[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int32(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(320), k_0 * T.int32(8) + (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) // T.int32(64))
                                    v1 = T.axis.spatial(T.int32(1280), i0_0_i1_0_i2_0_fused % T.int32(20) * T.int32(64) + (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) % T.int32(64))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(2), T.int32(1), T.int32(1), T.int32(2), T.int32(4), T.int32(1), T.int32(4), T.int32(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(2), i0_4 + i0_1_i1_1_i2_1_fused // T.int32(2) + i0_3)
                                v_i1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused // T.int32(20) * T.int32(16) + i0_2_i1_2_i2_2_fused // T.int32(16) * T.int32(4) + i1_3 * T.int32(4) + i1_4)
                                v_i2 = T.axis.spatial(T.int32(1280), i2_4 + i0_0_i1_0_i2_0_fused % T.int32(20) * T.int32(64) + i0_1_i1_1_i2_1_fused % T.int32(2) * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(16) * T.int32(2) + i2_3)
                                v_k = T.axis.reduce(T.int32(320), k_0 * T.int32(8) + k_1 * T.int32(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv105_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv105_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(4), T.int32(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(2), i0_1_i1_1_i2_1_fused // T.int32(2) + ax0)
                            v1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused // T.int32(20) * T.int32(16) + i0_2_i1_2_i2_2_fused // T.int32(16) * T.int32(4) + ax1)
                            v2 = T.axis.spatial(T.int32(1280), i0_0_i1_0_i2_0_fused % T.int32(20) * T.int32(64) + i0_1_i1_1_i2_1_fused % T.int32(2) * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(16) * T.int32(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2], param_1[v2], lv112[v0, v1, v2])
                            T.writes(T_multiply[v0, v1, v2])
                            T_multiply[v0, v1, v2] = (matmul_local[v0, v1, v2] + param_1[v2]) * lv112[v0, v1, v2]

    @T.prim_func
    def fused_matmul15_add9_add10(lv113: T.Buffer((T.int32(2), T.int32(4096), T.int32(1280)), "float32"), param_0: T.Buffer((T.int32(1280), T.int32(320)), "float32"), param_1: T.Buffer((T.int32(320),), "float32"), lv104: T.Buffer((T.int32(2), T.int32(4096), T.int32(320)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(4096), T.int32(320)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(2), T.int32(4096), T.int32(320)), scope="local")
        lv113_shared = T.alloc_buffer((T.int32(2), T.int32(4096), T.int32(1280)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(1280), T.int32(320)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(1280), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(2), T.int32(2), T.int32(1), T.int32(8), T.int32(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(2), i0_3_init + i0_4_init + i0_2_i1_2_i2_2_fused // T.int32(32))
                            v_i1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused // T.int32(5) * T.int32(16) + i1_3_init * T.int32(8) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(320), i2_4_init + i0_0_i1_0_i2_0_fused % T.int32(5) * T.int32(64) + i0_2_i1_2_i2_2_fused % T.int32(32) * T.int32(2) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(320)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("lv113_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) // T.int32(64))
                                    v1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused // T.int32(5) * T.int32(16) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(64) // T.int32(4))
                                    v2 = T.axis.spatial(T.int32(1280), k_0 * T.int32(4) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(4))
                                    T.reads(lv113[v0, v1, v2])
                                    T.writes(lv113_shared[v0, v1, v2])
                                    lv113_shared[v0, v1, v2] = lv113[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int32(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(1280), k_0 * T.int32(4) + (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) // T.int32(64))
                                    v1 = T.axis.spatial(T.int32(320), i0_0_i1_0_i2_0_fused % T.int32(5) * T.int32(64) + (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) % T.int32(64))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(1), T.int32(1), T.int32(2), T.int32(2), T.int32(4), T.int32(1), T.int32(8), T.int32(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(2), i0_3 + i0_4 + i0_2_i1_2_i2_2_fused // T.int32(32))
                                v_i1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused // T.int32(5) * T.int32(16) + i1_3 * T.int32(8) + i1_4)
                                v_i2 = T.axis.spatial(T.int32(320), i2_4 + i0_0_i1_0_i2_0_fused % T.int32(5) * T.int32(64) + i0_2_i1_2_i2_2_fused % T.int32(32) * T.int32(2) + i2_3)
                                v_k = T.axis.reduce(T.int32(1280), k_0 * T.int32(4) + k_1 * T.int32(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv113_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv113_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(16), T.int32(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(2), i0_2_i1_2_i2_2_fused // T.int32(32) + ax0)
                            v1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused // T.int32(5) * T.int32(16) + ax1)
                            v2 = T.axis.spatial(T.int32(320), i0_0_i1_0_i2_0_fused % T.int32(5) * T.int32(64) + i0_2_i1_2_i2_2_fused % T.int32(32) * T.int32(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2], param_1[v2], lv104[v0, v1, v2])
                            T.writes(T_add[v0, v1, v2])
                            T_add[v0, v1, v2] = matmul_local[v0, v1, v2] + param_1[v2] + lv104[v0, v1, v2]

    @T.prim_func
    def fused_matmul16_add14_strided_slice4(lv232: T.Buffer((T.int32(2), T.int32(1280)), "float32"), param_0: T.Buffer((T.int32(1280), T.int32(640)), "float32"), param_1: T.Buffer((T.int32(640),), "float32"), T_strided_slice_with_axes: T.Buffer((T.int32(2), T.int32(640)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(2), T.int32(640)), scope="local")
        lv232_shared = T.alloc_buffer((T.int32(2), T.int32(1280)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(1280), T.int32(640)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int32(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(2), i0_2_i1_2_fused // T.int32(32) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int32(640), i1_4_init + i0_0_i1_0_fused * T.int32(32) + i0_2_i1_2_fused % T.int32(32) + i1_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int32(64)):
                        for ax0_ax1_fused_0 in range(T.int32(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("lv232_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) // T.int32(20))
                                    v1 = T.axis.spatial(T.int32(1280), k_0 * T.int32(20) + (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) % T.int32(20))
                                    T.where(ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1 < T.int32(40))
                                    T.reads(lv232[v0, v1])
                                    T.writes(lv232_shared[v0, v1])
                                    lv232_shared[v0, v1] = lv232[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int32(10)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(1280), k_0 * T.int32(20) + (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) // T.int32(32))
                                    v1 = T.axis.spatial(T.int32(640), i0_0_i1_0_fused * T.int32(32) + (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) % T.int32(32))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int32(20), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(2), i0_2_i1_2_fused // T.int32(32) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int32(640), i1_4 + i0_0_i1_0_fused * T.int32(32) + i0_2_i1_2_fused % T.int32(32) + i1_3)
                                v_k = T.axis.reduce(T.int32(1280), k_0 * T.int32(20) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1], lv232_shared[v_i0, v_k], param_0_shared[v_k, v_i1])
                                T.writes(matmul_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1] = matmul_local[v_i0, v_i1] + lv232_shared[v_i0, v_k] * param_0_shared[v_k, v_i1]
                    for ax0, ax1 in T.grid(T.int32(1), T.int32(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(2), i0_2_i1_2_fused // T.int32(32) + ax0)
                            v1 = T.axis.spatial(T.int32(640), i0_0_i1_0_fused * T.int32(32) + i0_2_i1_2_fused % T.int32(32) + ax1)
                            T.reads(matmul_local[v0, v1], param_1[v1])
                            T.writes(T_strided_slice_with_axes[v0, v1])
                            T_strided_slice_with_axes[v0, v1] = matmul_local[v0, v1] + param_1[v1]

    @T.prim_func
    def fused_matmul17_add17_add18(lv278: T.Buffer((T.int32(2), T.int32(1024), T.int32(640)), "float32"), param_0: T.Buffer((T.int32(640), T.int32(640)), "float32"), param_1: T.Buffer((T.int32(640),), "float32"), lv254: T.Buffer((T.int32(2), T.int32(1024), T.int32(640)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(1024), T.int32(640)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(2), T.int32(1024), T.int32(640)), scope="local")
        lv278_shared = T.alloc_buffer((T.int32(2), T.int32(1024), T.int32(640)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(640), T.int32(640)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(640), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(2), T.int32(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(2), i0_3_init + i0_4_init + i0_2_i1_2_i2_2_fused // T.int32(32))
                            v_i1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused // T.int32(10) * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(32) // T.int32(16) * T.int32(8) + i1_3_init * T.int32(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(640), i0_0_i1_0_i2_0_fused % T.int32(10) * T.int32(64) + i0_1_i1_1_i2_1_fused * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(16) * T.int32(2) + i2_3_init * T.int32(2) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(80)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("lv278_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) // T.int32(128))
                                    v1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused // T.int32(10) * T.int32(16) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(128) // T.int32(8))
                                    v2 = T.axis.spatial(T.int32(640), k_0 * T.int32(8) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(8))
                                    T.reads(lv278[v0, v1, v2])
                                    T.writes(lv278_shared[v0, v1, v2])
                                    lv278_shared[v0, v1, v2] = lv278[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int32(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(640), k_0 * T.int32(8) + (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) // T.int32(64))
                                    v1 = T.axis.spatial(T.int32(640), i0_0_i1_0_i2_0_fused % T.int32(10) * T.int32(64) + (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) % T.int32(64))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(1), T.int32(1), T.int32(4), T.int32(1), T.int32(8), T.int32(1), T.int32(2), T.int32(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(2), i0_3 + i0_4 + i0_2_i1_2_i2_2_fused // T.int32(32))
                                v_i1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused // T.int32(10) * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(32) // T.int32(16) * T.int32(8) + i1_3 * T.int32(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int32(640), i0_0_i1_0_i2_0_fused % T.int32(10) * T.int32(64) + i0_1_i1_1_i2_1_fused * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(16) * T.int32(2) + i2_3 * T.int32(2) + i2_4)
                                v_k = T.axis.reduce(T.int32(640), k_0 * T.int32(8) + k_1 * T.int32(8) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv278_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv278_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(8), T.int32(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(2), i0_2_i1_2_i2_2_fused // T.int32(32) + ax0)
                            v1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused // T.int32(10) * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(32) // T.int32(16) * T.int32(8) + ax1)
                            v2 = T.axis.spatial(T.int32(640), i0_0_i1_0_i2_0_fused % T.int32(10) * T.int32(64) + i0_1_i1_1_i2_1_fused * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(16) * T.int32(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2], param_1[v2], lv254[v0, v1, v2])
                            T.writes(T_add[v0, v1, v2])
                            T_add[v0, v1, v2] = matmul_local[v0, v1, v2] + param_1[v2] + lv254[v0, v1, v2]

    @T.prim_func
    def fused_matmul18_multiply8(lv264: T.Buffer((T.int32(16), T.int32(1024), T.int32(80)), "float32"), lv271: T.Buffer((T.int32(16), T.int32(80), T.int32(1024)), "float32"), T_multiply: T.Buffer((T.int32(16), T.int32(1024), T.int32(1024)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(16), T.int32(1024), T.int32(1024)), scope="local")
        lv264_shared = T.alloc_buffer((T.int32(16), T.int32(1024), T.int32(80)), scope="shared")
        lv271_shared = T.alloc_buffer((T.int32(16), T.int32(80), T.int32(1024)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(8192), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(1), T.int32(2), T.int32(1), T.int32(16), T.int32(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(16), i0_4_init + i0_0_i1_0_i2_0_fused // T.int32(512) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused % T.int32(512) // T.int32(16) * T.int32(32) + i0_2_i1_2_i2_2_fused // T.int32(32) * T.int32(16) + i1_3_init * T.int32(16) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(1024), i2_4_init + i0_0_i1_0_i2_0_fused % T.int32(16) * T.int32(64) + i0_2_i1_2_i2_2_fused % T.int32(32) * T.int32(2) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(20)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("lv264_shared"):
                                    v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(512))
                                    v1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused % T.int32(512) // T.int32(16) * T.int32(32) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) // T.int32(4))
                                    v2 = T.axis.spatial(T.int32(80), k_0 * T.int32(4) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(4))
                                    T.reads(lv264[v0, v1, v2])
                                    T.writes(lv264_shared[v0, v1, v2])
                                    lv264_shared[v0, v1, v2] = lv264[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int32(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("lv271_shared"):
                                    v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(512))
                                    v1 = T.axis.spatial(T.int32(80), k_0 * T.int32(4) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) // T.int32(64))
                                    v2 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused % T.int32(16) * T.int32(64) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(64))
                                    T.reads(lv271[v0, v1, v2])
                                    T.writes(lv271_shared[v0, v1, v2])
                                    lv271_shared[v0, v1, v2] = lv271[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(4), T.int32(1), T.int32(16), T.int32(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(16), i0_4 + i0_0_i1_0_i2_0_fused // T.int32(512) + i0_3)
                                v_i1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused % T.int32(512) // T.int32(16) * T.int32(32) + i0_2_i1_2_i2_2_fused // T.int32(32) * T.int32(16) + i1_3 * T.int32(16) + i1_4)
                                v_i2 = T.axis.spatial(T.int32(1024), i2_4 + i0_0_i1_0_i2_0_fused % T.int32(16) * T.int32(64) + i0_2_i1_2_i2_2_fused % T.int32(32) * T.int32(2) + i2_3)
                                v_k = T.axis.reduce(T.int32(80), k_0 * T.int32(4) + k_1 * T.int32(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv264_shared[v_i0, v_i1, v_k], lv271_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv264_shared[v_i0, v_i1, v_k] * lv271_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(16), T.int32(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(512) + ax0)
                            v1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused % T.int32(512) // T.int32(16) * T.int32(32) + i0_2_i1_2_i2_2_fused // T.int32(32) * T.int32(16) + ax1)
                            v2 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused % T.int32(16) * T.int32(64) + i0_2_i1_2_i2_2_fused % T.int32(32) * T.int32(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(T_multiply[v0, v1, v2])
                            T_multiply[v0, v1, v2] = matmul_local[v0, v1, v2] * T.float32(0.11180339753627777)

    @T.prim_func
    def fused_matmul21_multiply9(lv292: T.Buffer((T.int32(16), T.int32(1024), T.int32(80)), "float32"), lv299: T.Buffer((T.int32(16), T.int32(80), T.int32(77)), "float32"), T_multiply: T.Buffer((T.int32(16), T.int32(1024), T.int32(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(16), T.int32(1024), T.int32(77)), scope="local")
        lv292_shared = T.alloc_buffer((T.int32(16), T.int32(1024), T.int32(80)), scope="shared")
        lv299_shared = T.alloc_buffer((T.int32(16), T.int32(80), T.int32(77)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(154), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(16), i0_4_init + i0_0_i1_0_i2_0_fused // T.int32(32) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused % T.int32(32) * T.int32(32) + i0_1_i1_1_i2_1_fused * T.int32(8) + i0_2_i1_2_i2_2_fused // T.int32(77) * T.int32(4) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(77), i2_4_init + i0_2_i1_2_i2_2_fused % T.int32(77) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(5)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(154), thread="threadIdx.x"):
                                with T.block("lv292_shared"):
                                    v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(32))
                                    v1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused % T.int32(32) * T.int32(32) + (ax0_ax1_ax2_fused_0 * T.int32(154) + ax0_ax1_ax2_fused_1) // T.int32(16))
                                    v2 = T.axis.spatial(T.int32(80), k_0 * T.int32(16) + (ax0_ax1_ax2_fused_0 * T.int32(154) + ax0_ax1_ax2_fused_1) % T.int32(16))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int32(154) + ax0_ax1_ax2_fused_1 < T.int32(512))
                                    T.reads(lv292[v0, v1, v2])
                                    T.writes(lv292_shared[v0, v1, v2])
                                    lv292_shared[v0, v1, v2] = lv292[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int32(8)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(154), thread="threadIdx.x"):
                                with T.block("lv299_shared"):
                                    v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(32))
                                    v1 = T.axis.spatial(T.int32(80), k_0 * T.int32(16) + (ax0_ax1_ax2_fused_0 * T.int32(154) + ax0_ax1_ax2_fused_1) // T.int32(77))
                                    v2 = T.axis.spatial(T.int32(77), (ax0_ax1_ax2_fused_0 * T.int32(154) + ax0_ax1_ax2_fused_1) % T.int32(77))
                                    T.reads(lv299[v0, v1, v2])
                                    T.writes(lv299_shared[v0, v1, v2])
                                    lv299_shared[v0, v1, v2] = lv299[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(4), T.int32(1), T.int32(4), T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(16), i0_4 + i0_0_i1_0_i2_0_fused // T.int32(32) + i0_3)
                                v_i1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused % T.int32(32) * T.int32(32) + i0_1_i1_1_i2_1_fused * T.int32(8) + i0_2_i1_2_i2_2_fused // T.int32(77) * T.int32(4) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int32(77), i2_4 + i0_2_i1_2_i2_2_fused % T.int32(77) + i2_3)
                                v_k = T.axis.reduce(T.int32(80), k_0 * T.int32(16) + k_1 * T.int32(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv292_shared[v_i0, v_i1, v_k], lv299_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv292_shared[v_i0, v_i1, v_k] * lv299_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(4), T.int32(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(32) + ax0)
                            v1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused % T.int32(32) * T.int32(32) + i0_1_i1_1_i2_1_fused * T.int32(8) + i0_2_i1_2_i2_2_fused // T.int32(77) * T.int32(4) + ax1)
                            v2 = T.axis.spatial(T.int32(77), i0_2_i1_2_i2_2_fused % T.int32(77) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(T_multiply[v0, v1, v2])
                            T_multiply[v0, v1, v2] = matmul_local[v0, v1, v2] * T.float32(0.11180339753627777)

    @T.prim_func
    def fused_matmul23_add19_gelu1(lv311: T.Buffer((T.int32(2), T.int32(1024), T.int32(640)), "float32"), param_0: T.Buffer((T.int32(640), T.int32(2560)), "float32"), param_1: T.Buffer((T.int32(2560),), "float32"), T_multiply: T.Buffer((T.int32(2), T.int32(1024), T.int32(2560)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul = T.alloc_buffer((T.int32(2), T.int32(1024), T.int32(2560)))
        matmul_local = T.alloc_buffer((T.int32(2), T.int32(1024), T.int32(2560)), scope="local")
        lv311_shared = T.alloc_buffer((T.int32(2), T.int32(1024), T.int32(640)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(640), T.int32(2560)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(1280), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(64), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(2), i0_4_init + i0_0_i1_0_i2_0_fused // T.int32(640) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused % T.int32(640) // T.int32(40) * T.int32(64) + i0_1_i1_1_i2_1_fused // T.int32(4) * T.int32(4) + i0_2_i1_2_i2_2_fused // T.int32(16) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(2560), i2_4_init + i0_0_i1_0_i2_0_fused % T.int32(40) * T.int32(64) + i0_1_i1_1_i2_1_fused % T.int32(4) * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(16) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(256), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(160)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("lv311_shared"):
                                    v0 = T.axis.spatial(T.int32(2), i0_0_i1_0_i2_0_fused // T.int32(640))
                                    v1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused % T.int32(640) // T.int32(40) * T.int32(64) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) // T.int32(4))
                                    v2 = T.axis.spatial(T.int32(640), k_0 * T.int32(4) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(4))
                                    T.reads(lv311[v0, v1, v2])
                                    T.writes(lv311_shared[v0, v1, v2])
                                    lv311_shared[v0, v1, v2] = lv311[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int32(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(640), k_0 * T.int32(4) + (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) // T.int32(64))
                                    v1 = T.axis.spatial(T.int32(2560), i0_0_i1_0_i2_0_fused % T.int32(40) * T.int32(64) + (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) % T.int32(64))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(2), i0_4 + i0_0_i1_0_i2_0_fused // T.int32(640) + i0_3)
                                v_i1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused % T.int32(640) // T.int32(40) * T.int32(64) + i0_1_i1_1_i2_1_fused // T.int32(4) * T.int32(4) + i0_2_i1_2_i2_2_fused // T.int32(16) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int32(2560), i2_4 + i0_0_i1_0_i2_0_fused % T.int32(40) * T.int32(64) + i0_1_i1_1_i2_1_fused % T.int32(4) * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(16) + i2_3)
                                v_k = T.axis.reduce(T.int32(640), k_0 * T.int32(4) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv311_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(256), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv311_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(2), i0_0_i1_0_i2_0_fused // T.int32(640) + ax0)
                            v1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused % T.int32(640) // T.int32(40) * T.int32(64) + i0_1_i1_1_i2_1_fused // T.int32(4) * T.int32(4) + i0_2_i1_2_i2_2_fused // T.int32(16) + ax1)
                            v2 = T.axis.spatial(T.int32(2560), i0_0_i1_0_i2_0_fused % T.int32(40) * T.int32(64) + i0_1_i1_1_i2_1_fused % T.int32(4) * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(16) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int32(80)):
                    with T.block("T_multiply_2"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_fused_0 * T.int32(65536) + ax0_ax1_ax2_fused_1 * T.int32(256) + ax0_ax1_ax2_fused_2) // T.int32(2621440))
                        v_ax1 = T.axis.spatial(T.int32(1024), (ax0_ax1_ax2_fused_0 * T.int32(65536) + ax0_ax1_ax2_fused_1 * T.int32(256) + ax0_ax1_ax2_fused_2) % T.int32(2621440) // T.int32(2560))
                        v_ax2 = T.axis.spatial(T.int32(2560), (ax0_ax1_ax2_fused_0 * T.int32(65536) + ax0_ax1_ax2_fused_1 * T.int32(256) + ax0_ax1_ax2_fused_2) % T.int32(2560))
                        T.reads(matmul[v_ax0, v_ax1, v_ax2], param_1[v_ax2])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2])
                        T_multiply[v_ax0, v_ax1, v_ax2] = (matmul[v_ax0, v_ax1, v_ax2] + param_1[v_ax2]) * (T.float32(0.5) + T.erf((matmul[v_ax0, v_ax1, v_ax2] + param_1[v_ax2]) * T.float32(0.70710678118654757)) * T.float32(0.5))

    @T.prim_func
    def fused_matmul23_add19_multiply10(lv311: T.Buffer((T.int32(2), T.int32(1024), T.int32(640)), "float32"), param_0: T.Buffer((T.int32(640), T.int32(2560)), "float32"), param_1: T.Buffer((T.int32(2560),), "float32"), lv318: T.Buffer((T.int32(2), T.int32(1024), T.int32(2560)), "float32"), T_multiply: T.Buffer((T.int32(2), T.int32(1024), T.int32(2560)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(2), T.int32(1024), T.int32(2560)), scope="local")
        lv311_shared = T.alloc_buffer((T.int32(2), T.int32(1024), T.int32(640)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(640), T.int32(2560)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(2048), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(4), T.int32(5), T.int32(1), T.int32(1), T.int32(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(2), i0_4_init + i0_0_i1_0_i2_0_fused // T.int32(1024) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused % T.int32(1024) // T.int32(32) * T.int32(32) + i0_2_i1_2_i2_2_fused // T.int32(8) * T.int32(4) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(2560), i0_0_i1_0_i2_0_fused % T.int32(32) * T.int32(80) + i0_2_i1_2_i2_2_fused % T.int32(8) * T.int32(10) + i2_3_init * T.int32(2) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(160)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("lv311_shared"):
                                    v0 = T.axis.spatial(T.int32(2), i0_0_i1_0_i2_0_fused // T.int32(1024))
                                    v1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused % T.int32(1024) // T.int32(32) * T.int32(32) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) // T.int32(4))
                                    v2 = T.axis.spatial(T.int32(640), k_0 * T.int32(4) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(4))
                                    T.reads(lv311[v0, v1, v2])
                                    T.writes(lv311_shared[v0, v1, v2])
                                    lv311_shared[v0, v1, v2] = lv311[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int32(5)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(640), k_0 * T.int32(4) + (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) // T.int32(80))
                                    v1 = T.axis.spatial(T.int32(2560), i0_0_i1_0_i2_0_fused % T.int32(32) * T.int32(80) + (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) % T.int32(80))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(1), T.int32(1), T.int32(4), T.int32(5), T.int32(4), T.int32(1), T.int32(1), T.int32(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(2), i0_4 + i0_0_i1_0_i2_0_fused // T.int32(1024) + i0_3)
                                v_i1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused % T.int32(1024) // T.int32(32) * T.int32(32) + i0_2_i1_2_i2_2_fused // T.int32(8) * T.int32(4) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int32(2560), i0_0_i1_0_i2_0_fused % T.int32(32) * T.int32(80) + i0_2_i1_2_i2_2_fused % T.int32(8) * T.int32(10) + i2_3 * T.int32(2) + i2_4)
                                v_k = T.axis.reduce(T.int32(640), k_0 * T.int32(4) + k_1 * T.int32(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv311_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv311_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(4), T.int32(10)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(2), i0_0_i1_0_i2_0_fused // T.int32(1024) + ax0)
                            v1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused % T.int32(1024) // T.int32(32) * T.int32(32) + i0_2_i1_2_i2_2_fused // T.int32(8) * T.int32(4) + ax1)
                            v2 = T.axis.spatial(T.int32(2560), i0_0_i1_0_i2_0_fused % T.int32(32) * T.int32(80) + i0_2_i1_2_i2_2_fused % T.int32(8) * T.int32(10) + ax2)
                            T.reads(matmul_local[v0, v1, v2], param_1[v2], lv318[v0, v1, v2])
                            T.writes(T_multiply[v0, v1, v2])
                            T_multiply[v0, v1, v2] = (matmul_local[v0, v1, v2] + param_1[v2]) * lv318[v0, v1, v2]

    @T.prim_func
    def fused_matmul24_add17_add18(lv319: T.Buffer((T.int32(2), T.int32(1024), T.int32(2560)), "float32"), param_0: T.Buffer((T.int32(2560), T.int32(640)), "float32"), param_1: T.Buffer((T.int32(640),), "float32"), lv310: T.Buffer((T.int32(2), T.int32(1024), T.int32(640)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(1024), T.int32(640)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(2), T.int32(1024), T.int32(640)), scope="local")
        lv319_shared = T.alloc_buffer((T.int32(2), T.int32(1024), T.int32(2560)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(2560), T.int32(640)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(1280), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(1), T.int32(2), T.int32(1), T.int32(8), T.int32(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(2), i0_4_init + i0_0_i1_0_i2_0_fused // T.int32(640) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused % T.int32(640) // T.int32(20) * T.int32(32) + i0_2_i1_2_i2_2_fused // T.int32(8) * T.int32(8) + i1_3_init * T.int32(8) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(640), i2_4_init + i0_0_i1_0_i2_0_fused % T.int32(20) * T.int32(32) + i0_1_i1_1_i2_1_fused * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(8) * T.int32(2) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(320)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(8)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                                with T.block("lv319_shared"):
                                    v0 = T.axis.spatial(T.int32(2), i0_0_i1_0_i2_0_fused // T.int32(640))
                                    v1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused % T.int32(640) // T.int32(20) * T.int32(32) + (ax0_ax1_ax2_fused_0 * T.int32(32) + ax0_ax1_ax2_fused_1) // T.int32(8))
                                    v2 = T.axis.spatial(T.int32(2560), k_0 * T.int32(8) + (ax0_ax1_ax2_fused_0 * T.int32(32) + ax0_ax1_ax2_fused_1) % T.int32(8))
                                    T.reads(lv319[v0, v1, v2])
                                    T.writes(lv319_shared[v0, v1, v2])
                                    lv319_shared[v0, v1, v2] = lv319[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int32(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(2560), k_0 * T.int32(8) + (ax0_ax1_fused_0 * T.int32(32) + ax0_ax1_fused_1) // T.int32(32))
                                    v1 = T.axis.spatial(T.int32(640), i0_0_i1_0_i2_0_fused % T.int32(20) * T.int32(32) + (ax0_ax1_fused_0 * T.int32(32) + ax0_ax1_fused_1) % T.int32(32))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(8), T.int32(1), T.int32(8), T.int32(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(2), i0_4 + i0_0_i1_0_i2_0_fused // T.int32(640) + i0_3)
                                v_i1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused % T.int32(640) // T.int32(20) * T.int32(32) + i0_2_i1_2_i2_2_fused // T.int32(8) * T.int32(8) + i1_3 * T.int32(8) + i1_4)
                                v_i2 = T.axis.spatial(T.int32(640), i2_4 + i0_0_i1_0_i2_0_fused % T.int32(20) * T.int32(32) + i0_1_i1_1_i2_1_fused * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(8) * T.int32(2) + i2_3)
                                v_k = T.axis.reduce(T.int32(2560), k_0 * T.int32(8) + k_1 * T.int32(8) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv319_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv319_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(8), T.int32(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(2), i0_0_i1_0_i2_0_fused // T.int32(640) + ax0)
                            v1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused % T.int32(640) // T.int32(20) * T.int32(32) + i0_2_i1_2_i2_2_fused // T.int32(8) * T.int32(8) + ax1)
                            v2 = T.axis.spatial(T.int32(640), i0_0_i1_0_i2_0_fused % T.int32(20) * T.int32(32) + i0_1_i1_1_i2_1_fused * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(8) * T.int32(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2], param_1[v2], lv310[v0, v1, v2])
                            T.writes(T_add[v0, v1, v2])
                            T_add[v0, v1, v2] = matmul_local[v0, v1, v2] + param_1[v2] + lv310[v0, v1, v2]

    @T.prim_func
    def fused_matmul25_add24_add25(lv484: T.Buffer((T.int32(2), T.int32(256), T.int32(1280)), "float32"), param_0: T.Buffer((T.int32(1280), T.int32(1280)), "float32"), param_1: T.Buffer((T.int32(1280),), "float32"), lv460: T.Buffer((T.int32(2), T.int32(256), T.int32(1280)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(256), T.int32(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(2), T.int32(256), T.int32(1280)), scope="local")
        lv484_shared = T.alloc_buffer((T.int32(2), T.int32(256), T.int32(1280)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(1280), T.int32(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(640), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(2), T.int32(2), T.int32(2), T.int32(4), T.int32(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(2), i0_3_init * T.int32(2) + i0_4_init)
                            v_i1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused // T.int32(40) * T.int32(16) + i0_2_i1_2_i2_2_fused // T.int32(16) * T.int32(8) + i1_3_init * T.int32(4) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(1280), i2_4_init + i0_0_i1_0_i2_0_fused % T.int32(40) * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(16) * T.int32(2) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(320)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                                with T.block("lv484_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_fused_0 * T.int32(32) + ax0_ax1_ax2_fused_1) // T.int32(64))
                                    v1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused // T.int32(40) * T.int32(16) + (ax0_ax1_ax2_fused_0 * T.int32(32) + ax0_ax1_ax2_fused_1) % T.int32(64) // T.int32(4))
                                    v2 = T.axis.spatial(T.int32(1280), k_0 * T.int32(4) + (ax0_ax1_ax2_fused_0 * T.int32(32) + ax0_ax1_ax2_fused_1) % T.int32(4))
                                    T.reads(lv484[v0, v1, v2])
                                    T.writes(lv484_shared[v0, v1, v2])
                                    lv484_shared[v0, v1, v2] = lv484[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int32(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(1280), k_0 * T.int32(4) + (ax0_ax1_fused_0 * T.int32(32) + ax0_ax1_fused_1) // T.int32(32))
                                    v1 = T.axis.spatial(T.int32(1280), i0_0_i1_0_i2_0_fused % T.int32(40) * T.int32(32) + (ax0_ax1_fused_0 * T.int32(32) + ax0_ax1_fused_1) % T.int32(32))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(2), T.int32(1), T.int32(2), T.int32(2), T.int32(2), T.int32(2), T.int32(4), T.int32(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(2), i0_3 * T.int32(2) + i0_4)
                                v_i1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused // T.int32(40) * T.int32(16) + i0_2_i1_2_i2_2_fused // T.int32(16) * T.int32(8) + i1_3 * T.int32(4) + i1_4)
                                v_i2 = T.axis.spatial(T.int32(1280), i2_4 + i0_0_i1_0_i2_0_fused % T.int32(40) * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(16) * T.int32(2) + i2_3)
                                v_k = T.axis.reduce(T.int32(1280), k_0 * T.int32(4) + k_1 * T.int32(2) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv484_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv484_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(2), T.int32(8), T.int32(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(2), ax0)
                            v1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused // T.int32(40) * T.int32(16) + i0_2_i1_2_i2_2_fused // T.int32(16) * T.int32(8) + ax1)
                            v2 = T.axis.spatial(T.int32(1280), i0_0_i1_0_i2_0_fused % T.int32(40) * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(16) * T.int32(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2], param_1[v2], lv460[v0, v1, v2])
                            T.writes(T_add[v0, v1, v2])
                            T_add[v0, v1, v2] = matmul_local[v0, v1, v2] + param_1[v2] + lv460[v0, v1, v2]

    @T.prim_func
    def fused_matmul26_multiply11(lv470: T.Buffer((T.int32(16), T.int32(256), T.int32(160)), "float32"), lv477: T.Buffer((T.int32(16), T.int32(160), T.int32(256)), "float32"), T_multiply: T.Buffer((T.int32(16), T.int32(256), T.int32(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(16), T.int32(256), T.int32(256)), scope="local")
        lv470_shared = T.alloc_buffer((T.int32(16), T.int32(256), T.int32(160)), scope="shared")
        lv477_shared = T.alloc_buffer((T.int32(16), T.int32(160), T.int32(256)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(4)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(16), i0_4_init + i0_0_i1_0_i2_0_fused // T.int32(16) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused % T.int32(16) // T.int32(4) * T.int32(64) + i0_1_i1_1_i2_1_fused * T.int32(32) + i0_2_i1_2_i2_2_fused // T.int32(16) * T.int32(2) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused % T.int32(4) * T.int32(64) + i0_2_i1_2_i2_2_fused % T.int32(16) * T.int32(4) + i2_3_init * T.int32(4) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(5)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(8)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                                with T.block("lv470_shared"):
                                    v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(16))
                                    v1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused % T.int32(16) // T.int32(4) * T.int32(64) + (ax0_ax1_ax2_fused_0 * T.int32(256) + ax0_ax1_ax2_fused_1) // T.int32(32))
                                    v2 = T.axis.spatial(T.int32(160), k_0 * T.int32(32) + (ax0_ax1_ax2_fused_0 * T.int32(256) + ax0_ax1_ax2_fused_1) % T.int32(32))
                                    T.reads(lv470[v0, v1, v2])
                                    T.writes(lv470_shared[v0, v1, v2])
                                    lv470_shared[v0, v1, v2] = lv470[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int32(8)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                                with T.block("lv477_shared"):
                                    v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(16))
                                    v1 = T.axis.spatial(T.int32(160), k_0 * T.int32(32) + (ax0_ax1_ax2_fused_0 * T.int32(256) + ax0_ax1_ax2_fused_1) // T.int32(64))
                                    v2 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused % T.int32(4) * T.int32(64) + (ax0_ax1_ax2_fused_0 * T.int32(256) + ax0_ax1_ax2_fused_1) % T.int32(64))
                                    T.reads(lv477[v0, v1, v2])
                                    T.writes(lv477_shared[v0, v1, v2])
                                    lv477_shared[v0, v1, v2] = lv477[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(8), T.int32(1), T.int32(2), T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(4)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(16), i0_4 + i0_0_i1_0_i2_0_fused // T.int32(16) + i0_3)
                                v_i1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused % T.int32(16) // T.int32(4) * T.int32(64) + i0_1_i1_1_i2_1_fused * T.int32(32) + i0_2_i1_2_i2_2_fused // T.int32(16) * T.int32(2) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused % T.int32(4) * T.int32(64) + i0_2_i1_2_i2_2_fused % T.int32(16) * T.int32(4) + i2_3 * T.int32(4) + i2_4)
                                v_k = T.axis.reduce(T.int32(160), k_0 * T.int32(32) + k_1 * T.int32(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv470_shared[v_i0, v_i1, v_k], lv477_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv470_shared[v_i0, v_i1, v_k] * lv477_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(2), T.int32(4)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(16) + ax0)
                            v1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused % T.int32(16) // T.int32(4) * T.int32(64) + i0_1_i1_1_i2_1_fused * T.int32(32) + i0_2_i1_2_i2_2_fused // T.int32(16) * T.int32(2) + ax1)
                            v2 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused % T.int32(4) * T.int32(64) + i0_2_i1_2_i2_2_fused % T.int32(16) * T.int32(4) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(T_multiply[v0, v1, v2])
                            T_multiply[v0, v1, v2] = matmul_local[v0, v1, v2] * T.float32(0.079056940972805023)

    @T.prim_func
    def fused_matmul29_multiply12(lv498: T.Buffer((T.int32(16), T.int32(256), T.int32(160)), "float32"), lv505: T.Buffer((T.int32(16), T.int32(160), T.int32(77)), "float32"), T_multiply: T.Buffer((T.int32(16), T.int32(256), T.int32(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(16), T.int32(256), T.int32(77)), scope="local")
        lv498_shared = T.alloc_buffer((T.int32(16), T.int32(256), T.int32(160)), scope="shared")
        lv505_shared = T.alloc_buffer((T.int32(16), T.int32(160), T.int32(77)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(224), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(1), T.int32(11), T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(16), i0_4_init + i0_0_i1_0_i2_0_fused // T.int32(8) * T.int32(2) + i0_1_i1_1_i2_1_fused + i0_3_init)
                            v_i1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused % T.int32(8) * T.int32(32) + i0_2_i1_2_i2_2_fused // T.int32(7) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(77), i2_4_init + i0_2_i1_2_i2_2_fused % T.int32(7) * T.int32(11) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(10)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(5)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(224), thread="threadIdx.x"):
                                with T.block("lv498_shared"):
                                    v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(8) * T.int32(2) + (ax0_ax1_ax2_fused_0 * T.int32(224) + ax0_ax1_ax2_fused_1) // T.int32(512))
                                    v1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused % T.int32(8) * T.int32(32) + (ax0_ax1_ax2_fused_0 * T.int32(224) + ax0_ax1_ax2_fused_1) % T.int32(512) // T.int32(16))
                                    v2 = T.axis.spatial(T.int32(160), k_0 * T.int32(16) + (ax0_ax1_ax2_fused_0 * T.int32(224) + ax0_ax1_ax2_fused_1) % T.int32(16))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int32(224) + ax0_ax1_ax2_fused_1 < T.int32(1024))
                                    T.reads(lv498[v0, v1, v2])
                                    T.writes(lv498_shared[v0, v1, v2])
                                    lv498_shared[v0, v1, v2] = lv498[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int32(11)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(224), thread="threadIdx.x"):
                                with T.block("lv505_shared"):
                                    v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(8) * T.int32(2) + (ax0_ax1_ax2_fused_0 * T.int32(224) + ax0_ax1_ax2_fused_1) // T.int32(1232))
                                    v1 = T.axis.spatial(T.int32(160), k_0 * T.int32(16) + (ax0_ax1_ax2_fused_0 * T.int32(224) + ax0_ax1_ax2_fused_1) % T.int32(1232) // T.int32(77))
                                    v2 = T.axis.spatial(T.int32(77), (ax0_ax1_ax2_fused_0 * T.int32(224) + ax0_ax1_ax2_fused_1) % T.int32(77))
                                    T.reads(lv505[v0, v1, v2])
                                    T.writes(lv505_shared[v0, v1, v2])
                                    lv505_shared[v0, v1, v2] = lv505[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(4), T.int32(1), T.int32(1), T.int32(11), T.int32(4), T.int32(1), T.int32(1), T.int32(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(16), i0_4 + i0_0_i1_0_i2_0_fused // T.int32(8) * T.int32(2) + i0_1_i1_1_i2_1_fused + i0_3)
                                v_i1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused % T.int32(8) * T.int32(32) + i0_2_i1_2_i2_2_fused // T.int32(7) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int32(77), i2_4 + i0_2_i1_2_i2_2_fused % T.int32(7) * T.int32(11) + i2_3)
                                v_k = T.axis.reduce(T.int32(160), k_0 * T.int32(16) + k_1 * T.int32(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv498_shared[v_i0, v_i1, v_k], lv505_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv498_shared[v_i0, v_i1, v_k] * lv505_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(1), T.int32(11)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(8) * T.int32(2) + i0_1_i1_1_i2_1_fused + ax0)
                            v1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused % T.int32(8) * T.int32(32) + i0_2_i1_2_i2_2_fused // T.int32(7) + ax1)
                            v2 = T.axis.spatial(T.int32(77), i0_2_i1_2_i2_2_fused % T.int32(7) * T.int32(11) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(T_multiply[v0, v1, v2])
                            T_multiply[v0, v1, v2] = matmul_local[v0, v1, v2] * T.float32(0.079056940972805023)

    @T.prim_func
    def fused_matmul31_add26_gelu2(lv517: T.Buffer((T.int32(2), T.int32(256), T.int32(1280)), "float32"), param_0: T.Buffer((T.int32(1280), T.int32(5120)), "float32"), param_1: T.Buffer((T.int32(5120),), "float32"), T_multiply: T.Buffer((T.int32(2), T.int32(256), T.int32(5120)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul = T.alloc_buffer((T.int32(2), T.int32(256), T.int32(5120)))
        matmul_local = T.alloc_buffer((T.int32(2), T.int32(256), T.int32(5120)), scope="local")
        lv517_shared = T.alloc_buffer((T.int32(2), T.int32(256), T.int32(1280)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(1280), T.int32(5120)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(640), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(16), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(2), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(2), i0_4_init + i0_3_init)
                            v_i1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused // T.int32(80) * T.int32(32) + i0_1_i1_1_i2_1_fused // T.int32(4) * T.int32(8) + i0_2_i1_2_i2_2_fused // T.int32(16) * T.int32(2) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(5120), i2_4_init + i0_0_i1_0_i2_0_fused % T.int32(80) * T.int32(64) + i0_1_i1_1_i2_1_fused % T.int32(4) * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(16) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(256), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(320)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("lv517_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) // T.int32(128))
                                    v1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused // T.int32(80) * T.int32(32) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(128) // T.int32(4))
                                    v2 = T.axis.spatial(T.int32(1280), k_0 * T.int32(4) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(4))
                                    T.reads(lv517[v0, v1, v2])
                                    T.writes(lv517_shared[v0, v1, v2])
                                    lv517_shared[v0, v1, v2] = lv517[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int32(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(1280), k_0 * T.int32(4) + (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) // T.int32(64))
                                    v1 = T.axis.spatial(T.int32(5120), i0_0_i1_0_i2_0_fused % T.int32(80) * T.int32(64) + (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) % T.int32(64))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(4), T.int32(2), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(2), i0_4 + i0_3)
                                v_i1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused // T.int32(80) * T.int32(32) + i0_1_i1_1_i2_1_fused // T.int32(4) * T.int32(8) + i0_2_i1_2_i2_2_fused // T.int32(16) * T.int32(2) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int32(5120), i2_4 + i0_0_i1_0_i2_0_fused % T.int32(80) * T.int32(64) + i0_1_i1_1_i2_1_fused % T.int32(4) * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(16) + i2_3)
                                v_k = T.axis.reduce(T.int32(1280), k_0 * T.int32(4) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv517_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(256), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv517_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(2), T.int32(2), T.int32(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(2), ax0)
                            v1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused // T.int32(80) * T.int32(32) + i0_1_i1_1_i2_1_fused // T.int32(4) * T.int32(8) + i0_2_i1_2_i2_2_fused // T.int32(16) * T.int32(2) + ax1)
                            v2 = T.axis.spatial(T.int32(5120), i0_0_i1_0_i2_0_fused % T.int32(80) * T.int32(64) + i0_1_i1_1_i2_1_fused % T.int32(4) * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(16) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int32(40)):
                    with T.block("T_multiply_2"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_fused_0 * T.int32(65536) + ax0_ax1_ax2_fused_1 * T.int32(256) + ax0_ax1_ax2_fused_2) // T.int32(1310720))
                        v_ax1 = T.axis.spatial(T.int32(256), (ax0_ax1_ax2_fused_0 * T.int32(65536) + ax0_ax1_ax2_fused_1 * T.int32(256) + ax0_ax1_ax2_fused_2) % T.int32(1310720) // T.int32(5120))
                        v_ax2 = T.axis.spatial(T.int32(5120), (ax0_ax1_ax2_fused_0 * T.int32(65536) + ax0_ax1_ax2_fused_1 * T.int32(256) + ax0_ax1_ax2_fused_2) % T.int32(5120))
                        T.reads(matmul[v_ax0, v_ax1, v_ax2], param_1[v_ax2])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2])
                        T_multiply[v_ax0, v_ax1, v_ax2] = (matmul[v_ax0, v_ax1, v_ax2] + param_1[v_ax2]) * (T.float32(0.5) + T.erf((matmul[v_ax0, v_ax1, v_ax2] + param_1[v_ax2]) * T.float32(0.70710678118654757)) * T.float32(0.5))

    @T.prim_func
    def fused_matmul31_add26_multiply13(lv517: T.Buffer((T.int32(2), T.int32(256), T.int32(1280)), "float32"), param_0: T.Buffer((T.int32(1280), T.int32(5120)), "float32"), param_1: T.Buffer((T.int32(5120),), "float32"), lv524: T.Buffer((T.int32(2), T.int32(256), T.int32(5120)), "float32"), T_multiply: T.Buffer((T.int32(2), T.int32(256), T.int32(5120)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(2), T.int32(256), T.int32(5120)), scope="local")
        lv517_shared = T.alloc_buffer((T.int32(2), T.int32(256), T.int32(1280)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(1280), T.int32(5120)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(1280), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(2), T.int32(1), T.int32(2), T.int32(1), T.int32(2), T.int32(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(2), i0_4_init + i0_3_init)
                            v_i1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused // T.int32(160) * T.int32(32) + i0_1_i1_1_i2_1_fused // T.int32(2) * T.int32(16) + i0_2_i1_2_i2_2_fused // T.int32(8) * T.int32(2) + i1_3_init * T.int32(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(5120), i2_4_init + i0_0_i1_0_i2_0_fused % T.int32(160) * T.int32(32) + i0_1_i1_1_i2_1_fused % T.int32(2) * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(8) * T.int32(2) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(160)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(8)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("lv517_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) // T.int32(256))
                                    v1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused // T.int32(160) * T.int32(32) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(256) // T.int32(8))
                                    v2 = T.axis.spatial(T.int32(1280), k_0 * T.int32(8) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(8))
                                    T.reads(lv517[v0, v1, v2])
                                    T.writes(lv517_shared[v0, v1, v2])
                                    lv517_shared[v0, v1, v2] = lv517[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int32(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(1280), k_0 * T.int32(8) + (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) // T.int32(32))
                                    v1 = T.axis.spatial(T.int32(5120), i0_0_i1_0_i2_0_fused % T.int32(160) * T.int32(32) + (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) % T.int32(32))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(4), T.int32(2), T.int32(1), T.int32(2), T.int32(2), T.int32(1), T.int32(2), T.int32(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(2), i0_4 + i0_3)
                                v_i1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused // T.int32(160) * T.int32(32) + i0_1_i1_1_i2_1_fused // T.int32(2) * T.int32(16) + i0_2_i1_2_i2_2_fused // T.int32(8) * T.int32(2) + i1_3 * T.int32(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int32(5120), i2_4 + i0_0_i1_0_i2_0_fused % T.int32(160) * T.int32(32) + i0_1_i1_1_i2_1_fused % T.int32(2) * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(8) * T.int32(2) + i2_3)
                                v_k = T.axis.reduce(T.int32(1280), k_0 * T.int32(8) + k_1 * T.int32(2) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv517_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv517_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(2), T.int32(2), T.int32(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(2), ax0)
                            v1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused // T.int32(160) * T.int32(32) + i0_1_i1_1_i2_1_fused // T.int32(2) * T.int32(16) + i0_2_i1_2_i2_2_fused // T.int32(8) * T.int32(2) + ax1)
                            v2 = T.axis.spatial(T.int32(5120), i0_0_i1_0_i2_0_fused % T.int32(160) * T.int32(32) + i0_1_i1_1_i2_1_fused % T.int32(2) * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(8) * T.int32(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2], param_1[v2], lv524[v0, v1, v2])
                            T.writes(T_multiply[v0, v1, v2])
                            T_multiply[v0, v1, v2] = (matmul_local[v0, v1, v2] + param_1[v2]) * lv524[v0, v1, v2]

    @T.prim_func
    def fused_matmul32_add24_add25(lv525: T.Buffer((T.int32(2), T.int32(256), T.int32(5120)), "float32"), param_0: T.Buffer((T.int32(5120), T.int32(1280)), "float32"), param_1: T.Buffer((T.int32(1280),), "float32"), lv516: T.Buffer((T.int32(2), T.int32(256), T.int32(1280)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(256), T.int32(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(2), T.int32(256), T.int32(1280)), scope="local")
        lv525_shared = T.alloc_buffer((T.int32(2), T.int32(256), T.int32(5120)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(5120), T.int32(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(160), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(1), T.int32(2), T.int32(2), T.int32(2), T.int32(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(2), i0_3_init * T.int32(2) + i0_4_init)
                            v_i1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused // T.int32(32) * T.int32(64) + i0_1_i1_1_i2_1_fused * T.int32(32) + i0_2_i1_2_i2_2_fused // T.int32(10) * T.int32(2) + i1_3_init * T.int32(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(1280), i0_0_i1_0_i2_0_fused % T.int32(32) * T.int32(40) + i0_2_i1_2_i2_2_fused % T.int32(10) * T.int32(4) + i2_3_init * T.int32(2) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(256)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(16)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(160), thread="threadIdx.x"):
                                with T.block("lv525_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_fused_0 * T.int32(160) + ax0_ax1_ax2_fused_1) // T.int32(1280))
                                    v1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused // T.int32(32) * T.int32(64) + (ax0_ax1_ax2_fused_0 * T.int32(160) + ax0_ax1_ax2_fused_1) % T.int32(1280) // T.int32(20))
                                    v2 = T.axis.spatial(T.int32(5120), k_0 * T.int32(20) + (ax0_ax1_ax2_fused_0 * T.int32(160) + ax0_ax1_ax2_fused_1) % T.int32(20))
                                    T.reads(lv525[v0, v1, v2])
                                    T.writes(lv525_shared[v0, v1, v2])
                                    lv525_shared[v0, v1, v2] = lv525[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int32(5)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(160), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(5120), k_0 * T.int32(20) + (ax0_ax1_fused_0 * T.int32(160) + ax0_ax1_fused_1) // T.int32(40))
                                    v1 = T.axis.spatial(T.int32(1280), i0_0_i1_0_i2_0_fused % T.int32(32) * T.int32(40) + (ax0_ax1_fused_0 * T.int32(160) + ax0_ax1_fused_1) % T.int32(40))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(5), T.int32(1), T.int32(1), T.int32(2), T.int32(4), T.int32(2), T.int32(2), T.int32(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(2), i0_3 * T.int32(2) + i0_4)
                                v_i1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused // T.int32(32) * T.int32(64) + i0_1_i1_1_i2_1_fused * T.int32(32) + i0_2_i1_2_i2_2_fused // T.int32(10) * T.int32(2) + i1_3 * T.int32(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int32(1280), i0_0_i1_0_i2_0_fused % T.int32(32) * T.int32(40) + i0_2_i1_2_i2_2_fused % T.int32(10) * T.int32(4) + i2_3 * T.int32(2) + i2_4)
                                v_k = T.axis.reduce(T.int32(5120), k_0 * T.int32(20) + k_1 * T.int32(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv525_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv525_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(2), T.int32(2), T.int32(4)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(2), ax0)
                            v1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused // T.int32(32) * T.int32(64) + i0_1_i1_1_i2_1_fused * T.int32(32) + i0_2_i1_2_i2_2_fused // T.int32(10) * T.int32(2) + ax1)
                            v2 = T.axis.spatial(T.int32(1280), i0_0_i1_0_i2_0_fused % T.int32(32) * T.int32(40) + i0_2_i1_2_i2_2_fused % T.int32(10) * T.int32(4) + ax2)
                            T.reads(matmul_local[v0, v1, v2], param_1[v2], lv516[v0, v1, v2])
                            T.writes(T_add[v0, v1, v2])
                            T_add[v0, v1, v2] = matmul_local[v0, v1, v2] + param_1[v2] + lv516[v0, v1, v2]

    @T.prim_func
    def fused_matmul33_add30_add31(lv725: T.Buffer((T.int32(2), T.int32(64), T.int32(1280)), "float32"), param_0: T.Buffer((T.int32(1280), T.int32(1280)), "float32"), param_1: T.Buffer((T.int32(1280),), "float32"), lv701: T.Buffer((T.int32(2), T.int32(64), T.int32(1280)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(64), T.int32(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(2), T.int32(64), T.int32(1280)), scope="local")
        lv725_shared = T.alloc_buffer((T.int32(2), T.int32(64), T.int32(1280)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(1280), T.int32(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(2), T.int32(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(2), i0_4_init + i0_0_i1_0_i2_0_fused // T.int32(10) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int32(64), i0_0_i1_0_i2_0_fused % T.int32(10) // T.int32(5) * T.int32(32) + i0_1_i1_1_i2_1_fused * T.int32(16) + i0_2_i1_2_i2_2_fused // T.int32(128) * T.int32(8) + i1_3_init * T.int32(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(1280), i0_0_i1_0_i2_0_fused % T.int32(5) * T.int32(256) + i0_2_i1_2_i2_2_fused % T.int32(128) * T.int32(2) + i2_3_init * T.int32(2) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(80)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                                with T.block("lv725_shared"):
                                    v0 = T.axis.spatial(T.int32(2), i0_0_i1_0_i2_0_fused // T.int32(10))
                                    v1 = T.axis.spatial(T.int32(64), i0_0_i1_0_i2_0_fused % T.int32(10) // T.int32(5) * T.int32(32) + (ax0_ax1_ax2_fused_0 * T.int32(256) + ax0_ax1_ax2_fused_1) // T.int32(16))
                                    v2 = T.axis.spatial(T.int32(1280), k_0 * T.int32(16) + (ax0_ax1_ax2_fused_0 * T.int32(256) + ax0_ax1_ax2_fused_1) % T.int32(16))
                                    T.reads(lv725[v0, v1, v2])
                                    T.writes(lv725_shared[v0, v1, v2])
                                    lv725_shared[v0, v1, v2] = lv725[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int32(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(1280), k_0 * T.int32(16) + (ax0_ax1_fused_0 * T.int32(256) + ax0_ax1_fused_1) // T.int32(256))
                                    v1 = T.axis.spatial(T.int32(1280), i0_0_i1_0_i2_0_fused % T.int32(5) * T.int32(256) + (ax0_ax1_fused_0 * T.int32(256) + ax0_ax1_fused_1) % T.int32(256))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(8), T.int32(1), T.int32(4), T.int32(1), T.int32(2), T.int32(1), T.int32(2), T.int32(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(2), i0_4 + i0_0_i1_0_i2_0_fused // T.int32(10) + i0_3)
                                v_i1 = T.axis.spatial(T.int32(64), i0_0_i1_0_i2_0_fused % T.int32(10) // T.int32(5) * T.int32(32) + i0_1_i1_1_i2_1_fused * T.int32(16) + i0_2_i1_2_i2_2_fused // T.int32(128) * T.int32(8) + i1_3 * T.int32(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int32(1280), i0_0_i1_0_i2_0_fused % T.int32(5) * T.int32(256) + i0_2_i1_2_i2_2_fused % T.int32(128) * T.int32(2) + i2_3 * T.int32(2) + i2_4)
                                v_k = T.axis.reduce(T.int32(1280), k_0 * T.int32(16) + k_1 * T.int32(2) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv725_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv725_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(8), T.int32(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(2), i0_0_i1_0_i2_0_fused // T.int32(10) + ax0)
                            v1 = T.axis.spatial(T.int32(64), i0_0_i1_0_i2_0_fused % T.int32(10) // T.int32(5) * T.int32(32) + i0_1_i1_1_i2_1_fused * T.int32(16) + i0_2_i1_2_i2_2_fused // T.int32(128) * T.int32(8) + ax1)
                            v2 = T.axis.spatial(T.int32(1280), i0_0_i1_0_i2_0_fused % T.int32(5) * T.int32(256) + i0_2_i1_2_i2_2_fused % T.int32(128) * T.int32(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2], param_1[v2], lv701[v0, v1, v2])
                            T.writes(T_add[v0, v1, v2])
                            T_add[v0, v1, v2] = matmul_local[v0, v1, v2] + param_1[v2] + lv701[v0, v1, v2]

    @T.prim_func
    def fused_matmul34_multiply14(lv711: T.Buffer((T.int32(16), T.int32(64), T.int32(160)), "float32"), lv718: T.Buffer((T.int32(16), T.int32(160), T.int32(64)), "float32"), T_multiply: T.Buffer((T.int32(16), T.int32(64), T.int32(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(16), T.int32(64), T.int32(64)), scope="local")
        lv711_shared = T.alloc_buffer((T.int32(16), T.int32(64), T.int32(160)), scope="shared")
        lv718_shared = T.alloc_buffer((T.int32(16), T.int32(160), T.int32(64)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(16), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(1), T.int32(4), T.int32(2), T.int32(1), T.int32(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(2) * T.int32(2) + i0_3_init * T.int32(2) + i0_4_init)
                            v_i1 = T.axis.spatial(T.int32(64), i0_2_i1_2_i2_2_fused // T.int32(4) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(64), i0_0_i1_0_i2_0_fused % T.int32(2) * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(4) * T.int32(8) + i2_3_init * T.int32(2) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(8)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(10)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                                with T.block("lv711_shared"):
                                    v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(2) * T.int32(2) + (ax0_ax1_ax2_fused_0 * T.int32(256) + ax0_ax1_ax2_fused_1) // T.int32(1280))
                                    v1 = T.axis.spatial(T.int32(64), (ax0_ax1_ax2_fused_0 * T.int32(256) + ax0_ax1_ax2_fused_1) % T.int32(1280) // T.int32(20))
                                    v2 = T.axis.spatial(T.int32(160), k_0 * T.int32(20) + (ax0_ax1_ax2_fused_0 * T.int32(256) + ax0_ax1_ax2_fused_1) % T.int32(20))
                                    T.reads(lv711[v0, v1, v2])
                                    T.writes(lv711_shared[v0, v1, v2])
                                    lv711_shared[v0, v1, v2] = lv711[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int32(5)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                                with T.block("lv718_shared"):
                                    v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(2) * T.int32(2) + (ax0_ax1_ax2_fused_0 * T.int32(256) + ax0_ax1_ax2_fused_1) // T.int32(640))
                                    v1 = T.axis.spatial(T.int32(160), k_0 * T.int32(20) + (ax0_ax1_ax2_fused_0 * T.int32(256) + ax0_ax1_ax2_fused_1) % T.int32(640) // T.int32(32))
                                    v2 = T.axis.spatial(T.int32(64), i0_0_i1_0_i2_0_fused % T.int32(2) * T.int32(32) + (ax0_ax1_ax2_fused_0 * T.int32(256) + ax0_ax1_ax2_fused_1) % T.int32(32))
                                    T.reads(lv718[v0, v1, v2])
                                    T.writes(lv718_shared[v0, v1, v2])
                                    lv718_shared[v0, v1, v2] = lv718[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(20), T.int32(1), T.int32(1), T.int32(4), T.int32(1), T.int32(2), T.int32(1), T.int32(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(2) * T.int32(2) + i0_3 * T.int32(2) + i0_4)
                                v_i1 = T.axis.spatial(T.int32(64), i0_2_i1_2_i2_2_fused // T.int32(4) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int32(64), i0_0_i1_0_i2_0_fused % T.int32(2) * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(4) * T.int32(8) + i2_3 * T.int32(2) + i2_4)
                                v_k = T.axis.reduce(T.int32(160), k_0 * T.int32(20) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv711_shared[v_i0, v_i1, v_k], lv718_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv711_shared[v_i0, v_i1, v_k] * lv718_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(2), T.int32(1), T.int32(8)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(2) * T.int32(2) + ax0)
                            v1 = T.axis.spatial(T.int32(64), i0_2_i1_2_i2_2_fused // T.int32(4) + ax1)
                            v2 = T.axis.spatial(T.int32(64), i0_0_i1_0_i2_0_fused % T.int32(2) * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(4) * T.int32(8) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(T_multiply[v0, v1, v2])
                            T_multiply[v0, v1, v2] = matmul_local[v0, v1, v2] * T.float32(0.079056940972805023)

    @T.prim_func
    def fused_matmul36_multiply15(lv739: T.Buffer((T.int32(16), T.int32(64), T.int32(160)), "float32"), lv746: T.Buffer((T.int32(16), T.int32(160), T.int32(77)), "float32"), T_multiply: T.Buffer((T.int32(16), T.int32(64), T.int32(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(16), T.int32(64), T.int32(77)), scope="local")
        lv739_shared = T.alloc_buffer((T.int32(16), T.int32(64), T.int32(160)), scope="shared")
        lv746_shared = T.alloc_buffer((T.int32(16), T.int32(160), T.int32(77)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(28), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(176), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(8), T.int32(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(16), i0_4_init + i0_0_i1_0_i2_0_fused // T.int32(7) * T.int32(4) + i0_2_i1_2_i2_2_fused // T.int32(88) * T.int32(2) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int32(64), i0_2_i1_2_i2_2_fused % T.int32(88) // T.int32(11) * T.int32(8) + i1_3_init * T.int32(8) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(77), i2_4_init + i0_0_i1_0_i2_0_fused % T.int32(7) * T.int32(11) + i0_2_i1_2_i2_2_fused % T.int32(11) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(20)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(12)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(176), thread="threadIdx.x"):
                                with T.block("lv739_shared"):
                                    v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(7) * T.int32(4) + (ax0_ax1_ax2_fused_0 * T.int32(176) + ax0_ax1_ax2_fused_1) // T.int32(512))
                                    v1 = T.axis.spatial(T.int32(64), (ax0_ax1_ax2_fused_0 * T.int32(176) + ax0_ax1_ax2_fused_1) % T.int32(512) // T.int32(8))
                                    v2 = T.axis.spatial(T.int32(160), k_0 * T.int32(8) + (ax0_ax1_ax2_fused_0 * T.int32(176) + ax0_ax1_ax2_fused_1) % T.int32(8))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int32(176) + ax0_ax1_ax2_fused_1 < T.int32(2048))
                                    T.reads(lv739[v0, v1, v2])
                                    T.writes(lv739_shared[v0, v1, v2])
                                    lv739_shared[v0, v1, v2] = lv739[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int32(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(176), thread="threadIdx.x"):
                                with T.block("lv746_shared"):
                                    v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(7) * T.int32(4) + (ax0_ax1_ax2_fused_0 * T.int32(176) + ax0_ax1_ax2_fused_1) // T.int32(88))
                                    v1 = T.axis.spatial(T.int32(160), k_0 * T.int32(8) + (ax0_ax1_ax2_fused_0 * T.int32(176) + ax0_ax1_ax2_fused_1) % T.int32(88) // T.int32(11))
                                    v2 = T.axis.spatial(T.int32(77), i0_0_i1_0_i2_0_fused % T.int32(7) * T.int32(11) + (ax0_ax1_ax2_fused_0 * T.int32(176) + ax0_ax1_ax2_fused_1) % T.int32(11))
                                    T.reads(lv746[v0, v1, v2])
                                    T.writes(lv746_shared[v0, v1, v2])
                                    lv746_shared[v0, v1, v2] = lv746[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(8), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(8), T.int32(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(16), i0_4 + i0_0_i1_0_i2_0_fused // T.int32(7) * T.int32(4) + i0_2_i1_2_i2_2_fused // T.int32(88) * T.int32(2) + i0_3)
                                v_i1 = T.axis.spatial(T.int32(64), i0_2_i1_2_i2_2_fused % T.int32(88) // T.int32(11) * T.int32(8) + i1_3 * T.int32(8) + i1_4)
                                v_i2 = T.axis.spatial(T.int32(77), i2_4 + i0_0_i1_0_i2_0_fused % T.int32(7) * T.int32(11) + i0_2_i1_2_i2_2_fused % T.int32(11) + i2_3)
                                v_k = T.axis.reduce(T.int32(160), k_0 * T.int32(8) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv739_shared[v_i0, v_i1, v_k], lv746_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv739_shared[v_i0, v_i1, v_k] * lv746_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(2), T.int32(8), T.int32(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(7) * T.int32(4) + i0_2_i1_2_i2_2_fused // T.int32(88) * T.int32(2) + ax0)
                            v1 = T.axis.spatial(T.int32(64), i0_2_i1_2_i2_2_fused % T.int32(88) // T.int32(11) * T.int32(8) + ax1)
                            v2 = T.axis.spatial(T.int32(77), i0_0_i1_0_i2_0_fused % T.int32(7) * T.int32(11) + i0_2_i1_2_i2_2_fused % T.int32(11) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(T_multiply[v0, v1, v2])
                            T_multiply[v0, v1, v2] = matmul_local[v0, v1, v2] * T.float32(0.079056940972805023)

    @T.prim_func
    def fused_matmul38_add32_gelu3(lv758: T.Buffer((T.int32(2), T.int32(64), T.int32(1280)), "float32"), param_0: T.Buffer((T.int32(1280), T.int32(5120)), "float32"), param_1: T.Buffer((T.int32(5120),), "float32"), T_multiply: T.Buffer((T.int32(2), T.int32(64), T.int32(5120)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul = T.alloc_buffer((T.int32(2), T.int32(64), T.int32(5120)))
        matmul_local = T.alloc_buffer((T.int32(2), T.int32(64), T.int32(5120)), scope="local")
        lv758_shared = T.alloc_buffer((T.int32(2), T.int32(64), T.int32(1280)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(1280), T.int32(5120)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(1), T.int32(4), T.int32(1), T.int32(2), T.int32(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(2), i0_4_init + i0_0_i1_0_i2_0_fused // T.int32(160) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int32(64), i0_1_i1_1_i2_1_fused * T.int32(16) + i0_2_i1_2_i2_2_fused // T.int32(8) * T.int32(2) + i1_3_init * T.int32(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(5120), i2_4_init + i0_0_i1_0_i2_0_fused % T.int32(160) * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(8) * T.int32(4) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(256), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(160)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(8)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("lv758_shared"):
                                    v0 = T.axis.spatial(T.int32(2), i0_0_i1_0_i2_0_fused // T.int32(160))
                                    v1 = T.axis.spatial(T.int32(64), (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) // T.int32(8))
                                    v2 = T.axis.spatial(T.int32(1280), k_0 * T.int32(8) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(8))
                                    T.reads(lv758[v0, v1, v2])
                                    T.writes(lv758_shared[v0, v1, v2])
                                    lv758_shared[v0, v1, v2] = lv758[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int32(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(1280), k_0 * T.int32(8) + (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) // T.int32(32))
                                    v1 = T.axis.spatial(T.int32(5120), i0_0_i1_0_i2_0_fused % T.int32(160) * T.int32(32) + (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) % T.int32(32))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(4), T.int32(8), T.int32(1), T.int32(2), T.int32(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(2), i0_4 + i0_0_i1_0_i2_0_fused // T.int32(160) + i0_3)
                                v_i1 = T.axis.spatial(T.int32(64), i0_1_i1_1_i2_1_fused * T.int32(16) + i0_2_i1_2_i2_2_fused // T.int32(8) * T.int32(2) + i1_3 * T.int32(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int32(5120), i2_4 + i0_0_i1_0_i2_0_fused % T.int32(160) * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(8) * T.int32(4) + i2_3)
                                v_k = T.axis.reduce(T.int32(1280), k_0 * T.int32(8) + k_1 * T.int32(8) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv758_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(256), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv758_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(2), T.int32(4)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(2), i0_0_i1_0_i2_0_fused // T.int32(160) + ax0)
                            v1 = T.axis.spatial(T.int32(64), i0_1_i1_1_i2_1_fused * T.int32(16) + i0_2_i1_2_i2_2_fused // T.int32(8) * T.int32(2) + ax1)
                            v2 = T.axis.spatial(T.int32(5120), i0_0_i1_0_i2_0_fused % T.int32(160) * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(8) * T.int32(4) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int32(10)):
                    with T.block("T_multiply_2"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_fused_0 * T.int32(65536) + ax0_ax1_ax2_fused_1 * T.int32(256) + ax0_ax1_ax2_fused_2) // T.int32(327680))
                        v_ax1 = T.axis.spatial(T.int32(64), (ax0_ax1_ax2_fused_0 * T.int32(65536) + ax0_ax1_ax2_fused_1 * T.int32(256) + ax0_ax1_ax2_fused_2) % T.int32(327680) // T.int32(5120))
                        v_ax2 = T.axis.spatial(T.int32(5120), (ax0_ax1_ax2_fused_0 * T.int32(65536) + ax0_ax1_ax2_fused_1 * T.int32(256) + ax0_ax1_ax2_fused_2) % T.int32(5120))
                        T.reads(matmul[v_ax0, v_ax1, v_ax2], param_1[v_ax2])
                        T.writes(T_multiply[v_ax0, v_ax1, v_ax2])
                        T_multiply[v_ax0, v_ax1, v_ax2] = (matmul[v_ax0, v_ax1, v_ax2] + param_1[v_ax2]) * (T.float32(0.5) + T.erf((matmul[v_ax0, v_ax1, v_ax2] + param_1[v_ax2]) * T.float32(0.70710678118654757)) * T.float32(0.5))

    @T.prim_func
    def fused_matmul38_add32_multiply16(lv758: T.Buffer((T.int32(2), T.int32(64), T.int32(1280)), "float32"), param_0: T.Buffer((T.int32(1280), T.int32(5120)), "float32"), param_1: T.Buffer((T.int32(5120),), "float32"), lv765: T.Buffer((T.int32(2), T.int32(64), T.int32(5120)), "float32"), T_multiply: T.Buffer((T.int32(2), T.int32(64), T.int32(5120)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(2), T.int32(64), T.int32(5120)), scope="local")
        lv758_shared = T.alloc_buffer((T.int32(2), T.int32(64), T.int32(1280)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(1280), T.int32(5120)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(8), T.int32(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(2), i0_4_init + i0_1_i1_1_i2_1_fused + i0_3_init)
                            v_i1 = T.axis.spatial(T.int32(64), i0_0_i1_0_i2_0_fused // T.int32(160) * T.int32(32) + i0_2_i1_2_i2_2_fused // T.int32(16) * T.int32(8) + i1_3_init * T.int32(8) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(5120), i0_0_i1_0_i2_0_fused % T.int32(160) * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(16) * T.int32(2) + i2_3_init * T.int32(2) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(320)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("lv758_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) // T.int32(128))
                                    v1 = T.axis.spatial(T.int32(64), i0_0_i1_0_i2_0_fused // T.int32(160) * T.int32(32) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(128) // T.int32(4))
                                    v2 = T.axis.spatial(T.int32(1280), k_0 * T.int32(4) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(4))
                                    T.reads(lv758[v0, v1, v2])
                                    T.writes(lv758_shared[v0, v1, v2])
                                    lv758_shared[v0, v1, v2] = lv758[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int32(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(1280), k_0 * T.int32(4) + (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) // T.int32(32))
                                    v1 = T.axis.spatial(T.int32(5120), i0_0_i1_0_i2_0_fused % T.int32(160) * T.int32(32) + (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) % T.int32(32))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(1), T.int32(8), T.int32(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(2), i0_4 + i0_1_i1_1_i2_1_fused + i0_3)
                                v_i1 = T.axis.spatial(T.int32(64), i0_0_i1_0_i2_0_fused // T.int32(160) * T.int32(32) + i0_2_i1_2_i2_2_fused // T.int32(16) * T.int32(8) + i1_3 * T.int32(8) + i1_4)
                                v_i2 = T.axis.spatial(T.int32(5120), i0_0_i1_0_i2_0_fused % T.int32(160) * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(16) * T.int32(2) + i2_3 * T.int32(2) + i2_4)
                                v_k = T.axis.reduce(T.int32(1280), k_0 * T.int32(4) + k_1 * T.int32(2) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv758_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv758_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(8), T.int32(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(2), i0_1_i1_1_i2_1_fused + ax0)
                            v1 = T.axis.spatial(T.int32(64), i0_0_i1_0_i2_0_fused // T.int32(160) * T.int32(32) + i0_2_i1_2_i2_2_fused // T.int32(16) * T.int32(8) + ax1)
                            v2 = T.axis.spatial(T.int32(5120), i0_0_i1_0_i2_0_fused % T.int32(160) * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(16) * T.int32(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2], param_1[v2], lv765[v0, v1, v2])
                            T.writes(T_multiply[v0, v1, v2])
                            T_multiply[v0, v1, v2] = (matmul_local[v0, v1, v2] + param_1[v2]) * lv765[v0, v1, v2]

    @T.prim_func
    def fused_matmul39_add30_add31(lv766: T.Buffer((T.int32(2), T.int32(64), T.int32(5120)), "float32"), param_0: T.Buffer((T.int32(5120), T.int32(1280)), "float32"), param_1: T.Buffer((T.int32(1280),), "float32"), lv757: T.Buffer((T.int32(2), T.int32(64), T.int32(1280)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(64), T.int32(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(2), T.int32(64), T.int32(1280)), scope="local")
        lv766_shared = T.alloc_buffer((T.int32(2), T.int32(64), T.int32(5120)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(5120), T.int32(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(40), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(2), T.int32(2), T.int32(1), T.int32(8), T.int32(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(2), i0_3_init + i0_4_init + i0_2_i1_2_i2_2_fused // T.int32(64))
                            v_i1 = T.axis.spatial(T.int32(64), i0_0_i1_0_i2_0_fused // T.int32(10) * T.int32(16) + i1_3_init * T.int32(8) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(1280), i2_4_init + i0_0_i1_0_i2_0_fused % T.int32(10) * T.int32(128) + i0_2_i1_2_i2_2_fused % T.int32(64) * T.int32(2) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(640)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("lv766_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_fused_0 * T.int32(128) + ax0_ax1_ax2_fused_1) // T.int32(128))
                                    v1 = T.axis.spatial(T.int32(64), i0_0_i1_0_i2_0_fused // T.int32(10) * T.int32(16) + (ax0_ax1_ax2_fused_0 * T.int32(128) + ax0_ax1_ax2_fused_1) % T.int32(128) // T.int32(8))
                                    v2 = T.axis.spatial(T.int32(5120), k_0 * T.int32(8) + (ax0_ax1_ax2_fused_0 * T.int32(128) + ax0_ax1_ax2_fused_1) % T.int32(8))
                                    T.reads(lv766[v0, v1, v2])
                                    T.writes(lv766_shared[v0, v1, v2])
                                    lv766_shared[v0, v1, v2] = lv766[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int32(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(5120), k_0 * T.int32(8) + (ax0_ax1_fused_0 * T.int32(128) + ax0_ax1_fused_1) // T.int32(128))
                                    v1 = T.axis.spatial(T.int32(1280), i0_0_i1_0_i2_0_fused % T.int32(10) * T.int32(128) + (ax0_ax1_fused_0 * T.int32(128) + ax0_ax1_fused_1) % T.int32(128))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(1), T.int32(1), T.int32(2), T.int32(2), T.int32(8), T.int32(1), T.int32(8), T.int32(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(2), i0_3 + i0_4 + i0_2_i1_2_i2_2_fused // T.int32(64))
                                v_i1 = T.axis.spatial(T.int32(64), i0_0_i1_0_i2_0_fused // T.int32(10) * T.int32(16) + i1_3 * T.int32(8) + i1_4)
                                v_i2 = T.axis.spatial(T.int32(1280), i2_4 + i0_0_i1_0_i2_0_fused % T.int32(10) * T.int32(128) + i0_2_i1_2_i2_2_fused % T.int32(64) * T.int32(2) + i2_3)
                                v_k = T.axis.reduce(T.int32(5120), k_0 * T.int32(8) + k_1 * T.int32(8) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv766_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv766_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(16), T.int32(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(2), i0_2_i1_2_i2_2_fused // T.int32(64) + ax0)
                            v1 = T.axis.spatial(T.int32(64), i0_0_i1_0_i2_0_fused // T.int32(10) * T.int32(16) + ax1)
                            v2 = T.axis.spatial(T.int32(1280), i0_0_i1_0_i2_0_fused % T.int32(10) * T.int32(128) + i0_2_i1_2_i2_2_fused % T.int32(64) * T.int32(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2], param_1[v2], lv757[v0, v1, v2])
                            T.writes(T_add[v0, v1, v2])
                            T_add[v0, v1, v2] = matmul_local[v0, v1, v2] + param_1[v2] + lv757[v0, v1, v2]

    @T.prim_func
    def fused_matmul3_add3_multiply1_sigmoid_multiply2(lv48: T.Buffer((T.int32(1), T.int32(77), T.int32(768)), "float32"), param_0: T.Buffer((T.int32(768), T.int32(3072)), "float32"), param_1: T.Buffer((T.int32(3072),), "float32"), T_multiply: T.Buffer((T.int32(1), T.int32(77), T.int32(3072)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul = T.alloc_buffer((T.int32(1), T.int32(77), T.int32(3072)))
        matmul_local = T.alloc_buffer((T.int32(1), T.int32(77), T.int32(3072)), scope="local")
        lv48_shared = T.alloc_buffer((T.int32(1), T.int32(77), T.int32(768)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(768), T.int32(3072)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(48), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(224), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(11), T.int32(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(1), i0_4_init + i0_3_init)
                            v_i1 = T.axis.spatial(T.int32(77), i0_2_i1_2_i2_2_fused // T.int32(32) * T.int32(11) + i1_3_init * T.int32(11) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(3072), i0_0_i1_0_i2_0_fused * T.int32(64) + i0_2_i1_2_i2_2_fused % T.int32(32) * T.int32(2) + i2_3_init * T.int32(2) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(24)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(11)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(224), thread="threadIdx.x"):
                                with T.block("lv48_shared"):
                                    v0 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v1 = T.axis.spatial(T.int32(77), (ax0_ax1_ax2_fused_0 * T.int32(224) + ax0_ax1_ax2_fused_1) // T.int32(32))
                                    v2 = T.axis.spatial(T.int32(768), k_0 * T.int32(32) + (ax0_ax1_ax2_fused_0 * T.int32(224) + ax0_ax1_ax2_fused_1) % T.int32(32))
                                    T.reads(lv48[v0, v1, v2])
                                    T.writes(lv48_shared[v0, v1, v2])
                                    lv48_shared[v0, v1, v2] = lv48[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int32(10)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(224), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(768), k_0 * T.int32(32) + (ax0_ax1_fused_0 * T.int32(224) + ax0_ax1_fused_1) // T.int32(64))
                                    v1 = T.axis.spatial(T.int32(3072), i0_0_i1_0_i2_0_fused * T.int32(64) + (ax0_ax1_fused_0 * T.int32(224) + ax0_ax1_fused_1) % T.int32(64))
                                    T.where(ax0_ax1_fused_0 * T.int32(224) + ax0_ax1_fused_1 < T.int32(2048))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(8), T.int32(1), T.int32(1), T.int32(1), T.int32(4), T.int32(1), T.int32(11), T.int32(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(1), i0_4 + i0_3)
                                v_i1 = T.axis.spatial(T.int32(77), i0_2_i1_2_i2_2_fused // T.int32(32) * T.int32(11) + i1_3 * T.int32(11) + i1_4)
                                v_i2 = T.axis.spatial(T.int32(3072), i0_0_i1_0_i2_0_fused * T.int32(64) + i0_2_i1_2_i2_2_fused % T.int32(32) * T.int32(2) + i2_3 * T.int32(2) + i2_4)
                                v_k = T.axis.reduce(T.int32(768), k_0 * T.int32(32) + k_1 * T.int32(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv48_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv48_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(11), T.int32(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(1), ax0)
                            v1 = T.axis.spatial(T.int32(77), i0_2_i1_2_i2_2_fused // T.int32(32) * T.int32(11) + ax1)
                            v2 = T.axis.spatial(T.int32(3072), i0_0_i1_0_i2_0_fused * T.int32(64) + i0_2_i1_2_i2_2_fused % T.int32(32) * T.int32(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int32(3696), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                with T.block("T_multiply_1"):
                    v_ax0 = T.axis.spatial(T.int32(1), T.int32(0))
                    v_ax1 = T.axis.spatial(T.int32(77), (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) // T.int32(3072))
                    v_ax2 = T.axis.spatial(T.int32(3072), (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(3072))
                    T.reads(matmul[v_ax0, v_ax1, v_ax2], param_1[v_ax2])
                    T.writes(T_multiply[v_ax0, v_ax1, v_ax2])
                    T_multiply[v_ax0, v_ax1, v_ax2] = (matmul[v_ax0, v_ax1, v_ax2] + param_1[v_ax2]) * T.sigmoid(T.float32(1.7020000219345093) * (matmul[v_ax0, v_ax1, v_ax2] + param_1[v_ax2]))

    @T.prim_func
    def fused_matmul40_add39(lv20: T.Buffer((T.int32(1), T.int32(4096), T.int32(512)), "float32"), param_0: T.Buffer((T.int32(512), T.int32(512)), "float32"), param_1: T.Buffer((T.int32(512),), "float32"), T_add: T.Buffer((T.int32(1), T.int32(4096), T.int32(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(1), T.int32(4096), T.int32(512)), scope="local")
        lv20_shared = T.alloc_buffer((T.int32(1), T.int32(4096), T.int32(512)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(512), T.int32(512)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(1024), thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(2), T.int32(4), T.int32(1), T.int32(2), T.int32(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(1), i0_4_init + i0_3_init)
                            v_i1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused // T.int32(16) * T.int32(64) + i0_2_i1_2_i2_2_fused // T.int32(8) * T.int32(4) + i1_3_init * T.int32(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(512), i2_4_init + i0_0_i1_0_i2_0_fused % T.int32(16) * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(8) * T.int32(4) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(16)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(16)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("lv20_shared"):
                                    v0 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused // T.int32(16) * T.int32(64) + (ax0_ax1_ax2_fused_0 * T.int32(128) + ax0_ax1_ax2_fused_1) // T.int32(32))
                                    v2 = T.axis.spatial(T.int32(512), k_0 * T.int32(32) + (ax0_ax1_ax2_fused_0 * T.int32(128) + ax0_ax1_ax2_fused_1) % T.int32(32))
                                    T.reads(lv20[v0, v1, v2])
                                    T.writes(lv20_shared[v0, v1, v2])
                                    lv20_shared[v0, v1, v2] = lv20[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int32(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(512), k_0 * T.int32(32) + (ax0_ax1_fused_0 * T.int32(128) + ax0_ax1_fused_1) // T.int32(32))
                                    v1 = T.axis.spatial(T.int32(512), i0_0_i1_0_i2_0_fused % T.int32(16) * T.int32(32) + (ax0_ax1_fused_0 * T.int32(128) + ax0_ax1_fused_1) % T.int32(32))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(8), T.int32(1), T.int32(2), T.int32(4), T.int32(4), T.int32(1), T.int32(2), T.int32(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(1), i0_4 + i0_3)
                                v_i1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused // T.int32(16) * T.int32(64) + i0_2_i1_2_i2_2_fused // T.int32(8) * T.int32(4) + i1_3 * T.int32(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int32(512), i2_4 + i0_0_i1_0_i2_0_fused % T.int32(16) * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(8) * T.int32(4) + i2_3)
                                v_k = T.axis.reduce(T.int32(512), k_0 * T.int32(32) + k_1 * T.int32(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv20_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv20_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(4), T.int32(4)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(1), ax0)
                            v1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused // T.int32(16) * T.int32(64) + i0_2_i1_2_i2_2_fused // T.int32(8) * T.int32(4) + ax1)
                            v2 = T.axis.spatial(T.int32(512), i0_0_i1_0_i2_0_fused % T.int32(16) * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(8) * T.int32(4) + ax2)
                            T.reads(matmul_local[v0, v1, v2], param_1[v2])
                            T.writes(T_add[v0, v1, v2])
                            T_add[v0, v1, v2] = matmul_local[v0, v1, v2] + param_1[v2]

    @T.prim_func
    def fused_matmul41_multiply17_cast2(lv32: T.Buffer((T.int32(1), T.int32(4096), T.int32(512)), "float32"), lv39: T.Buffer((T.int32(1), T.int32(512), T.int32(4096)), "float32"), compute: T.Buffer((T.int32(1), T.int32(4096), T.int32(4096)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(1), T.int32(4096), T.int32(4096)), scope="local")
        lv32_shared = T.alloc_buffer((T.int32(1), T.int32(4096), T.int32(512)), scope="shared")
        lv39_shared = T.alloc_buffer((T.int32(1), T.int32(512), T.int32(4096)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(4096), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(16), T.int32(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(1), i0_4_init + i0_3_init)
                            v_i1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused // T.int32(64) * T.int32(64) + i0_2_i1_2_i2_2_fused // T.int32(16) * T.int32(16) + i1_3_init * T.int32(16) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(4096), i2_4_init + i0_0_i1_0_i2_0_fused % T.int32(64) * T.int32(64) + i0_1_i1_1_i2_1_fused * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(16) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(128)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("lv32_shared"):
                                    v0 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused // T.int32(64) * T.int32(64) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) // T.int32(4))
                                    v2 = T.axis.spatial(T.int32(512), k_0 * T.int32(4) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(4))
                                    T.reads(lv32[v0, v1, v2])
                                    T.writes(lv32_shared[v0, v1, v2])
                                    lv32_shared[v0, v1, v2] = lv32[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int32(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("lv39_shared"):
                                    v0 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v1 = T.axis.spatial(T.int32(512), k_0 * T.int32(4) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) // T.int32(64))
                                    v2 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused % T.int32(64) * T.int32(64) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(64))
                                    T.reads(lv39[v0, v1, v2])
                                    T.writes(lv39_shared[v0, v1, v2])
                                    lv39_shared[v0, v1, v2] = lv39[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(16), T.int32(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(1), i0_4 + i0_3)
                                v_i1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused // T.int32(64) * T.int32(64) + i0_2_i1_2_i2_2_fused // T.int32(16) * T.int32(16) + i1_3 * T.int32(16) + i1_4)
                                v_i2 = T.axis.spatial(T.int32(4096), i2_4 + i0_0_i1_0_i2_0_fused % T.int32(64) * T.int32(64) + i0_1_i1_1_i2_1_fused * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(16) + i2_3)
                                v_k = T.axis.reduce(T.int32(512), k_0 * T.int32(4) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv32_shared[T.int32(0), v_i1, v_k], lv39_shared[T.int32(0), v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv32_shared[T.int32(0), v_i1, v_k] * lv39_shared[T.int32(0), v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(16), T.int32(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(1), ax0)
                            v1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused // T.int32(64) * T.int32(64) + i0_2_i1_2_i2_2_fused // T.int32(16) * T.int32(16) + ax1)
                            v2 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused % T.int32(64) * T.int32(64) + i0_1_i1_1_i2_1_fused * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(16) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(compute[v0, v1, v2])
                            compute[v0, v1, v2] = matmul_local[v0, v1, v2] * T.float32(0.044194173067808151)

    @T.prim_func
    def fused_matmul4_add1_add(lv54: T.Buffer((T.int32(1), T.int32(77), T.int32(3072)), "float32"), param_0: T.Buffer((T.int32(3072), T.int32(768)), "float32"), param_1: T.Buffer((T.int32(768),), "float32"), lv47: T.Buffer((T.int32(1), T.int32(77), T.int32(768)), "float32"), T_add: T.Buffer((T.int32(1), T.int32(77), T.int32(768)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(1), T.int32(77), T.int32(768)), scope="local")
        lv54_shared = T.alloc_buffer((T.int32(1), T.int32(77), T.int32(3072)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(3072), T.int32(768)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(48), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(88), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(7), T.int32(2), T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(1), i0_4_init + i0_3_init)
                            v_i1 = T.axis.spatial(T.int32(77), i0_2_i1_2_i2_2_fused // T.int32(8) * T.int32(7) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(768), i2_4_init + i0_0_i1_0_i2_0_fused * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(8) * T.int32(2) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(192)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(14)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(88), thread="threadIdx.x"):
                                with T.block("lv54_shared"):
                                    v0 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v1 = T.axis.spatial(T.int32(77), (ax0_ax1_ax2_fused_0 * T.int32(88) + ax0_ax1_ax2_fused_1) // T.int32(16))
                                    v2 = T.axis.spatial(T.int32(3072), k_0 * T.int32(16) + (ax0_ax1_ax2_fused_0 * T.int32(88) + ax0_ax1_ax2_fused_1) % T.int32(16))
                                    T.reads(lv54[v0, v1, v2])
                                    T.writes(lv54_shared[v0, v1, v2])
                                    lv54_shared[v0, v1, v2] = lv54[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int32(3)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(88), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(3072), k_0 * T.int32(16) + (ax0_ax1_fused_0 * T.int32(88) + ax0_ax1_fused_1) // T.int32(16))
                                    v1 = T.axis.spatial(T.int32(768), i0_0_i1_0_i2_0_fused * T.int32(16) + (ax0_ax1_fused_0 * T.int32(88) + ax0_ax1_fused_1) % T.int32(16))
                                    T.where(ax0_ax1_fused_0 * T.int32(88) + ax0_ax1_fused_1 < T.int32(256))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(1), T.int32(1), T.int32(7), T.int32(2), T.int32(16), T.int32(1), T.int32(1), T.int32(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(1), i0_4 + i0_3)
                                v_i1 = T.axis.spatial(T.int32(77), i0_2_i1_2_i2_2_fused // T.int32(8) * T.int32(7) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int32(768), i2_4 + i0_0_i1_0_i2_0_fused * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(8) * T.int32(2) + i2_3)
                                v_k = T.axis.reduce(T.int32(3072), k_0 * T.int32(16) + k_1 * T.int32(16) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv54_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv54_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(7), T.int32(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(1), ax0)
                            v1 = T.axis.spatial(T.int32(77), i0_2_i1_2_i2_2_fused // T.int32(8) * T.int32(7) + ax1)
                            v2 = T.axis.spatial(T.int32(768), i0_0_i1_0_i2_0_fused * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(8) * T.int32(2) + ax2)
                            T.reads(lv47[v0, v1, v2], matmul_local[v0, v1, v2], param_1[v2])
                            T.writes(T_add[v0, v1, v2])
                            T_add[v0, v1, v2] = lv47[v0, v1, v2] + (matmul_local[v0, v1, v2] + param_1[v2])

    @T.prim_func
    def fused_matmul5_add4_silu(lv13: T.Buffer((T.int32(2), T.int32(320)), "float32"), param_0: T.Buffer((T.int32(320), T.int32(1280)), "float32"), param_1: T.Buffer((T.int32(1280),), "float32"), T_multiply: T.Buffer((T.int32(2), T.int32(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul = T.alloc_buffer((T.int32(2), T.int32(1280)))
        matmul_local = T.alloc_buffer((T.int32(2), T.int32(1280)), scope="local")
        lv13_shared = T.alloc_buffer((T.int32(2), T.int32(320)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(320), T.int32(1280)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int32(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(16), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(2), i0_3_init + i0_4_init + i0_0_i1_0_fused // T.int32(10))
                            v_i1 = T.axis.spatial(T.int32(1280), i1_4_init + i0_0_i1_0_fused % T.int32(10) * T.int32(128) + i0_2_i1_2_fused + i1_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int32(32)):
                        for ax0_ax1_fused_0 in range(T.int32(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("lv13_shared"):
                                    v0 = T.axis.spatial(T.int32(2), i0_0_i1_0_fused // T.int32(10))
                                    v1 = T.axis.spatial(T.int32(320), k_0 * T.int32(10) + (ax0_ax1_fused_0 * T.int32(128) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int32(128) + ax0_ax1_fused_1 < T.int32(10))
                                    T.reads(lv13[v0, v1])
                                    T.writes(lv13_shared[v0, v1])
                                    lv13_shared[v0, v1] = lv13[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int32(10)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(320), k_0 * T.int32(10) + (ax0_ax1_fused_0 * T.int32(128) + ax0_ax1_fused_1) // T.int32(128))
                                    v1 = T.axis.spatial(T.int32(1280), i0_0_i1_0_fused % T.int32(10) * T.int32(128) + (ax0_ax1_fused_0 * T.int32(128) + ax0_ax1_fused_1) % T.int32(128))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int32(10), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(2), i0_3 + i0_4 + i0_0_i1_0_fused // T.int32(10))
                                v_i1 = T.axis.spatial(T.int32(1280), i1_4 + i0_0_i1_0_fused % T.int32(10) * T.int32(128) + i0_2_i1_2_fused + i1_3)
                                v_k = T.axis.reduce(T.int32(320), k_0 * T.int32(10) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1], lv13_shared[v_i0, v_k], param_0_shared[v_k, v_i1])
                                T.writes(matmul_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1] = matmul_local[v_i0, v_i1] + lv13_shared[v_i0, v_k] * param_0_shared[v_k, v_i1]
                    for ax0, ax1 in T.grid(T.int32(1), T.int32(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(2), i0_0_i1_0_fused // T.int32(10) + ax0)
                            v1 = T.axis.spatial(T.int32(1280), i0_0_i1_0_fused % T.int32(10) * T.int32(128) + i0_2_i1_2_fused + ax1)
                            T.reads(matmul_local[v0, v1])
                            T.writes(matmul[v0, v1])
                            matmul[v0, v1] = matmul_local[v0, v1]
        for ax0_ax1_fused_0 in T.thread_binding(T.int32(10), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(16), "pragma_unroll_explicit": T.int32(1)}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_fused_0 * T.int32(256) + ax0_ax1_fused_1) // T.int32(1280))
                    v_ax1 = T.axis.spatial(T.int32(1280), (ax0_ax1_fused_0 * T.int32(256) + ax0_ax1_fused_1) % T.int32(1280))
                    T.reads(matmul[v_ax0, v_ax1], param_1[v_ax1])
                    T.writes(T_multiply[v_ax0, v_ax1])
                    T_multiply[v_ax0, v_ax1] = (matmul[v_ax0, v_ax1] + param_1[v_ax1]) * T.sigmoid(matmul[v_ax0, v_ax1] + param_1[v_ax1])

    @T.prim_func
    def fused_matmul6_add4(lv17: T.Buffer((T.int32(2), T.int32(1280)), "float32"), param_0: T.Buffer((T.int32(1280), T.int32(1280)), "float32"), param_1: T.Buffer((T.int32(1280),), "float32"), T_add: T.Buffer((T.int32(2), T.int32(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(2), T.int32(1280)), scope="local")
        lv17_shared = T.alloc_buffer((T.int32(2), T.int32(1280)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(1280), T.int32(1280)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int32(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(2), i0_3_init + i0_4_init + i0_0_i1_0_fused // T.int32(10))
                            v_i1 = T.axis.spatial(T.int32(1280), i1_4_init + i0_0_i1_0_fused % T.int32(10) * T.int32(128) + i0_2_i1_2_fused + i1_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int32(128)):
                        for ax0_ax1_fused_0 in range(T.int32(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("lv17_shared"):
                                    v0 = T.axis.spatial(T.int32(2), i0_0_i1_0_fused // T.int32(10))
                                    v1 = T.axis.spatial(T.int32(1280), k_0 * T.int32(10) + (ax0_ax1_fused_0 * T.int32(128) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int32(128) + ax0_ax1_fused_1 < T.int32(10))
                                    T.reads(lv17[v0, v1])
                                    T.writes(lv17_shared[v0, v1])
                                    lv17_shared[v0, v1] = lv17[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int32(10)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(1280), k_0 * T.int32(10) + (ax0_ax1_fused_0 * T.int32(128) + ax0_ax1_fused_1) // T.int32(128))
                                    v1 = T.axis.spatial(T.int32(1280), i0_0_i1_0_fused % T.int32(10) * T.int32(128) + (ax0_ax1_fused_0 * T.int32(128) + ax0_ax1_fused_1) % T.int32(128))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int32(10), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(2), i0_3 + i0_4 + i0_0_i1_0_fused // T.int32(10))
                                v_i1 = T.axis.spatial(T.int32(1280), i1_4 + i0_0_i1_0_fused % T.int32(10) * T.int32(128) + i0_2_i1_2_fused + i1_3)
                                v_k = T.axis.reduce(T.int32(1280), k_0 * T.int32(10) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1], lv17_shared[v_i0, v_k], param_0_shared[v_k, v_i1])
                                T.writes(matmul_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1] = matmul_local[v_i0, v_i1] + lv17_shared[v_i0, v_k] * param_0_shared[v_k, v_i1]
                    for ax0, ax1 in T.grid(T.int32(1), T.int32(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(2), i0_0_i1_0_fused // T.int32(10) + ax0)
                            v1 = T.axis.spatial(T.int32(1280), i0_0_i1_0_fused % T.int32(10) * T.int32(128) + i0_2_i1_2_fused + ax1)
                            T.reads(matmul_local[v0, v1], param_1[v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = matmul_local[v0, v1] + param_1[v1]

    @T.prim_func
    def fused_matmul6_add4_strided_slice5(lv438: T.Buffer((T.int32(2), T.int32(1280)), "float32"), param_0: T.Buffer((T.int32(1280), T.int32(1280)), "float32"), param_1: T.Buffer((T.int32(1280),), "float32"), T_strided_slice_with_axes: T.Buffer((T.int32(2), T.int32(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(2), T.int32(1280)), scope="local")
        lv438_shared = T.alloc_buffer((T.int32(2), T.int32(1280)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(1280), T.int32(1280)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int32(40), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(16), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(2), i0_2_i1_2_fused // T.int32(32) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int32(1280), i1_4_init + i0_0_i1_0_fused * T.int32(32) + i0_2_i1_2_fused % T.int32(32) + i1_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int32(20)):
                        for ax0_ax1_fused_0 in range(T.int32(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("lv438_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) // T.int32(64))
                                    v1 = T.axis.spatial(T.int32(1280), k_0 * T.int32(64) + (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) % T.int32(64))
                                    T.reads(lv438[v0, v1])
                                    T.writes(lv438_shared[v0, v1])
                                    lv438_shared[v0, v1] = lv438[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int32(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(1280), k_0 * T.int32(64) + (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) // T.int32(32))
                                    v1 = T.axis.spatial(T.int32(1280), i0_0_i1_0_fused * T.int32(32) + (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) % T.int32(32))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int32(4), T.int32(1), T.int32(1), T.int32(16), T.int32(1), T.int32(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(2), i0_2_i1_2_fused // T.int32(32) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int32(1280), i1_4 + i0_0_i1_0_fused * T.int32(32) + i0_2_i1_2_fused % T.int32(32) + i1_3)
                                v_k = T.axis.reduce(T.int32(1280), k_0 * T.int32(64) + k_1 * T.int32(16) + k_2)
                                T.reads(matmul_local[v_i0, v_i1], lv438_shared[v_i0, v_k], param_0_shared[v_k, v_i1])
                                T.writes(matmul_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1] = matmul_local[v_i0, v_i1] + lv438_shared[v_i0, v_k] * param_0_shared[v_k, v_i1]
                    for ax0, ax1 in T.grid(T.int32(1), T.int32(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(2), i0_2_i1_2_fused // T.int32(32) + ax0)
                            v1 = T.axis.spatial(T.int32(1280), i0_0_i1_0_fused * T.int32(32) + i0_2_i1_2_fused % T.int32(32) + ax1)
                            T.reads(matmul_local[v0, v1], param_1[v1])
                            T.writes(T_strided_slice_with_axes[v0, v1])
                            T_strided_slice_with_axes[v0, v1] = matmul_local[v0, v1] + param_1[v1]

    @T.prim_func
    def fused_matmul7_add6_strided_slice3(lv29: T.Buffer((T.int32(2), T.int32(1280)), "float32"), param_0: T.Buffer((T.int32(1280), T.int32(320)), "float32"), param_1: T.Buffer((T.int32(320),), "float32"), T_strided_slice_with_axes: T.Buffer((T.int32(2), T.int32(320)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(2), T.int32(320)), scope="local")
        lv29_shared = T.alloc_buffer((T.int32(2), T.int32(1280)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(1280), T.int32(320)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int32(10), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(2), i0_2_i1_2_fused // T.int32(32) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int32(320), i1_4_init + i0_0_i1_0_fused * T.int32(32) + i0_2_i1_2_fused % T.int32(32) + i1_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int32(10)):
                        for ax0_ax1_fused_0 in range(T.int32(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("lv29_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) // T.int32(128))
                                    v1 = T.axis.spatial(T.int32(1280), k_0 * T.int32(128) + (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) % T.int32(128))
                                    T.reads(lv29[v0, v1])
                                    T.writes(lv29_shared[v0, v1])
                                    lv29_shared[v0, v1] = lv29[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int32(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(1280), k_0 * T.int32(128) + (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) // T.int32(32))
                                    v1 = T.axis.spatial(T.int32(320), i0_0_i1_0_fused * T.int32(32) + (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) % T.int32(32))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int32(64), T.int32(1), T.int32(1), T.int32(2), T.int32(1), T.int32(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(2), i0_2_i1_2_fused // T.int32(32) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int32(320), i1_4 + i0_0_i1_0_fused * T.int32(32) + i0_2_i1_2_fused % T.int32(32) + i1_3)
                                v_k = T.axis.reduce(T.int32(1280), k_0 * T.int32(128) + k_1 * T.int32(2) + k_2)
                                T.reads(matmul_local[v_i0, v_i1], lv29_shared[v_i0, v_k], param_0_shared[v_k, v_i1])
                                T.writes(matmul_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1] = matmul_local[v_i0, v_i1] + lv29_shared[v_i0, v_k] * param_0_shared[v_k, v_i1]
                    for ax0, ax1 in T.grid(T.int32(1), T.int32(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(2), i0_2_i1_2_fused // T.int32(32) + ax0)
                            v1 = T.axis.spatial(T.int32(320), i0_0_i1_0_fused * T.int32(32) + i0_2_i1_2_fused % T.int32(32) + ax1)
                            T.reads(matmul_local[v0, v1], param_1[v1])
                            T.writes(T_strided_slice_with_axes[v0, v1])
                            T_strided_slice_with_axes[v0, v1] = matmul_local[v0, v1] + param_1[v1]

    @T.prim_func
    def fused_matmul8_add9_add10(lv72: T.Buffer((T.int32(2), T.int32(4096), T.int32(320)), "float32"), param_0: T.Buffer((T.int32(320), T.int32(320)), "float32"), param_1: T.Buffer((T.int32(320),), "float32"), lv48: T.Buffer((T.int32(2), T.int32(4096), T.int32(320)), "float32"), T_add: T.Buffer((T.int32(2), T.int32(4096), T.int32(320)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(2), T.int32(4096), T.int32(320)), scope="local")
        lv72_shared = T.alloc_buffer((T.int32(2), T.int32(4096), T.int32(320)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(320), T.int32(320)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(1280), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(2), T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(2), i0_4_init + i0_3_init)
                            v_i1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused // T.int32(5) * T.int32(16) + i0_1_i1_1_i2_1_fused * T.int32(4) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(320), i0_0_i1_0_i2_0_fused % T.int32(5) * T.int32(64) + i0_2_i1_2_i2_2_fused * T.int32(2) + i2_3_init * T.int32(2) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(80)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                                with T.block("lv72_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_fused_0 * T.int32(32) + ax0_ax1_ax2_fused_1) // T.int32(64))
                                    v1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused // T.int32(5) * T.int32(16) + (ax0_ax1_ax2_fused_0 * T.int32(32) + ax0_ax1_ax2_fused_1) % T.int32(64) // T.int32(4))
                                    v2 = T.axis.spatial(T.int32(320), k_0 * T.int32(4) + (ax0_ax1_ax2_fused_0 * T.int32(32) + ax0_ax1_ax2_fused_1) % T.int32(4))
                                    T.reads(lv72[v0, v1, v2])
                                    T.writes(lv72_shared[v0, v1, v2])
                                    lv72_shared[v0, v1, v2] = lv72[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int32(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(320), k_0 * T.int32(4) + (ax0_ax1_fused_0 * T.int32(32) + ax0_ax1_fused_1) // T.int32(64))
                                    v1 = T.axis.spatial(T.int32(320), i0_0_i1_0_i2_0_fused % T.int32(5) * T.int32(64) + (ax0_ax1_fused_0 * T.int32(32) + ax0_ax1_fused_1) % T.int32(64))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(1), T.int32(2), T.int32(4), T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(2), i0_4 + i0_3)
                                v_i1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused // T.int32(5) * T.int32(16) + i0_1_i1_1_i2_1_fused * T.int32(4) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int32(320), i0_0_i1_0_i2_0_fused % T.int32(5) * T.int32(64) + i0_2_i1_2_i2_2_fused * T.int32(2) + i2_3 * T.int32(2) + i2_4)
                                v_k = T.axis.reduce(T.int32(320), k_0 * T.int32(4) + k_1 * T.int32(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv72_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv72_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(2), T.int32(4), T.int32(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(2), ax0)
                            v1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused // T.int32(5) * T.int32(16) + i0_1_i1_1_i2_1_fused * T.int32(4) + ax1)
                            v2 = T.axis.spatial(T.int32(320), i0_0_i1_0_i2_0_fused % T.int32(5) * T.int32(64) + i0_2_i1_2_i2_2_fused * T.int32(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2], param_1[v2], lv48[v0, v1, v2])
                            T.writes(T_add[v0, v1, v2])
                            T_add[v0, v1, v2] = matmul_local[v0, v1, v2] + param_1[v2] + lv48[v0, v1, v2]

    @T.prim_func
    def fused_matmul9_multiply5(lv58: T.Buffer((T.int32(16), T.int32(4096), T.int32(40)), "float32"), lv65: T.Buffer((T.int32(16), T.int32(40), T.int32(4096)), "float32"), T_multiply: T.Buffer((T.int32(16), T.int32(4096), T.int32(4096)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(16), T.int32(4096), T.int32(4096)), scope="local")
        lv58_shared = T.alloc_buffer((T.int32(16), T.int32(4096), T.int32(40)), scope="shared")
        lv65_shared = T.alloc_buffer((T.int32(16), T.int32(40), T.int32(4096)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(131072), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(16), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(32), T.int32(2), T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(16), i0_4_init + i0_0_i1_0_i2_0_fused // T.int32(8192) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused % T.int32(8192) // T.int32(64) * T.int32(32) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(4096), i2_4_init + i0_0_i1_0_i2_0_fused % T.int32(64) * T.int32(64) + i0_2_i1_2_i2_2_fused * T.int32(2) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(10)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                                with T.block("lv58_shared"):
                                    v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(8192))
                                    v1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused % T.int32(8192) // T.int32(64) * T.int32(32) + (ax0_ax1_ax2_fused_0 * T.int32(32) + ax0_ax1_ax2_fused_1) // T.int32(4))
                                    v2 = T.axis.spatial(T.int32(40), k_0 * T.int32(4) + (ax0_ax1_ax2_fused_0 * T.int32(32) + ax0_ax1_ax2_fused_1) % T.int32(4))
                                    T.reads(lv58[v0, v1, v2])
                                    T.writes(lv58_shared[v0, v1, v2])
                                    lv58_shared[v0, v1, v2] = lv58[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int32(8)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                                with T.block("lv65_shared"):
                                    v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(8192))
                                    v1 = T.axis.spatial(T.int32(40), k_0 * T.int32(4) + (ax0_ax1_ax2_fused_0 * T.int32(32) + ax0_ax1_ax2_fused_1) // T.int32(64))
                                    v2 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused % T.int32(64) * T.int32(64) + (ax0_ax1_ax2_fused_0 * T.int32(32) + ax0_ax1_ax2_fused_1) % T.int32(64))
                                    T.reads(lv65[v0, v1, v2])
                                    T.writes(lv65_shared[v0, v1, v2])
                                    lv65_shared[v0, v1, v2] = lv65[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(1), T.int32(1), T.int32(32), T.int32(2), T.int32(4), T.int32(1), T.int32(1), T.int32(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(16), i0_4 + i0_0_i1_0_i2_0_fused // T.int32(8192) + i0_3)
                                v_i1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused % T.int32(8192) // T.int32(64) * T.int32(32) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int32(4096), i2_4 + i0_0_i1_0_i2_0_fused % T.int32(64) * T.int32(64) + i0_2_i1_2_i2_2_fused * T.int32(2) + i2_3)
                                v_k = T.axis.reduce(T.int32(40), k_0 * T.int32(4) + k_1 * T.int32(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv58_shared[v_i0, v_i1, v_k], lv65_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv58_shared[v_i0, v_i1, v_k] * lv65_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(32), T.int32(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(8192) + ax0)
                            v1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused % T.int32(8192) // T.int32(64) * T.int32(32) + ax1)
                            v2 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused % T.int32(64) * T.int32(64) + i0_2_i1_2_i2_2_fused * T.int32(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(T_multiply[v0, v1, v2])
                            T_multiply[v0, v1, v2] = matmul_local[v0, v1, v2] * T.float32(0.15811388194561005)

    @T.prim_func
    def fused_matmul_add1(lv14: T.Buffer((T.int32(1), T.int32(77), T.int32(768)), "float32"), param_0: T.Buffer((T.int32(768), T.int32(768)), "float32"), param_1: T.Buffer((T.int32(768),), "float32"), T_add: T.Buffer((T.int32(1), T.int32(77), T.int32(768)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(1), T.int32(77), T.int32(768)), scope="local")
        lv14_shared = T.alloc_buffer((T.int32(1), T.int32(77), T.int32(768)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(768), T.int32(768)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(48), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(88), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(7), T.int32(2), T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(1), i0_4_init + i0_3_init)
                            v_i1 = T.axis.spatial(T.int32(77), i0_2_i1_2_i2_2_fused // T.int32(8) * T.int32(7) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(768), i2_4_init + i0_0_i1_0_i2_0_fused * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(8) * T.int32(2) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(48)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(14)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(88), thread="threadIdx.x"):
                                with T.block("lv14_shared"):
                                    v0 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v1 = T.axis.spatial(T.int32(77), (ax0_ax1_ax2_fused_0 * T.int32(88) + ax0_ax1_ax2_fused_1) // T.int32(16))
                                    v2 = T.axis.spatial(T.int32(768), k_0 * T.int32(16) + (ax0_ax1_ax2_fused_0 * T.int32(88) + ax0_ax1_ax2_fused_1) % T.int32(16))
                                    T.reads(lv14[v0, v1, v2])
                                    T.writes(lv14_shared[v0, v1, v2])
                                    lv14_shared[v0, v1, v2] = lv14[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int32(3)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(88), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(768), k_0 * T.int32(16) + (ax0_ax1_fused_0 * T.int32(88) + ax0_ax1_fused_1) // T.int32(16))
                                    v1 = T.axis.spatial(T.int32(768), i0_0_i1_0_i2_0_fused * T.int32(16) + (ax0_ax1_fused_0 * T.int32(88) + ax0_ax1_fused_1) % T.int32(16))
                                    T.where(ax0_ax1_fused_0 * T.int32(88) + ax0_ax1_fused_1 < T.int32(256))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(1), T.int32(1), T.int32(7), T.int32(2), T.int32(16), T.int32(1), T.int32(1), T.int32(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(1), i0_4 + i0_3)
                                v_i1 = T.axis.spatial(T.int32(77), i0_2_i1_2_i2_2_fused // T.int32(8) * T.int32(7) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int32(768), i2_4 + i0_0_i1_0_i2_0_fused * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(8) * T.int32(2) + i2_3)
                                v_k = T.axis.reduce(T.int32(768), k_0 * T.int32(16) + k_1 * T.int32(16) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv14_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv14_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(7), T.int32(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(1), ax0)
                            v1 = T.axis.spatial(T.int32(77), i0_2_i1_2_i2_2_fused // T.int32(8) * T.int32(7) + ax1)
                            v2 = T.axis.spatial(T.int32(768), i0_0_i1_0_i2_0_fused * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(8) * T.int32(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2], param_1[v2])
                            T.writes(T_add[v0, v1, v2])
                            T_add[v0, v1, v2] = matmul_local[v0, v1, v2] + param_1[v2]

    @T.prim_func
    def fused_matmul_add1_add(lv43: T.Buffer((T.int32(1), T.int32(77), T.int32(768)), "float32"), param_0: T.Buffer((T.int32(768), T.int32(768)), "float32"), param_1: T.Buffer((T.int32(768),), "float32"), lv9: T.Buffer((T.int32(1), T.int32(77), T.int32(768)), "float32"), T_add: T.Buffer((T.int32(1), T.int32(77), T.int32(768)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(1), T.int32(77), T.int32(768)), scope="local")
        lv43_shared = T.alloc_buffer((T.int32(1), T.int32(77), T.int32(768)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(768), T.int32(768)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(48), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(16), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(88), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(7), T.int32(2), T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(1), i0_4_init + i0_3_init)
                            v_i1 = T.axis.spatial(T.int32(77), i0_2_i1_2_i2_2_fused // T.int32(8) * T.int32(7) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(768), i2_4_init + i0_0_i1_0_i2_0_fused * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(8) * T.int32(2) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(48)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(14)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(88), thread="threadIdx.x"):
                                with T.block("lv43_shared"):
                                    v0 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v1 = T.axis.spatial(T.int32(77), (ax0_ax1_ax2_fused_0 * T.int32(88) + ax0_ax1_ax2_fused_1) // T.int32(16))
                                    v2 = T.axis.spatial(T.int32(768), k_0 * T.int32(16) + (ax0_ax1_ax2_fused_0 * T.int32(88) + ax0_ax1_ax2_fused_1) % T.int32(16))
                                    T.reads(lv43[v0, v1, v2])
                                    T.writes(lv43_shared[v0, v1, v2])
                                    lv43_shared[v0, v1, v2] = lv43[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int32(3)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(88), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(768), k_0 * T.int32(16) + (ax0_ax1_fused_0 * T.int32(88) + ax0_ax1_fused_1) // T.int32(16))
                                    v1 = T.axis.spatial(T.int32(768), i0_0_i1_0_i2_0_fused * T.int32(16) + (ax0_ax1_fused_0 * T.int32(88) + ax0_ax1_fused_1) % T.int32(16))
                                    T.where(ax0_ax1_fused_0 * T.int32(88) + ax0_ax1_fused_1 < T.int32(256))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(1), T.int32(1), T.int32(7), T.int32(2), T.int32(16), T.int32(1), T.int32(1), T.int32(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(1), i0_4 + i0_3)
                                v_i1 = T.axis.spatial(T.int32(77), i0_2_i1_2_i2_2_fused // T.int32(8) * T.int32(7) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int32(768), i2_4 + i0_0_i1_0_i2_0_fused * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(8) * T.int32(2) + i2_3)
                                v_k = T.axis.reduce(T.int32(768), k_0 * T.int32(16) + k_1 * T.int32(16) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv43_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv43_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(7), T.int32(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(1), ax0)
                            v1 = T.axis.spatial(T.int32(77), i0_2_i1_2_i2_2_fused // T.int32(8) * T.int32(7) + ax1)
                            v2 = T.axis.spatial(T.int32(768), i0_0_i1_0_i2_0_fused * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(8) * T.int32(2) + ax2)
                            T.reads(lv9[v0, v1, v2], matmul_local[v0, v1, v2], param_1[v2])
                            T.writes(T_add[v0, v1, v2])
                            T_add[v0, v1, v2] = lv9[v0, v1, v2] + (matmul_local[v0, v1, v2] + param_1[v2])

    @T.prim_func
    def fused_matmul_add1_multiply(lv14: T.Buffer((T.int32(1), T.int32(77), T.int32(768)), "float32"), param_0: T.Buffer((T.int32(768), T.int32(768)), "float32"), param_1: T.Buffer((T.int32(768),), "float32"), T_multiply: T.Buffer((T.int32(1), T.int32(77), T.int32(768)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(1), T.int32(77), T.int32(768)), scope="local")
        lv14_shared = T.alloc_buffer((T.int32(1), T.int32(77), T.int32(768)), scope="shared")
        param_0_shared = T.alloc_buffer((T.int32(768), T.int32(768)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(48), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(16), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(88), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(7), T.int32(2), T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(1), i0_4_init + i0_3_init)
                            v_i1 = T.axis.spatial(T.int32(77), i0_2_i1_2_i2_2_fused // T.int32(8) * T.int32(7) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(768), i2_4_init + i0_0_i1_0_i2_0_fused * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(8) * T.int32(2) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(48)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(14)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(88), thread="threadIdx.x"):
                                with T.block("lv14_shared"):
                                    v0 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v1 = T.axis.spatial(T.int32(77), (ax0_ax1_ax2_fused_0 * T.int32(88) + ax0_ax1_ax2_fused_1) // T.int32(16))
                                    v2 = T.axis.spatial(T.int32(768), k_0 * T.int32(16) + (ax0_ax1_ax2_fused_0 * T.int32(88) + ax0_ax1_ax2_fused_1) % T.int32(16))
                                    T.reads(lv14[v0, v1, v2])
                                    T.writes(lv14_shared[v0, v1, v2])
                                    lv14_shared[v0, v1, v2] = lv14[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int32(3)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(88), thread="threadIdx.x"):
                                with T.block("param_0_shared"):
                                    v0 = T.axis.spatial(T.int32(768), k_0 * T.int32(16) + (ax0_ax1_fused_0 * T.int32(88) + ax0_ax1_fused_1) // T.int32(16))
                                    v1 = T.axis.spatial(T.int32(768), i0_0_i1_0_i2_0_fused * T.int32(16) + (ax0_ax1_fused_0 * T.int32(88) + ax0_ax1_fused_1) % T.int32(16))
                                    T.where(ax0_ax1_fused_0 * T.int32(88) + ax0_ax1_fused_1 < T.int32(256))
                                    T.reads(param_0[v0, v1])
                                    T.writes(param_0_shared[v0, v1])
                                    param_0_shared[v0, v1] = param_0[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(1), T.int32(1), T.int32(7), T.int32(2), T.int32(16), T.int32(1), T.int32(1), T.int32(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(1), i0_4 + i0_3)
                                v_i1 = T.axis.spatial(T.int32(77), i0_2_i1_2_i2_2_fused // T.int32(8) * T.int32(7) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int32(768), i2_4 + i0_0_i1_0_i2_0_fused * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(8) * T.int32(2) + i2_3)
                                v_k = T.axis.reduce(T.int32(768), k_0 * T.int32(16) + k_1 * T.int32(16) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], lv14_shared[v_i0, v_i1, v_k], param_0_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + lv14_shared[v_i0, v_i1, v_k] * param_0_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(7), T.int32(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(1), ax0)
                            v1 = T.axis.spatial(T.int32(77), i0_2_i1_2_i2_2_fused // T.int32(8) * T.int32(7) + ax1)
                            v2 = T.axis.spatial(T.int32(768), i0_0_i1_0_i2_0_fused * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(8) * T.int32(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2], param_1[v2])
                            T.writes(T_multiply[v0, v1, v2])
                            T_multiply[v0, v1, v2] = (matmul_local[v0, v1, v2] + param_1[v2]) * T.float32(0.125)

    @T.prim_func
    def fused_reshape14_transpose11_reshape15(lv51: T.Buffer((T.int32(2), T.int32(4096), T.int32(320)), "float32"), T_reshape: T.Buffer((T.int32(16), T.int32(4096), T.int32(40)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(40)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(1310720))
                        v_ax1 = T.axis.spatial(T.int32(4096), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(1310720) // T.int32(320))
                        v_ax2 = T.axis.spatial(T.int32(8), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(320) // T.int32(40))
                        v_ax3 = T.axis.spatial(T.int32(40), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(40))
                        T.reads(lv51[(((v_ax2 * T.int32(40) + v_ax3) // T.int32(320) + v_ax1) // T.int32(4096) + v_ax0) % T.int32(2), ((v_ax2 * T.int32(40) + v_ax3) // T.int32(320) + v_ax1) % T.int32(4096), (v_ax2 * T.int32(40) + v_ax3) % T.int32(320)])
                        T.writes(T_reshape[v_ax0 * T.int32(8) + v_ax2, v_ax1, v_ax3])
                        T_reshape[v_ax2 + v_ax0 * T.int32(8), v_ax1, v_ax3] = lv51[(((v_ax2 * T.int32(40) + v_ax3) // T.int32(320) + v_ax1) // T.int32(4096) + v_ax0) % T.int32(2), ((v_ax2 * T.int32(40) + v_ax3) // T.int32(320) + v_ax1) % T.int32(4096), (v_ax2 * T.int32(40) + v_ax3) % T.int32(320)]

    @T.prim_func
    def fused_reshape14_transpose11_reshape15_transpose12(lv53: T.Buffer((T.int32(2), T.int32(4096), T.int32(320)), "float32"), T_transpose: T.Buffer((T.int32(16), T.int32(40), T.int32(4096)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(40)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(1310720))
                        v_ax1 = T.axis.spatial(T.int32(4096), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(1310720) // T.int32(320))
                        v_ax2 = T.axis.spatial(T.int32(8), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(320) // T.int32(40))
                        v_ax3 = T.axis.spatial(T.int32(40), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(40))
                        T.reads(lv53[(((v_ax2 * T.int32(40) + v_ax3) // T.int32(320) + v_ax1) // T.int32(4096) + v_ax0) % T.int32(2), ((v_ax2 * T.int32(40) + v_ax3) // T.int32(320) + v_ax1) % T.int32(4096), (v_ax2 * T.int32(40) + v_ax3) % T.int32(320)])
                        T.writes(T_transpose[v_ax0 * T.int32(8) + v_ax2, v_ax3, v_ax1])
                        T_transpose[v_ax2 + v_ax0 * T.int32(8), v_ax3, v_ax1] = lv53[(((v_ax2 * T.int32(40) + v_ax3) // T.int32(320) + v_ax1) // T.int32(4096) + v_ax0) % T.int32(2), ((v_ax2 * T.int32(40) + v_ax3) // T.int32(320) + v_ax1) % T.int32(4096), (v_ax2 * T.int32(40) + v_ax3) % T.int32(320)]

    @T.prim_func
    def fused_reshape16_transpose13_reshape17(lv69: T.Buffer((T.int32(16), T.int32(4096), T.int32(40)), "float32"), T_reshape: T.Buffer((T.int32(2), T.int32(4096), T.int32(320)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(40)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(1310720))
                        v_ax1 = T.axis.spatial(T.int32(8), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(1310720) // T.int32(163840))
                        v_ax2 = T.axis.spatial(T.int32(4096), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(163840) // T.int32(40))
                        v_ax3 = T.axis.spatial(T.int32(40), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(40))
                        T.reads(lv69[(v_ax0 * T.int32(8) + (v_ax3 // T.int32(40) + v_ax2) // T.int32(4096) + v_ax1) % T.int32(16), (v_ax3 // T.int32(40) + v_ax2) % T.int32(4096), v_ax3 % T.int32(40)])
                        T.writes(T_reshape[v_ax0, v_ax2, v_ax1 * T.int32(40) + v_ax3])
                        T_reshape[v_ax0, v_ax2, v_ax3 + v_ax1 * T.int32(40)] = lv69[(v_ax0 * T.int32(8) + (v_ax3 // T.int32(40) + v_ax2) // T.int32(4096) + v_ax1) % T.int32(16), (v_ax3 // T.int32(40) + v_ax2) % T.int32(4096), v_ax3 % T.int32(40)]

    @T.prim_func
    def fused_reshape18_transpose15_reshape19(lv83: T.Buffer((T.int32(2), T.int32(77), T.int32(320)), "float32"), T_reshape: T.Buffer((T.int32(16), T.int32(77), T.int32(40)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int32(770), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(24640))
                    v_ax1 = T.axis.spatial(T.int32(77), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(24640) // T.int32(320))
                    v_ax2 = T.axis.spatial(T.int32(8), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(320) // T.int32(40))
                    v_ax3 = T.axis.spatial(T.int32(40), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(40))
                    T.reads(lv83[(((v_ax2 * T.int32(40) + v_ax3) // T.int32(320) + v_ax1) // T.int32(77) + v_ax0) % T.int32(2), ((v_ax2 * T.int32(40) + v_ax3) // T.int32(320) + v_ax1) % T.int32(77), (v_ax2 * T.int32(40) + v_ax3) % T.int32(320)])
                    T.writes(T_reshape[v_ax0 * T.int32(8) + v_ax2, v_ax1, v_ax3])
                    T_reshape[v_ax2 + v_ax0 * T.int32(8), v_ax1, v_ax3] = lv83[(((v_ax2 * T.int32(40) + v_ax3) // T.int32(320) + v_ax1) // T.int32(77) + v_ax0) % T.int32(2), ((v_ax2 * T.int32(40) + v_ax3) // T.int32(320) + v_ax1) % T.int32(77), (v_ax2 * T.int32(40) + v_ax3) % T.int32(320)]

    @T.prim_func
    def fused_reshape18_transpose15_reshape19_transpose16(lv81: T.Buffer((T.int32(2), T.int32(77), T.int32(320)), "float32"), T_transpose: T.Buffer((T.int32(16), T.int32(40), T.int32(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int32(770), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(24640))
                    v_ax1 = T.axis.spatial(T.int32(77), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(24640) // T.int32(320))
                    v_ax2 = T.axis.spatial(T.int32(8), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(320) // T.int32(40))
                    v_ax3 = T.axis.spatial(T.int32(40), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(40))
                    T.reads(lv81[(((v_ax2 * T.int32(40) + v_ax3) // T.int32(320) + v_ax1) // T.int32(77) + v_ax0) % T.int32(2), ((v_ax2 * T.int32(40) + v_ax3) // T.int32(320) + v_ax1) % T.int32(77), (v_ax2 * T.int32(40) + v_ax3) % T.int32(320)])
                    T.writes(T_transpose[v_ax0 * T.int32(8) + v_ax2, v_ax3, v_ax1])
                    T_transpose[v_ax2 + v_ax0 * T.int32(8), v_ax3, v_ax1] = lv81[(((v_ax2 * T.int32(40) + v_ax3) // T.int32(320) + v_ax1) // T.int32(77) + v_ax0) % T.int32(2), ((v_ax2 * T.int32(40) + v_ax3) // T.int32(320) + v_ax1) % T.int32(77), (v_ax2 * T.int32(40) + v_ax3) % T.int32(320)]

    @T.prim_func
    def fused_reshape20_transpose17(lv117: T.Buffer((T.int32(2), T.int32(4096), T.int32(320)), "float32"), T_transpose: T.Buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(40)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(1310720))
                        v_ax1 = T.axis.spatial(T.int32(64), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(1310720) // T.int32(20480))
                        v_ax2 = T.axis.spatial(T.int32(64), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(20480) // T.int32(320))
                        v_ax3 = T.axis.spatial(T.int32(320), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(320))
                        T.reads(lv117[((v_ax1 * T.int32(64) + v_ax3 // T.int32(320) + v_ax2) // T.int32(4096) + v_ax0) % T.int32(2), (v_ax1 * T.int32(64) + v_ax3 // T.int32(320) + v_ax2) % T.int32(4096), v_ax3 % T.int32(320)])
                        T.writes(T_transpose[v_ax0, v_ax3, v_ax1, v_ax2])
                        T_transpose[v_ax0, v_ax3, v_ax1, v_ax2] = lv117[((v_ax1 * T.int32(64) + v_ax3 // T.int32(320) + v_ax2) // T.int32(4096) + v_ax0) % T.int32(2), (v_ax1 * T.int32(64) + v_ax3 // T.int32(320) + v_ax2) % T.int32(4096), v_ax3 % T.int32(320)]

    @T.prim_func
    def fused_reshape24_transpose21_reshape25(lv257: T.Buffer((T.int32(2), T.int32(1024), T.int32(640)), "float32"), T_reshape: T.Buffer((T.int32(16), T.int32(1024), T.int32(80)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(20)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(655360))
                        v_ax1 = T.axis.spatial(T.int32(1024), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(655360) // T.int32(640))
                        v_ax2 = T.axis.spatial(T.int32(8), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(640) // T.int32(80))
                        v_ax3 = T.axis.spatial(T.int32(80), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(80))
                        T.reads(lv257[(((v_ax2 * T.int32(80) + v_ax3) // T.int32(640) + v_ax1) // T.int32(1024) + v_ax0) % T.int32(2), ((v_ax2 * T.int32(80) + v_ax3) // T.int32(640) + v_ax1) % T.int32(1024), (v_ax2 * T.int32(80) + v_ax3) % T.int32(640)])
                        T.writes(T_reshape[v_ax0 * T.int32(8) + v_ax2, v_ax1, v_ax3])
                        T_reshape[v_ax2 + v_ax0 * T.int32(8), v_ax1, v_ax3] = lv257[(((v_ax2 * T.int32(80) + v_ax3) // T.int32(640) + v_ax1) // T.int32(1024) + v_ax0) % T.int32(2), ((v_ax2 * T.int32(80) + v_ax3) // T.int32(640) + v_ax1) % T.int32(1024), (v_ax2 * T.int32(80) + v_ax3) % T.int32(640)]

    @T.prim_func
    def fused_reshape24_transpose21_reshape25_transpose22(lv259: T.Buffer((T.int32(2), T.int32(1024), T.int32(640)), "float32"), T_transpose: T.Buffer((T.int32(16), T.int32(80), T.int32(1024)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(20)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(655360))
                        v_ax1 = T.axis.spatial(T.int32(1024), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(655360) // T.int32(640))
                        v_ax2 = T.axis.spatial(T.int32(8), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(640) // T.int32(80))
                        v_ax3 = T.axis.spatial(T.int32(80), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(80))
                        T.reads(lv259[(((v_ax2 * T.int32(80) + v_ax3) // T.int32(640) + v_ax1) // T.int32(1024) + v_ax0) % T.int32(2), ((v_ax2 * T.int32(80) + v_ax3) // T.int32(640) + v_ax1) % T.int32(1024), (v_ax2 * T.int32(80) + v_ax3) % T.int32(640)])
                        T.writes(T_transpose[v_ax0 * T.int32(8) + v_ax2, v_ax3, v_ax1])
                        T_transpose[v_ax2 + v_ax0 * T.int32(8), v_ax3, v_ax1] = lv259[(((v_ax2 * T.int32(80) + v_ax3) // T.int32(640) + v_ax1) // T.int32(1024) + v_ax0) % T.int32(2), ((v_ax2 * T.int32(80) + v_ax3) // T.int32(640) + v_ax1) % T.int32(1024), (v_ax2 * T.int32(80) + v_ax3) % T.int32(640)]

    @T.prim_func
    def fused_reshape26_transpose23_reshape27(lv275: T.Buffer((T.int32(16), T.int32(1024), T.int32(80)), "float32"), T_reshape: T.Buffer((T.int32(2), T.int32(1024), T.int32(640)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(20)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(655360))
                        v_ax1 = T.axis.spatial(T.int32(8), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(655360) // T.int32(81920))
                        v_ax2 = T.axis.spatial(T.int32(1024), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(81920) // T.int32(80))
                        v_ax3 = T.axis.spatial(T.int32(80), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(80))
                        T.reads(lv275[(v_ax0 * T.int32(8) + (v_ax3 // T.int32(80) + v_ax2) // T.int32(1024) + v_ax1) % T.int32(16), (v_ax3 // T.int32(80) + v_ax2) % T.int32(1024), v_ax3 % T.int32(80)])
                        T.writes(T_reshape[v_ax0, v_ax2, v_ax1 * T.int32(80) + v_ax3])
                        T_reshape[v_ax0, v_ax2, v_ax3 + v_ax1 * T.int32(80)] = lv275[(v_ax0 * T.int32(8) + (v_ax3 // T.int32(80) + v_ax2) // T.int32(1024) + v_ax1) % T.int32(16), (v_ax3 // T.int32(80) + v_ax2) % T.int32(1024), v_ax3 % T.int32(80)]

    @T.prim_func
    def fused_reshape28_transpose25_reshape29(lv289: T.Buffer((T.int32(2), T.int32(77), T.int32(640)), "float32"), T_reshape: T.Buffer((T.int32(16), T.int32(77), T.int32(80)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int32(3080), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) // T.int32(49280))
                    v_ax1 = T.axis.spatial(T.int32(77), (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) % T.int32(49280) // T.int32(640))
                    v_ax2 = T.axis.spatial(T.int32(8), (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) % T.int32(640) // T.int32(80))
                    v_ax3 = T.axis.spatial(T.int32(80), (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) % T.int32(80))
                    T.reads(lv289[(((v_ax2 * T.int32(80) + v_ax3) // T.int32(640) + v_ax1) // T.int32(77) + v_ax0) % T.int32(2), ((v_ax2 * T.int32(80) + v_ax3) // T.int32(640) + v_ax1) % T.int32(77), (v_ax2 * T.int32(80) + v_ax3) % T.int32(640)])
                    T.writes(T_reshape[v_ax0 * T.int32(8) + v_ax2, v_ax1, v_ax3])
                    T_reshape[v_ax2 + v_ax0 * T.int32(8), v_ax1, v_ax3] = lv289[(((v_ax2 * T.int32(80) + v_ax3) // T.int32(640) + v_ax1) // T.int32(77) + v_ax0) % T.int32(2), ((v_ax2 * T.int32(80) + v_ax3) // T.int32(640) + v_ax1) % T.int32(77), (v_ax2 * T.int32(80) + v_ax3) % T.int32(640)]

    @T.prim_func
    def fused_reshape28_transpose25_reshape29_transpose26(lv287: T.Buffer((T.int32(2), T.int32(77), T.int32(640)), "float32"), T_transpose: T.Buffer((T.int32(16), T.int32(80), T.int32(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int32(3080), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) // T.int32(49280))
                    v_ax1 = T.axis.spatial(T.int32(77), (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) % T.int32(49280) // T.int32(640))
                    v_ax2 = T.axis.spatial(T.int32(8), (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) % T.int32(640) // T.int32(80))
                    v_ax3 = T.axis.spatial(T.int32(80), (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) % T.int32(80))
                    T.reads(lv287[(((v_ax2 * T.int32(80) + v_ax3) // T.int32(640) + v_ax1) // T.int32(77) + v_ax0) % T.int32(2), ((v_ax2 * T.int32(80) + v_ax3) // T.int32(640) + v_ax1) % T.int32(77), (v_ax2 * T.int32(80) + v_ax3) % T.int32(640)])
                    T.writes(T_transpose[v_ax0 * T.int32(8) + v_ax2, v_ax3, v_ax1])
                    T_transpose[v_ax2 + v_ax0 * T.int32(8), v_ax3, v_ax1] = lv287[(((v_ax2 * T.int32(80) + v_ax3) // T.int32(640) + v_ax1) // T.int32(77) + v_ax0) % T.int32(2), ((v_ax2 * T.int32(80) + v_ax3) // T.int32(640) + v_ax1) % T.int32(77), (v_ax2 * T.int32(80) + v_ax3) % T.int32(640)]

    @T.prim_func
    def fused_reshape2_reshape2_add(lv3: T.Buffer((T.int32(77), T.int32(768)), "float32"), lv7: T.Buffer((T.int32(77), T.int32(768)), "float32"), T_add: T.Buffer((T.int32(1), T.int32(77), T.int32(768)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int32(924), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                with T.block("T_add"):
                    v_ax0 = T.axis.spatial(T.int32(1), T.int32(0))
                    v_ax1 = T.axis.spatial(T.int32(77), (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) // T.int32(768))
                    v_ax2 = T.axis.spatial(T.int32(768), (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(768))
                    T.reads(lv3[(v_ax2 // T.int32(768) + v_ax1) % T.int32(77), v_ax2 % T.int32(768)], lv7[(v_ax2 // T.int32(768) + v_ax1) % T.int32(77), v_ax2 % T.int32(768)])
                    T.writes(T_add[v_ax0, v_ax1, v_ax2])
                    T_add[v_ax0, v_ax1, v_ax2] = lv3[(v_ax2 // T.int32(768) + v_ax1) % T.int32(77), v_ax2 % T.int32(768)] + lv7[(v_ax2 // T.int32(768) + v_ax1) % T.int32(77), v_ax2 % T.int32(768)]

    @T.prim_func
    def fused_reshape30_transpose29(lv323: T.Buffer((T.int32(2), T.int32(1024), T.int32(640)), "float32"), T_transpose: T.Buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(20)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(655360))
                        v_ax1 = T.axis.spatial(T.int32(32), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(655360) // T.int32(20480))
                        v_ax2 = T.axis.spatial(T.int32(32), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(20480) // T.int32(640))
                        v_ax3 = T.axis.spatial(T.int32(640), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(640))
                        T.reads(lv323[((v_ax1 * T.int32(32) + v_ax3 // T.int32(640) + v_ax2) // T.int32(1024) + v_ax0) % T.int32(2), (v_ax1 * T.int32(32) + v_ax3 // T.int32(640) + v_ax2) % T.int32(1024), v_ax3 % T.int32(640)])
                        T.writes(T_transpose[v_ax0, v_ax3, v_ax1, v_ax2])
                        T_transpose[v_ax0, v_ax3, v_ax1, v_ax2] = lv323[((v_ax1 * T.int32(32) + v_ax3 // T.int32(640) + v_ax2) // T.int32(1024) + v_ax0) % T.int32(2), (v_ax1 * T.int32(32) + v_ax3 // T.int32(640) + v_ax2) % T.int32(1024), v_ax3 % T.int32(640)]

    @T.prim_func
    def fused_reshape34_transpose31_reshape35(lv463: T.Buffer((T.int32(2), T.int32(256), T.int32(1280)), "float32"), T_reshape: T.Buffer((T.int32(16), T.int32(256), T.int32(160)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(10)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(327680))
                        v_ax1 = T.axis.spatial(T.int32(256), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(327680) // T.int32(1280))
                        v_ax2 = T.axis.spatial(T.int32(8), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(1280) // T.int32(160))
                        v_ax3 = T.axis.spatial(T.int32(160), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(160))
                        T.reads(lv463[(((v_ax2 * T.int32(160) + v_ax3) // T.int32(1280) + v_ax1) // T.int32(256) + v_ax0) % T.int32(2), ((v_ax2 * T.int32(160) + v_ax3) // T.int32(1280) + v_ax1) % T.int32(256), (v_ax2 * T.int32(160) + v_ax3) % T.int32(1280)])
                        T.writes(T_reshape[v_ax0 * T.int32(8) + v_ax2, v_ax1, v_ax3])
                        T_reshape[v_ax2 + v_ax0 * T.int32(8), v_ax1, v_ax3] = lv463[(((v_ax2 * T.int32(160) + v_ax3) // T.int32(1280) + v_ax1) // T.int32(256) + v_ax0) % T.int32(2), ((v_ax2 * T.int32(160) + v_ax3) // T.int32(1280) + v_ax1) % T.int32(256), (v_ax2 * T.int32(160) + v_ax3) % T.int32(1280)]

    @T.prim_func
    def fused_reshape34_transpose31_reshape35_transpose32(lv465: T.Buffer((T.int32(2), T.int32(256), T.int32(1280)), "float32"), T_transpose: T.Buffer((T.int32(16), T.int32(160), T.int32(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(10)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(327680))
                        v_ax1 = T.axis.spatial(T.int32(256), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(327680) // T.int32(1280))
                        v_ax2 = T.axis.spatial(T.int32(8), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(1280) // T.int32(160))
                        v_ax3 = T.axis.spatial(T.int32(160), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(160))
                        T.reads(lv465[(((v_ax2 * T.int32(160) + v_ax3) // T.int32(1280) + v_ax1) // T.int32(256) + v_ax0) % T.int32(2), ((v_ax2 * T.int32(160) + v_ax3) // T.int32(1280) + v_ax1) % T.int32(256), (v_ax2 * T.int32(160) + v_ax3) % T.int32(1280)])
                        T.writes(T_transpose[v_ax0 * T.int32(8) + v_ax2, v_ax3, v_ax1])
                        T_transpose[v_ax2 + v_ax0 * T.int32(8), v_ax3, v_ax1] = lv465[(((v_ax2 * T.int32(160) + v_ax3) // T.int32(1280) + v_ax1) // T.int32(256) + v_ax0) % T.int32(2), ((v_ax2 * T.int32(160) + v_ax3) // T.int32(1280) + v_ax1) % T.int32(256), (v_ax2 * T.int32(160) + v_ax3) % T.int32(1280)]

    @T.prim_func
    def fused_reshape36_transpose33_reshape37(lv481: T.Buffer((T.int32(16), T.int32(256), T.int32(160)), "float32"), T_reshape: T.Buffer((T.int32(2), T.int32(256), T.int32(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(10)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(327680))
                        v_ax1 = T.axis.spatial(T.int32(8), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(327680) // T.int32(40960))
                        v_ax2 = T.axis.spatial(T.int32(256), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(40960) // T.int32(160))
                        v_ax3 = T.axis.spatial(T.int32(160), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(160))
                        T.reads(lv481[(v_ax0 * T.int32(8) + (v_ax3 // T.int32(160) + v_ax2) // T.int32(256) + v_ax1) % T.int32(16), (v_ax3 // T.int32(160) + v_ax2) % T.int32(256), v_ax3 % T.int32(160)])
                        T.writes(T_reshape[v_ax0, v_ax2, v_ax1 * T.int32(160) + v_ax3])
                        T_reshape[v_ax0, v_ax2, v_ax3 + v_ax1 * T.int32(160)] = lv481[(v_ax0 * T.int32(8) + (v_ax3 // T.int32(160) + v_ax2) // T.int32(256) + v_ax1) % T.int32(16), (v_ax3 // T.int32(160) + v_ax2) % T.int32(256), v_ax3 % T.int32(160)]

    @T.prim_func
    def fused_reshape38_transpose35_reshape39(lv495: T.Buffer((T.int32(2), T.int32(77), T.int32(1280)), "float32"), T_reshape: T.Buffer((T.int32(16), T.int32(77), T.int32(160)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int32(6160), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) // T.int32(98560))
                    v_ax1 = T.axis.spatial(T.int32(77), (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) % T.int32(98560) // T.int32(1280))
                    v_ax2 = T.axis.spatial(T.int32(8), (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) % T.int32(1280) // T.int32(160))
                    v_ax3 = T.axis.spatial(T.int32(160), (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) % T.int32(160))
                    T.reads(lv495[(((v_ax2 * T.int32(160) + v_ax3) // T.int32(1280) + v_ax1) // T.int32(77) + v_ax0) % T.int32(2), ((v_ax2 * T.int32(160) + v_ax3) // T.int32(1280) + v_ax1) % T.int32(77), (v_ax2 * T.int32(160) + v_ax3) % T.int32(1280)])
                    T.writes(T_reshape[v_ax0 * T.int32(8) + v_ax2, v_ax1, v_ax3])
                    T_reshape[v_ax2 + v_ax0 * T.int32(8), v_ax1, v_ax3] = lv495[(((v_ax2 * T.int32(160) + v_ax3) // T.int32(1280) + v_ax1) // T.int32(77) + v_ax0) % T.int32(2), ((v_ax2 * T.int32(160) + v_ax3) // T.int32(1280) + v_ax1) % T.int32(77), (v_ax2 * T.int32(160) + v_ax3) % T.int32(1280)]

    @T.prim_func
    def fused_reshape38_transpose35_reshape39_transpose36(lv493: T.Buffer((T.int32(2), T.int32(77), T.int32(1280)), "float32"), T_transpose: T.Buffer((T.int32(16), T.int32(160), T.int32(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int32(3080), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(98560))
                    v_ax1 = T.axis.spatial(T.int32(77), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(98560) // T.int32(1280))
                    v_ax2 = T.axis.spatial(T.int32(8), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(1280) // T.int32(160))
                    v_ax3 = T.axis.spatial(T.int32(160), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(160))
                    T.reads(lv493[(((v_ax2 * T.int32(160) + v_ax3) // T.int32(1280) + v_ax1) // T.int32(77) + v_ax0) % T.int32(2), ((v_ax2 * T.int32(160) + v_ax3) // T.int32(1280) + v_ax1) % T.int32(77), (v_ax2 * T.int32(160) + v_ax3) % T.int32(1280)])
                    T.writes(T_transpose[v_ax0 * T.int32(8) + v_ax2, v_ax3, v_ax1])
                    T_transpose[v_ax2 + v_ax0 * T.int32(8), v_ax3, v_ax1] = lv493[(((v_ax2 * T.int32(160) + v_ax3) // T.int32(1280) + v_ax1) // T.int32(77) + v_ax0) % T.int32(2), ((v_ax2 * T.int32(160) + v_ax3) // T.int32(1280) + v_ax1) % T.int32(77), (v_ax2 * T.int32(160) + v_ax3) % T.int32(1280)]

    @T.prim_func
    def fused_reshape3_transpose1_reshape4(lv26: T.Buffer((T.int32(1), T.int32(77), T.int32(768)), "float32"), T_reshape: T.Buffer((T.int32(12), T.int32(77), T.int32(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int32(1848), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int32(1), T.int32(0))
                    v_ax1 = T.axis.spatial(T.int32(77), (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) // T.int32(768))
                    v_ax2 = T.axis.spatial(T.int32(12), (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) % T.int32(768) // T.int32(64))
                    v_ax3 = T.axis.spatial(T.int32(64), (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) % T.int32(64))
                    T.reads(lv26[T.int32(0), ((v_ax2 * T.int32(64) + v_ax3) // T.int32(768) + v_ax1) % T.int32(77), (v_ax2 * T.int32(64) + v_ax3) % T.int32(768)])
                    T.writes(T_reshape[v_ax2, v_ax1, v_ax3])
                    T_reshape[v_ax2, v_ax1, v_ax3] = lv26[T.int32(0), ((v_ax2 * T.int32(64) + v_ax3) // T.int32(768) + v_ax1) % T.int32(77), (v_ax2 * T.int32(64) + v_ax3) % T.int32(768)]

    @T.prim_func
    def fused_reshape3_transpose1_reshape4_transpose2(lv21: T.Buffer((T.int32(1), T.int32(77), T.int32(768)), "float32"), T_transpose: T.Buffer((T.int32(12), T.int32(64), T.int32(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int32(1848), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int32(1), T.int32(0))
                    v_ax1 = T.axis.spatial(T.int32(77), (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) // T.int32(768))
                    v_ax2 = T.axis.spatial(T.int32(12), (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) % T.int32(768) // T.int32(64))
                    v_ax3 = T.axis.spatial(T.int32(64), (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) % T.int32(64))
                    T.reads(lv21[T.int32(0), ((v_ax2 * T.int32(64) + v_ax3) // T.int32(768) + v_ax1) % T.int32(77), (v_ax2 * T.int32(64) + v_ax3) % T.int32(768)])
                    T.writes(T_transpose[v_ax2, v_ax3, v_ax1])
                    T_transpose[v_ax2, v_ax3, v_ax1] = lv21[T.int32(0), ((v_ax2 * T.int32(64) + v_ax3) // T.int32(768) + v_ax1) % T.int32(77), (v_ax2 * T.int32(64) + v_ax3) % T.int32(768)]

    @T.prim_func
    def fused_reshape40_transpose39(lv529: T.Buffer((T.int32(2), T.int32(256), T.int32(1280)), "float32"), T_transpose: T.Buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(10)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(327680))
                        v_ax1 = T.axis.spatial(T.int32(16), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(327680) // T.int32(20480))
                        v_ax2 = T.axis.spatial(T.int32(16), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(20480) // T.int32(1280))
                        v_ax3 = T.axis.spatial(T.int32(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(1280))
                        T.reads(lv529[((v_ax1 * T.int32(16) + v_ax3 // T.int32(1280) + v_ax2) // T.int32(256) + v_ax0) % T.int32(2), (v_ax1 * T.int32(16) + v_ax3 // T.int32(1280) + v_ax2) % T.int32(256), v_ax3 % T.int32(1280)])
                        T.writes(T_transpose[v_ax0, v_ax3, v_ax1, v_ax2])
                        T_transpose[v_ax0, v_ax3, v_ax1, v_ax2] = lv529[((v_ax1 * T.int32(16) + v_ax3 // T.int32(1280) + v_ax2) // T.int32(256) + v_ax0) % T.int32(2), (v_ax1 * T.int32(16) + v_ax3 // T.int32(1280) + v_ax2) % T.int32(256), v_ax3 % T.int32(1280)]

    @T.prim_func
    def fused_reshape42_transpose41_reshape43(lv704: T.Buffer((T.int32(2), T.int32(64), T.int32(1280)), "float32"), T_reshape: T.Buffer((T.int32(16), T.int32(64), T.int32(160)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int32(1280), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(81920))
                    v_ax1 = T.axis.spatial(T.int32(64), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(81920) // T.int32(1280))
                    v_ax2 = T.axis.spatial(T.int32(8), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(1280) // T.int32(160))
                    v_ax3 = T.axis.spatial(T.int32(160), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(160))
                    T.reads(lv704[(((v_ax2 * T.int32(160) + v_ax3) // T.int32(1280) + v_ax1) // T.int32(64) + v_ax0) % T.int32(2), ((v_ax2 * T.int32(160) + v_ax3) // T.int32(1280) + v_ax1) % T.int32(64), (v_ax2 * T.int32(160) + v_ax3) % T.int32(1280)])
                    T.writes(T_reshape[v_ax0 * T.int32(8) + v_ax2, v_ax1, v_ax3])
                    T_reshape[v_ax2 + v_ax0 * T.int32(8), v_ax1, v_ax3] = lv704[(((v_ax2 * T.int32(160) + v_ax3) // T.int32(1280) + v_ax1) // T.int32(64) + v_ax0) % T.int32(2), ((v_ax2 * T.int32(160) + v_ax3) // T.int32(1280) + v_ax1) % T.int32(64), (v_ax2 * T.int32(160) + v_ax3) % T.int32(1280)]

    @T.prim_func
    def fused_reshape42_transpose41_reshape43_transpose42(lv706: T.Buffer((T.int32(2), T.int32(64), T.int32(1280)), "float32"), T_transpose: T.Buffer((T.int32(16), T.int32(160), T.int32(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(3)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(81920))
                        v_ax1 = T.axis.spatial(T.int32(64), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(81920) // T.int32(1280))
                        v_ax2 = T.axis.spatial(T.int32(8), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(1280) // T.int32(160))
                        v_ax3 = T.axis.spatial(T.int32(160), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(160))
                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) * T.int32(256) + ax0_ax1_ax2_ax3_fused_2 < T.int32(163840))
                        T.reads(lv706[(((v_ax2 * T.int32(160) + v_ax3) // T.int32(1280) + v_ax1) // T.int32(64) + v_ax0) % T.int32(2), ((v_ax2 * T.int32(160) + v_ax3) // T.int32(1280) + v_ax1) % T.int32(64), (v_ax2 * T.int32(160) + v_ax3) % T.int32(1280)])
                        T.writes(T_transpose[v_ax0 * T.int32(8) + v_ax2, v_ax3, v_ax1])
                        T_transpose[v_ax2 + v_ax0 * T.int32(8), v_ax3, v_ax1] = lv706[(((v_ax2 * T.int32(160) + v_ax3) // T.int32(1280) + v_ax1) // T.int32(64) + v_ax0) % T.int32(2), ((v_ax2 * T.int32(160) + v_ax3) // T.int32(1280) + v_ax1) % T.int32(64), (v_ax2 * T.int32(160) + v_ax3) % T.int32(1280)]

    @T.prim_func
    def fused_reshape44_transpose43_reshape45(lv722: T.Buffer((T.int32(16), T.int32(64), T.int32(160)), "float32"), T_reshape: T.Buffer((T.int32(2), T.int32(64), T.int32(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(3)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(81920))
                        v_ax1 = T.axis.spatial(T.int32(8), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(81920) // T.int32(10240))
                        v_ax2 = T.axis.spatial(T.int32(64), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(10240) // T.int32(160))
                        v_ax3 = T.axis.spatial(T.int32(160), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(160))
                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) * T.int32(256) + ax0_ax1_ax2_ax3_fused_2 < T.int32(163840))
                        T.reads(lv722[(v_ax0 * T.int32(8) + (v_ax3 // T.int32(160) + v_ax2) // T.int32(64) + v_ax1) % T.int32(16), (v_ax3 // T.int32(160) + v_ax2) % T.int32(64), v_ax3 % T.int32(160)])
                        T.writes(T_reshape[v_ax0, v_ax2, v_ax1 * T.int32(160) + v_ax3])
                        T_reshape[v_ax0, v_ax2, v_ax3 + v_ax1 * T.int32(160)] = lv722[(v_ax0 * T.int32(8) + (v_ax3 // T.int32(160) + v_ax2) // T.int32(64) + v_ax1) % T.int32(16), (v_ax3 // T.int32(160) + v_ax2) % T.int32(64), v_ax3 % T.int32(160)]

    @T.prim_func
    def fused_reshape46_transpose44(lv770: T.Buffer((T.int32(2), T.int32(64), T.int32(1280)), "float32"), T_transpose: T.Buffer((T.int32(2), T.int32(1280), T.int32(8), T.int32(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(3)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(81920))
                        v_ax1 = T.axis.spatial(T.int32(8), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(81920) // T.int32(10240))
                        v_ax2 = T.axis.spatial(T.int32(8), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(10240) // T.int32(1280))
                        v_ax3 = T.axis.spatial(T.int32(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(1280))
                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) * T.int32(256) + ax0_ax1_ax2_ax3_fused_2 < T.int32(163840))
                        T.reads(lv770[((v_ax1 * T.int32(8) + v_ax3 // T.int32(1280) + v_ax2) // T.int32(64) + v_ax0) % T.int32(2), (v_ax1 * T.int32(8) + v_ax3 // T.int32(1280) + v_ax2) % T.int32(64), v_ax3 % T.int32(1280)])
                        T.writes(T_transpose[v_ax0, v_ax3, v_ax1, v_ax2])
                        T_transpose[v_ax0, v_ax3, v_ax1, v_ax2] = lv770[((v_ax1 * T.int32(8) + v_ax3 // T.int32(1280) + v_ax2) // T.int32(64) + v_ax0) % T.int32(2), (v_ax1 * T.int32(8) + v_ax3 // T.int32(1280) + v_ax2) % T.int32(64), v_ax3 % T.int32(1280)]

    @T.prim_func
    def fused_reshape49_transpose45(lv18: T.Buffer((T.int32(1), T.int32(512), T.int32(64), T.int32(64)), "float32"), T_transpose: T.Buffer((T.int32(1), T.int32(4096), T.int32(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int32(32)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int32(1), T.int32(0))
                        v_ax1 = T.axis.spatial(T.int32(512), (ax0_ax1_ax2_fused_0 * T.int32(65536) + ax0_ax1_ax2_fused_1 * T.int32(256) + ax0_ax1_ax2_fused_2) // T.int32(4096))
                        v_ax2 = T.axis.spatial(T.int32(4096), (ax0_ax1_ax2_fused_0 * T.int32(65536) + ax0_ax1_ax2_fused_1 * T.int32(256) + ax0_ax1_ax2_fused_2) % T.int32(4096))
                        T.reads(lv18[T.int32(0), (v_ax2 // T.int32(4096) + v_ax1) % T.int32(512), v_ax2 % T.int32(4096) // T.int32(64), v_ax2 % T.int32(64)])
                        T.writes(T_transpose[v_ax0, v_ax2, v_ax1])
                        T_transpose[v_ax0, v_ax2, v_ax1] = lv18[T.int32(0), (v_ax2 // T.int32(4096) + v_ax1) % T.int32(512), v_ax2 % T.int32(4096) // T.int32(64), v_ax2 % T.int32(64)]

    @T.prim_func
    def fused_reshape50_transpose47_reshape51(lv23: T.Buffer((T.int32(1), T.int32(4096), T.int32(512)), "float32"), T_reshape: T.Buffer((T.int32(1), T.int32(4096), T.int32(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(32)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int32(1), T.int32(0))
                        v_ax1 = T.axis.spatial(T.int32(4096), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(512))
                        v_ax2 = T.axis.spatial(T.int32(1), T.int32(0))
                        v_ax3 = T.axis.spatial(T.int32(512), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(512))
                        T.reads(lv23[T.int32(0), (v_ax3 // T.int32(512) + v_ax1 + v_ax2) % T.int32(4096), v_ax3 % T.int32(512)])
                        T.writes(T_reshape[T.int32(0), v_ax1, v_ax3])
                        T_reshape[T.int32(0), v_ax1, v_ax3] = lv23[T.int32(0), (v_ax3 // T.int32(512) + v_ax1 + v_ax2) % T.int32(4096), v_ax3 % T.int32(512)]

    @T.prim_func
    def fused_reshape50_transpose47_reshape51_transpose48(lv26: T.Buffer((T.int32(1), T.int32(4096), T.int32(512)), "float32"), T_transpose: T.Buffer((T.int32(1), T.int32(512), T.int32(4096)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(32)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int32(1), T.int32(0))
                        v_ax1 = T.axis.spatial(T.int32(4096), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(512))
                        v_ax2 = T.axis.spatial(T.int32(1), T.int32(0))
                        v_ax3 = T.axis.spatial(T.int32(512), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(512))
                        T.reads(lv26[T.int32(0), (v_ax3 // T.int32(512) + v_ax1 + v_ax2) % T.int32(4096), v_ax3 % T.int32(512)])
                        T.writes(T_transpose[T.int32(0), v_ax3, v_ax1])
                        T_transpose[T.int32(0), v_ax3, v_ax1] = lv26[T.int32(0), (v_ax3 // T.int32(512) + v_ax1 + v_ax2) % T.int32(4096), v_ax3 % T.int32(512)]

    @T.prim_func
    def fused_reshape52_transpose49_reshape53(lv45: T.Buffer((T.int32(1), T.int32(4096), T.int32(512)), "float32"), T_reshape: T.Buffer((T.int32(1), T.int32(4096), T.int32(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(32)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int32(1), T.int32(0))
                        v_ax1 = T.axis.spatial(T.int32(1), T.int32(0))
                        v_ax2 = T.axis.spatial(T.int32(4096), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(512))
                        v_ax3 = T.axis.spatial(T.int32(512), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(512))
                        T.reads(lv45[T.int32(0), (v_ax3 // T.int32(512) + v_ax2) % T.int32(4096), v_ax3 % T.int32(512)])
                        T.writes(T_reshape[T.int32(0), v_ax2, v_ax3])
                        T_reshape[T.int32(0), v_ax2, v_ax3] = lv45[T.int32(0), (v_ax3 // T.int32(512) + v_ax2) % T.int32(4096), v_ax3 % T.int32(512)]

    @T.prim_func
    def fused_reshape5_add2_reshape6(lv35: T.Buffer((T.int32(12), T.int32(77), T.int32(77)), "float32"), param_0: T.Buffer((T.int32(1), T.int32(1), T.int32(77), T.int32(77)), "float32"), T_reshape: T.Buffer((T.int32(12), T.int32(77), T.int32(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int32(1112), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int32(1), T.int32(0))
                    v_ax1 = T.axis.spatial(T.int32(12), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(5929))
                    v_ax2 = T.axis.spatial(T.int32(77), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(5929) // T.int32(77))
                    v_ax3 = T.axis.spatial(T.int32(77), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(77))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1 < T.int32(71148))
                    T.reads(lv35[((v_ax3 // T.int32(77) + v_ax2) // T.int32(77) + v_ax1) % T.int32(12), (v_ax3 // T.int32(77) + v_ax2) % T.int32(77), v_ax3 % T.int32(77)], param_0[v_ax0, T.int32(0), v_ax2, v_ax3])
                    T.writes(T_reshape[v_ax1, v_ax2, v_ax3])
                    T_reshape[v_ax1, v_ax2, v_ax3] = lv35[((v_ax3 // T.int32(77) + v_ax2) // T.int32(77) + v_ax1) % T.int32(12), (v_ax3 // T.int32(77) + v_ax2) % T.int32(77), v_ax3 % T.int32(77)] + param_0[v_ax0, T.int32(0), v_ax2, v_ax3]

    @T.prim_func
    def fused_reshape7_transpose3_reshape8(lv40: T.Buffer((T.int32(12), T.int32(77), T.int32(64)), "float32"), T_reshape: T.Buffer((T.int32(1), T.int32(77), T.int32(768)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int32(924), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int32(1), T.int32(0))
                    v_ax1 = T.axis.spatial(T.int32(12), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) // T.int32(4928))
                    v_ax2 = T.axis.spatial(T.int32(77), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(4928) // T.int32(64))
                    v_ax3 = T.axis.spatial(T.int32(64), (ax0_ax1_ax2_ax3_fused_0 * T.int32(64) + ax0_ax1_ax2_ax3_fused_1) % T.int32(64))
                    T.reads(lv40[((v_ax3 // T.int32(64) + v_ax2) // T.int32(77) + v_ax1) % T.int32(12), (v_ax3 // T.int32(64) + v_ax2) % T.int32(77), v_ax3 % T.int32(64)])
                    T.writes(T_reshape[T.int32(0), v_ax2, v_ax1 * T.int32(64) + v_ax3])
                    T_reshape[T.int32(0), v_ax2, v_ax3 + v_ax1 * T.int32(64)] = lv40[((v_ax3 // T.int32(64) + v_ax2) // T.int32(77) + v_ax1) % T.int32(12), (v_ax3 // T.int32(64) + v_ax2) % T.int32(77), v_ax3 % T.int32(64)]

    @T.prim_func
    def fused_reshape_cast_reshape1(inp_0: T.Buffer((T.int32(1), T.int32(77)), "int32"), T_reshape: T.Buffer((T.int32(77),), "int32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int32(3), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int32(1), T.int32(0))
                    v_ax1 = T.axis.spatial(T.int32(77), ax0_ax1_fused_0 * T.int32(32) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int32(32) + ax0_ax1_fused_1 < T.int32(77))
                    T.reads(inp_0[T.int32(0), v_ax1 % T.int32(77)])
                    T.writes(T_reshape[v_ax1])
                    T_reshape[v_ax1] = T.Cast("int32", inp_0[T.int32(0), v_ax1 % T.int32(77)])

    @T.prim_func
    def fused_softmax9_cast2(lv42: T.Buffer((T.int32(1), T.int32(4096), T.int32(4096)), "float32"), compute: T.Buffer((T.int32(1), T.int32(4096), T.int32(4096)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int32(1), T.int32(4096)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int32(1), T.int32(4096)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int32(4096), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(16), "pragma_unroll_explicit": T.int32(1)}):
            for ax0, ax1, ax2_0 in T.grid(T.int32(1), T.int32(1), T.int32(16)):
                for ax2_1 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int32(1), ax0)
                        v_i1 = T.axis.spatial(T.int32(4096), i0_i1_fused + ax1)
                        v_k = T.axis.reduce(T.int32(4096), ax2_0 * T.int32(256) + ax2_1)
                        T.reads(lv42[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], lv42[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int32(1), T.int32(1), T.int32(16)):
                for ax2_1 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int32(1), ax0)
                        v_i1 = T.axis.spatial(T.int32(4096), i0_i1_fused + ax1)
                        v_k = T.axis.reduce(T.int32(4096), ax2_0 * T.int32(256) + ax2_1)
                        T.reads(lv42[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(lv42[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int32(16)):
                for i2_1 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int32(1), T.int32(0))
                        v_i1 = T.axis.spatial(T.int32(4096), i0_i1_fused)
                        v_i2 = T.axis.spatial(T.int32(4096), i2_0 * T.int32(256) + i2_1)
                        T.reads(lv42[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(compute[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        compute[v_i0, v_i1, v_i2] = T.exp(lv42[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def fused_transpose19_reshape23(lv252: T.Buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), "float32"), T_reshape: T.Buffer((T.int32(2), T.int32(1024), T.int32(640)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(20)):
                    with T.block("T_transpose"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(655360))
                        v_ax1 = T.axis.spatial(T.int32(32), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(655360) // T.int32(20480))
                        v_ax2 = T.axis.spatial(T.int32(32), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(20480) // T.int32(640))
                        v_ax3 = T.axis.spatial(T.int32(640), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(640))
                        T.reads(lv252[v_ax0, v_ax3, v_ax1, v_ax2])
                        T.writes(T_reshape[v_ax0, v_ax1 * T.int32(32) + v_ax2, v_ax3])
                        T_reshape[v_ax0, v_ax2 + v_ax1 * T.int32(32), v_ax3] = lv252[v_ax0, v_ax3, v_ax1, v_ax2]

    @T.prim_func
    def fused_transpose30_reshape33(lv458: T.Buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), "float32"), T_reshape: T.Buffer((T.int32(2), T.int32(256), T.int32(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(10)):
                    with T.block("T_transpose"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(327680))
                        v_ax1 = T.axis.spatial(T.int32(16), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(327680) // T.int32(20480))
                        v_ax2 = T.axis.spatial(T.int32(16), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(20480) // T.int32(1280))
                        v_ax3 = T.axis.spatial(T.int32(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(1280))
                        T.reads(lv458[v_ax0, v_ax3, v_ax1, v_ax2])
                        T.writes(T_reshape[v_ax0, v_ax1 * T.int32(16) + v_ax2, v_ax3])
                        T_reshape[v_ax0, v_ax2 + v_ax1 * T.int32(16), v_ax3] = lv458[v_ax0, v_ax3, v_ax1, v_ax2]

    @T.prim_func
    def fused_transpose40_reshape41(lv699: T.Buffer((T.int32(2), T.int32(1280), T.int32(8), T.int32(8)), "float32"), T_reshape: T.Buffer((T.int32(2), T.int32(64), T.int32(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(3)):
                    with T.block("T_transpose"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(81920))
                        v_ax1 = T.axis.spatial(T.int32(8), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(81920) // T.int32(10240))
                        v_ax2 = T.axis.spatial(T.int32(8), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(10240) // T.int32(1280))
                        v_ax3 = T.axis.spatial(T.int32(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(1280))
                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) * T.int32(256) + ax0_ax1_ax2_ax3_fused_2 < T.int32(163840))
                        T.reads(lv699[v_ax0, v_ax3, v_ax1, v_ax2])
                        T.writes(T_reshape[v_ax0, v_ax1 * T.int32(8) + v_ax2, v_ax3])
                        T_reshape[v_ax0, v_ax2 + v_ax1 * T.int32(8), v_ax3] = lv699[v_ax0, v_ax3, v_ax1, v_ax2]

    @T.prim_func
    def fused_transpose48_reshape54_add38_divide4(lv51: T.Buffer((T.int32(1), T.int32(4096), T.int32(512)), "float32"), lv17: T.Buffer((T.int32(1), T.int32(512), T.int32(64), T.int32(64)), "float32"), T_divide: T.Buffer((T.int32(1), T.int32(512), T.int32(64), T.int32(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int32(32)):
                    with T.block("T_transpose"):
                        v_ax0 = T.axis.spatial(T.int32(1), T.int32(0))
                        v_ax1 = T.axis.spatial(T.int32(512), (ax0_ax1_ax2_fused_0 * T.int32(65536) + ax0_ax1_ax2_fused_1 * T.int32(256) + ax0_ax1_ax2_fused_2) // T.int32(4096))
                        v_ax2 = T.axis.spatial(T.int32(4096), (ax0_ax1_ax2_fused_0 * T.int32(65536) + ax0_ax1_ax2_fused_1 * T.int32(256) + ax0_ax1_ax2_fused_2) % T.int32(4096))
                        T.reads(lv51[v_ax0, v_ax2, v_ax1], lv17[T.int32(0), v_ax1, v_ax2 // T.int32(64), v_ax2 % T.int32(64)])
                        T.writes(T_divide[T.int32(0), v_ax1, v_ax2 // T.int32(64), v_ax2 % T.int32(64)])
                        T_divide[T.int32(0), v_ax1, v_ax2 // T.int32(64), v_ax2 % T.int32(64)] = lv51[v_ax0, v_ax2, v_ax1] + lv17[T.int32(0), v_ax1, v_ax2 // T.int32(64), v_ax2 % T.int32(64)]

    @T.prim_func
    def fused_transpose9_reshape13(lv46: T.Buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), "float32"), T_reshape: T.Buffer((T.int32(2), T.int32(4096), T.int32(320)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int32(40)):
                    with T.block("T_transpose"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) // T.int32(1310720))
                        v_ax1 = T.axis.spatial(T.int32(64), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(1310720) // T.int32(20480))
                        v_ax2 = T.axis.spatial(T.int32(64), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(20480) // T.int32(320))
                        v_ax3 = T.axis.spatial(T.int32(320), (ax0_ax1_ax2_ax3_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_fused_2) % T.int32(320))
                        T.reads(lv46[v_ax0, v_ax3, v_ax1, v_ax2])
                        T.writes(T_reshape[v_ax0, v_ax1 * T.int32(64) + v_ax2, v_ax3])
                        T_reshape[v_ax0, v_ax2 + v_ax1 * T.int32(64), v_ax3] = lv46[v_ax0, v_ax3, v_ax1, v_ax2]

    @T.prim_func
    def group_norm1(rxplaceholder: T.Buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), "float32"), rxplaceholder_1: T.Buffer((T.int32(320),), "float32"), rxplaceholder_2: T.Buffer((T.int32(320),), "float32"), T_reshape: T.Buffer((T.int32(2), T.int32(320), T.int32(64), T.int32(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int32(2), T.int32(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int32(2), T.int32(32)))
        for ax0_ax1_fused in T.thread_binding(T.int32(64), thread="blockIdx.x"):
            for k2_k3_k4_fused_0 in range(T.int32(640)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int32(2), ax0_ax1_fused // T.int32(32))
                        v_ax1 = T.axis.spatial(T.int32(32), ax0_ax1_fused % T.int32(32))
                        v_k2 = T.axis.reduce(T.int32(10), (k2_k3_k4_fused_0 * T.int32(64) + k2_k3_k4_fused_1) // T.int32(4096))
                        v_k3 = T.axis.reduce(T.int32(64), (k2_k3_k4_fused_0 * T.int32(64) + k2_k3_k4_fused_1) % T.int32(4096) // T.int32(64))
                        v_k4 = T.axis.reduce(T.int32(64), (k2_k3_k4_fused_0 * T.int32(64) + k2_k3_k4_fused_1) % T.int32(64))
                        T.reads(rxplaceholder[((v_ax1 * T.int32(10) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) // T.int32(320) + v_ax0) % T.int32(2), (v_ax1 * T.int32(10) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) % T.int32(320), (v_k4 // T.int32(64) + v_k3) % T.int32(64), v_k4 % T.int32(64)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + rxplaceholder[((v_ax1 * T.int32(10) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) // T.int32(320) + v_ax0) % T.int32(2), (v_ax1 * T.int32(10) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) % T.int32(320), (v_k4 // T.int32(64) + v_k3) % T.int32(64), v_k4 % T.int32(64)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + rxplaceholder[((v_ax1 * T.int32(10) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) // T.int32(320) + v_ax0) % T.int32(2), (v_ax1 * T.int32(10) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) % T.int32(320), (v_k4 // T.int32(64) + v_k3) % T.int32(64), v_k4 % T.int32(64)] * rxplaceholder[((v_ax1 * T.int32(10) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) // T.int32(320) + v_ax0) % T.int32(2), (v_ax1 * T.int32(10) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) % T.int32(320), (v_k4 // T.int32(64) + v_k3) % T.int32(64), v_k4 % T.int32(64)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_ax4_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_ax4_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_ax4_fused_0 in range(T.int32(40)):
                    with T.block("T_group_norm"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_ax4_fused_2) // T.int32(1310720))
                        v_ax1 = T.axis.spatial(T.int32(32), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int32(1310720) // T.int32(40960))
                        v_ax2 = T.axis.spatial(T.int32(10), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int32(40960) // T.int32(4096))
                        v_ax3 = T.axis.spatial(T.int32(64), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int32(4096) // T.int32(64))
                        v_ax4 = T.axis.spatial(T.int32(64), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int32(64))
                        T.reads(rxplaceholder[((v_ax1 * T.int32(10) + (v_ax4 // T.int32(64) + v_ax3) // T.int32(64) + v_ax2) // T.int32(320) + v_ax0) % T.int32(2), (v_ax1 * T.int32(10) + (v_ax4 // T.int32(64) + v_ax3) // T.int32(64) + v_ax2) % T.int32(320), (v_ax4 // T.int32(64) + v_ax3) % T.int32(64), v_ax4 % T.int32(64)], rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1], rxplaceholder_1[(v_ax1 * T.int32(10) + v_ax2) % T.int32(320)], rxplaceholder_2[(v_ax1 * T.int32(10) + v_ax2) % T.int32(320)])
                        T.writes(T_reshape[v_ax0, v_ax1 * T.int32(10) + v_ax2, v_ax3, v_ax4])
                        T_reshape[v_ax0, v_ax2 + v_ax1 * T.int32(10), v_ax3, v_ax4] = (rxplaceholder[((v_ax1 * T.int32(10) + (v_ax4 // T.int32(64) + v_ax3) // T.int32(64) + v_ax2) // T.int32(320) + v_ax0) % T.int32(2), (v_ax1 * T.int32(10) + (v_ax4 // T.int32(64) + v_ax3) // T.int32(64) + v_ax2) % T.int32(320), (v_ax4 // T.int32(64) + v_ax3) % T.int32(64), v_ax4 % T.int32(64)] - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(2.4414062500000001e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[v_ax0, v_ax1] * T.float32(2.4414062500000001e-05) - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(2.4414062500000001e-05) * (rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(2.4414062500000001e-05)) + T.float32(9.9999999999999995e-07)) * rxplaceholder_1[(v_ax1 * T.int32(10) + v_ax2) % T.int32(320)] + rxplaceholder_2[(v_ax1 * T.int32(10) + v_ax2) % T.int32(320)]

    @T.prim_func
    def group_norm18(rxplaceholder: T.Buffer((T.int32(1), T.int32(512), T.int32(64), T.int32(64)), "float32"), rxplaceholder_1: T.Buffer((T.int32(512),), "float32"), rxplaceholder_2: T.Buffer((T.int32(512),), "float32"), T_reshape: T.Buffer((T.int32(1), T.int32(512), T.int32(64), T.int32(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int32(1), T.int32(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int32(1), T.int32(32)))
        for ax0_ax1_fused in T.thread_binding(T.int32(32), thread="blockIdx.x"):
            for k2_k3_k4_fused_0 in range(T.int32(256)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int32(1), T.int32(0))
                        v_ax1 = T.axis.spatial(T.int32(32), ax0_ax1_fused)
                        v_k2 = T.axis.reduce(T.int32(16), (k2_k3_k4_fused_0 * T.int32(256) + k2_k3_k4_fused_1) // T.int32(4096))
                        v_k3 = T.axis.reduce(T.int32(64), (k2_k3_k4_fused_0 * T.int32(256) + k2_k3_k4_fused_1) % T.int32(4096) // T.int32(64))
                        v_k4 = T.axis.reduce(T.int32(64), (k2_k3_k4_fused_0 * T.int32(256) + k2_k3_k4_fused_1) % T.int32(64))
                        T.reads(rxplaceholder[T.int32(0), (v_ax1 * T.int32(16) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) % T.int32(512), (v_k4 // T.int32(64) + v_k3) % T.int32(64), v_k4 % T.int32(64)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + rxplaceholder[T.int32(0), (v_ax1 * T.int32(16) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) % T.int32(512), (v_k4 // T.int32(64) + v_k3) % T.int32(64), v_k4 % T.int32(64)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + rxplaceholder[T.int32(0), (v_ax1 * T.int32(16) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) % T.int32(512), (v_k4 // T.int32(64) + v_k3) % T.int32(64), v_k4 % T.int32(64)] * rxplaceholder[T.int32(0), (v_ax1 * T.int32(16) + (v_k4 // T.int32(64) + v_k3) // T.int32(64) + v_k2) % T.int32(512), (v_k4 // T.int32(64) + v_k3) % T.int32(64), v_k4 % T.int32(64)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_ax4_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_ax4_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_ax4_fused_0 in range(T.int32(32)):
                    with T.block("T_group_norm"):
                        v_ax0 = T.axis.spatial(T.int32(1), T.int32(0))
                        v_ax1 = T.axis.spatial(T.int32(32), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_ax4_fused_2) // T.int32(65536))
                        v_ax2 = T.axis.spatial(T.int32(16), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int32(65536) // T.int32(4096))
                        v_ax3 = T.axis.spatial(T.int32(64), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int32(4096) // T.int32(64))
                        v_ax4 = T.axis.spatial(T.int32(64), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int32(64))
                        T.reads(rxplaceholder[T.int32(0), (v_ax1 * T.int32(16) + (v_ax4 // T.int32(64) + v_ax3) // T.int32(64) + v_ax2) % T.int32(512), (v_ax4 // T.int32(64) + v_ax3) % T.int32(64), v_ax4 % T.int32(64)], rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1], rxplaceholder_1[(v_ax1 * T.int32(16) + v_ax2) % T.int32(512)], rxplaceholder_2[(v_ax1 * T.int32(16) + v_ax2) % T.int32(512)])
                        T.writes(T_reshape[T.int32(0), v_ax1 * T.int32(16) + v_ax2, v_ax3, v_ax4])
                        T_reshape[T.int32(0), v_ax2 + v_ax1 * T.int32(16), v_ax3, v_ax4] = (rxplaceholder[T.int32(0), (v_ax1 * T.int32(16) + (v_ax4 // T.int32(64) + v_ax3) // T.int32(64) + v_ax2) % T.int32(512), (v_ax4 // T.int32(64) + v_ax3) % T.int32(64), v_ax4 % T.int32(64)] - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(1.52587890625e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[v_ax0, v_ax1] * T.float32(1.52587890625e-05) - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(1.52587890625e-05) * (rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(1.52587890625e-05)) + T.float32(9.9999999999999995e-07)) * rxplaceholder_1[(v_ax1 * T.int32(16) + v_ax2) % T.int32(512)] + rxplaceholder_2[(v_ax1 * T.int32(16) + v_ax2) % T.int32(512)]

    @T.prim_func
    def group_norm4(rxplaceholder: T.Buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), "float32"), rxplaceholder_1: T.Buffer((T.int32(640),), "float32"), rxplaceholder_2: T.Buffer((T.int32(640),), "float32"), T_reshape: T.Buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int32(2), T.int32(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int32(2), T.int32(32)))
        for ax0_ax1_fused in T.thread_binding(T.int32(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for k2_k3_k4_fused_0 in range(T.int32(80)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int32(2), ax0_ax1_fused // T.int32(32))
                        v_ax1 = T.axis.spatial(T.int32(32), ax0_ax1_fused % T.int32(32))
                        v_k2 = T.axis.reduce(T.int32(20), (k2_k3_k4_fused_0 * T.int32(256) + k2_k3_k4_fused_1) // T.int32(1024))
                        v_k3 = T.axis.reduce(T.int32(32), (k2_k3_k4_fused_0 * T.int32(256) + k2_k3_k4_fused_1) % T.int32(1024) // T.int32(32))
                        v_k4 = T.axis.reduce(T.int32(32), (k2_k3_k4_fused_0 * T.int32(256) + k2_k3_k4_fused_1) % T.int32(32))
                        T.reads(rxplaceholder[((v_ax1 * T.int32(20) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) // T.int32(640) + v_ax0) % T.int32(2), (v_ax1 * T.int32(20) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) % T.int32(640), (v_k4 // T.int32(32) + v_k3) % T.int32(32), v_k4 % T.int32(32)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + rxplaceholder[((v_ax1 * T.int32(20) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) // T.int32(640) + v_ax0) % T.int32(2), (v_ax1 * T.int32(20) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) % T.int32(640), (v_k4 // T.int32(32) + v_k3) % T.int32(32), v_k4 % T.int32(32)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + rxplaceholder[((v_ax1 * T.int32(20) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) // T.int32(640) + v_ax0) % T.int32(2), (v_ax1 * T.int32(20) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) % T.int32(640), (v_k4 // T.int32(32) + v_k3) % T.int32(32), v_k4 % T.int32(32)] * rxplaceholder[((v_ax1 * T.int32(20) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) // T.int32(640) + v_ax0) % T.int32(2), (v_ax1 * T.int32(20) + (v_k4 // T.int32(32) + v_k3) // T.int32(32) + v_k2) % T.int32(640), (v_k4 // T.int32(32) + v_k3) % T.int32(32), v_k4 % T.int32(32)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_ax4_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for ax0_ax1_ax2_ax3_ax4_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_ax4_fused_0 in range(T.int32(20)):
                    with T.block("T_group_norm"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_ax4_fused_2) // T.int32(655360))
                        v_ax1 = T.axis.spatial(T.int32(32), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int32(655360) // T.int32(20480))
                        v_ax2 = T.axis.spatial(T.int32(20), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int32(20480) // T.int32(1024))
                        v_ax3 = T.axis.spatial(T.int32(32), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int32(1024) // T.int32(32))
                        v_ax4 = T.axis.spatial(T.int32(32), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int32(32))
                        T.reads(rxplaceholder[((v_ax1 * T.int32(20) + (v_ax4 // T.int32(32) + v_ax3) // T.int32(32) + v_ax2) // T.int32(640) + v_ax0) % T.int32(2), (v_ax1 * T.int32(20) + (v_ax4 // T.int32(32) + v_ax3) // T.int32(32) + v_ax2) % T.int32(640), (v_ax4 // T.int32(32) + v_ax3) % T.int32(32), v_ax4 % T.int32(32)], rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1], rxplaceholder_1[(v_ax1 * T.int32(20) + v_ax2) % T.int32(640)], rxplaceholder_2[(v_ax1 * T.int32(20) + v_ax2) % T.int32(640)])
                        T.writes(T_reshape[v_ax0, v_ax1 * T.int32(20) + v_ax2, v_ax3, v_ax4])
                        T_reshape[v_ax0, v_ax2 + v_ax1 * T.int32(20), v_ax3, v_ax4] = (rxplaceholder[((v_ax1 * T.int32(20) + (v_ax4 // T.int32(32) + v_ax3) // T.int32(32) + v_ax2) // T.int32(640) + v_ax0) % T.int32(2), (v_ax1 * T.int32(20) + (v_ax4 // T.int32(32) + v_ax3) // T.int32(32) + v_ax2) % T.int32(640), (v_ax4 // T.int32(32) + v_ax3) % T.int32(32), v_ax4 % T.int32(32)] - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(4.8828125000000003e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[v_ax0, v_ax1] * T.float32(4.8828125000000003e-05) - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(4.8828125000000003e-05) * (rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(4.8828125000000003e-05)) + T.float32(9.9999999999999995e-07)) * rxplaceholder_1[(v_ax1 * T.int32(20) + v_ax2) % T.int32(640)] + rxplaceholder_2[(v_ax1 * T.int32(20) + v_ax2) % T.int32(640)]

    @T.prim_func
    def group_norm7(rxplaceholder: T.Buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), "float32"), rxplaceholder_1: T.Buffer((T.int32(1280),), "float32"), rxplaceholder_2: T.Buffer((T.int32(1280),), "float32"), T_reshape: T.Buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int32(2), T.int32(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int32(2), T.int32(32)))
        for ax0_ax1_fused in T.thread_binding(T.int32(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(16), "pragma_unroll_explicit": T.int32(1)}):
            for k2_k3_k4_fused_0 in range(T.int32(160)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int32(2), ax0_ax1_fused // T.int32(32))
                        v_ax1 = T.axis.spatial(T.int32(32), ax0_ax1_fused % T.int32(32))
                        v_k2 = T.axis.reduce(T.int32(40), (k2_k3_k4_fused_0 * T.int32(64) + k2_k3_k4_fused_1) // T.int32(256))
                        v_k3 = T.axis.reduce(T.int32(16), (k2_k3_k4_fused_0 * T.int32(64) + k2_k3_k4_fused_1) % T.int32(256) // T.int32(16))
                        v_k4 = T.axis.reduce(T.int32(16), (k2_k3_k4_fused_0 * T.int32(64) + k2_k3_k4_fused_1) % T.int32(16))
                        T.reads(rxplaceholder[((v_ax1 * T.int32(40) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) // T.int32(1280) + v_ax0) % T.int32(2), (v_ax1 * T.int32(40) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) % T.int32(1280), (v_k4 // T.int32(16) + v_k3) % T.int32(16), v_k4 % T.int32(16)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + rxplaceholder[((v_ax1 * T.int32(40) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) // T.int32(1280) + v_ax0) % T.int32(2), (v_ax1 * T.int32(40) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) % T.int32(1280), (v_k4 // T.int32(16) + v_k3) % T.int32(16), v_k4 % T.int32(16)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + rxplaceholder[((v_ax1 * T.int32(40) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) // T.int32(1280) + v_ax0) % T.int32(2), (v_ax1 * T.int32(40) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) % T.int32(1280), (v_k4 // T.int32(16) + v_k3) % T.int32(16), v_k4 % T.int32(16)] * rxplaceholder[((v_ax1 * T.int32(40) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) // T.int32(1280) + v_ax0) % T.int32(2), (v_ax1 * T.int32(40) + (v_k4 // T.int32(16) + v_k3) // T.int32(16) + v_k2) % T.int32(1280), (v_k4 // T.int32(16) + v_k3) % T.int32(16), v_k4 % T.int32(16)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_ax4_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(16), "pragma_unroll_explicit": T.int32(1)}):
            for ax0_ax1_ax2_ax3_ax4_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_ax4_fused_0 in range(T.int32(10)):
                    with T.block("T_group_norm"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_ax4_fused_2) // T.int32(327680))
                        v_ax1 = T.axis.spatial(T.int32(32), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int32(327680) // T.int32(10240))
                        v_ax2 = T.axis.spatial(T.int32(40), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int32(10240) // T.int32(256))
                        v_ax3 = T.axis.spatial(T.int32(16), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int32(256) // T.int32(16))
                        v_ax4 = T.axis.spatial(T.int32(16), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int32(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int32(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int32(16))
                        T.reads(rxplaceholder[((v_ax1 * T.int32(40) + (v_ax4 // T.int32(16) + v_ax3) // T.int32(16) + v_ax2) // T.int32(1280) + v_ax0) % T.int32(2), (v_ax1 * T.int32(40) + (v_ax4 // T.int32(16) + v_ax3) // T.int32(16) + v_ax2) % T.int32(1280), (v_ax4 // T.int32(16) + v_ax3) % T.int32(16), v_ax4 % T.int32(16)], rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1], rxplaceholder_1[(v_ax1 * T.int32(40) + v_ax2) % T.int32(1280)], rxplaceholder_2[(v_ax1 * T.int32(40) + v_ax2) % T.int32(1280)])
                        T.writes(T_reshape[v_ax0, v_ax1 * T.int32(40) + v_ax2, v_ax3, v_ax4])
                        T_reshape[v_ax0, v_ax2 + v_ax1 * T.int32(40), v_ax3, v_ax4] = (rxplaceholder[((v_ax1 * T.int32(40) + (v_ax4 // T.int32(16) + v_ax3) // T.int32(16) + v_ax2) // T.int32(1280) + v_ax0) % T.int32(2), (v_ax1 * T.int32(40) + (v_ax4 // T.int32(16) + v_ax3) // T.int32(16) + v_ax2) % T.int32(1280), (v_ax4 // T.int32(16) + v_ax3) % T.int32(16), v_ax4 % T.int32(16)] - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(9.7656250000000005e-05)) * T.rsqrt(rxplaceholder_red_temp_v1[v_ax0, v_ax1] * T.float32(9.7656250000000005e-05) - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(9.7656250000000005e-05) * (rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(9.7656250000000005e-05)) + T.float32(9.9999999999999995e-07)) * rxplaceholder_1[(v_ax1 * T.int32(40) + v_ax2) % T.int32(1280)] + rxplaceholder_2[(v_ax1 * T.int32(40) + v_ax2) % T.int32(1280)]

    @T.prim_func
    def group_norm9(rxplaceholder: T.Buffer((T.int32(2), T.int32(1280), T.int32(8), T.int32(8)), "float32"), rxplaceholder_1: T.Buffer((T.int32(1280),), "float32"), rxplaceholder_2: T.Buffer((T.int32(1280),), "float32"), T_reshape: T.Buffer((T.int32(2), T.int32(1280), T.int32(8), T.int32(8)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int32(2), T.int32(32)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int32(2), T.int32(32)))
        for ax0_ax1_fused in T.thread_binding(T.int32(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for k2_k3_k4_fused_0 in range(T.int32(80)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int32(2), ax0_ax1_fused // T.int32(32))
                        v_ax1 = T.axis.spatial(T.int32(32), ax0_ax1_fused % T.int32(32))
                        v_k2 = T.axis.reduce(T.int32(40), (k2_k3_k4_fused_0 * T.int32(32) + k2_k3_k4_fused_1) // T.int32(64))
                        v_k3 = T.axis.reduce(T.int32(8), (k2_k3_k4_fused_0 * T.int32(32) + k2_k3_k4_fused_1) % T.int32(64) // T.int32(8))
                        v_k4 = T.axis.reduce(T.int32(8), (k2_k3_k4_fused_0 * T.int32(32) + k2_k3_k4_fused_1) % T.int32(8))
                        T.reads(rxplaceholder[((v_ax1 * T.int32(40) + (v_k4 // T.int32(8) + v_k3) // T.int32(8) + v_k2) // T.int32(1280) + v_ax0) % T.int32(2), (v_ax1 * T.int32(40) + (v_k4 // T.int32(8) + v_k3) // T.int32(8) + v_k2) % T.int32(1280), (v_k4 // T.int32(8) + v_k3) % T.int32(8), v_k4 % T.int32(8)])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + rxplaceholder[((v_ax1 * T.int32(40) + (v_k4 // T.int32(8) + v_k3) // T.int32(8) + v_k2) // T.int32(1280) + v_ax0) % T.int32(2), (v_ax1 * T.int32(40) + (v_k4 // T.int32(8) + v_k3) // T.int32(8) + v_k2) % T.int32(1280), (v_k4 // T.int32(8) + v_k3) % T.int32(8), v_k4 % T.int32(8)]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + rxplaceholder[((v_ax1 * T.int32(40) + (v_k4 // T.int32(8) + v_k3) // T.int32(8) + v_k2) // T.int32(1280) + v_ax0) % T.int32(2), (v_ax1 * T.int32(40) + (v_k4 // T.int32(8) + v_k3) // T.int32(8) + v_k2) % T.int32(1280), (v_k4 // T.int32(8) + v_k3) % T.int32(8), v_k4 % T.int32(8)] * rxplaceholder[((v_ax1 * T.int32(40) + (v_k4 // T.int32(8) + v_k3) // T.int32(8) + v_k2) // T.int32(1280) + v_ax0) % T.int32(2), (v_ax1 * T.int32(40) + (v_k4 // T.int32(8) + v_k3) // T.int32(8) + v_k2) % T.int32(1280), (v_k4 // T.int32(8) + v_k3) % T.int32(8), v_k4 % T.int32(8)]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_ax3_ax4_fused_0 in T.thread_binding(T.int32(640), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for ax0_ax1_ax2_ax3_ax4_fused_1 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                with T.block("T_group_norm"):
                    v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_ax4_fused_1) // T.int32(81920))
                    v_ax1 = T.axis.spatial(T.int32(32), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_ax4_fused_1) % T.int32(81920) // T.int32(2560))
                    v_ax2 = T.axis.spatial(T.int32(40), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_ax4_fused_1) % T.int32(2560) // T.int32(64))
                    v_ax3 = T.axis.spatial(T.int32(8), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_ax4_fused_1) % T.int32(64) // T.int32(8))
                    v_ax4 = T.axis.spatial(T.int32(8), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_ax4_fused_1) % T.int32(8))
                    T.reads(rxplaceholder[((v_ax1 * T.int32(40) + (v_ax4 // T.int32(8) + v_ax3) // T.int32(8) + v_ax2) // T.int32(1280) + v_ax0) % T.int32(2), (v_ax1 * T.int32(40) + (v_ax4 // T.int32(8) + v_ax3) // T.int32(8) + v_ax2) % T.int32(1280), (v_ax4 // T.int32(8) + v_ax3) % T.int32(8), v_ax4 % T.int32(8)], rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1], rxplaceholder_1[(v_ax1 * T.int32(40) + v_ax2) % T.int32(1280)], rxplaceholder_2[(v_ax1 * T.int32(40) + v_ax2) % T.int32(1280)])
                    T.writes(T_reshape[v_ax0, v_ax1 * T.int32(40) + v_ax2, v_ax3, v_ax4])
                    T_reshape[v_ax0, v_ax2 + v_ax1 * T.int32(40), v_ax3, v_ax4] = (rxplaceholder[((v_ax1 * T.int32(40) + (v_ax4 // T.int32(8) + v_ax3) // T.int32(8) + v_ax2) // T.int32(1280) + v_ax0) % T.int32(2), (v_ax1 * T.int32(40) + (v_ax4 // T.int32(8) + v_ax3) // T.int32(8) + v_ax2) % T.int32(1280), (v_ax4 // T.int32(8) + v_ax3) % T.int32(8), v_ax4 % T.int32(8)] - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00039062500000000002)) * T.rsqrt(rxplaceholder_red_temp_v1[v_ax0, v_ax1] * T.float32(0.00039062500000000002) - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00039062500000000002) * (rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00039062500000000002)) + T.float32(9.9999999999999995e-07)) * rxplaceholder_1[(v_ax1 * T.int32(40) + v_ax2) % T.int32(1280)] + rxplaceholder_2[(v_ax1 * T.int32(40) + v_ax2) % T.int32(1280)]

    @T.prim_func
    def layer_norm(rxplaceholder: T.Buffer((T.int32(1), T.int32(77), T.int32(768)), "float32"), rxplaceholder_1: T.Buffer((T.int32(768),), "float32"), rxplaceholder_2: T.Buffer((T.int32(768),), "float32"), T_layer_norm: T.Buffer((T.int32(1), T.int32(77), T.int32(768)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int32(1), T.int32(77)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int32(1), T.int32(77)))
        for ax0_ax1_fused in T.thread_binding(T.int32(77), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(16), "pragma_unroll_explicit": T.int32(1)}):
            for k2_0 in range(T.int32(96)):
                for k2_1 in T.thread_binding(T.int32(8), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int32(1), T.int32(0))
                        v_ax1 = T.axis.spatial(T.int32(77), ax0_ax1_fused)
                        v_k2 = T.axis.reduce(T.int32(768), k2_0 * T.int32(8) + k2_1)
                        T.reads(rxplaceholder[v_ax0, v_ax1, v_k2])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + rxplaceholder[v_ax0, v_ax1, v_k2]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + rxplaceholder[v_ax0, v_ax1, v_k2] * rxplaceholder[v_ax0, v_ax1, v_k2]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int32(1848), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(16), "pragma_unroll_explicit": T.int32(1)}):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                with T.block("T_layer_norm"):
                    v_ax0 = T.axis.spatial(T.int32(1), T.int32(0))
                    v_ax1 = T.axis.spatial(T.int32(77), (ax0_ax1_ax2_fused_0 * T.int32(32) + ax0_ax1_ax2_fused_1) // T.int32(768))
                    v_ax2 = T.axis.spatial(T.int32(768), (ax0_ax1_ax2_fused_0 * T.int32(32) + ax0_ax1_ax2_fused_1) % T.int32(768))
                    T.reads(rxplaceholder[v_ax0, v_ax1, v_ax2], rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1], rxplaceholder_1[v_ax2], rxplaceholder_2[v_ax2])
                    T.writes(T_layer_norm[v_ax0, v_ax1, v_ax2])
                    T_layer_norm[v_ax0, v_ax1, v_ax2] = (rxplaceholder[v_ax0, v_ax1, v_ax2] - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0013020833333333333)) * T.rsqrt(rxplaceholder_red_temp_v1[v_ax0, v_ax1] * T.float32(0.0013020833333333333) - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0013020833333333333) * (rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0013020833333333333)) + T.float32(1.0000000000000001e-05)) * rxplaceholder_1[v_ax2] + rxplaceholder_2[v_ax2]

    @T.prim_func
    def layer_norm1(rxplaceholder: T.Buffer((T.int32(2), T.int32(4096), T.int32(320)), "float32"), rxplaceholder_1: T.Buffer((T.int32(320),), "float32"), rxplaceholder_2: T.Buffer((T.int32(320),), "float32"), T_layer_norm: T.Buffer((T.int32(2), T.int32(4096), T.int32(320)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int32(2), T.int32(4096)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int32(2), T.int32(4096)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int32(128), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                with T.block("rxplaceholder_red_temp_init"):
                    v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) // T.int32(4096))
                    v_ax1 = T.axis.spatial(T.int32(4096), (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) % T.int32(4096))
                    T.reads()
                    T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                    rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                    rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                for k2 in range(T.int32(320)):
                    with T.block("rxplaceholder_red_temp_update"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) // T.int32(4096))
                        v_ax1 = T.axis.spatial(T.int32(4096), (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) % T.int32(4096))
                        v_k2 = T.axis.reduce(T.int32(320), k2)
                        T.reads(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1], rxplaceholder[v_ax0, v_ax1, v_k2])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + rxplaceholder[v_ax0, v_ax1, v_k2]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + rxplaceholder[v_ax0, v_ax1, v_k2] * rxplaceholder[v_ax0, v_ax1, v_k2]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int32(40)):
                    with T.block("T_layer_norm"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_fused_0 * T.int32(65536) + ax0_ax1_ax2_fused_1 * T.int32(256) + ax0_ax1_ax2_fused_2) // T.int32(1310720))
                        v_ax1 = T.axis.spatial(T.int32(4096), (ax0_ax1_ax2_fused_0 * T.int32(65536) + ax0_ax1_ax2_fused_1 * T.int32(256) + ax0_ax1_ax2_fused_2) % T.int32(1310720) // T.int32(320))
                        v_ax2 = T.axis.spatial(T.int32(320), (ax0_ax1_ax2_fused_0 * T.int32(65536) + ax0_ax1_ax2_fused_1 * T.int32(256) + ax0_ax1_ax2_fused_2) % T.int32(320))
                        T.reads(rxplaceholder[v_ax0, v_ax1, v_ax2], rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1], rxplaceholder_1[v_ax2], rxplaceholder_2[v_ax2])
                        T.writes(T_layer_norm[v_ax0, v_ax1, v_ax2])
                        T_layer_norm[v_ax0, v_ax1, v_ax2] = (rxplaceholder[v_ax0, v_ax1, v_ax2] - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0031250000000000002)) * T.rsqrt(rxplaceholder_red_temp_v1[v_ax0, v_ax1] * T.float32(0.0031250000000000002) - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0031250000000000002) * (rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0031250000000000002)) + T.float32(1.0000000000000001e-05)) * rxplaceholder_1[v_ax2] + rxplaceholder_2[v_ax2]

    @T.prim_func
    def layer_norm2(rxplaceholder: T.Buffer((T.int32(2), T.int32(1024), T.int32(640)), "float32"), rxplaceholder_1: T.Buffer((T.int32(640),), "float32"), rxplaceholder_2: T.Buffer((T.int32(640),), "float32"), T_layer_norm: T.Buffer((T.int32(2), T.int32(1024), T.int32(640)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int32(2), T.int32(1024)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int32(2), T.int32(1024)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int32(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                with T.block("rxplaceholder_red_temp_init"):
                    v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_fused_0 * T.int32(32) + ax0_ax1_fused_1) // T.int32(1024))
                    v_ax1 = T.axis.spatial(T.int32(1024), (ax0_ax1_fused_0 * T.int32(32) + ax0_ax1_fused_1) % T.int32(1024))
                    T.reads()
                    T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                    rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                    rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                for k2 in range(T.int32(640)):
                    with T.block("rxplaceholder_red_temp_update"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_fused_0 * T.int32(32) + ax0_ax1_fused_1) // T.int32(1024))
                        v_ax1 = T.axis.spatial(T.int32(1024), (ax0_ax1_fused_0 * T.int32(32) + ax0_ax1_fused_1) % T.int32(1024))
                        v_k2 = T.axis.reduce(T.int32(640), k2)
                        T.reads(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1], rxplaceholder[v_ax0, v_ax1, v_k2])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + rxplaceholder[v_ax0, v_ax1, v_k2]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + rxplaceholder[v_ax0, v_ax1, v_k2] * rxplaceholder[v_ax0, v_ax1, v_k2]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int32(20)):
                    with T.block("T_layer_norm"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_fused_0 * T.int32(65536) + ax0_ax1_ax2_fused_1 * T.int32(256) + ax0_ax1_ax2_fused_2) // T.int32(655360))
                        v_ax1 = T.axis.spatial(T.int32(1024), (ax0_ax1_ax2_fused_0 * T.int32(65536) + ax0_ax1_ax2_fused_1 * T.int32(256) + ax0_ax1_ax2_fused_2) % T.int32(655360) // T.int32(640))
                        v_ax2 = T.axis.spatial(T.int32(640), (ax0_ax1_ax2_fused_0 * T.int32(65536) + ax0_ax1_ax2_fused_1 * T.int32(256) + ax0_ax1_ax2_fused_2) % T.int32(640))
                        T.reads(rxplaceholder[v_ax0, v_ax1, v_ax2], rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1], rxplaceholder_1[v_ax2], rxplaceholder_2[v_ax2])
                        T.writes(T_layer_norm[v_ax0, v_ax1, v_ax2])
                        T_layer_norm[v_ax0, v_ax1, v_ax2] = (rxplaceholder[v_ax0, v_ax1, v_ax2] - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0015625000000000001)) * T.rsqrt(rxplaceholder_red_temp_v1[v_ax0, v_ax1] * T.float32(0.0015625000000000001) - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0015625000000000001) * (rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0015625000000000001)) + T.float32(1.0000000000000001e-05)) * rxplaceholder_1[v_ax2] + rxplaceholder_2[v_ax2]

    @T.prim_func
    def layer_norm3(rxplaceholder: T.Buffer((T.int32(2), T.int32(256), T.int32(1280)), "float32"), rxplaceholder_1: T.Buffer((T.int32(1280),), "float32"), rxplaceholder_2: T.Buffer((T.int32(1280),), "float32"), T_layer_norm: T.Buffer((T.int32(2), T.int32(256), T.int32(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int32(2), T.int32(256)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int32(2), T.int32(256)))
        for ax0_ax1_fused in T.thread_binding(T.int32(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for k2_0 in range(T.int32(20)):
                for k2_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int32(2), ax0_ax1_fused // T.int32(256))
                        v_ax1 = T.axis.spatial(T.int32(256), ax0_ax1_fused % T.int32(256))
                        v_k2 = T.axis.reduce(T.int32(1280), k2_0 * T.int32(64) + k2_1)
                        T.reads(rxplaceholder[v_ax0, v_ax1, v_k2])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + rxplaceholder[v_ax0, v_ax1, v_k2]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + rxplaceholder[v_ax0, v_ax1, v_k2] * rxplaceholder[v_ax0, v_ax1, v_k2]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int32(10)):
                    with T.block("T_layer_norm"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_fused_0 * T.int32(65536) + ax0_ax1_ax2_fused_1 * T.int32(256) + ax0_ax1_ax2_fused_2) // T.int32(327680))
                        v_ax1 = T.axis.spatial(T.int32(256), (ax0_ax1_ax2_fused_0 * T.int32(65536) + ax0_ax1_ax2_fused_1 * T.int32(256) + ax0_ax1_ax2_fused_2) % T.int32(327680) // T.int32(1280))
                        v_ax2 = T.axis.spatial(T.int32(1280), (ax0_ax1_ax2_fused_0 * T.int32(65536) + ax0_ax1_ax2_fused_1 * T.int32(256) + ax0_ax1_ax2_fused_2) % T.int32(1280))
                        T.reads(rxplaceholder[v_ax0, v_ax1, v_ax2], rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1], rxplaceholder_1[v_ax2], rxplaceholder_2[v_ax2])
                        T.writes(T_layer_norm[v_ax0, v_ax1, v_ax2])
                        T_layer_norm[v_ax0, v_ax1, v_ax2] = (rxplaceholder[v_ax0, v_ax1, v_ax2] - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00078125000000000004)) * T.rsqrt(rxplaceholder_red_temp_v1[v_ax0, v_ax1] * T.float32(0.00078125000000000004) - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00078125000000000004) * (rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00078125000000000004)) + T.float32(1.0000000000000001e-05)) * rxplaceholder_1[v_ax2] + rxplaceholder_2[v_ax2]

    @T.prim_func
    def layer_norm4(rxplaceholder: T.Buffer((T.int32(2), T.int32(64), T.int32(1280)), "float32"), rxplaceholder_1: T.Buffer((T.int32(1280),), "float32"), rxplaceholder_2: T.Buffer((T.int32(1280),), "float32"), T_layer_norm: T.Buffer((T.int32(2), T.int32(64), T.int32(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        rxplaceholder_red_temp_v0 = T.alloc_buffer((T.int32(2), T.int32(64)))
        rxplaceholder_red_temp_v1 = T.alloc_buffer((T.int32(2), T.int32(64)))
        for ax0_ax1_fused in T.thread_binding(T.int32(128), thread="blockIdx.x"):
            for k2_0 in range(T.int32(80)):
                for k2_1 in T.thread_binding(T.int32(16), thread="threadIdx.x"):
                    with T.block("rxplaceholder_red_temp"):
                        v_ax0 = T.axis.spatial(T.int32(2), ax0_ax1_fused // T.int32(64))
                        v_ax1 = T.axis.spatial(T.int32(64), ax0_ax1_fused % T.int32(64))
                        v_k2 = T.axis.reduce(T.int32(1280), k2_0 * T.int32(16) + k2_1)
                        T.reads(rxplaceholder[v_ax0, v_ax1, v_k2])
                        T.writes(rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            rxplaceholder_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            rxplaceholder_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_rxplaceholder_red_temp_v0: T.float32 = rxplaceholder_red_temp_v0[v_ax0, v_ax1] + rxplaceholder[v_ax0, v_ax1, v_k2]
                        v_rxplaceholder_red_temp_v1: T.float32 = rxplaceholder_red_temp_v1[v_ax0, v_ax1] + rxplaceholder[v_ax0, v_ax1, v_k2] * rxplaceholder[v_ax0, v_ax1, v_k2]
                        rxplaceholder_red_temp_v0[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v0
                        rxplaceholder_red_temp_v1[v_ax0, v_ax1] = v_rxplaceholder_red_temp_v1
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int32(3)):
                    with T.block("T_layer_norm"):
                        v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_fused_0 * T.int32(65536) + ax0_ax1_ax2_fused_1 * T.int32(256) + ax0_ax1_ax2_fused_2) // T.int32(81920))
                        v_ax1 = T.axis.spatial(T.int32(64), (ax0_ax1_ax2_fused_0 * T.int32(65536) + ax0_ax1_ax2_fused_1 * T.int32(256) + ax0_ax1_ax2_fused_2) % T.int32(81920) // T.int32(1280))
                        v_ax2 = T.axis.spatial(T.int32(1280), (ax0_ax1_ax2_fused_0 * T.int32(65536) + ax0_ax1_ax2_fused_1 * T.int32(256) + ax0_ax1_ax2_fused_2) % T.int32(1280))
                        T.where((ax0_ax1_ax2_fused_0 * T.int32(256) + ax0_ax1_ax2_fused_1) * T.int32(256) + ax0_ax1_ax2_fused_2 < T.int32(163840))
                        T.reads(rxplaceholder[v_ax0, v_ax1, v_ax2], rxplaceholder_red_temp_v0[v_ax0, v_ax1], rxplaceholder_red_temp_v1[v_ax0, v_ax1], rxplaceholder_1[v_ax2], rxplaceholder_2[v_ax2])
                        T.writes(T_layer_norm[v_ax0, v_ax1, v_ax2])
                        T_layer_norm[v_ax0, v_ax1, v_ax2] = (rxplaceholder[v_ax0, v_ax1, v_ax2] - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00078125000000000004)) * T.rsqrt(rxplaceholder_red_temp_v1[v_ax0, v_ax1] * T.float32(0.00078125000000000004) - rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00078125000000000004) * (rxplaceholder_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00078125000000000004)) + T.float32(1.0000000000000001e-05)) * rxplaceholder_1[v_ax2] + rxplaceholder_2[v_ax2]

    @T.prim_func
    def matmul1(rxplaceholder: T.Buffer((T.int32(12), T.int32(77), T.int32(64)), "float32"), rxplaceholder_1: T.Buffer((T.int32(12), T.int32(64), T.int32(77)), "float32"), matmul: T.Buffer((T.int32(12), T.int32(77), T.int32(77)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(12), T.int32(77), T.int32(77)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int32(12), T.int32(77), T.int32(64)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int32(12), T.int32(64), T.int32(77)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(33), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(154), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(7), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(12), i0_3_init + i0_4_init + i0_0_i1_0_i2_0_fused // T.int32(11) * T.int32(4) + i0_1_i1_1_i2_1_fused * T.int32(2) + i0_2_i1_2_i2_2_fused // T.int32(77))
                            v_i1 = T.axis.spatial(T.int32(77), i0_0_i1_0_i2_0_fused % T.int32(11) * T.int32(7) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(77), i2_4_init + i0_2_i1_2_i2_2_fused % T.int32(77) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(8)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(154), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(12), i0_0_i1_0_i2_0_fused // T.int32(11) * T.int32(4) + (ax0_ax1_ax2_fused_0 * T.int32(154) + ax0_ax1_ax2_fused_1) // T.int32(56))
                                    v1 = T.axis.spatial(T.int32(77), i0_0_i1_0_i2_0_fused % T.int32(11) * T.int32(7) + (ax0_ax1_ax2_fused_0 * T.int32(154) + ax0_ax1_ax2_fused_1) % T.int32(56) // T.int32(8))
                                    v2 = T.axis.spatial(T.int32(64), k_0 * T.int32(8) + (ax0_ax1_ax2_fused_0 * T.int32(154) + ax0_ax1_ax2_fused_1) % T.int32(8))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int32(154) + ax0_ax1_ax2_fused_1 < T.int32(224))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int32(16)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(154), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(12), i0_0_i1_0_i2_0_fused // T.int32(11) * T.int32(4) + (ax0_ax1_ax2_fused_0 * T.int32(154) + ax0_ax1_ax2_fused_1) // T.int32(616))
                                    v1 = T.axis.spatial(T.int32(64), k_0 * T.int32(8) + (ax0_ax1_ax2_fused_0 * T.int32(154) + ax0_ax1_ax2_fused_1) % T.int32(616) // T.int32(77))
                                    v2 = T.axis.spatial(T.int32(77), (ax0_ax1_ax2_fused_0 * T.int32(154) + ax0_ax1_ax2_fused_1) % T.int32(77))
                                    T.reads(rxplaceholder_1[v0, v1, v2])
                                    T.writes(rxplaceholder_shared_1[v0, v1, v2])
                                    rxplaceholder_shared_1[v0, v1, v2] = rxplaceholder_1[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(4), T.int32(1), T.int32(7), T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(12), i0_3 + i0_4 + i0_0_i1_0_i2_0_fused // T.int32(11) * T.int32(4) + i0_1_i1_1_i2_1_fused * T.int32(2) + i0_2_i1_2_i2_2_fused // T.int32(77))
                                v_i1 = T.axis.spatial(T.int32(77), i0_0_i1_0_i2_0_fused % T.int32(11) * T.int32(7) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int32(77), i2_4 + i0_2_i1_2_i2_2_fused % T.int32(77) + i2_3)
                                v_k = T.axis.reduce(T.int32(64), k_0 * T.int32(8) + k_1 * T.int32(2) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[v_i0, v_i1, v_k], rxplaceholder_shared_1[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[v_i0, v_i1, v_k] * rxplaceholder_shared_1[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(7), T.int32(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(12), i0_0_i1_0_i2_0_fused // T.int32(11) * T.int32(4) + i0_1_i1_1_i2_1_fused * T.int32(2) + i0_2_i1_2_i2_2_fused // T.int32(77) + ax0)
                            v1 = T.axis.spatial(T.int32(77), i0_0_i1_0_i2_0_fused % T.int32(11) * T.int32(7) + ax1)
                            v2 = T.axis.spatial(T.int32(77), i0_2_i1_2_i2_2_fused % T.int32(77) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul10(rxplaceholder: T.Buffer((T.int32(16), T.int32(4096), T.int32(4096)), "float32"), rxplaceholder_1: T.Buffer((T.int32(16), T.int32(4096), T.int32(40)), "float32"), matmul: T.Buffer((T.int32(16), T.int32(4096), T.int32(40)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(16), T.int32(4096), T.int32(40)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int32(16), T.int32(4096), T.int32(4096)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int32(16), T.int32(4096), T.int32(40)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(1), T.int32(5), T.int32(1), T.int32(2), T.int32(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(16), i0_4_init + i0_0_i1_0_i2_0_fused // T.int32(64) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused % T.int32(64) * T.int32(64) + i0_1_i1_1_i2_1_fused * T.int32(32) + i0_2_i1_2_i2_2_fused // T.int32(8) * T.int32(2) + i1_3_init * T.int32(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(40), i2_4_init + i0_2_i1_2_i2_2_fused % T.int32(8) * T.int32(5) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(128)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(16)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(64))
                                    v1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused % T.int32(64) * T.int32(64) + (ax0_ax1_ax2_fused_0 * T.int32(128) + ax0_ax1_ax2_fused_1) // T.int32(32))
                                    v2 = T.axis.spatial(T.int32(4096), k_0 * T.int32(32) + (ax0_ax1_ax2_fused_0 * T.int32(128) + ax0_ax1_ax2_fused_1) % T.int32(32))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int32(10)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(64))
                                    v1 = T.axis.spatial(T.int32(4096), k_0 * T.int32(32) + (ax0_ax1_ax2_fused_0 * T.int32(128) + ax0_ax1_ax2_fused_1) // T.int32(40))
                                    v2 = T.axis.spatial(T.int32(40), (ax0_ax1_ax2_fused_0 * T.int32(128) + ax0_ax1_ax2_fused_1) % T.int32(40))
                                    T.reads(rxplaceholder_1[v0, v1, v2])
                                    T.writes(rxplaceholder_shared_1[v0, v1, v2])
                                    rxplaceholder_shared_1[v0, v1, v2] = rxplaceholder_1[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(8), T.int32(1), T.int32(1), T.int32(5), T.int32(4), T.int32(1), T.int32(2), T.int32(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(16), i0_4 + i0_0_i1_0_i2_0_fused // T.int32(64) + i0_3)
                                v_i1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused % T.int32(64) * T.int32(64) + i0_1_i1_1_i2_1_fused * T.int32(32) + i0_2_i1_2_i2_2_fused // T.int32(8) * T.int32(2) + i1_3 * T.int32(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int32(40), i2_4 + i0_2_i1_2_i2_2_fused % T.int32(8) * T.int32(5) + i2_3)
                                v_k = T.axis.reduce(T.int32(4096), k_0 * T.int32(32) + k_1 * T.int32(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[v_i0, v_i1, v_k], rxplaceholder_shared_1[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[v_i0, v_i1, v_k] * rxplaceholder_shared_1[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(2), T.int32(5)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(64) + ax0)
                            v1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused % T.int32(64) * T.int32(64) + i0_1_i1_1_i2_1_fused * T.int32(32) + i0_2_i1_2_i2_2_fused // T.int32(8) * T.int32(2) + ax1)
                            v2 = T.axis.spatial(T.int32(40), i0_2_i1_2_i2_2_fused % T.int32(8) * T.int32(5) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul11(rxplaceholder: T.Buffer((T.int32(2), T.int32(77), T.int32(768)), "float32"), rxplaceholder_1: T.Buffer((T.int32(768), T.int32(320)), "float32"), matmul: T.Buffer((T.int32(2), T.int32(77), T.int32(320)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(2), T.int32(77), T.int32(320)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int32(2), T.int32(77), T.int32(768)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int32(768), T.int32(320)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(35), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(176), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(2), i0_4_init + i0_1_i1_1_i2_1_fused + i0_3_init)
                            v_i1 = T.axis.spatial(T.int32(77), i0_0_i1_0_i2_0_fused // T.int32(5) * T.int32(11) + i0_2_i1_2_i2_2_fused // T.int32(16) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(320), i2_4_init + i0_0_i1_0_i2_0_fused % T.int32(5) * T.int32(64) + i0_2_i1_2_i2_2_fused % T.int32(16) * T.int32(4) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(32)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(3)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(176), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_fused_0 * T.int32(176) + ax0_ax1_ax2_fused_1) // T.int32(264))
                                    v1 = T.axis.spatial(T.int32(77), i0_0_i1_0_i2_0_fused // T.int32(5) * T.int32(11) + (ax0_ax1_ax2_fused_0 * T.int32(176) + ax0_ax1_ax2_fused_1) % T.int32(264) // T.int32(24))
                                    v2 = T.axis.spatial(T.int32(768), k_0 * T.int32(24) + (ax0_ax1_ax2_fused_0 * T.int32(176) + ax0_ax1_ax2_fused_1) % T.int32(24))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int32(9)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(176), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(768), k_0 * T.int32(24) + (ax0_ax1_fused_0 * T.int32(176) + ax0_ax1_fused_1) // T.int32(64))
                                    v1 = T.axis.spatial(T.int32(320), i0_0_i1_0_i2_0_fused % T.int32(5) * T.int32(64) + (ax0_ax1_fused_0 * T.int32(176) + ax0_ax1_fused_1) % T.int32(64))
                                    T.where(ax0_ax1_fused_0 * T.int32(176) + ax0_ax1_fused_1 < T.int32(1536))
                                    T.reads(rxplaceholder_1[v0, v1])
                                    T.writes(rxplaceholder_shared_1[v0, v1])
                                    rxplaceholder_shared_1[v0, v1] = rxplaceholder_1[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(24), T.int32(1), T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(2), i0_4 + i0_1_i1_1_i2_1_fused + i0_3)
                                v_i1 = T.axis.spatial(T.int32(77), i0_0_i1_0_i2_0_fused // T.int32(5) * T.int32(11) + i0_2_i1_2_i2_2_fused // T.int32(16) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int32(320), i2_4 + i0_0_i1_0_i2_0_fused % T.int32(5) * T.int32(64) + i0_2_i1_2_i2_2_fused % T.int32(16) * T.int32(4) + i2_3)
                                v_k = T.axis.reduce(T.int32(768), k_0 * T.int32(24) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[v_i0, v_i1, v_k], rxplaceholder_shared_1[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[v_i0, v_i1, v_k] * rxplaceholder_shared_1[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(1), T.int32(4)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(2), i0_1_i1_1_i2_1_fused + ax0)
                            v1 = T.axis.spatial(T.int32(77), i0_0_i1_0_i2_0_fused // T.int32(5) * T.int32(11) + i0_2_i1_2_i2_2_fused // T.int32(16) + ax1)
                            v2 = T.axis.spatial(T.int32(320), i0_0_i1_0_i2_0_fused % T.int32(5) * T.int32(64) + i0_2_i1_2_i2_2_fused % T.int32(16) * T.int32(4) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul13(rxplaceholder: T.Buffer((T.int32(16), T.int32(4096), T.int32(77)), "float32"), rxplaceholder_1: T.Buffer((T.int32(16), T.int32(77), T.int32(40)), "float32"), matmul: T.Buffer((T.int32(16), T.int32(4096), T.int32(40)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(16), T.int32(4096), T.int32(40)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int32(16), T.int32(4096), T.int32(77)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int32(16), T.int32(77), T.int32(40)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(5), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(1), T.int32(4), T.int32(1), T.int32(2), T.int32(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(16), i0_4_init + i0_0_i1_0_i2_0_fused // T.int32(64) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused % T.int32(64) * T.int32(64) + i0_2_i1_2_i2_2_fused // T.int32(2) * T.int32(2) + i1_3_init * T.int32(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(40), i2_4_init + i0_1_i1_1_i2_1_fused * T.int32(8) + i0_2_i1_2_i2_2_fused % T.int32(2) * T.int32(4) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(11)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(7)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(64))
                                    v1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused % T.int32(64) * T.int32(64) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) // T.int32(7))
                                    v2 = T.axis.spatial(T.int32(77), k_0 * T.int32(7) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(7))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int32(5)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(64))
                                    v1 = T.axis.spatial(T.int32(77), k_0 * T.int32(7) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) // T.int32(40))
                                    v2 = T.axis.spatial(T.int32(40), (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(40))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1 < T.int32(280))
                                    T.reads(rxplaceholder_1[v0, v1, v2])
                                    T.writes(rxplaceholder_shared_1[v0, v1, v2])
                                    rxplaceholder_shared_1[v0, v1, v2] = rxplaceholder_1[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(7), T.int32(1), T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(2), T.int32(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(16), i0_4 + i0_0_i1_0_i2_0_fused // T.int32(64) + i0_3)
                                v_i1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused % T.int32(64) * T.int32(64) + i0_2_i1_2_i2_2_fused // T.int32(2) * T.int32(2) + i1_3 * T.int32(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int32(40), i2_4 + i0_1_i1_1_i2_1_fused * T.int32(8) + i0_2_i1_2_i2_2_fused % T.int32(2) * T.int32(4) + i2_3)
                                v_k = T.axis.reduce(T.int32(77), k_0 * T.int32(7) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[v_i0, v_i1, v_k], rxplaceholder_shared_1[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[v_i0, v_i1, v_k] * rxplaceholder_shared_1[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(2), T.int32(4)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(64) + ax0)
                            v1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused % T.int32(64) * T.int32(64) + i0_2_i1_2_i2_2_fused // T.int32(2) * T.int32(2) + ax1)
                            v2 = T.axis.spatial(T.int32(40), i0_1_i1_1_i2_1_fused * T.int32(8) + i0_2_i1_2_i2_2_fused % T.int32(2) * T.int32(4) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul17(rxplaceholder: T.Buffer((T.int32(2), T.int32(1024), T.int32(640)), "float32"), rxplaceholder_1: T.Buffer((T.int32(640), T.int32(640)), "float32"), matmul: T.Buffer((T.int32(2), T.int32(1024), T.int32(640)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(2), T.int32(1024), T.int32(640)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int32(2), T.int32(1024), T.int32(640)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int32(640), T.int32(640)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(640), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(4), T.int32(1), T.int32(2), T.int32(1), T.int32(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(2), i0_3_init * T.int32(2) + i0_4_init)
                            v_i1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused // T.int32(10) * T.int32(16) + i0_1_i1_1_i2_1_fused * T.int32(4) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(640), i0_0_i1_0_i2_0_fused % T.int32(10) * T.int32(64) + i0_2_i1_2_i2_2_fused * T.int32(2) + i2_3_init * T.int32(2) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(160)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_fused_0 * T.int32(32) + ax0_ax1_ax2_fused_1) // T.int32(64))
                                    v1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused // T.int32(10) * T.int32(16) + (ax0_ax1_ax2_fused_0 * T.int32(32) + ax0_ax1_ax2_fused_1) % T.int32(64) // T.int32(4))
                                    v2 = T.axis.spatial(T.int32(640), k_0 * T.int32(4) + (ax0_ax1_ax2_fused_0 * T.int32(32) + ax0_ax1_ax2_fused_1) % T.int32(4))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int32(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(640), k_0 * T.int32(4) + (ax0_ax1_fused_0 * T.int32(32) + ax0_ax1_fused_1) // T.int32(64))
                                    v1 = T.axis.spatial(T.int32(640), i0_0_i1_0_i2_0_fused % T.int32(10) * T.int32(64) + (ax0_ax1_fused_0 * T.int32(32) + ax0_ax1_fused_1) % T.int32(64))
                                    T.reads(rxplaceholder_1[v0, v1])
                                    T.writes(rxplaceholder_shared_1[v0, v1])
                                    rxplaceholder_shared_1[v0, v1] = rxplaceholder_1[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(1), T.int32(1), T.int32(4), T.int32(1), T.int32(4), T.int32(2), T.int32(1), T.int32(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(2), i0_3 * T.int32(2) + i0_4)
                                v_i1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused // T.int32(10) * T.int32(16) + i0_1_i1_1_i2_1_fused * T.int32(4) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int32(640), i0_0_i1_0_i2_0_fused % T.int32(10) * T.int32(64) + i0_2_i1_2_i2_2_fused * T.int32(2) + i2_3 * T.int32(2) + i2_4)
                                v_k = T.axis.reduce(T.int32(640), k_0 * T.int32(4) + k_1 * T.int32(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[v_i0, v_i1, v_k], rxplaceholder_shared_1[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[v_i0, v_i1, v_k] * rxplaceholder_shared_1[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(2), T.int32(4), T.int32(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(2), ax0)
                            v1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused // T.int32(10) * T.int32(16) + i0_1_i1_1_i2_1_fused * T.int32(4) + ax1)
                            v2 = T.axis.spatial(T.int32(640), i0_0_i1_0_i2_0_fused % T.int32(10) * T.int32(64) + i0_2_i1_2_i2_2_fused * T.int32(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul19(rxplaceholder: T.Buffer((T.int32(16), T.int32(1024), T.int32(1024)), "float32"), rxplaceholder_1: T.Buffer((T.int32(16), T.int32(1024), T.int32(80)), "float32"), matmul: T.Buffer((T.int32(16), T.int32(1024), T.int32(80)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(16), T.int32(1024), T.int32(80)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int32(16), T.int32(1024), T.int32(1024)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int32(16), T.int32(1024), T.int32(80)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(2560), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(8), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(16), i0_3_init + i0_4_init + i0_0_i1_0_i2_0_fused // T.int32(320) * T.int32(2) + i0_2_i1_2_i2_2_fused // T.int32(16))
                            v_i1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused % T.int32(320) // T.int32(5) * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(16) // T.int32(8) * T.int32(8) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(80), i2_4_init + i0_0_i1_0_i2_0_fused % T.int32(5) * T.int32(16) + i0_1_i1_1_i2_1_fused * T.int32(8) + i0_2_i1_2_i2_2_fused % T.int32(8) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(128)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(8)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(320) * T.int32(2) + (ax0_ax1_ax2_fused_0 * T.int32(32) + ax0_ax1_ax2_fused_1) // T.int32(128))
                                    v1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused % T.int32(320) // T.int32(5) * T.int32(16) + (ax0_ax1_ax2_fused_0 * T.int32(32) + ax0_ax1_ax2_fused_1) % T.int32(128) // T.int32(8))
                                    v2 = T.axis.spatial(T.int32(1024), k_0 * T.int32(8) + (ax0_ax1_ax2_fused_0 * T.int32(32) + ax0_ax1_ax2_fused_1) % T.int32(8))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int32(8)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(320) * T.int32(2) + (ax0_ax1_ax2_fused_0 * T.int32(32) + ax0_ax1_ax2_fused_1) // T.int32(128))
                                    v1 = T.axis.spatial(T.int32(1024), k_0 * T.int32(8) + (ax0_ax1_ax2_fused_0 * T.int32(32) + ax0_ax1_ax2_fused_1) % T.int32(128) // T.int32(16))
                                    v2 = T.axis.spatial(T.int32(80), i0_0_i1_0_i2_0_fused % T.int32(5) * T.int32(16) + (ax0_ax1_ax2_fused_0 * T.int32(32) + ax0_ax1_ax2_fused_1) % T.int32(16))
                                    T.reads(rxplaceholder_1[v0, v1, v2])
                                    T.writes(rxplaceholder_shared_1[v0, v1, v2])
                                    rxplaceholder_shared_1[v0, v1, v2] = rxplaceholder_1[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(1), T.int32(1), T.int32(8), T.int32(1), T.int32(8), T.int32(1), T.int32(1), T.int32(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(16), i0_3 + i0_4 + i0_0_i1_0_i2_0_fused // T.int32(320) * T.int32(2) + i0_2_i1_2_i2_2_fused // T.int32(16))
                                v_i1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused % T.int32(320) // T.int32(5) * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(16) // T.int32(8) * T.int32(8) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int32(80), i2_4 + i0_0_i1_0_i2_0_fused % T.int32(5) * T.int32(16) + i0_1_i1_1_i2_1_fused * T.int32(8) + i0_2_i1_2_i2_2_fused % T.int32(8) + i2_3)
                                v_k = T.axis.reduce(T.int32(1024), k_0 * T.int32(8) + k_1 * T.int32(8) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[v_i0, v_i1, v_k], rxplaceholder_shared_1[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[v_i0, v_i1, v_k] * rxplaceholder_shared_1[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(8), T.int32(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(320) * T.int32(2) + i0_2_i1_2_i2_2_fused // T.int32(16) + ax0)
                            v1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused % T.int32(320) // T.int32(5) * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(16) // T.int32(8) * T.int32(8) + ax1)
                            v2 = T.axis.spatial(T.int32(80), i0_0_i1_0_i2_0_fused % T.int32(5) * T.int32(16) + i0_1_i1_1_i2_1_fused * T.int32(8) + i0_2_i1_2_i2_2_fused % T.int32(8) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul2(rxplaceholder: T.Buffer((T.int32(12), T.int32(77), T.int32(77)), "float32"), rxplaceholder_1: T.Buffer((T.int32(12), T.int32(77), T.int32(64)), "float32"), matmul: T.Buffer((T.int32(12), T.int32(77), T.int32(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(12), T.int32(77), T.int32(64)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int32(12), T.int32(77), T.int32(77)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int32(12), T.int32(77), T.int32(64)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(132), thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(224), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(12), i0_4_init + i0_0_i1_0_i2_0_fused // T.int32(11) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int32(77), i0_0_i1_0_i2_0_fused % T.int32(11) * T.int32(7) + i0_2_i1_2_i2_2_fused // T.int32(32) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(64), i2_4_init + i0_1_i1_1_i2_1_fused * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(32) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(11)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(1)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(224), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(12), i0_0_i1_0_i2_0_fused // T.int32(11))
                                    v1 = T.axis.spatial(T.int32(77), i0_0_i1_0_i2_0_fused % T.int32(11) * T.int32(7) + (ax0_ax1_ax2_fused_0 * T.int32(224) + ax0_ax1_ax2_fused_1) // T.int32(7))
                                    v2 = T.axis.spatial(T.int32(77), k_0 * T.int32(7) + (ax0_ax1_ax2_fused_0 * T.int32(224) + ax0_ax1_ax2_fused_1) % T.int32(7))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int32(224) + ax0_ax1_ax2_fused_1 < T.int32(49))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int32(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(224), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(12), i0_0_i1_0_i2_0_fused // T.int32(11))
                                    v1 = T.axis.spatial(T.int32(77), k_0 * T.int32(7) + (ax0_ax1_ax2_fused_0 * T.int32(224) + ax0_ax1_ax2_fused_1) // T.int32(64))
                                    v2 = T.axis.spatial(T.int32(64), (ax0_ax1_ax2_fused_0 * T.int32(224) + ax0_ax1_ax2_fused_1) % T.int32(64))
                                    T.reads(rxplaceholder_1[v0, v1, v2])
                                    T.writes(rxplaceholder_shared_1[v0, v1, v2])
                                    rxplaceholder_shared_1[v0, v1, v2] = rxplaceholder_1[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(7), T.int32(1), T.int32(1), T.int32(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(12), i0_4 + i0_0_i1_0_i2_0_fused // T.int32(11) + i0_3)
                                v_i1 = T.axis.spatial(T.int32(77), i0_0_i1_0_i2_0_fused % T.int32(11) * T.int32(7) + i0_2_i1_2_i2_2_fused // T.int32(32) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int32(64), i2_4 + i0_1_i1_1_i2_1_fused * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(32) + i2_3)
                                v_k = T.axis.reduce(T.int32(77), k_0 * T.int32(7) + k_1 * T.int32(7) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[v_i0, v_i1, v_k], rxplaceholder_shared_1[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[v_i0, v_i1, v_k] * rxplaceholder_shared_1[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(12), i0_0_i1_0_i2_0_fused // T.int32(11) + ax0)
                            v1 = T.axis.spatial(T.int32(77), i0_0_i1_0_i2_0_fused % T.int32(11) * T.int32(7) + i0_2_i1_2_i2_2_fused // T.int32(32) + ax1)
                            v2 = T.axis.spatial(T.int32(64), i0_1_i1_1_i2_1_fused * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(32) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul20(rxplaceholder: T.Buffer((T.int32(2), T.int32(77), T.int32(768)), "float32"), rxplaceholder_1: T.Buffer((T.int32(768), T.int32(640)), "float32"), matmul: T.Buffer((T.int32(2), T.int32(77), T.int32(640)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(2), T.int32(77), T.int32(640)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int32(2), T.int32(77), T.int32(768)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int32(768), T.int32(640)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(110), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(14), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(2), i0_3_init + i0_4_init + i0_2_i1_2_i2_2_fused // T.int32(32))
                            v_i1 = T.axis.spatial(T.int32(77), i1_4_init + i0_0_i1_0_i2_0_fused // T.int32(10) * T.int32(7) + i0_1_i1_1_i2_1_fused // T.int32(2) + i1_3_init)
                            v_i2 = T.axis.spatial(T.int32(640), i2_4_init + i0_0_i1_0_i2_0_fused % T.int32(10) * T.int32(64) + i0_1_i1_1_i2_1_fused % T.int32(2) * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(32) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(96)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) // T.int32(56))
                                    v1 = T.axis.spatial(T.int32(77), i0_0_i1_0_i2_0_fused // T.int32(10) * T.int32(7) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(56) // T.int32(8))
                                    v2 = T.axis.spatial(T.int32(768), k_0 * T.int32(8) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(8))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1 < T.int32(112))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int32(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(768), k_0 * T.int32(8) + (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) // T.int32(64))
                                    v1 = T.axis.spatial(T.int32(640), i0_0_i1_0_i2_0_fused % T.int32(10) * T.int32(64) + (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) % T.int32(64))
                                    T.reads(rxplaceholder_1[v0, v1])
                                    T.writes(rxplaceholder_shared_1[v0, v1])
                                    rxplaceholder_shared_1[v0, v1] = rxplaceholder_1[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(8), T.int32(1), T.int32(1), T.int32(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(2), i0_3 + i0_4 + i0_2_i1_2_i2_2_fused // T.int32(32))
                                v_i1 = T.axis.spatial(T.int32(77), i1_4 + i0_0_i1_0_i2_0_fused // T.int32(10) * T.int32(7) + i0_1_i1_1_i2_1_fused // T.int32(2) + i1_3)
                                v_i2 = T.axis.spatial(T.int32(640), i2_4 + i0_0_i1_0_i2_0_fused % T.int32(10) * T.int32(64) + i0_1_i1_1_i2_1_fused % T.int32(2) * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(32) + i2_3)
                                v_k = T.axis.reduce(T.int32(768), k_0 * T.int32(8) + k_1 * T.int32(8) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[v_i0, v_i1, v_k], rxplaceholder_shared_1[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[v_i0, v_i1, v_k] * rxplaceholder_shared_1[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(2), i0_2_i1_2_i2_2_fused // T.int32(32) + ax0)
                            v1 = T.axis.spatial(T.int32(77), i0_0_i1_0_i2_0_fused // T.int32(10) * T.int32(7) + i0_1_i1_1_i2_1_fused // T.int32(2) + ax1)
                            v2 = T.axis.spatial(T.int32(640), i0_0_i1_0_i2_0_fused % T.int32(10) * T.int32(64) + i0_1_i1_1_i2_1_fused % T.int32(2) * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(32) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul22(rxplaceholder: T.Buffer((T.int32(16), T.int32(1024), T.int32(77)), "float32"), rxplaceholder_1: T.Buffer((T.int32(16), T.int32(77), T.int32(80)), "float32"), matmul: T.Buffer((T.int32(16), T.int32(1024), T.int32(80)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(16), T.int32(1024), T.int32(80)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int32(16), T.int32(1024), T.int32(77)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int32(16), T.int32(77), T.int32(80)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(1280), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(8), T.int32(1), T.int32(1), T.int32(1), T.int32(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(16), i0_3_init + i0_4_init + i0_0_i1_0_i2_0_fused // T.int32(320) * T.int32(4) + i0_2_i1_2_i2_2_fused // T.int32(16))
                            v_i1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused % T.int32(320) // T.int32(5) * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(16) // T.int32(8) * T.int32(8) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(80), i0_0_i1_0_i2_0_fused % T.int32(5) * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(8) * T.int32(2) + i2_3_init * T.int32(2) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(11)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(7)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(320) * T.int32(4) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) // T.int32(112))
                                    v1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused % T.int32(320) // T.int32(5) * T.int32(16) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(112) // T.int32(7))
                                    v2 = T.axis.spatial(T.int32(77), k_0 * T.int32(7) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(7))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int32(7)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(320) * T.int32(4) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) // T.int32(112))
                                    v1 = T.axis.spatial(T.int32(77), k_0 * T.int32(7) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(112) // T.int32(16))
                                    v2 = T.axis.spatial(T.int32(80), i0_0_i1_0_i2_0_fused % T.int32(5) * T.int32(16) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(16))
                                    T.reads(rxplaceholder_1[v0, v1, v2])
                                    T.writes(rxplaceholder_shared_1[v0, v1, v2])
                                    rxplaceholder_shared_1[v0, v1, v2] = rxplaceholder_1[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(1), T.int32(1), T.int32(8), T.int32(1), T.int32(7), T.int32(1), T.int32(1), T.int32(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(16), i0_3 + i0_4 + i0_0_i1_0_i2_0_fused // T.int32(320) * T.int32(4) + i0_2_i1_2_i2_2_fused // T.int32(16))
                                v_i1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused % T.int32(320) // T.int32(5) * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(16) // T.int32(8) * T.int32(8) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int32(80), i0_0_i1_0_i2_0_fused % T.int32(5) * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(8) * T.int32(2) + i2_3 * T.int32(2) + i2_4)
                                v_k = T.axis.reduce(T.int32(77), k_0 * T.int32(7) + k_1 * T.int32(7) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[v_i0, v_i1, v_k], rxplaceholder_shared_1[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[v_i0, v_i1, v_k] * rxplaceholder_shared_1[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(8), T.int32(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(320) * T.int32(4) + i0_2_i1_2_i2_2_fused // T.int32(16) + ax0)
                            v1 = T.axis.spatial(T.int32(1024), i0_0_i1_0_i2_0_fused % T.int32(320) // T.int32(5) * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(16) // T.int32(8) * T.int32(8) + ax1)
                            v2 = T.axis.spatial(T.int32(80), i0_0_i1_0_i2_0_fused % T.int32(5) * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(8) * T.int32(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul25(rxplaceholder: T.Buffer((T.int32(2), T.int32(256), T.int32(1280)), "float32"), rxplaceholder_1: T.Buffer((T.int32(1280), T.int32(1280)), "float32"), matmul: T.Buffer((T.int32(2), T.int32(256), T.int32(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(2), T.int32(256), T.int32(1280)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int32(2), T.int32(256), T.int32(1280)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int32(1280), T.int32(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(8), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(1), T.int32(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(2), i0_4_init + i0_1_i1_1_i2_1_fused // T.int32(4) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused // T.int32(20) * T.int32(16) + i0_1_i1_1_i2_1_fused % T.int32(4) * T.int32(4) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(1280), i0_0_i1_0_i2_0_fused % T.int32(20) * T.int32(64) + i0_2_i1_2_i2_2_fused * T.int32(2) + i2_3_init * T.int32(2) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(320)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_fused_0 * T.int32(32) + ax0_ax1_ax2_fused_1) // T.int32(64))
                                    v1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused // T.int32(20) * T.int32(16) + (ax0_ax1_ax2_fused_0 * T.int32(32) + ax0_ax1_ax2_fused_1) % T.int32(64) // T.int32(4))
                                    v2 = T.axis.spatial(T.int32(1280), k_0 * T.int32(4) + (ax0_ax1_ax2_fused_0 * T.int32(32) + ax0_ax1_ax2_fused_1) % T.int32(4))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int32(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(1280), k_0 * T.int32(4) + (ax0_ax1_fused_0 * T.int32(32) + ax0_ax1_fused_1) // T.int32(64))
                                    v1 = T.axis.spatial(T.int32(1280), i0_0_i1_0_i2_0_fused % T.int32(20) * T.int32(64) + (ax0_ax1_fused_0 * T.int32(32) + ax0_ax1_fused_1) % T.int32(64))
                                    T.reads(rxplaceholder_1[v0, v1])
                                    T.writes(rxplaceholder_shared_1[v0, v1])
                                    rxplaceholder_shared_1[v0, v1] = rxplaceholder_1[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(1), T.int32(1), T.int32(4), T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(2), i0_4 + i0_1_i1_1_i2_1_fused // T.int32(4) + i0_3)
                                v_i1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused // T.int32(20) * T.int32(16) + i0_1_i1_1_i2_1_fused % T.int32(4) * T.int32(4) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int32(1280), i0_0_i1_0_i2_0_fused % T.int32(20) * T.int32(64) + i0_2_i1_2_i2_2_fused * T.int32(2) + i2_3 * T.int32(2) + i2_4)
                                v_k = T.axis.reduce(T.int32(1280), k_0 * T.int32(4) + k_1 * T.int32(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[v_i0, v_i1, v_k], rxplaceholder_shared_1[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[v_i0, v_i1, v_k] * rxplaceholder_shared_1[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(4), T.int32(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(2), i0_1_i1_1_i2_1_fused // T.int32(4) + ax0)
                            v1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused // T.int32(20) * T.int32(16) + i0_1_i1_1_i2_1_fused % T.int32(4) * T.int32(4) + ax1)
                            v2 = T.axis.spatial(T.int32(1280), i0_0_i1_0_i2_0_fused % T.int32(20) * T.int32(64) + i0_2_i1_2_i2_2_fused * T.int32(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul27(rxplaceholder: T.Buffer((T.int32(16), T.int32(256), T.int32(256)), "float32"), rxplaceholder_1: T.Buffer((T.int32(16), T.int32(256), T.int32(160)), "float32"), matmul: T.Buffer((T.int32(16), T.int32(256), T.int32(160)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(16), T.int32(256), T.int32(160)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int32(16), T.int32(256), T.int32(256)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int32(16), T.int32(256), T.int32(160)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(160), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(2), T.int32(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(16), i0_4_init + i0_0_i1_0_i2_0_fused // T.int32(10) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused % T.int32(10) // T.int32(5) * T.int32(128) + i0_1_i1_1_i2_1_fused * T.int32(64) + i0_2_i1_2_i2_2_fused // T.int32(16) * T.int32(8) + i1_3_init * T.int32(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(160), i0_0_i1_0_i2_0_fused % T.int32(5) * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(16) * T.int32(2) + i2_3_init * T.int32(2) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(16)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(16)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(10))
                                    v1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused % T.int32(10) // T.int32(5) * T.int32(128) + (ax0_ax1_ax2_fused_0 * T.int32(128) + ax0_ax1_ax2_fused_1) // T.int32(16))
                                    v2 = T.axis.spatial(T.int32(256), k_0 * T.int32(16) + (ax0_ax1_ax2_fused_0 * T.int32(128) + ax0_ax1_ax2_fused_1) % T.int32(16))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int32(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(10))
                                    v1 = T.axis.spatial(T.int32(256), k_0 * T.int32(16) + (ax0_ax1_ax2_fused_0 * T.int32(128) + ax0_ax1_ax2_fused_1) // T.int32(32))
                                    v2 = T.axis.spatial(T.int32(160), i0_0_i1_0_i2_0_fused % T.int32(5) * T.int32(32) + (ax0_ax1_ax2_fused_0 * T.int32(128) + ax0_ax1_ax2_fused_1) % T.int32(32))
                                    T.reads(rxplaceholder_1[v0, v1, v2])
                                    T.writes(rxplaceholder_shared_1[v0, v1, v2])
                                    rxplaceholder_shared_1[v0, v1, v2] = rxplaceholder_1[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(8), T.int32(1), T.int32(4), T.int32(1), T.int32(2), T.int32(1), T.int32(2), T.int32(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(16), i0_4 + i0_0_i1_0_i2_0_fused // T.int32(10) + i0_3)
                                v_i1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused % T.int32(10) // T.int32(5) * T.int32(128) + i0_1_i1_1_i2_1_fused * T.int32(64) + i0_2_i1_2_i2_2_fused // T.int32(16) * T.int32(8) + i1_3 * T.int32(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int32(160), i0_0_i1_0_i2_0_fused % T.int32(5) * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(16) * T.int32(2) + i2_3 * T.int32(2) + i2_4)
                                v_k = T.axis.reduce(T.int32(256), k_0 * T.int32(16) + k_1 * T.int32(2) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[v_i0, v_i1, v_k], rxplaceholder_shared_1[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[v_i0, v_i1, v_k] * rxplaceholder_shared_1[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(8), T.int32(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(10) + ax0)
                            v1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused % T.int32(10) // T.int32(5) * T.int32(128) + i0_1_i1_1_i2_1_fused * T.int32(64) + i0_2_i1_2_i2_2_fused // T.int32(16) * T.int32(8) + ax1)
                            v2 = T.axis.spatial(T.int32(160), i0_0_i1_0_i2_0_fused % T.int32(5) * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(16) * T.int32(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul28(rxplaceholder: T.Buffer((T.int32(2), T.int32(77), T.int32(768)), "float32"), rxplaceholder_1: T.Buffer((T.int32(768), T.int32(1280)), "float32"), matmul: T.Buffer((T.int32(2), T.int32(77), T.int32(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(2), T.int32(77), T.int32(1280)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int32(2), T.int32(77), T.int32(768)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int32(768), T.int32(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(44), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(224), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(2), i0_3_init + i0_4_init + i0_2_i1_2_i2_2_fused // T.int32(112))
                            v_i1 = T.axis.spatial(T.int32(77), i0_1_i1_1_i2_1_fused // T.int32(4) * T.int32(7) + i0_2_i1_2_i2_2_fused % T.int32(112) // T.int32(16) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(1280), i2_4_init + i0_0_i1_0_i2_0_fused * T.int32(64) + i0_1_i1_1_i2_1_fused % T.int32(4) * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(16) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(32)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(17)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(224), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_fused_0 * T.int32(224) + ax0_ax1_ax2_fused_1) // T.int32(1848))
                                    v1 = T.axis.spatial(T.int32(77), (ax0_ax1_ax2_fused_0 * T.int32(224) + ax0_ax1_ax2_fused_1) % T.int32(1848) // T.int32(24))
                                    v2 = T.axis.spatial(T.int32(768), k_0 * T.int32(24) + (ax0_ax1_ax2_fused_0 * T.int32(224) + ax0_ax1_ax2_fused_1) % T.int32(24))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int32(224) + ax0_ax1_ax2_fused_1 < T.int32(3696))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int32(7)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(224), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(768), k_0 * T.int32(24) + (ax0_ax1_fused_0 * T.int32(224) + ax0_ax1_fused_1) // T.int32(64))
                                    v1 = T.axis.spatial(T.int32(1280), i0_0_i1_0_i2_0_fused * T.int32(64) + (ax0_ax1_fused_0 * T.int32(224) + ax0_ax1_fused_1) % T.int32(64))
                                    T.where(ax0_ax1_fused_0 * T.int32(224) + ax0_ax1_fused_1 < T.int32(1536))
                                    T.reads(rxplaceholder_1[v0, v1])
                                    T.writes(rxplaceholder_shared_1[v0, v1])
                                    rxplaceholder_shared_1[v0, v1] = rxplaceholder_1[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(8), T.int32(1), T.int32(1), T.int32(1), T.int32(3), T.int32(1), T.int32(1), T.int32(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(2), i0_3 + i0_4 + i0_2_i1_2_i2_2_fused // T.int32(112))
                                v_i1 = T.axis.spatial(T.int32(77), i0_1_i1_1_i2_1_fused // T.int32(4) * T.int32(7) + i0_2_i1_2_i2_2_fused % T.int32(112) // T.int32(16) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int32(1280), i2_4 + i0_0_i1_0_i2_0_fused * T.int32(64) + i0_1_i1_1_i2_1_fused % T.int32(4) * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(16) + i2_3)
                                v_k = T.axis.reduce(T.int32(768), k_0 * T.int32(24) + k_1 * T.int32(3) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[v_i0, v_i1, v_k], rxplaceholder_shared_1[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[v_i0, v_i1, v_k] * rxplaceholder_shared_1[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(1), T.int32(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(2), i0_2_i1_2_i2_2_fused // T.int32(112) + ax0)
                            v1 = T.axis.spatial(T.int32(77), i0_1_i1_1_i2_1_fused // T.int32(4) * T.int32(7) + i0_2_i1_2_i2_2_fused % T.int32(112) // T.int32(16) + ax1)
                            v2 = T.axis.spatial(T.int32(1280), i0_0_i1_0_i2_0_fused * T.int32(64) + i0_1_i1_1_i2_1_fused % T.int32(4) * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(16) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul30(rxplaceholder: T.Buffer((T.int32(16), T.int32(256), T.int32(77)), "float32"), rxplaceholder_1: T.Buffer((T.int32(16), T.int32(77), T.int32(160)), "float32"), matmul: T.Buffer((T.int32(16), T.int32(256), T.int32(160)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(16), T.int32(256), T.int32(160)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int32(16), T.int32(256), T.int32(77)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int32(16), T.int32(77), T.int32(160)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(2), T.int32(4)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(16), i0_3_init + i0_4_init + i0_0_i1_0_i2_0_fused // T.int32(80) * T.int32(4) + i0_2_i1_2_i2_2_fused // T.int32(16))
                            v_i1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused % T.int32(80) // T.int32(10) * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(16) // T.int32(2) * T.int32(4) + i1_3_init * T.int32(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(160), i0_0_i1_0_i2_0_fused % T.int32(10) * T.int32(16) + i0_1_i1_1_i2_1_fused * T.int32(8) + i0_2_i1_2_i2_2_fused % T.int32(2) * T.int32(4) + i2_3_init * T.int32(4) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(7)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(22)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(80) * T.int32(4) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) // T.int32(352))
                                    v1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused % T.int32(80) // T.int32(10) * T.int32(32) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(352) // T.int32(11))
                                    v2 = T.axis.spatial(T.int32(77), k_0 * T.int32(11) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(11))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int32(11)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(80) * T.int32(4) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) // T.int32(176))
                                    v1 = T.axis.spatial(T.int32(77), k_0 * T.int32(11) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(176) // T.int32(16))
                                    v2 = T.axis.spatial(T.int32(160), i0_0_i1_0_i2_0_fused % T.int32(10) * T.int32(16) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(16))
                                    T.reads(rxplaceholder_1[v0, v1, v2])
                                    T.writes(rxplaceholder_shared_1[v0, v1, v2])
                                    rxplaceholder_shared_1[v0, v1, v2] = rxplaceholder_1[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(11), T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(2), T.int32(4)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(16), i0_3 + i0_4 + i0_0_i1_0_i2_0_fused // T.int32(80) * T.int32(4) + i0_2_i1_2_i2_2_fused // T.int32(16))
                                v_i1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused % T.int32(80) // T.int32(10) * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(16) // T.int32(2) * T.int32(4) + i1_3 * T.int32(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int32(160), i0_0_i1_0_i2_0_fused % T.int32(10) * T.int32(16) + i0_1_i1_1_i2_1_fused * T.int32(8) + i0_2_i1_2_i2_2_fused % T.int32(2) * T.int32(4) + i2_3 * T.int32(4) + i2_4)
                                v_k = T.axis.reduce(T.int32(77), k_0 * T.int32(11) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[v_i0, v_i1, v_k], rxplaceholder_shared_1[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[v_i0, v_i1, v_k] * rxplaceholder_shared_1[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(4), T.int32(4)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(80) * T.int32(4) + i0_2_i1_2_i2_2_fused // T.int32(16) + ax0)
                            v1 = T.axis.spatial(T.int32(256), i0_0_i1_0_i2_0_fused % T.int32(80) // T.int32(10) * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(16) // T.int32(2) * T.int32(4) + ax1)
                            v2 = T.axis.spatial(T.int32(160), i0_0_i1_0_i2_0_fused % T.int32(10) * T.int32(16) + i0_1_i1_1_i2_1_fused * T.int32(8) + i0_2_i1_2_i2_2_fused % T.int32(2) * T.int32(4) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul33(rxplaceholder: T.Buffer((T.int32(2), T.int32(64), T.int32(1280)), "float32"), rxplaceholder_1: T.Buffer((T.int32(1280), T.int32(1280)), "float32"), matmul: T.Buffer((T.int32(2), T.int32(64), T.int32(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(2), T.int32(64), T.int32(1280)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int32(2), T.int32(64), T.int32(1280)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int32(1280), T.int32(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(160), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(2), T.int32(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(2), i0_4_init + i0_0_i1_0_i2_0_fused // T.int32(80) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int32(64), i0_0_i1_0_i2_0_fused % T.int32(80) // T.int32(40) * T.int32(32) + i0_2_i1_2_i2_2_fused // T.int32(16) * T.int32(8) + i1_3_init * T.int32(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(1280), i0_0_i1_0_i2_0_fused % T.int32(40) * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(16) * T.int32(2) + i2_3_init * T.int32(2) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(320)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(2), i0_0_i1_0_i2_0_fused // T.int32(80))
                                    v1 = T.axis.spatial(T.int32(64), i0_0_i1_0_i2_0_fused % T.int32(80) // T.int32(40) * T.int32(32) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) // T.int32(4))
                                    v2 = T.axis.spatial(T.int32(1280), k_0 * T.int32(4) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(4))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int32(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(1280), k_0 * T.int32(4) + (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) // T.int32(32))
                                    v1 = T.axis.spatial(T.int32(1280), i0_0_i1_0_i2_0_fused % T.int32(40) * T.int32(32) + (ax0_ax1_fused_0 * T.int32(64) + ax0_ax1_fused_1) % T.int32(32))
                                    T.reads(rxplaceholder_1[v0, v1])
                                    T.writes(rxplaceholder_shared_1[v0, v1])
                                    rxplaceholder_shared_1[v0, v1] = rxplaceholder_1[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(2), T.int32(1), T.int32(4), T.int32(1), T.int32(2), T.int32(1), T.int32(2), T.int32(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(2), i0_4 + i0_0_i1_0_i2_0_fused // T.int32(80) + i0_3)
                                v_i1 = T.axis.spatial(T.int32(64), i0_0_i1_0_i2_0_fused % T.int32(80) // T.int32(40) * T.int32(32) + i0_2_i1_2_i2_2_fused // T.int32(16) * T.int32(8) + i1_3 * T.int32(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int32(1280), i0_0_i1_0_i2_0_fused % T.int32(40) * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(16) * T.int32(2) + i2_3 * T.int32(2) + i2_4)
                                v_k = T.axis.reduce(T.int32(1280), k_0 * T.int32(4) + k_1 * T.int32(2) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[v_i0, v_i1, v_k], rxplaceholder_shared_1[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[v_i0, v_i1, v_k] * rxplaceholder_shared_1[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(8), T.int32(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(2), i0_0_i1_0_i2_0_fused // T.int32(80) + ax0)
                            v1 = T.axis.spatial(T.int32(64), i0_0_i1_0_i2_0_fused % T.int32(80) // T.int32(40) * T.int32(32) + i0_2_i1_2_i2_2_fused // T.int32(16) * T.int32(8) + ax1)
                            v2 = T.axis.spatial(T.int32(1280), i0_0_i1_0_i2_0_fused % T.int32(40) * T.int32(32) + i0_2_i1_2_i2_2_fused % T.int32(16) * T.int32(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul35(rxplaceholder: T.Buffer((T.int32(16), T.int32(64), T.int32(64)), "float32"), rxplaceholder_1: T.Buffer((T.int32(16), T.int32(64), T.int32(160)), "float32"), matmul: T.Buffer((T.int32(16), T.int32(64), T.int32(160)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(16), T.int32(64), T.int32(160)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int32(16), T.int32(64), T.int32(64)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int32(16), T.int32(64), T.int32(160)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(16), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(1), T.int32(5), T.int32(1), T.int32(1), T.int32(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(16), i0_3_init + i0_4_init + i0_0_i1_0_i2_0_fused // T.int32(8) * T.int32(4) + i0_2_i1_2_i2_2_fused // T.int32(64))
                            v_i1 = T.axis.spatial(T.int32(64), i0_0_i1_0_i2_0_fused % T.int32(8) // T.int32(2) * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(64) // T.int32(4) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(160), i0_0_i1_0_i2_0_fused % T.int32(2) * T.int32(80) + i0_1_i1_1_i2_1_fused * T.int32(40) + i0_2_i1_2_i2_2_fused % T.int32(4) * T.int32(10) + i2_3_init * T.int32(2) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(8)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(8) * T.int32(4) + (ax0_ax1_ax2_fused_0 * T.int32(256) + ax0_ax1_ax2_fused_1) // T.int32(128))
                                    v1 = T.axis.spatial(T.int32(64), i0_0_i1_0_i2_0_fused % T.int32(8) // T.int32(2) * T.int32(16) + (ax0_ax1_ax2_fused_0 * T.int32(256) + ax0_ax1_ax2_fused_1) % T.int32(128) // T.int32(8))
                                    v2 = T.axis.spatial(T.int32(64), k_0 * T.int32(8) + (ax0_ax1_ax2_fused_0 * T.int32(256) + ax0_ax1_ax2_fused_1) % T.int32(8))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int32(10)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(8) * T.int32(4) + (ax0_ax1_ax2_fused_0 * T.int32(256) + ax0_ax1_ax2_fused_1) // T.int32(640))
                                    v1 = T.axis.spatial(T.int32(64), k_0 * T.int32(8) + (ax0_ax1_ax2_fused_0 * T.int32(256) + ax0_ax1_ax2_fused_1) % T.int32(640) // T.int32(80))
                                    v2 = T.axis.spatial(T.int32(160), i0_0_i1_0_i2_0_fused % T.int32(2) * T.int32(80) + (ax0_ax1_ax2_fused_0 * T.int32(256) + ax0_ax1_ax2_fused_1) % T.int32(80))
                                    T.reads(rxplaceholder_1[v0, v1, v2])
                                    T.writes(rxplaceholder_shared_1[v0, v1, v2])
                                    rxplaceholder_shared_1[v0, v1, v2] = rxplaceholder_1[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(4), T.int32(1), T.int32(1), T.int32(5), T.int32(2), T.int32(1), T.int32(1), T.int32(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(16), i0_3 + i0_4 + i0_0_i1_0_i2_0_fused // T.int32(8) * T.int32(4) + i0_2_i1_2_i2_2_fused // T.int32(64))
                                v_i1 = T.axis.spatial(T.int32(64), i0_0_i1_0_i2_0_fused % T.int32(8) // T.int32(2) * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(64) // T.int32(4) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int32(160), i0_0_i1_0_i2_0_fused % T.int32(2) * T.int32(80) + i0_1_i1_1_i2_1_fused * T.int32(40) + i0_2_i1_2_i2_2_fused % T.int32(4) * T.int32(10) + i2_3 * T.int32(2) + i2_4)
                                v_k = T.axis.reduce(T.int32(64), k_0 * T.int32(8) + k_1 * T.int32(2) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[v_i0, v_i1, v_k], rxplaceholder_shared_1[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[v_i0, v_i1, v_k] * rxplaceholder_shared_1[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(1), T.int32(10)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(8) * T.int32(4) + i0_2_i1_2_i2_2_fused // T.int32(64) + ax0)
                            v1 = T.axis.spatial(T.int32(64), i0_0_i1_0_i2_0_fused % T.int32(8) // T.int32(2) * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(64) // T.int32(4) + ax1)
                            v2 = T.axis.spatial(T.int32(160), i0_0_i1_0_i2_0_fused % T.int32(2) * T.int32(80) + i0_1_i1_1_i2_1_fused * T.int32(40) + i0_2_i1_2_i2_2_fused % T.int32(4) * T.int32(10) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul37(rxplaceholder: T.Buffer((T.int32(16), T.int32(64), T.int32(77)), "float32"), rxplaceholder_1: T.Buffer((T.int32(16), T.int32(77), T.int32(160)), "float32"), matmul: T.Buffer((T.int32(16), T.int32(64), T.int32(160)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(16), T.int32(64), T.int32(160)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int32(16), T.int32(64), T.int32(77)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int32(16), T.int32(77), T.int32(160)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(16), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(8), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(40), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(4)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(16), i0_4_init + i0_0_i1_0_i2_0_fused // T.int32(4) + i0_3_init)
                            v_i1 = T.axis.spatial(T.int32(64), i0_0_i1_0_i2_0_fused % T.int32(4) * T.int32(16) + i0_1_i1_1_i2_1_fused * T.int32(2) + i0_2_i1_2_i2_2_fused // T.int32(20) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(160), i0_2_i1_2_i2_2_fused % T.int32(20) * T.int32(8) + i2_3_init * T.int32(4) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(7)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(5)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(40), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(4))
                                    v1 = T.axis.spatial(T.int32(64), i0_0_i1_0_i2_0_fused % T.int32(4) * T.int32(16) + (ax0_ax1_ax2_fused_0 * T.int32(40) + ax0_ax1_ax2_fused_1) // T.int32(11))
                                    v2 = T.axis.spatial(T.int32(77), k_0 * T.int32(11) + (ax0_ax1_ax2_fused_0 * T.int32(40) + ax0_ax1_ax2_fused_1) % T.int32(11))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int32(40) + ax0_ax1_ax2_fused_1 < T.int32(176))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int32(44)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(40), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(4))
                                    v1 = T.axis.spatial(T.int32(77), k_0 * T.int32(11) + (ax0_ax1_ax2_fused_0 * T.int32(40) + ax0_ax1_ax2_fused_1) // T.int32(160))
                                    v2 = T.axis.spatial(T.int32(160), (ax0_ax1_ax2_fused_0 * T.int32(40) + ax0_ax1_ax2_fused_1) % T.int32(160))
                                    T.reads(rxplaceholder_1[v0, v1, v2])
                                    T.writes(rxplaceholder_shared_1[v0, v1, v2])
                                    rxplaceholder_shared_1[v0, v1, v2] = rxplaceholder_1[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(11), T.int32(1), T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(4)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(16), i0_4 + i0_0_i1_0_i2_0_fused // T.int32(4) + i0_3)
                                v_i1 = T.axis.spatial(T.int32(64), i0_0_i1_0_i2_0_fused % T.int32(4) * T.int32(16) + i0_1_i1_1_i2_1_fused * T.int32(2) + i0_2_i1_2_i2_2_fused // T.int32(20) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int32(160), i0_2_i1_2_i2_2_fused % T.int32(20) * T.int32(8) + i2_3 * T.int32(4) + i2_4)
                                v_k = T.axis.reduce(T.int32(77), k_0 * T.int32(11) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[v_i0, v_i1, v_k], rxplaceholder_shared_1[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[v_i0, v_i1, v_k] * rxplaceholder_shared_1[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(1), T.int32(8)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(16), i0_0_i1_0_i2_0_fused // T.int32(4) + ax0)
                            v1 = T.axis.spatial(T.int32(64), i0_0_i1_0_i2_0_fused % T.int32(4) * T.int32(16) + i0_1_i1_1_i2_1_fused * T.int32(2) + i0_2_i1_2_i2_2_fused // T.int32(20) + ax1)
                            v2 = T.axis.spatial(T.int32(160), i0_2_i1_2_i2_2_fused % T.int32(20) * T.int32(8) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul42(rxplaceholder: T.Buffer((T.int32(1), T.int32(4096), T.int32(4096)), "float32"), rxplaceholder_1: T.Buffer((T.int32(1), T.int32(4096), T.int32(512)), "float32"), matmul: T.Buffer((T.int32(1), T.int32(4096), T.int32(512)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(1), T.int32(4096), T.int32(512)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int32(1), T.int32(4096), T.int32(4096)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int32(1), T.int32(4096), T.int32(512)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(2), T.int32(1), T.int32(1), T.int32(1), T.int32(4)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(1), i0_4_init + i0_3_init)
                            v_i1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused // T.int32(8) * T.int32(32) + i0_1_i1_1_i2_1_fused * T.int32(8) + i0_2_i1_2_i2_2_fused // T.int32(16) * T.int32(2) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(512), i0_0_i1_0_i2_0_fused % T.int32(8) * T.int32(64) + i0_2_i1_2_i2_2_fused % T.int32(16) * T.int32(4) + i2_3_init * T.int32(4) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(512)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused // T.int32(8) * T.int32(32) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) // T.int32(8))
                                    v2 = T.axis.spatial(T.int32(4096), k_0 * T.int32(8) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(8))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int32(8)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(1), T.int32(0))
                                    v1 = T.axis.spatial(T.int32(4096), k_0 * T.int32(8) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) // T.int32(64))
                                    v2 = T.axis.spatial(T.int32(512), i0_0_i1_0_i2_0_fused % T.int32(8) * T.int32(64) + (ax0_ax1_ax2_fused_0 * T.int32(64) + ax0_ax1_ax2_fused_1) % T.int32(64))
                                    T.reads(rxplaceholder_1[v0, v1, v2])
                                    T.writes(rxplaceholder_shared_1[v0, v1, v2])
                                    rxplaceholder_shared_1[v0, v1, v2] = rxplaceholder_1[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(1), T.int32(1), T.int32(2), T.int32(1), T.int32(8), T.int32(1), T.int32(1), T.int32(4)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(1), i0_4 + i0_3)
                                v_i1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused // T.int32(8) * T.int32(32) + i0_1_i1_1_i2_1_fused * T.int32(8) + i0_2_i1_2_i2_2_fused // T.int32(16) * T.int32(2) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int32(512), i0_0_i1_0_i2_0_fused % T.int32(8) * T.int32(64) + i0_2_i1_2_i2_2_fused % T.int32(16) * T.int32(4) + i2_3 * T.int32(4) + i2_4)
                                v_k = T.axis.reduce(T.int32(4096), k_0 * T.int32(8) + k_1 * T.int32(8) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[T.int32(0), v_i1, v_k], rxplaceholder_shared_1[T.int32(0), v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[T.int32(0), v_i1, v_k] * rxplaceholder_shared_1[T.int32(0), v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(1), T.int32(2), T.int32(4)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(1), ax0)
                            v1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused // T.int32(8) * T.int32(32) + i0_1_i1_1_i2_1_fused * T.int32(8) + i0_2_i1_2_i2_2_fused // T.int32(16) * T.int32(2) + ax1)
                            v2 = T.axis.spatial(T.int32(512), i0_0_i1_0_i2_0_fused % T.int32(8) * T.int32(64) + i0_2_i1_2_i2_2_fused % T.int32(16) * T.int32(4) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul8(rxplaceholder: T.Buffer((T.int32(2), T.int32(4096), T.int32(320)), "float32"), rxplaceholder_1: T.Buffer((T.int32(320), T.int32(320)), "float32"), matmul: T.Buffer((T.int32(2), T.int32(4096), T.int32(320)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int32(2), T.int32(4096), T.int32(320)), scope="local")
        rxplaceholder_shared = T.alloc_buffer((T.int32(2), T.int32(4096), T.int32(320)), scope="shared")
        rxplaceholder_shared_1 = T.alloc_buffer((T.int32(320), T.int32(320)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int32(1280), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int32(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int32(1), T.int32(4), T.int32(1), T.int32(2), T.int32(2), T.int32(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int32(2), i0_3_init * T.int32(2) + i0_4_init)
                            v_i1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused // T.int32(5) * T.int32(16) + i0_2_i1_2_i2_2_fused // T.int32(16) * T.int32(8) + i1_3_init * T.int32(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int32(320), i2_4_init + i0_0_i1_0_i2_0_fused % T.int32(5) * T.int32(64) + i0_1_i1_1_i2_1_fused * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(16) + i2_3_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int32(80)):
                        for ax0_ax1_ax2_fused_0 in range(T.int32(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_fused_0 * T.int32(32) + ax0_ax1_ax2_fused_1) // T.int32(64))
                                    v1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused // T.int32(5) * T.int32(16) + (ax0_ax1_ax2_fused_0 * T.int32(32) + ax0_ax1_ax2_fused_1) % T.int32(64) // T.int32(4))
                                    v2 = T.axis.spatial(T.int32(320), k_0 * T.int32(4) + (ax0_ax1_ax2_fused_0 * T.int32(32) + ax0_ax1_ax2_fused_1) % T.int32(4))
                                    T.reads(rxplaceholder[v0, v1, v2])
                                    T.writes(rxplaceholder_shared[v0, v1, v2])
                                    rxplaceholder_shared[v0, v1, v2] = rxplaceholder[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int32(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                                with T.block("rxplaceholder_shared"):
                                    v0 = T.axis.spatial(T.int32(320), k_0 * T.int32(4) + (ax0_ax1_fused_0 * T.int32(32) + ax0_ax1_fused_1) // T.int32(64))
                                    v1 = T.axis.spatial(T.int32(320), i0_0_i1_0_i2_0_fused % T.int32(5) * T.int32(64) + (ax0_ax1_fused_0 * T.int32(32) + ax0_ax1_fused_1) % T.int32(64))
                                    T.reads(rxplaceholder_1[v0, v1])
                                    T.writes(rxplaceholder_shared_1[v0, v1])
                                    rxplaceholder_shared_1[v0, v1] = rxplaceholder_1[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int32(4), T.int32(1), T.int32(4), T.int32(1), T.int32(1), T.int32(2), T.int32(2), T.int32(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int32(2), i0_3 * T.int32(2) + i0_4)
                                v_i1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused // T.int32(5) * T.int32(16) + i0_2_i1_2_i2_2_fused // T.int32(16) * T.int32(8) + i1_3 * T.int32(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int32(320), i2_4 + i0_0_i1_0_i2_0_fused % T.int32(5) * T.int32(64) + i0_1_i1_1_i2_1_fused * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(16) + i2_3)
                                v_k = T.axis.reduce(T.int32(320), k_0 * T.int32(4) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], rxplaceholder_shared[v_i0, v_i1, v_k], rxplaceholder_shared_1[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int32(1024), "meta_schedule.thread_extent_low_inclusive": T.int32(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + rxplaceholder_shared[v_i0, v_i1, v_k] * rxplaceholder_shared_1[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int32(2), T.int32(8), T.int32(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int32(2), ax0)
                            v1 = T.axis.spatial(T.int32(4096), i0_0_i1_0_i2_0_fused // T.int32(5) * T.int32(16) + i0_2_i1_2_i2_2_fused // T.int32(16) * T.int32(8) + ax1)
                            v2 = T.axis.spatial(T.int32(320), i0_0_i1_0_i2_0_fused % T.int32(5) * T.int32(64) + i0_1_i1_1_i2_1_fused * T.int32(16) + i0_2_i1_2_i2_2_fused % T.int32(16) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def reshape12(rxplaceholder: T.Buffer((T.int32(2), T.int32(320)), "float32"), T_reshape: T.Buffer((T.int32(2), T.int32(320), T.int32(1), T.int32(1)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int32(3), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) // T.int32(320))
                    v_ax1 = T.axis.spatial(T.int32(320), (ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1) % T.int32(320))
                    v_ax2 = T.axis.spatial(T.int32(1), T.int32(0))
                    v_ax3 = T.axis.spatial(T.int32(1), T.int32(0))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int32(256) + ax0_ax1_ax2_ax3_fused_1 < T.int32(640))
                    T.reads(rxplaceholder[((v_ax1 + v_ax2 + v_ax3) // T.int32(320) + v_ax0) % T.int32(2), (v_ax1 + v_ax2 + v_ax3) % T.int32(320)])
                    T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_reshape[v_ax0, v_ax1, v_ax2, v_ax3] = rxplaceholder[((v_ax1 + v_ax2 + v_ax3) // T.int32(320) + v_ax0) % T.int32(2), (v_ax1 + v_ax2 + v_ax3) % T.int32(320)]

    @T.prim_func
    def reshape22(rxplaceholder: T.Buffer((T.int32(2), T.int32(640)), "float32"), T_reshape: T.Buffer((T.int32(2), T.int32(640), T.int32(1), T.int32(1)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int32(10), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) // T.int32(640))
                    v_ax1 = T.axis.spatial(T.int32(640), (ax0_ax1_ax2_ax3_fused_0 * T.int32(128) + ax0_ax1_ax2_ax3_fused_1) % T.int32(640))
                    v_ax2 = T.axis.spatial(T.int32(1), T.int32(0))
                    v_ax3 = T.axis.spatial(T.int32(1), T.int32(0))
                    T.reads(rxplaceholder[((v_ax1 + v_ax2 + v_ax3) // T.int32(640) + v_ax0) % T.int32(2), (v_ax1 + v_ax2 + v_ax3) % T.int32(640)])
                    T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_reshape[v_ax0, v_ax1, v_ax2, v_ax3] = rxplaceholder[((v_ax1 + v_ax2 + v_ax3) // T.int32(640) + v_ax0) % T.int32(2), (v_ax1 + v_ax2 + v_ax3) % T.int32(640)]

    @T.prim_func
    def reshape32(rxplaceholder: T.Buffer((T.int32(2), T.int32(1280)), "float32"), T_reshape: T.Buffer((T.int32(2), T.int32(1280), T.int32(1), T.int32(1)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int32(80), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int32(2), (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) // T.int32(1280))
                    v_ax1 = T.axis.spatial(T.int32(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int32(32) + ax0_ax1_ax2_ax3_fused_1) % T.int32(1280))
                    v_ax2 = T.axis.spatial(T.int32(1), T.int32(0))
                    v_ax3 = T.axis.spatial(T.int32(1), T.int32(0))
                    T.reads(rxplaceholder[((v_ax1 + v_ax2 + v_ax3) // T.int32(1280) + v_ax0) % T.int32(2), (v_ax1 + v_ax2 + v_ax3) % T.int32(1280)])
                    T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_reshape[v_ax0, v_ax1, v_ax2, v_ax3] = rxplaceholder[((v_ax1 + v_ax2 + v_ax3) // T.int32(1280) + v_ax0) % T.int32(2), (v_ax1 + v_ax2 + v_ax3) % T.int32(1280)]

    @T.prim_func
    def resize2d(rxplaceholder: T.Buffer((T.int32(2), T.int32(1280), T.int32(8), T.int32(8)), "float32"), resize: T.Buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for i0_i1_i2_i3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for i0_i1_i2_i3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for i0_i1_i2_i3_fused_0 in range(T.int32(10)):
                    with T.block("resize"):
                        v_i0 = T.axis.spatial(T.int32(2), (i0_i1_i2_i3_fused_0 * T.int32(65536) + i0_i1_i2_i3_fused_1 * T.int32(256) + i0_i1_i2_i3_fused_2) // T.int32(327680))
                        v_i1 = T.axis.spatial(T.int32(1280), (i0_i1_i2_i3_fused_0 * T.int32(65536) + i0_i1_i2_i3_fused_1 * T.int32(256) + i0_i1_i2_i3_fused_2) % T.int32(327680) // T.int32(256))
                        v_i2 = T.axis.spatial(T.int32(16), (i0_i1_i2_i3_fused_0 * T.int32(65536) + i0_i1_i2_i3_fused_1 * T.int32(256) + i0_i1_i2_i3_fused_2) % T.int32(256) // T.int32(16))
                        v_i3 = T.axis.spatial(T.int32(16), (i0_i1_i2_i3_fused_0 * T.int32(65536) + i0_i1_i2_i3_fused_1 * T.int32(256) + i0_i1_i2_i3_fused_2) % T.int32(16))
                        T.reads(rxplaceholder[v_i0, v_i1, T.max(T.min(v_i2 // T.int32(2), T.int32(7)), T.int32(0)), T.max(T.min(v_i3 // T.int32(2), T.int32(7)), T.int32(0))])
                        T.writes(resize[v_i0, v_i1, v_i2, v_i3])
                        resize[v_i0, v_i1, v_i2, v_i3] = rxplaceholder[v_i0, v_i1, T.max(T.min(v_i2 // T.int32(2), T.int32(7)), T.int32(0)), T.max(T.min(v_i3 // T.int32(2), T.int32(7)), T.int32(0))]

    @T.prim_func
    def resize2d1(rxplaceholder: T.Buffer((T.int32(2), T.int32(1280), T.int32(16), T.int32(16)), "float32"), resize: T.Buffer((T.int32(2), T.int32(1280), T.int32(32), T.int32(32)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for i0_i1_i2_i3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for i0_i1_i2_i3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for i0_i1_i2_i3_fused_0 in range(T.int32(40)):
                    with T.block("resize"):
                        v_i0 = T.axis.spatial(T.int32(2), (i0_i1_i2_i3_fused_0 * T.int32(65536) + i0_i1_i2_i3_fused_1 * T.int32(256) + i0_i1_i2_i3_fused_2) // T.int32(1310720))
                        v_i1 = T.axis.spatial(T.int32(1280), (i0_i1_i2_i3_fused_0 * T.int32(65536) + i0_i1_i2_i3_fused_1 * T.int32(256) + i0_i1_i2_i3_fused_2) % T.int32(1310720) // T.int32(1024))
                        v_i2 = T.axis.spatial(T.int32(32), (i0_i1_i2_i3_fused_0 * T.int32(65536) + i0_i1_i2_i3_fused_1 * T.int32(256) + i0_i1_i2_i3_fused_2) % T.int32(1024) // T.int32(32))
                        v_i3 = T.axis.spatial(T.int32(32), (i0_i1_i2_i3_fused_0 * T.int32(65536) + i0_i1_i2_i3_fused_1 * T.int32(256) + i0_i1_i2_i3_fused_2) % T.int32(32))
                        T.reads(rxplaceholder[v_i0, v_i1, T.max(T.min(v_i2 // T.int32(2), T.int32(15)), T.int32(0)), T.max(T.min(v_i3 // T.int32(2), T.int32(15)), T.int32(0))])
                        T.writes(resize[v_i0, v_i1, v_i2, v_i3])
                        resize[v_i0, v_i1, v_i2, v_i3] = rxplaceholder[v_i0, v_i1, T.max(T.min(v_i2 // T.int32(2), T.int32(15)), T.int32(0)), T.max(T.min(v_i3 // T.int32(2), T.int32(15)), T.int32(0))]

    @T.prim_func
    def resize2d2(rxplaceholder: T.Buffer((T.int32(2), T.int32(640), T.int32(32), T.int32(32)), "float32"), resize: T.Buffer((T.int32(2), T.int32(640), T.int32(64), T.int32(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for i0_i1_i2_i3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for i0_i1_i2_i3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for i0_i1_i2_i3_fused_0 in range(T.int32(80)):
                    with T.block("resize"):
                        v_i0 = T.axis.spatial(T.int32(2), (i0_i1_i2_i3_fused_0 * T.int32(65536) + i0_i1_i2_i3_fused_1 * T.int32(256) + i0_i1_i2_i3_fused_2) // T.int32(2621440))
                        v_i1 = T.axis.spatial(T.int32(640), (i0_i1_i2_i3_fused_0 * T.int32(65536) + i0_i1_i2_i3_fused_1 * T.int32(256) + i0_i1_i2_i3_fused_2) % T.int32(2621440) // T.int32(4096))
                        v_i2 = T.axis.spatial(T.int32(64), (i0_i1_i2_i3_fused_0 * T.int32(65536) + i0_i1_i2_i3_fused_1 * T.int32(256) + i0_i1_i2_i3_fused_2) % T.int32(4096) // T.int32(64))
                        v_i3 = T.axis.spatial(T.int32(64), (i0_i1_i2_i3_fused_0 * T.int32(65536) + i0_i1_i2_i3_fused_1 * T.int32(256) + i0_i1_i2_i3_fused_2) % T.int32(64))
                        T.reads(rxplaceholder[v_i0, v_i1, T.max(T.min(v_i2 // T.int32(2), T.int32(31)), T.int32(0)), T.max(T.min(v_i3 // T.int32(2), T.int32(31)), T.int32(0))])
                        T.writes(resize[v_i0, v_i1, v_i2, v_i3])
                        resize[v_i0, v_i1, v_i2, v_i3] = rxplaceholder[v_i0, v_i1, T.max(T.min(v_i2 // T.int32(2), T.int32(31)), T.int32(0)), T.max(T.min(v_i3 // T.int32(2), T.int32(31)), T.int32(0))]

    @T.prim_func
    def resize2d3(rxplaceholder: T.Buffer((T.int32(1), T.int32(512), T.int32(64), T.int32(64)), "float32"), resize: T.Buffer((T.int32(1), T.int32(512), T.int32(128), T.int32(128)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for i0_i1_i2_i3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for i0_i1_i2_i3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for i0_i1_i2_i3_fused_0 in range(T.int32(128)):
                    with T.block("resize"):
                        v_i0 = T.axis.spatial(T.int32(1), T.int32(0))
                        v_i1 = T.axis.spatial(T.int32(512), (i0_i1_i2_i3_fused_0 * T.int32(65536) + i0_i1_i2_i3_fused_1 * T.int32(256) + i0_i1_i2_i3_fused_2) // T.int32(16384))
                        v_i2 = T.axis.spatial(T.int32(128), (i0_i1_i2_i3_fused_0 * T.int32(65536) + i0_i1_i2_i3_fused_1 * T.int32(256) + i0_i1_i2_i3_fused_2) % T.int32(16384) // T.int32(128))
                        v_i3 = T.axis.spatial(T.int32(128), (i0_i1_i2_i3_fused_0 * T.int32(65536) + i0_i1_i2_i3_fused_1 * T.int32(256) + i0_i1_i2_i3_fused_2) % T.int32(128))
                        T.reads(rxplaceholder[v_i0, v_i1, T.max(T.min(v_i2 // T.int32(2), T.int32(63)), T.int32(0)), T.max(T.min(v_i3 // T.int32(2), T.int32(63)), T.int32(0))])
                        T.writes(resize[v_i0, v_i1, v_i2, v_i3])
                        resize[v_i0, v_i1, v_i2, v_i3] = rxplaceholder[v_i0, v_i1, T.max(T.min(v_i2 // T.int32(2), T.int32(63)), T.int32(0)), T.max(T.min(v_i3 // T.int32(2), T.int32(63)), T.int32(0))]

    @T.prim_func
    def resize2d4(rxplaceholder: T.Buffer((T.int32(1), T.int32(512), T.int32(128), T.int32(128)), "float32"), resize: T.Buffer((T.int32(1), T.int32(512), T.int32(256), T.int32(256)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for i0_i1_i2_i3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for i0_i1_i2_i3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for i0_i1_i2_i3_fused_0 in range(T.int32(512)):
                    with T.block("resize"):
                        v_i0 = T.axis.spatial(T.int32(1), T.int32(0))
                        v_i1 = T.axis.spatial(T.int32(512), (i0_i1_i2_i3_fused_0 * T.int32(65536) + i0_i1_i2_i3_fused_1 * T.int32(256) + i0_i1_i2_i3_fused_2) // T.int32(65536))
                        v_i2 = T.axis.spatial(T.int32(256), (i0_i1_i2_i3_fused_0 * T.int32(65536) + i0_i1_i2_i3_fused_1 * T.int32(256) + i0_i1_i2_i3_fused_2) % T.int32(65536) // T.int32(256))
                        v_i3 = T.axis.spatial(T.int32(256), (i0_i1_i2_i3_fused_0 * T.int32(65536) + i0_i1_i2_i3_fused_1 * T.int32(256) + i0_i1_i2_i3_fused_2) % T.int32(256))
                        T.reads(rxplaceholder[v_i0, v_i1, T.max(T.min(v_i2 // T.int32(2), T.int32(127)), T.int32(0)), T.max(T.min(v_i3 // T.int32(2), T.int32(127)), T.int32(0))])
                        T.writes(resize[v_i0, v_i1, v_i2, v_i3])
                        resize[v_i0, v_i1, v_i2, v_i3] = rxplaceholder[v_i0, v_i1, T.max(T.min(v_i2 // T.int32(2), T.int32(127)), T.int32(0)), T.max(T.min(v_i3 // T.int32(2), T.int32(127)), T.int32(0))]

    @T.prim_func
    def resize2d5(rxplaceholder: T.Buffer((T.int32(1), T.int32(256), T.int32(256), T.int32(256)), "float32"), resize: T.Buffer((T.int32(1), T.int32(256), T.int32(512), T.int32(512)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.noalias": True})
        # with T.block("root"):
        for i0_i1_i2_i3_fused_1 in T.thread_binding(T.int32(256), thread="blockIdx.x"):
            for i0_i1_i2_i3_fused_2 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                for i0_i1_i2_i3_fused_0 in range(T.int32(1024)):
                    with T.block("resize"):
                        v_i0 = T.axis.spatial(T.int32(1), T.int32(0))
                        v_i1 = T.axis.spatial(T.int32(256), (i0_i1_i2_i3_fused_0 * T.int32(65536) + i0_i1_i2_i3_fused_1 * T.int32(256) + i0_i1_i2_i3_fused_2) // T.int32(262144))
                        v_i2 = T.axis.spatial(T.int32(512), (i0_i1_i2_i3_fused_0 * T.int32(65536) + i0_i1_i2_i3_fused_1 * T.int32(256) + i0_i1_i2_i3_fused_2) % T.int32(262144) // T.int32(512))
                        v_i3 = T.axis.spatial(T.int32(512), (i0_i1_i2_i3_fused_0 * T.int32(65536) + i0_i1_i2_i3_fused_1 * T.int32(256) + i0_i1_i2_i3_fused_2) % T.int32(512))
                        T.reads(rxplaceholder[v_i0, v_i1, T.max(T.min(v_i2 // T.int32(2), T.int32(255)), T.int32(0)), T.max(T.min(v_i3 // T.int32(2), T.int32(255)), T.int32(0))])
                        T.writes(resize[v_i0, v_i1, v_i2, v_i3])
                        resize[v_i0, v_i1, v_i2, v_i3] = rxplaceholder[v_i0, v_i1, T.max(T.min(v_i2 // T.int32(2), T.int32(255)), T.int32(0)), T.max(T.min(v_i3 // T.int32(2), T.int32(255)), T.int32(0))]

    @T.prim_func
    def silu(rxplaceholder: T.Buffer((T.int32(2), T.int32(1280)), "float32"), T_multiply: T.Buffer((T.int32(2), T.int32(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.noalias": True})
        # with T.block("root"):
        for i0_i1_fused_0 in T.thread_binding(T.int32(80), thread="blockIdx.x"):
            for i0_i1_fused_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                with T.block("compute"):
                    v_i0 = T.axis.spatial(T.int32(2), (i0_i1_fused_0 * T.int32(32) + i0_i1_fused_1) // T.int32(1280))
                    v_i1 = T.axis.spatial(T.int32(1280), (i0_i1_fused_0 * T.int32(32) + i0_i1_fused_1) % T.int32(1280))
                    T.reads(rxplaceholder[v_i0, v_i1])
                    T.writes(T_multiply[v_i0, v_i1])
                    T_multiply[v_i0, v_i1] = rxplaceholder[v_i0, v_i1] * T.sigmoid(rxplaceholder[v_i0, v_i1])

    @T.prim_func
    def softmax(rxplaceholder: T.Buffer((T.int32(12), T.int32(77), T.int32(77)), "float32"), T_softmax_norm: T.Buffer((T.int32(12), T.int32(77), T.int32(77)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int32(12), T.int32(77)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int32(12), T.int32(77)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int32(924), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for ax0, ax1, ax2_0 in T.grid(T.int32(1), T.int32(1), T.int32(3)):
                for ax2_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int32(12), i0_i1_fused // T.int32(77) + ax0)
                        v_i1 = T.axis.spatial(T.int32(77), i0_i1_fused % T.int32(77) + ax1)
                        v_k = T.axis.reduce(T.int32(77), ax2_0 * T.int32(32) + ax2_1)
                        T.where(ax2_0 * T.int32(32) + ax2_1 < T.int32(77))
                        T.reads(rxplaceholder[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], rxplaceholder[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int32(1), T.int32(1), T.int32(3)):
                for ax2_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int32(12), i0_i1_fused // T.int32(77) + ax0)
                        v_i1 = T.axis.spatial(T.int32(77), i0_i1_fused % T.int32(77) + ax1)
                        v_k = T.axis.reduce(T.int32(77), ax2_0 * T.int32(32) + ax2_1)
                        T.where(ax2_0 * T.int32(32) + ax2_1 < T.int32(77))
                        T.reads(rxplaceholder[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(rxplaceholder[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int32(3)):
                for i2_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int32(12), i0_i1_fused // T.int32(77))
                        v_i1 = T.axis.spatial(T.int32(77), i0_i1_fused % T.int32(77))
                        v_i2 = T.axis.spatial(T.int32(77), i2_0 * T.int32(32) + i2_1)
                        T.where(i2_0 * T.int32(32) + i2_1 < T.int32(77))
                        T.reads(rxplaceholder[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(rxplaceholder[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def softmax1(rxplaceholder: T.Buffer((T.int32(16), T.int32(4096), T.int32(4096)), "float32"), T_softmax_norm: T.Buffer((T.int32(16), T.int32(4096), T.int32(4096)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int32(16), T.int32(4096)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int32(16), T.int32(4096)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int32(65536), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for ax0, ax1, ax2_0 in T.grid(T.int32(1), T.int32(1), T.int32(16)):
                for ax2_1 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int32(16), i0_i1_fused // T.int32(4096) + ax0)
                        v_i1 = T.axis.spatial(T.int32(4096), i0_i1_fused % T.int32(4096) + ax1)
                        v_k = T.axis.reduce(T.int32(4096), ax2_0 * T.int32(256) + ax2_1)
                        T.reads(rxplaceholder[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], rxplaceholder[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int32(1), T.int32(1), T.int32(16)):
                for ax2_1 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int32(16), i0_i1_fused // T.int32(4096) + ax0)
                        v_i1 = T.axis.spatial(T.int32(4096), i0_i1_fused % T.int32(4096) + ax1)
                        v_k = T.axis.reduce(T.int32(4096), ax2_0 * T.int32(256) + ax2_1)
                        T.reads(rxplaceholder[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(rxplaceholder[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int32(16)):
                for i2_1 in T.thread_binding(T.int32(256), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int32(16), i0_i1_fused // T.int32(4096))
                        v_i1 = T.axis.spatial(T.int32(4096), i0_i1_fused % T.int32(4096))
                        v_i2 = T.axis.spatial(T.int32(4096), i2_0 * T.int32(256) + i2_1)
                        T.reads(rxplaceholder[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(rxplaceholder[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def softmax2(rxplaceholder: T.Buffer((T.int32(16), T.int32(4096), T.int32(77)), "float32"), T_softmax_norm: T.Buffer((T.int32(16), T.int32(4096), T.int32(77)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int32(16), T.int32(4096)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int32(16), T.int32(4096)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int32(65536), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(16), "pragma_unroll_explicit": T.int32(1)}):
            for ax0, ax1, ax2_0 in T.grid(T.int32(1), T.int32(1), T.int32(10)):
                for ax2_1 in T.thread_binding(T.int32(8), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int32(16), i0_i1_fused // T.int32(4096) + ax0)
                        v_i1 = T.axis.spatial(T.int32(4096), i0_i1_fused % T.int32(4096) + ax1)
                        v_k = T.axis.reduce(T.int32(77), ax2_0 * T.int32(8) + ax2_1)
                        T.where(ax2_0 * T.int32(8) + ax2_1 < T.int32(77))
                        T.reads(rxplaceholder[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], rxplaceholder[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int32(1), T.int32(1), T.int32(10)):
                for ax2_1 in T.thread_binding(T.int32(8), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int32(16), i0_i1_fused // T.int32(4096) + ax0)
                        v_i1 = T.axis.spatial(T.int32(4096), i0_i1_fused % T.int32(4096) + ax1)
                        v_k = T.axis.reduce(T.int32(77), ax2_0 * T.int32(8) + ax2_1)
                        T.where(ax2_0 * T.int32(8) + ax2_1 < T.int32(77))
                        T.reads(rxplaceholder[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(rxplaceholder[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int32(10)):
                for i2_1 in T.thread_binding(T.int32(8), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int32(16), i0_i1_fused // T.int32(4096))
                        v_i1 = T.axis.spatial(T.int32(4096), i0_i1_fused % T.int32(4096))
                        v_i2 = T.axis.spatial(T.int32(77), i2_0 * T.int32(8) + i2_1)
                        T.where(i2_0 * T.int32(8) + i2_1 < T.int32(77))
                        T.reads(rxplaceholder[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(rxplaceholder[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def softmax3(rxplaceholder: T.Buffer((T.int32(16), T.int32(1024), T.int32(1024)), "float32"), T_softmax_norm: T.Buffer((T.int32(16), T.int32(1024), T.int32(1024)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int32(16), T.int32(1024)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int32(16), T.int32(1024)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int32(16384), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(512), "pragma_unroll_explicit": T.int32(1)}):
            for ax0, ax1, ax2_0 in T.grid(T.int32(1), T.int32(1), T.int32(16)):
                for ax2_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int32(16), i0_i1_fused // T.int32(1024) + ax0)
                        v_i1 = T.axis.spatial(T.int32(1024), i0_i1_fused % T.int32(1024) + ax1)
                        v_k = T.axis.reduce(T.int32(1024), ax2_0 * T.int32(64) + ax2_1)
                        T.reads(rxplaceholder[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], rxplaceholder[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int32(1), T.int32(1), T.int32(16)):
                for ax2_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int32(16), i0_i1_fused // T.int32(1024) + ax0)
                        v_i1 = T.axis.spatial(T.int32(1024), i0_i1_fused % T.int32(1024) + ax1)
                        v_k = T.axis.reduce(T.int32(1024), ax2_0 * T.int32(64) + ax2_1)
                        T.reads(rxplaceholder[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(rxplaceholder[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int32(16)):
                for i2_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int32(16), i0_i1_fused // T.int32(1024))
                        v_i1 = T.axis.spatial(T.int32(1024), i0_i1_fused % T.int32(1024))
                        v_i2 = T.axis.spatial(T.int32(1024), i2_0 * T.int32(64) + i2_1)
                        T.reads(rxplaceholder[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(rxplaceholder[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def softmax4(rxplaceholder: T.Buffer((T.int32(16), T.int32(1024), T.int32(77)), "float32"), T_softmax_norm: T.Buffer((T.int32(16), T.int32(1024), T.int32(77)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int32(16), T.int32(1024)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int32(16), T.int32(1024)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int32(16384), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(16), "pragma_unroll_explicit": T.int32(1)}):
            for ax0, ax1, ax2_0 in T.grid(T.int32(1), T.int32(1), T.int32(10)):
                for ax2_1 in T.thread_binding(T.int32(8), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int32(16), i0_i1_fused // T.int32(1024) + ax0)
                        v_i1 = T.axis.spatial(T.int32(1024), i0_i1_fused % T.int32(1024) + ax1)
                        v_k = T.axis.reduce(T.int32(77), ax2_0 * T.int32(8) + ax2_1)
                        T.where(ax2_0 * T.int32(8) + ax2_1 < T.int32(77))
                        T.reads(rxplaceholder[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], rxplaceholder[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int32(1), T.int32(1), T.int32(10)):
                for ax2_1 in T.thread_binding(T.int32(8), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int32(16), i0_i1_fused // T.int32(1024) + ax0)
                        v_i1 = T.axis.spatial(T.int32(1024), i0_i1_fused % T.int32(1024) + ax1)
                        v_k = T.axis.reduce(T.int32(77), ax2_0 * T.int32(8) + ax2_1)
                        T.where(ax2_0 * T.int32(8) + ax2_1 < T.int32(77))
                        T.reads(rxplaceholder[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(rxplaceholder[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int32(10)):
                for i2_1 in T.thread_binding(T.int32(8), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int32(16), i0_i1_fused // T.int32(1024))
                        v_i1 = T.axis.spatial(T.int32(1024), i0_i1_fused % T.int32(1024))
                        v_i2 = T.axis.spatial(T.int32(77), i2_0 * T.int32(8) + i2_1)
                        T.where(i2_0 * T.int32(8) + i2_1 < T.int32(77))
                        T.reads(rxplaceholder[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(rxplaceholder[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def softmax5(rxplaceholder: T.Buffer((T.int32(16), T.int32(256), T.int32(256)), "float32"), T_softmax_norm: T.Buffer((T.int32(16), T.int32(256), T.int32(256)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int32(16), T.int32(256)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int32(16), T.int32(256)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int32(4096), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(64), "pragma_unroll_explicit": T.int32(1)}):
            for ax0, ax1, ax2_0 in T.grid(T.int32(1), T.int32(1), T.int32(8)):
                for ax2_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int32(16), i0_i1_fused // T.int32(256) + ax0)
                        v_i1 = T.axis.spatial(T.int32(256), i0_i1_fused % T.int32(256) + ax1)
                        v_k = T.axis.reduce(T.int32(256), ax2_0 * T.int32(32) + ax2_1)
                        T.reads(rxplaceholder[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], rxplaceholder[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int32(1), T.int32(1), T.int32(8)):
                for ax2_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int32(16), i0_i1_fused // T.int32(256) + ax0)
                        v_i1 = T.axis.spatial(T.int32(256), i0_i1_fused % T.int32(256) + ax1)
                        v_k = T.axis.reduce(T.int32(256), ax2_0 * T.int32(32) + ax2_1)
                        T.reads(rxplaceholder[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(rxplaceholder[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int32(8)):
                for i2_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int32(16), i0_i1_fused // T.int32(256))
                        v_i1 = T.axis.spatial(T.int32(256), i0_i1_fused % T.int32(256))
                        v_i2 = T.axis.spatial(T.int32(256), i2_0 * T.int32(32) + i2_1)
                        T.reads(rxplaceholder[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(rxplaceholder[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def softmax6(rxplaceholder: T.Buffer((T.int32(16), T.int32(256), T.int32(77)), "float32"), T_softmax_norm: T.Buffer((T.int32(16), T.int32(256), T.int32(77)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int32(16), T.int32(256)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int32(16), T.int32(256)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int32(4096), thread="blockIdx.x"):
            for ax0, ax1, ax2_0 in T.grid(T.int32(1), T.int32(1), T.int32(3)):
                for ax2_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int32(16), i0_i1_fused // T.int32(256) + ax0)
                        v_i1 = T.axis.spatial(T.int32(256), i0_i1_fused % T.int32(256) + ax1)
                        v_k = T.axis.reduce(T.int32(77), ax2_0 * T.int32(32) + ax2_1)
                        T.where(ax2_0 * T.int32(32) + ax2_1 < T.int32(77))
                        T.reads(rxplaceholder[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], rxplaceholder[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int32(1), T.int32(1), T.int32(3)):
                for ax2_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int32(16), i0_i1_fused // T.int32(256) + ax0)
                        v_i1 = T.axis.spatial(T.int32(256), i0_i1_fused % T.int32(256) + ax1)
                        v_k = T.axis.reduce(T.int32(77), ax2_0 * T.int32(32) + ax2_1)
                        T.where(ax2_0 * T.int32(32) + ax2_1 < T.int32(77))
                        T.reads(rxplaceholder[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(rxplaceholder[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int32(3)):
                for i2_1 in T.thread_binding(T.int32(32), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int32(16), i0_i1_fused // T.int32(256))
                        v_i1 = T.axis.spatial(T.int32(256), i0_i1_fused % T.int32(256))
                        v_i2 = T.axis.spatial(T.int32(77), i2_0 * T.int32(32) + i2_1)
                        T.where(i2_0 * T.int32(32) + i2_1 < T.int32(77))
                        T.reads(rxplaceholder[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(rxplaceholder[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def softmax7(rxplaceholder: T.Buffer((T.int32(16), T.int32(64), T.int32(64)), "float32"), T_softmax_norm: T.Buffer((T.int32(16), T.int32(64), T.int32(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int32(16), T.int32(64)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int32(16), T.int32(64)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int32(1024), thread="blockIdx.x"):
            for ax0, ax1, ax2_0 in T.grid(T.int32(1), T.int32(1), T.int32(4)):
                for ax2_1 in T.thread_binding(T.int32(16), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int32(16), i0_i1_fused // T.int32(64) + ax0)
                        v_i1 = T.axis.spatial(T.int32(64), i0_i1_fused % T.int32(64) + ax1)
                        v_k = T.axis.reduce(T.int32(64), ax2_0 * T.int32(16) + ax2_1)
                        T.reads(rxplaceholder[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], rxplaceholder[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int32(1), T.int32(1), T.int32(4)):
                for ax2_1 in T.thread_binding(T.int32(16), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int32(16), i0_i1_fused // T.int32(64) + ax0)
                        v_i1 = T.axis.spatial(T.int32(64), i0_i1_fused % T.int32(64) + ax1)
                        v_k = T.axis.reduce(T.int32(64), ax2_0 * T.int32(16) + ax2_1)
                        T.reads(rxplaceholder[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(rxplaceholder[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int32(4)):
                for i2_1 in T.thread_binding(T.int32(16), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int32(16), i0_i1_fused // T.int32(64))
                        v_i1 = T.axis.spatial(T.int32(64), i0_i1_fused % T.int32(64))
                        v_i2 = T.axis.spatial(T.int32(64), i2_0 * T.int32(16) + i2_1)
                        T.reads(rxplaceholder[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(rxplaceholder[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def softmax8(rxplaceholder: T.Buffer((T.int32(16), T.int32(64), T.int32(77)), "float32"), T_softmax_norm: T.Buffer((T.int32(16), T.int32(64), T.int32(77)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.noalias": True})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int32(16), T.int32(64)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int32(16), T.int32(64)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int32(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int32(1024), "pragma_unroll_explicit": T.int32(1)}):
            for ax0, ax1, ax2_0 in T.grid(T.int32(1), T.int32(1), T.int32(2)):
                for ax2_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int32(16), i0_i1_fused // T.int32(64) + ax0)
                        v_i1 = T.axis.spatial(T.int32(64), i0_i1_fused % T.int32(64) + ax1)
                        v_k = T.axis.reduce(T.int32(77), ax2_0 * T.int32(64) + ax2_1)
                        T.where(ax2_0 * T.int32(64) + ax2_1 < T.int32(77))
                        T.reads(rxplaceholder[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], rxplaceholder[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int32(1), T.int32(1), T.int32(2)):
                for ax2_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int32(16), i0_i1_fused // T.int32(64) + ax0)
                        v_i1 = T.axis.spatial(T.int32(64), i0_i1_fused % T.int32(64) + ax1)
                        v_k = T.axis.reduce(T.int32(77), ax2_0 * T.int32(64) + ax2_1)
                        T.where(ax2_0 * T.int32(64) + ax2_1 < T.int32(77))
                        T.reads(rxplaceholder[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(rxplaceholder[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int32(2)):
                for i2_1 in T.thread_binding(T.int32(64), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int32(16), i0_i1_fused // T.int32(64))
                        v_i1 = T.axis.spatial(T.int32(64), i0_i1_fused % T.int32(64))
                        v_i2 = T.axis.spatial(T.int32(77), i2_0 * T.int32(64) + i2_1)
                        T.where(i2_0 * T.int32(64) + i2_1 < T.int32(77))
                        T.reads(rxplaceholder[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(rxplaceholder[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def take(rxplaceholder: T.Buffer((T.int32(49408), T.int32(768)), "float32"), rxplaceholder_1: T.Buffer((T.int32(77),), "int32"), T_take: T.Buffer((T.int32(77), T.int32(768)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 8, "tir.noalias": True})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int32(462), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int32(128), thread="threadIdx.x"):
                with T.block("T_take"):
                    v_ax0 = T.axis.spatial(T.int32(77), (ax0_ax1_fused_0 * T.int32(128) + ax0_ax1_fused_1) // T.int32(768))
                    v_ax1 = T.axis.spatial(T.int32(768), (ax0_ax1_fused_0 * T.int32(128) + ax0_ax1_fused_1) % T.int32(768))
                    T.reads(rxplaceholder[rxplaceholder_1[v_ax0], v_ax1], rxplaceholder_1[v_ax0])
                    T.writes(T_take[v_ax0, v_ax1])
                    T_take[v_ax0, v_ax1] = rxplaceholder[rxplaceholder_1[v_ax0], v_ax1]

    @R.function
    def clip(inp_0: R.Tensor((1, 77), dtype="int32"), params: R.Tuple(R.Tensor((49408, 768), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((77, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"))) -> R.Tuple(R.Tensor((1, 77, 768), dtype="float32")):
        with R.dataflow():
            lv440 = R.call_tir(fused_reshape_cast_reshape1, (inp_0,), out_sinfo=R.Tensor((77,), dtype="int32"))
            lv: R.Tensor((49408, 768), dtype="float32") = params[0]
            lv3 = R.call_tir(take, (lv, lv440), out_sinfo=R.Tensor((77, 768), dtype="float32"))
            lv1: R.Tensor((77, 768), dtype="float32") = params[123]
            lv441 = R.call_tir(fused_reshape2_reshape2_add, (lv3, lv1), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2: R.Tensor((768,), dtype="float32") = params[1]
            lv3_1: R.Tensor((768,), dtype="float32") = params[2]
            lv14 = R.call_tir(layer_norm, (lv441, lv2, lv3_1), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv4: R.Tensor((768, 768), dtype="float32") = params[124]
            lv5: R.Tensor((768,), dtype="float32") = params[3]
            lv442 = R.call_tir(fused_matmul_add1_multiply, (lv14, lv4, lv5), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv6: R.Tensor((768, 768), dtype="float32") = params[125]
            lv7: R.Tensor((768,), dtype="float32") = params[4]
            lv443 = R.call_tir(fused_matmul_add1, (lv14, lv6, lv7), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv444 = R.call_tir(fused_reshape3_transpose1_reshape4_transpose2, (lv443,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv8: R.Tensor((768, 768), dtype="float32") = params[126]
            lv9: R.Tensor((768,), dtype="float32") = params[5]
            lv445 = R.call_tir(fused_matmul_add1, (lv14, lv8, lv9), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv446 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv445,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv447 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv442,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv35 = R.call_tir(matmul1, (lv447, lv444), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv448 = R.call_tir(fused_reshape5_add2_reshape6, (lv35, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv39 = R.call_tir(softmax, (lv448,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv40 = R.call_tir(matmul2, (lv39, lv446), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv449 = R.call_tir(fused_reshape7_transpose3_reshape8, (lv40,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv10: R.Tensor((768, 768), dtype="float32") = params[127]
            lv11: R.Tensor((768,), dtype="float32") = params[6]
            lv450 = R.call_tir(fused_matmul_add1_add, (lv449, lv10, lv11, lv441), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv12: R.Tensor((768,), dtype="float32") = params[7]
            lv13: R.Tensor((768,), dtype="float32") = params[8]
            lv48 = R.call_tir(layer_norm, (lv450, lv12, lv13), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv14_1: R.Tensor((768, 3072), dtype="float32") = params[128]
            lv15: R.Tensor((3072,), dtype="float32") = params[9]
            lv451 = R.call_tir(fused_matmul3_add3_multiply1_sigmoid_multiply2, (lv48, lv14_1, lv15), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv16: R.Tensor((3072, 768), dtype="float32") = params[129]
            lv17: R.Tensor((768,), dtype="float32") = params[10]
            lv452 = R.call_tir(fused_matmul4_add1_add, (lv451, lv16, lv17, lv450), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv18: R.Tensor((768,), dtype="float32") = params[11]
            lv19: R.Tensor((768,), dtype="float32") = params[12]
            lv59 = R.call_tir(layer_norm, (lv452, lv18, lv19), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv20: R.Tensor((768, 768), dtype="float32") = params[130]
            lv21: R.Tensor((768,), dtype="float32") = params[13]
            lv453 = R.call_tir(fused_matmul_add1_multiply, (lv59, lv20, lv21), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv22: R.Tensor((768, 768), dtype="float32") = params[131]
            lv23: R.Tensor((768,), dtype="float32") = params[14]
            lv454 = R.call_tir(fused_matmul_add1, (lv59, lv22, lv23), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv455 = R.call_tir(fused_reshape3_transpose1_reshape4_transpose2, (lv454,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv24: R.Tensor((768, 768), dtype="float32") = params[132]
            lv25: R.Tensor((768,), dtype="float32") = params[15]
            lv456 = R.call_tir(fused_matmul_add1, (lv59, lv24, lv25), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv457 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv456,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv458 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv453,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv80 = R.call_tir(matmul1, (lv458, lv455), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv459 = R.call_tir(fused_reshape5_add2_reshape6, (lv80, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv84 = R.call_tir(softmax, (lv459,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv85 = R.call_tir(matmul2, (lv84, lv457), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv460 = R.call_tir(fused_reshape7_transpose3_reshape8, (lv85,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv26: R.Tensor((768, 768), dtype="float32") = params[133]
            lv27: R.Tensor((768,), dtype="float32") = params[16]
            lv461 = R.call_tir(fused_matmul_add1_add, (lv460, lv26, lv27, lv452), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv28: R.Tensor((768,), dtype="float32") = params[17]
            lv29: R.Tensor((768,), dtype="float32") = params[18]
            lv93 = R.call_tir(layer_norm, (lv461, lv28, lv29), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv30: R.Tensor((768, 3072), dtype="float32") = params[134]
            lv31: R.Tensor((3072,), dtype="float32") = params[19]
            lv462 = R.call_tir(fused_matmul3_add3_multiply1_sigmoid_multiply2, (lv93, lv30, lv31), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv32: R.Tensor((3072, 768), dtype="float32") = params[135]
            lv33: R.Tensor((768,), dtype="float32") = params[20]
            lv463 = R.call_tir(fused_matmul4_add1_add, (lv462, lv32, lv33, lv461), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv34: R.Tensor((768,), dtype="float32") = params[21]
            lv35_1: R.Tensor((768,), dtype="float32") = params[22]
            lv104 = R.call_tir(layer_norm, (lv463, lv34, lv35_1), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv36: R.Tensor((768, 768), dtype="float32") = params[136]
            lv37: R.Tensor((768,), dtype="float32") = params[23]
            lv464 = R.call_tir(fused_matmul_add1_multiply, (lv104, lv36, lv37), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv38: R.Tensor((768, 768), dtype="float32") = params[137]
            lv39_1: R.Tensor((768,), dtype="float32") = params[24]
            lv465 = R.call_tir(fused_matmul_add1, (lv104, lv38, lv39_1), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv466 = R.call_tir(fused_reshape3_transpose1_reshape4_transpose2, (lv465,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv40_1: R.Tensor((768, 768), dtype="float32") = params[138]
            lv41: R.Tensor((768,), dtype="float32") = params[25]
            lv467 = R.call_tir(fused_matmul_add1, (lv104, lv40_1, lv41), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv468 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv467,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv469 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv464,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv125 = R.call_tir(matmul1, (lv469, lv466), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv470 = R.call_tir(fused_reshape5_add2_reshape6, (lv125, metadata["relax.expr.Constant"][2]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv129 = R.call_tir(softmax, (lv470,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv130 = R.call_tir(matmul2, (lv129, lv468), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv471 = R.call_tir(fused_reshape7_transpose3_reshape8, (lv130,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv42: R.Tensor((768, 768), dtype="float32") = params[139]
            lv43: R.Tensor((768,), dtype="float32") = params[26]
            lv472 = R.call_tir(fused_matmul_add1_add, (lv471, lv42, lv43, lv463), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv44: R.Tensor((768,), dtype="float32") = params[27]
            lv45: R.Tensor((768,), dtype="float32") = params[28]
            lv138 = R.call_tir(layer_norm, (lv472, lv44, lv45), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv46: R.Tensor((768, 3072), dtype="float32") = params[140]
            lv47: R.Tensor((3072,), dtype="float32") = params[29]
            lv473 = R.call_tir(fused_matmul3_add3_multiply1_sigmoid_multiply2, (lv138, lv46, lv47), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv48_1: R.Tensor((3072, 768), dtype="float32") = params[141]
            lv49: R.Tensor((768,), dtype="float32") = params[30]
            lv474 = R.call_tir(fused_matmul4_add1_add, (lv473, lv48_1, lv49, lv472), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv50: R.Tensor((768,), dtype="float32") = params[31]
            lv51: R.Tensor((768,), dtype="float32") = params[32]
            lv149 = R.call_tir(layer_norm, (lv474, lv50, lv51), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv52: R.Tensor((768, 768), dtype="float32") = params[142]
            lv53: R.Tensor((768,), dtype="float32") = params[33]
            lv475 = R.call_tir(fused_matmul_add1_multiply, (lv149, lv52, lv53), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv54: R.Tensor((768, 768), dtype="float32") = params[143]
            lv55: R.Tensor((768,), dtype="float32") = params[34]
            lv476 = R.call_tir(fused_matmul_add1, (lv149, lv54, lv55), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv477 = R.call_tir(fused_reshape3_transpose1_reshape4_transpose2, (lv476,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv56: R.Tensor((768, 768), dtype="float32") = params[144]
            lv57: R.Tensor((768,), dtype="float32") = params[35]
            lv478 = R.call_tir(fused_matmul_add1, (lv149, lv56, lv57), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv479 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv478,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv480 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv475,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv170 = R.call_tir(matmul1, (lv480, lv477), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv481 = R.call_tir(fused_reshape5_add2_reshape6, (lv170, metadata["relax.expr.Constant"][3]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv174 = R.call_tir(softmax, (lv481,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv175 = R.call_tir(matmul2, (lv174, lv479), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv482 = R.call_tir(fused_reshape7_transpose3_reshape8, (lv175,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv58: R.Tensor((768, 768), dtype="float32") = params[145]
            lv59_1: R.Tensor((768,), dtype="float32") = params[36]
            lv483 = R.call_tir(fused_matmul_add1_add, (lv482, lv58, lv59_1, lv474), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv60: R.Tensor((768,), dtype="float32") = params[37]
            lv61: R.Tensor((768,), dtype="float32") = params[38]
            lv183 = R.call_tir(layer_norm, (lv483, lv60, lv61), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv62: R.Tensor((768, 3072), dtype="float32") = params[146]
            lv63: R.Tensor((3072,), dtype="float32") = params[39]
            lv484 = R.call_tir(fused_matmul3_add3_multiply1_sigmoid_multiply2, (lv183, lv62, lv63), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv64: R.Tensor((3072, 768), dtype="float32") = params[147]
            lv65: R.Tensor((768,), dtype="float32") = params[40]
            lv485 = R.call_tir(fused_matmul4_add1_add, (lv484, lv64, lv65, lv483), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv66: R.Tensor((768,), dtype="float32") = params[41]
            lv67: R.Tensor((768,), dtype="float32") = params[42]
            lv194 = R.call_tir(layer_norm, (lv485, lv66, lv67), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv68: R.Tensor((768, 768), dtype="float32") = params[148]
            lv69: R.Tensor((768,), dtype="float32") = params[43]
            lv486 = R.call_tir(fused_matmul_add1_multiply, (lv194, lv68, lv69), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv70: R.Tensor((768, 768), dtype="float32") = params[149]
            lv71: R.Tensor((768,), dtype="float32") = params[44]
            lv487 = R.call_tir(fused_matmul_add1, (lv194, lv70, lv71), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv488 = R.call_tir(fused_reshape3_transpose1_reshape4_transpose2, (lv487,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv72: R.Tensor((768, 768), dtype="float32") = params[150]
            lv73: R.Tensor((768,), dtype="float32") = params[45]
            lv489 = R.call_tir(fused_matmul_add1, (lv194, lv72, lv73), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv490 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv489,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv491 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv486,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv215 = R.call_tir(matmul1, (lv491, lv488), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv492 = R.call_tir(fused_reshape5_add2_reshape6, (lv215, metadata["relax.expr.Constant"][4]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv219 = R.call_tir(softmax, (lv492,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv220 = R.call_tir(matmul2, (lv219, lv490), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv493 = R.call_tir(fused_reshape7_transpose3_reshape8, (lv220,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv74: R.Tensor((768, 768), dtype="float32") = params[151]
            lv75: R.Tensor((768,), dtype="float32") = params[46]
            lv494 = R.call_tir(fused_matmul_add1_add, (lv493, lv74, lv75, lv485), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv76: R.Tensor((768,), dtype="float32") = params[47]
            lv77: R.Tensor((768,), dtype="float32") = params[48]
            lv228 = R.call_tir(layer_norm, (lv494, lv76, lv77), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv78: R.Tensor((768, 3072), dtype="float32") = params[152]
            lv79: R.Tensor((3072,), dtype="float32") = params[49]
            lv495 = R.call_tir(fused_matmul3_add3_multiply1_sigmoid_multiply2, (lv228, lv78, lv79), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv80_1: R.Tensor((3072, 768), dtype="float32") = params[153]
            lv81: R.Tensor((768,), dtype="float32") = params[50]
            lv496 = R.call_tir(fused_matmul4_add1_add, (lv495, lv80_1, lv81, lv494), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv82: R.Tensor((768,), dtype="float32") = params[51]
            lv83: R.Tensor((768,), dtype="float32") = params[52]
            lv239 = R.call_tir(layer_norm, (lv496, lv82, lv83), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv84_1: R.Tensor((768, 768), dtype="float32") = params[154]
            lv85_1: R.Tensor((768,), dtype="float32") = params[53]
            lv497 = R.call_tir(fused_matmul_add1_multiply, (lv239, lv84_1, lv85_1), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv86: R.Tensor((768, 768), dtype="float32") = params[155]
            lv87: R.Tensor((768,), dtype="float32") = params[54]
            lv498 = R.call_tir(fused_matmul_add1, (lv239, lv86, lv87), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv499 = R.call_tir(fused_reshape3_transpose1_reshape4_transpose2, (lv498,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv88: R.Tensor((768, 768), dtype="float32") = params[156]
            lv89: R.Tensor((768,), dtype="float32") = params[55]
            lv500 = R.call_tir(fused_matmul_add1, (lv239, lv88, lv89), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv501 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv500,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv502 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv497,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv260 = R.call_tir(matmul1, (lv502, lv499), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv503 = R.call_tir(fused_reshape5_add2_reshape6, (lv260, metadata["relax.expr.Constant"][5]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv264 = R.call_tir(softmax, (lv503,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv265 = R.call_tir(matmul2, (lv264, lv501), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv504 = R.call_tir(fused_reshape7_transpose3_reshape8, (lv265,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv90: R.Tensor((768, 768), dtype="float32") = params[157]
            lv91: R.Tensor((768,), dtype="float32") = params[56]
            lv505 = R.call_tir(fused_matmul_add1_add, (lv504, lv90, lv91, lv496), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv92: R.Tensor((768,), dtype="float32") = params[57]
            lv93_1: R.Tensor((768,), dtype="float32") = params[58]
            lv273 = R.call_tir(layer_norm, (lv505, lv92, lv93_1), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv94: R.Tensor((768, 3072), dtype="float32") = params[158]
            lv95: R.Tensor((3072,), dtype="float32") = params[59]
            lv506 = R.call_tir(fused_matmul3_add3_multiply1_sigmoid_multiply2, (lv273, lv94, lv95), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv96: R.Tensor((3072, 768), dtype="float32") = params[159]
            lv97: R.Tensor((768,), dtype="float32") = params[60]
            lv507 = R.call_tir(fused_matmul4_add1_add, (lv506, lv96, lv97, lv505), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv98: R.Tensor((768,), dtype="float32") = params[61]
            lv99: R.Tensor((768,), dtype="float32") = params[62]
            lv284 = R.call_tir(layer_norm, (lv507, lv98, lv99), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv100: R.Tensor((768, 768), dtype="float32") = params[160]
            lv101: R.Tensor((768,), dtype="float32") = params[63]
            lv508 = R.call_tir(fused_matmul_add1_multiply, (lv284, lv100, lv101), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv102: R.Tensor((768, 768), dtype="float32") = params[161]
            lv103: R.Tensor((768,), dtype="float32") = params[64]
            lv509 = R.call_tir(fused_matmul_add1, (lv284, lv102, lv103), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv510 = R.call_tir(fused_reshape3_transpose1_reshape4_transpose2, (lv509,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv104_1: R.Tensor((768, 768), dtype="float32") = params[162]
            lv105: R.Tensor((768,), dtype="float32") = params[65]
            lv511 = R.call_tir(fused_matmul_add1, (lv284, lv104_1, lv105), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv512 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv511,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv513 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv508,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv305 = R.call_tir(matmul1, (lv513, lv510), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv514 = R.call_tir(fused_reshape5_add2_reshape6, (lv305, metadata["relax.expr.Constant"][6]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv309 = R.call_tir(softmax, (lv514,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv310 = R.call_tir(matmul2, (lv309, lv512), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv515 = R.call_tir(fused_reshape7_transpose3_reshape8, (lv310,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv106: R.Tensor((768, 768), dtype="float32") = params[163]
            lv107: R.Tensor((768,), dtype="float32") = params[66]
            lv516 = R.call_tir(fused_matmul_add1_add, (lv515, lv106, lv107, lv507), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv108: R.Tensor((768,), dtype="float32") = params[67]
            lv109: R.Tensor((768,), dtype="float32") = params[68]
            lv318 = R.call_tir(layer_norm, (lv516, lv108, lv109), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv110: R.Tensor((768, 3072), dtype="float32") = params[164]
            lv111: R.Tensor((3072,), dtype="float32") = params[69]
            lv517 = R.call_tir(fused_matmul3_add3_multiply1_sigmoid_multiply2, (lv318, lv110, lv111), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv112: R.Tensor((3072, 768), dtype="float32") = params[165]
            lv113: R.Tensor((768,), dtype="float32") = params[70]
            lv518 = R.call_tir(fused_matmul4_add1_add, (lv517, lv112, lv113, lv516), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv114: R.Tensor((768,), dtype="float32") = params[71]
            lv115: R.Tensor((768,), dtype="float32") = params[72]
            lv329 = R.call_tir(layer_norm, (lv518, lv114, lv115), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv116: R.Tensor((768, 768), dtype="float32") = params[166]
            lv117: R.Tensor((768,), dtype="float32") = params[73]
            lv519 = R.call_tir(fused_matmul_add1_multiply, (lv329, lv116, lv117), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv118: R.Tensor((768, 768), dtype="float32") = params[167]
            lv119: R.Tensor((768,), dtype="float32") = params[74]
            lv520 = R.call_tir(fused_matmul_add1, (lv329, lv118, lv119), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv521 = R.call_tir(fused_reshape3_transpose1_reshape4_transpose2, (lv520,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv120: R.Tensor((768, 768), dtype="float32") = params[168]
            lv121: R.Tensor((768,), dtype="float32") = params[75]
            lv522 = R.call_tir(fused_matmul_add1, (lv329, lv120, lv121), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv523 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv522,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv524 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv519,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv350 = R.call_tir(matmul1, (lv524, lv521), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv525 = R.call_tir(fused_reshape5_add2_reshape6, (lv350, metadata["relax.expr.Constant"][7]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv354 = R.call_tir(softmax, (lv525,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv355 = R.call_tir(matmul2, (lv354, lv523), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv526 = R.call_tir(fused_reshape7_transpose3_reshape8, (lv355,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv122: R.Tensor((768, 768), dtype="float32") = params[169]
            lv123: R.Tensor((768,), dtype="float32") = params[76]
            lv527 = R.call_tir(fused_matmul_add1_add, (lv526, lv122, lv123, lv518), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv124: R.Tensor((768,), dtype="float32") = params[77]
            lv125_1: R.Tensor((768,), dtype="float32") = params[78]
            lv363 = R.call_tir(layer_norm, (lv527, lv124, lv125_1), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv126: R.Tensor((768, 3072), dtype="float32") = params[170]
            lv127: R.Tensor((3072,), dtype="float32") = params[79]
            lv528 = R.call_tir(fused_matmul3_add3_multiply1_sigmoid_multiply2, (lv363, lv126, lv127), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv128: R.Tensor((3072, 768), dtype="float32") = params[171]
            lv129_1: R.Tensor((768,), dtype="float32") = params[80]
            lv529 = R.call_tir(fused_matmul4_add1_add, (lv528, lv128, lv129_1, lv527), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv130_1: R.Tensor((768,), dtype="float32") = params[81]
            lv131: R.Tensor((768,), dtype="float32") = params[82]
            lv374 = R.call_tir(layer_norm, (lv529, lv130_1, lv131), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv132: R.Tensor((768, 768), dtype="float32") = params[172]
            lv133: R.Tensor((768,), dtype="float32") = params[83]
            lv530 = R.call_tir(fused_matmul_add1_multiply, (lv374, lv132, lv133), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv134: R.Tensor((768, 768), dtype="float32") = params[173]
            lv135: R.Tensor((768,), dtype="float32") = params[84]
            lv531 = R.call_tir(fused_matmul_add1, (lv374, lv134, lv135), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv532 = R.call_tir(fused_reshape3_transpose1_reshape4_transpose2, (lv531,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv136: R.Tensor((768, 768), dtype="float32") = params[174]
            lv137: R.Tensor((768,), dtype="float32") = params[85]
            lv533 = R.call_tir(fused_matmul_add1, (lv374, lv136, lv137), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv534 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv533,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv535 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv530,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv395 = R.call_tir(matmul1, (lv535, lv532), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv536 = R.call_tir(fused_reshape5_add2_reshape6, (lv395, metadata["relax.expr.Constant"][8]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv399 = R.call_tir(softmax, (lv536,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv400 = R.call_tir(matmul2, (lv399, lv534), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv537 = R.call_tir(fused_reshape7_transpose3_reshape8, (lv400,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv138_1: R.Tensor((768, 768), dtype="float32") = params[175]
            lv139: R.Tensor((768,), dtype="float32") = params[86]
            lv538 = R.call_tir(fused_matmul_add1_add, (lv537, lv138_1, lv139, lv529), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv140: R.Tensor((768,), dtype="float32") = params[87]
            lv141: R.Tensor((768,), dtype="float32") = params[88]
            lv408 = R.call_tir(layer_norm, (lv538, lv140, lv141), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv142: R.Tensor((768, 3072), dtype="float32") = params[176]
            lv143: R.Tensor((3072,), dtype="float32") = params[89]
            lv539 = R.call_tir(fused_matmul3_add3_multiply1_sigmoid_multiply2, (lv408, lv142, lv143), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv144: R.Tensor((3072, 768), dtype="float32") = params[177]
            lv145: R.Tensor((768,), dtype="float32") = params[90]
            lv540 = R.call_tir(fused_matmul4_add1_add, (lv539, lv144, lv145, lv538), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv146: R.Tensor((768,), dtype="float32") = params[91]
            lv147: R.Tensor((768,), dtype="float32") = params[92]
            lv419 = R.call_tir(layer_norm, (lv540, lv146, lv147), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv148: R.Tensor((768, 768), dtype="float32") = params[178]
            lv149_1: R.Tensor((768,), dtype="float32") = params[93]
            lv541 = R.call_tir(fused_matmul_add1_multiply, (lv419, lv148, lv149_1), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv150: R.Tensor((768, 768), dtype="float32") = params[179]
            lv151: R.Tensor((768,), dtype="float32") = params[94]
            lv542 = R.call_tir(fused_matmul_add1, (lv419, lv150, lv151), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv543 = R.call_tir(fused_reshape3_transpose1_reshape4_transpose2, (lv542,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv152: R.Tensor((768, 768), dtype="float32") = params[180]
            lv153: R.Tensor((768,), dtype="float32") = params[95]
            lv544 = R.call_tir(fused_matmul_add1, (lv419, lv152, lv153), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv545 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv544,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv546 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv541,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv440_1 = R.call_tir(matmul1, (lv546, lv543), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv547 = R.call_tir(fused_reshape5_add2_reshape6, (lv440_1, metadata["relax.expr.Constant"][9]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv444_1 = R.call_tir(softmax, (lv547,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv445_1 = R.call_tir(matmul2, (lv444_1, lv545), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv548 = R.call_tir(fused_reshape7_transpose3_reshape8, (lv445_1,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv154: R.Tensor((768, 768), dtype="float32") = params[181]
            lv155: R.Tensor((768,), dtype="float32") = params[96]
            lv549 = R.call_tir(fused_matmul_add1_add, (lv548, lv154, lv155, lv540), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv156: R.Tensor((768,), dtype="float32") = params[97]
            lv157: R.Tensor((768,), dtype="float32") = params[98]
            lv453_1 = R.call_tir(layer_norm, (lv549, lv156, lv157), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv158: R.Tensor((768, 3072), dtype="float32") = params[182]
            lv159: R.Tensor((3072,), dtype="float32") = params[99]
            lv550 = R.call_tir(fused_matmul3_add3_multiply1_sigmoid_multiply2, (lv453_1, lv158, lv159), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv160: R.Tensor((3072, 768), dtype="float32") = params[183]
            lv161: R.Tensor((768,), dtype="float32") = params[100]
            lv551 = R.call_tir(fused_matmul4_add1_add, (lv550, lv160, lv161, lv549), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv162: R.Tensor((768,), dtype="float32") = params[101]
            lv163: R.Tensor((768,), dtype="float32") = params[102]
            lv464_1 = R.call_tir(layer_norm, (lv551, lv162, lv163), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv164: R.Tensor((768, 768), dtype="float32") = params[184]
            lv165: R.Tensor((768,), dtype="float32") = params[103]
            lv552 = R.call_tir(fused_matmul_add1_multiply, (lv464_1, lv164, lv165), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv166: R.Tensor((768, 768), dtype="float32") = params[185]
            lv167: R.Tensor((768,), dtype="float32") = params[104]
            lv553 = R.call_tir(fused_matmul_add1, (lv464_1, lv166, lv167), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv554 = R.call_tir(fused_reshape3_transpose1_reshape4_transpose2, (lv553,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv168: R.Tensor((768, 768), dtype="float32") = params[186]
            lv169: R.Tensor((768,), dtype="float32") = params[105]
            lv555 = R.call_tir(fused_matmul_add1, (lv464_1, lv168, lv169), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv556 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv555,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv557 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv552,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv485_1 = R.call_tir(matmul1, (lv557, lv554), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv558 = R.call_tir(fused_reshape5_add2_reshape6, (lv485_1, metadata["relax.expr.Constant"][10]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv489_1 = R.call_tir(softmax, (lv558,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv490_1 = R.call_tir(matmul2, (lv489_1, lv556), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv559 = R.call_tir(fused_reshape7_transpose3_reshape8, (lv490_1,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv170_1: R.Tensor((768, 768), dtype="float32") = params[187]
            lv171: R.Tensor((768,), dtype="float32") = params[106]
            lv560 = R.call_tir(fused_matmul_add1_add, (lv559, lv170_1, lv171, lv551), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv172: R.Tensor((768,), dtype="float32") = params[107]
            lv173: R.Tensor((768,), dtype="float32") = params[108]
            lv498_1 = R.call_tir(layer_norm, (lv560, lv172, lv173), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv174_1: R.Tensor((768, 3072), dtype="float32") = params[188]
            lv175_1: R.Tensor((3072,), dtype="float32") = params[109]
            lv561 = R.call_tir(fused_matmul3_add3_multiply1_sigmoid_multiply2, (lv498_1, lv174_1, lv175_1), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv176: R.Tensor((3072, 768), dtype="float32") = params[189]
            lv177: R.Tensor((768,), dtype="float32") = params[110]
            lv562 = R.call_tir(fused_matmul4_add1_add, (lv561, lv176, lv177, lv560), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv178: R.Tensor((768,), dtype="float32") = params[111]
            lv179: R.Tensor((768,), dtype="float32") = params[112]
            lv509_1 = R.call_tir(layer_norm, (lv562, lv178, lv179), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv180: R.Tensor((768, 768), dtype="float32") = params[190]
            lv181: R.Tensor((768,), dtype="float32") = params[113]
            lv563 = R.call_tir(fused_matmul_add1_multiply, (lv509_1, lv180, lv181), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv182: R.Tensor((768, 768), dtype="float32") = params[191]
            lv183_1: R.Tensor((768,), dtype="float32") = params[114]
            lv564 = R.call_tir(fused_matmul_add1, (lv509_1, lv182, lv183_1), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv565 = R.call_tir(fused_reshape3_transpose1_reshape4_transpose2, (lv564,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv184: R.Tensor((768, 768), dtype="float32") = params[192]
            lv185: R.Tensor((768,), dtype="float32") = params[115]
            lv566 = R.call_tir(fused_matmul_add1, (lv509_1, lv184, lv185), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv567 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv566,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv568 = R.call_tir(fused_reshape3_transpose1_reshape4, (lv563,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv530_1 = R.call_tir(matmul1, (lv568, lv565), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv569 = R.call_tir(fused_reshape5_add2_reshape6, (lv530_1, metadata["relax.expr.Constant"][11]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv534_1 = R.call_tir(softmax, (lv569,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv535_1 = R.call_tir(matmul2, (lv534_1, lv567), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv570 = R.call_tir(fused_reshape7_transpose3_reshape8, (lv535_1,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv186: R.Tensor((768, 768), dtype="float32") = params[193]
            lv187: R.Tensor((768,), dtype="float32") = params[116]
            lv571 = R.call_tir(fused_matmul_add1_add, (lv570, lv186, lv187, lv562), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv188: R.Tensor((768,), dtype="float32") = params[117]
            lv189: R.Tensor((768,), dtype="float32") = params[118]
            lv543_1 = R.call_tir(layer_norm, (lv571, lv188, lv189), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv190: R.Tensor((768, 3072), dtype="float32") = params[194]
            lv191: R.Tensor((3072,), dtype="float32") = params[119]
            lv572 = R.call_tir(fused_matmul3_add3_multiply1_sigmoid_multiply2, (lv543_1, lv190, lv191), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv192: R.Tensor((3072, 768), dtype="float32") = params[195]
            lv193: R.Tensor((768,), dtype="float32") = params[120]
            lv573 = R.call_tir(fused_matmul4_add1_add, (lv572, lv192, lv193, lv571), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv194_1: R.Tensor((768,), dtype="float32") = params[121]
            lv195: R.Tensor((768,), dtype="float32") = params[122]
            lv554_1 = R.call_tir(layer_norm, (lv573, lv194_1, lv195), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            gv: R.Tuple(R.Tensor((1, 77, 768), dtype="float32")) = (lv554_1,)
            R.output(gv)
        return gv

    @R.function
    def unet(inp_0: R.Tensor((2, 4, 64, 64), dtype="float32"), inp_1: R.Tensor((1,), dtype="int32"), inp_2: R.Tensor((2, 77, 768), dtype="float32"), params: R.Tuple(R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((320, 4, 3, 3), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 3, 3), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 3, 3), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 3, 3), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 3, 3), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 1, 1), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 1, 1), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 1, 1), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 1, 1), dtype="float32"), R.Tensor((320, 320, 3, 3), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((640, 320, 3, 3), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 3, 3), dtype="float32"), R.Tensor((640, 320, 1, 1), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 3, 3), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 3, 3), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 1, 1), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 1, 1), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 1, 1), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 1, 1), dtype="float32"), R.Tensor((640, 640, 3, 3), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((1280, 640, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280, 640, 1, 1), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((1280, 2560, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280, 2560, 1, 1), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((1280, 2560, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280, 2560, 1, 1), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((1280, 2560, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280, 2560, 1, 1), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((1280, 2560, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280, 2560, 1, 1), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((1280, 2560, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280, 2560, 1, 1), dtype="float32"), R.Tensor((1920,), dtype="float32"), R.Tensor((1920,), dtype="float32"), R.Tensor((1280, 1920, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280, 1920, 1, 1), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1920,), dtype="float32"), R.Tensor((1920,), dtype="float32"), R.Tensor((640, 1920, 3, 3), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 3, 3), dtype="float32"), R.Tensor((640, 1920, 1, 1), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((640, 1280, 3, 3), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 3, 3), dtype="float32"), R.Tensor((640, 1280, 1, 1), dtype="float32"), R.Tensor((960,), dtype="float32"), R.Tensor((960,), dtype="float32"), R.Tensor((640, 960, 3, 3), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 3, 3), dtype="float32"), R.Tensor((640, 960, 1, 1), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 1, 1), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 1, 1), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 1, 1), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 1, 1), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 1, 1), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 1, 1), dtype="float32"), R.Tensor((640, 640, 3, 3), dtype="float32"), R.Tensor((960,), dtype="float32"), R.Tensor((960,), dtype="float32"), R.Tensor((320, 960, 3, 3), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 3, 3), dtype="float32"), R.Tensor((320, 960, 1, 1), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((320, 640, 3, 3), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 3, 3), dtype="float32"), R.Tensor((320, 640, 1, 1), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((320, 640, 3, 3), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 3, 3), dtype="float32"), R.Tensor((320, 640, 1, 1), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 1, 1), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 1, 1), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 1, 1), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 1, 1), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 1, 1), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 1, 1), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((4, 320, 3, 3), dtype="float32"), R.Tensor((320, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1280, 320), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((768, 320), dtype="float32"), R.Tensor((768, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 1280), dtype="float32"), R.Tensor((320, 1280), dtype="float32"), R.Tensor((1280, 320), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1280, 320), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((768, 320), dtype="float32"), R.Tensor((768, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 1280), dtype="float32"), R.Tensor((320, 1280), dtype="float32"), R.Tensor((1280, 320), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1280, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((768, 640), dtype="float32"), R.Tensor((768, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 2560), dtype="float32"), R.Tensor((640, 2560), dtype="float32"), R.Tensor((2560, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1280, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((768, 640), dtype="float32"), R.Tensor((768, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 2560), dtype="float32"), R.Tensor((640, 2560), dtype="float32"), R.Tensor((2560, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1280, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((768, 640), dtype="float32"), R.Tensor((768, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 2560), dtype="float32"), R.Tensor((640, 2560), dtype="float32"), R.Tensor((2560, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1280, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((768, 640), dtype="float32"), R.Tensor((768, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 2560), dtype="float32"), R.Tensor((640, 2560), dtype="float32"), R.Tensor((2560, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1280, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((768, 640), dtype="float32"), R.Tensor((768, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 2560), dtype="float32"), R.Tensor((640, 2560), dtype="float32"), R.Tensor((2560, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1280, 320), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((768, 320), dtype="float32"), R.Tensor((768, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 1280), dtype="float32"), R.Tensor((320, 1280), dtype="float32"), R.Tensor((1280, 320), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1280, 320), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((768, 320), dtype="float32"), R.Tensor((768, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 1280), dtype="float32"), R.Tensor((320, 1280), dtype="float32"), R.Tensor((1280, 320), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1280, 320), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((768, 320), dtype="float32"), R.Tensor((768, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 1280), dtype="float32"), R.Tensor((320, 1280), dtype="float32"), R.Tensor((1280, 320), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 4, 1, 1), dtype="float32"))) -> R.Tensor((2, 4, 64, 64), dtype="float32"):
        with R.dataflow():
            lv = R.call_tir(fused_broadcast_to_strided_slice_reshape9_cast1_multiply3_multiply4_sin_cos_concatenate_strided_slice1_reshape10_strided_slice2_reshape10_concatenate, (inp_1, metadata["relax.expr.Constant"][12]), out_sinfo=R.Tensor((2, 320), dtype="float32"))
            lv196: R.Tensor((320, 1280), dtype="float32") = params[420]
            lv197: R.Tensor((1280,), dtype="float32") = params[0]
            lv1 = R.call_tir(fused_matmul5_add4_silu, (lv, lv196, lv197), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv198: R.Tensor((1280, 1280), dtype="float32") = params[421]
            lv199: R.Tensor((1280,), dtype="float32") = params[1]
            lv2 = R.call_tir(fused_matmul6_add4, (lv1, lv198, lv199), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv200: R.Tensor((320, 4, 3, 3), dtype="float32") = params[2]
            lv201: R.Tensor((1, 320, 1, 1), dtype="float32") = params[422]
            lv3 = R.call_tir(fused_conv2d_add5, (inp_0, lv200, lv201), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv202: R.Tensor((320,), dtype="float32") = params[3]
            lv203: R.Tensor((320,), dtype="float32") = params[4]
            lv4 = R.call_tir(fused_group_norm_silu1, (lv3, lv202, lv203), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv29 = R.call_tir(silu, (lv2,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv204: R.Tensor((1280, 320), dtype="float32") = params[424]
            lv205: R.Tensor((320,), dtype="float32") = params[6]
            lv5 = R.call_tir(fused_matmul7_add6_strided_slice3, (lv29, lv204, lv205), out_sinfo=R.Tensor((2, 320), dtype="float32"))
            lv34 = R.call_tir(reshape12, (lv5,), out_sinfo=R.Tensor((2, 320, 1, 1), dtype="float32"))
            lv206: R.Tensor((320, 320, 3, 3), dtype="float32") = params[5]
            lv207: R.Tensor((1, 320, 1, 1), dtype="float32") = params[423]
            lv6 = R.call_tir(fused_conv2d1_add5_add7, (lv4, lv206, lv207, lv34), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv208: R.Tensor((320,), dtype="float32") = params[7]
            lv209: R.Tensor((320,), dtype="float32") = params[8]
            lv7 = R.call_tir(fused_group_norm_silu1, (lv6, lv208, lv209), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv210: R.Tensor((320, 320, 3, 3), dtype="float32") = params[9]
            lv211: R.Tensor((1, 320, 1, 1), dtype="float32") = params[425]
            lv8 = R.call_tir(fused_conv2d1_add5_add8_divide, (lv7, lv210, lv211, lv3), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv212: R.Tensor((320,), dtype="float32") = params[17]
            lv213: R.Tensor((320,), dtype="float32") = params[18]
            lv43 = R.call_tir(group_norm1, (lv8, lv212, lv213), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv214: R.Tensor((320, 320, 1, 1), dtype="float32") = params[19]
            lv215: R.Tensor((1, 320, 1, 1), dtype="float32") = params[426]
            lv9 = R.call_tir(fused_conv2d2_add5, (lv43, lv214, lv215), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv10 = R.call_tir(fused_transpose9_reshape13, (lv9,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv216: R.Tensor((320,), dtype="float32") = params[20]
            lv217: R.Tensor((320,), dtype="float32") = params[21]
            lv49 = R.call_tir(layer_norm1, (lv10, lv216, lv217), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv218: R.Tensor((320, 320), dtype="float32") = params[427]
            lv51 = R.call_tir(matmul8, (lv49, lv218), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv219: R.Tensor((320, 320), dtype="float32") = params[428]
            lv53 = R.call_tir(matmul8, (lv49, lv219), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv220: R.Tensor((320, 320), dtype="float32") = params[429]
            lv55 = R.call_tir(matmul8, (lv49, lv220), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv11 = R.call_tir(fused_reshape14_transpose11_reshape15, (lv51,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv12 = R.call_tir(fused_reshape14_transpose11_reshape15_transpose12, (lv53,), out_sinfo=R.Tensor((16, 40, 4096), dtype="float32"))
            lv13 = R.call_tir(fused_reshape14_transpose11_reshape15, (lv55,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv14 = R.call_tir(fused_matmul9_multiply5, (lv11, lv12), out_sinfo=R.Tensor((16, 4096, 4096), dtype="float32"))
            lv68 = R.call_tir(softmax1, (lv14,), out_sinfo=R.Tensor((16, 4096, 4096), dtype="float32"))
            lv69 = R.call_tir(matmul10, (lv68, lv13), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv15 = R.call_tir(fused_reshape16_transpose13_reshape17, (lv69,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv221: R.Tensor((320, 320), dtype="float32") = params[430]
            lv222: R.Tensor((320,), dtype="float32") = params[22]
            lv16 = R.call_tir(fused_matmul8_add9_add10, (lv15, lv221, lv222, lv10), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv223: R.Tensor((320,), dtype="float32") = params[23]
            lv224: R.Tensor((320,), dtype="float32") = params[24]
            lv77 = R.call_tir(layer_norm1, (lv16, lv223, lv224), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv225: R.Tensor((320, 320), dtype="float32") = params[431]
            lv79 = R.call_tir(matmul8, (lv77, lv225), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv226: R.Tensor((768, 320), dtype="float32") = params[432]
            lv81 = R.call_tir(matmul11, (inp_2, lv226), out_sinfo=R.Tensor((2, 77, 320), dtype="float32"))
            lv227: R.Tensor((768, 320), dtype="float32") = params[433]
            lv83 = R.call_tir(matmul11, (inp_2, lv227), out_sinfo=R.Tensor((2, 77, 320), dtype="float32"))
            lv17 = R.call_tir(fused_reshape14_transpose11_reshape15, (lv79,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv18 = R.call_tir(fused_reshape18_transpose15_reshape19_transpose16, (lv81,), out_sinfo=R.Tensor((16, 40, 77), dtype="float32"))
            lv19 = R.call_tir(fused_reshape18_transpose15_reshape19, (lv83,), out_sinfo=R.Tensor((16, 77, 40), dtype="float32"))
            lv20 = R.call_tir(fused_matmul12_multiply6, (lv17, lv18), out_sinfo=R.Tensor((16, 4096, 77), dtype="float32"))
            lv96 = R.call_tir(softmax2, (lv20,), out_sinfo=R.Tensor((16, 4096, 77), dtype="float32"))
            lv97 = R.call_tir(matmul13, (lv96, lv19), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv21 = R.call_tir(fused_reshape16_transpose13_reshape17, (lv97,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv228: R.Tensor((320, 320), dtype="float32") = params[434]
            lv229: R.Tensor((320,), dtype="float32") = params[25]
            lv22 = R.call_tir(fused_matmul8_add9_add10, (lv21, lv228, lv229, lv16), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv230: R.Tensor((320,), dtype="float32") = params[26]
            lv231: R.Tensor((320,), dtype="float32") = params[27]
            lv105 = R.call_tir(layer_norm1, (lv22, lv230, lv231), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv232: R.Tensor((320, 1280), dtype="float32") = params[436]
            lv233: R.Tensor((1280,), dtype="float32") = params[29]
            lv23 = R.call_tir(fused_matmul14_add11_gelu, (lv105, lv232, lv233), out_sinfo=R.Tensor((2, 4096, 1280), dtype="float32"))
            lv234: R.Tensor((320, 1280), dtype="float32") = params[435]
            lv235: R.Tensor((1280,), dtype="float32") = params[28]
            lv24 = R.call_tir(fused_matmul14_add11_multiply7, (lv105, lv234, lv235, lv23), out_sinfo=R.Tensor((2, 4096, 1280), dtype="float32"))
            lv236: R.Tensor((1280, 320), dtype="float32") = params[437]
            lv237: R.Tensor((320,), dtype="float32") = params[30]
            lv25 = R.call_tir(fused_matmul15_add9_add10, (lv24, lv236, lv237, lv22), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv26 = R.call_tir(fused_reshape20_transpose17, (lv25,), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv238: R.Tensor((320, 320, 1, 1), dtype="float32") = params[31]
            lv239: R.Tensor((1, 320, 1, 1), dtype="float32") = params[438]
            lv27 = R.call_tir(fused_conv2d2_add5_add8, (lv26, lv238, lv239, lv8), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv240: R.Tensor((320,), dtype="float32") = params[10]
            lv241: R.Tensor((320,), dtype="float32") = params[11]
            lv28 = R.call_tir(fused_group_norm_silu1, (lv27, lv240, lv241), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv129 = R.call_tir(silu, (lv2,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv242: R.Tensor((1280, 320), dtype="float32") = params[439]
            lv243: R.Tensor((320,), dtype="float32") = params[13]
            lv29_1 = R.call_tir(fused_matmul7_add6_strided_slice3, (lv129, lv242, lv243), out_sinfo=R.Tensor((2, 320), dtype="float32"))
            lv134 = R.call_tir(reshape12, (lv29_1,), out_sinfo=R.Tensor((2, 320, 1, 1), dtype="float32"))
            lv244: R.Tensor((320, 320, 3, 3), dtype="float32") = params[12]
            lv245: R.Tensor((1, 320, 1, 1), dtype="float32") = params[440]
            lv30 = R.call_tir(fused_conv2d1_add5_add7, (lv28, lv244, lv245, lv134), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv246: R.Tensor((320,), dtype="float32") = params[14]
            lv247: R.Tensor((320,), dtype="float32") = params[15]
            lv31 = R.call_tir(fused_group_norm_silu1, (lv30, lv246, lv247), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv248: R.Tensor((320, 320, 3, 3), dtype="float32") = params[16]
            lv249: R.Tensor((1, 320, 1, 1), dtype="float32") = params[441]
            lv32 = R.call_tir(fused_conv2d1_add5_add8_divide, (lv31, lv248, lv249, lv27), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv250: R.Tensor((320,), dtype="float32") = params[32]
            lv251: R.Tensor((320,), dtype="float32") = params[33]
            lv143 = R.call_tir(group_norm1, (lv32, lv250, lv251), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv252: R.Tensor((320, 320, 1, 1), dtype="float32") = params[34]
            lv253: R.Tensor((1, 320, 1, 1), dtype="float32") = params[442]
            lv33 = R.call_tir(fused_conv2d2_add5, (lv143, lv252, lv253), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv34_1 = R.call_tir(fused_transpose9_reshape13, (lv33,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv254: R.Tensor((320,), dtype="float32") = params[35]
            lv255: R.Tensor((320,), dtype="float32") = params[36]
            lv149 = R.call_tir(layer_norm1, (lv34_1, lv254, lv255), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv256: R.Tensor((320, 320), dtype="float32") = params[443]
            lv151 = R.call_tir(matmul8, (lv149, lv256), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv257: R.Tensor((320, 320), dtype="float32") = params[444]
            lv153 = R.call_tir(matmul8, (lv149, lv257), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv258: R.Tensor((320, 320), dtype="float32") = params[445]
            lv155 = R.call_tir(matmul8, (lv149, lv258), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv35 = R.call_tir(fused_reshape14_transpose11_reshape15, (lv151,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv36 = R.call_tir(fused_reshape14_transpose11_reshape15_transpose12, (lv153,), out_sinfo=R.Tensor((16, 40, 4096), dtype="float32"))
            lv37 = R.call_tir(fused_reshape14_transpose11_reshape15, (lv155,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv38 = R.call_tir(fused_matmul9_multiply5, (lv35, lv36), out_sinfo=R.Tensor((16, 4096, 4096), dtype="float32"))
            lv168 = R.call_tir(softmax1, (lv38,), out_sinfo=R.Tensor((16, 4096, 4096), dtype="float32"))
            lv169 = R.call_tir(matmul10, (lv168, lv37), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv39 = R.call_tir(fused_reshape16_transpose13_reshape17, (lv169,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv259: R.Tensor((320, 320), dtype="float32") = params[446]
            lv260: R.Tensor((320,), dtype="float32") = params[37]
            lv40 = R.call_tir(fused_matmul8_add9_add10, (lv39, lv259, lv260, lv34_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv261: R.Tensor((320,), dtype="float32") = params[38]
            lv262: R.Tensor((320,), dtype="float32") = params[39]
            lv177 = R.call_tir(layer_norm1, (lv40, lv261, lv262), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv263: R.Tensor((320, 320), dtype="float32") = params[447]
            lv179 = R.call_tir(matmul8, (lv177, lv263), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv264: R.Tensor((768, 320), dtype="float32") = params[448]
            lv181 = R.call_tir(matmul11, (inp_2, lv264), out_sinfo=R.Tensor((2, 77, 320), dtype="float32"))
            lv265: R.Tensor((768, 320), dtype="float32") = params[449]
            lv183 = R.call_tir(matmul11, (inp_2, lv265), out_sinfo=R.Tensor((2, 77, 320), dtype="float32"))
            lv41 = R.call_tir(fused_reshape14_transpose11_reshape15, (lv179,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv42 = R.call_tir(fused_reshape18_transpose15_reshape19_transpose16, (lv181,), out_sinfo=R.Tensor((16, 40, 77), dtype="float32"))
            lv43_1 = R.call_tir(fused_reshape18_transpose15_reshape19, (lv183,), out_sinfo=R.Tensor((16, 77, 40), dtype="float32"))
            lv44 = R.call_tir(fused_matmul12_multiply6, (lv41, lv42), out_sinfo=R.Tensor((16, 4096, 77), dtype="float32"))
            lv196_1 = R.call_tir(softmax2, (lv44,), out_sinfo=R.Tensor((16, 4096, 77), dtype="float32"))
            lv197_1 = R.call_tir(matmul13, (lv196_1, lv43_1), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv45 = R.call_tir(fused_reshape16_transpose13_reshape17, (lv197_1,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv266: R.Tensor((320, 320), dtype="float32") = params[450]
            lv267: R.Tensor((320,), dtype="float32") = params[40]
            lv46 = R.call_tir(fused_matmul8_add9_add10, (lv45, lv266, lv267, lv40), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv268: R.Tensor((320,), dtype="float32") = params[41]
            lv269: R.Tensor((320,), dtype="float32") = params[42]
            lv205_1 = R.call_tir(layer_norm1, (lv46, lv268, lv269), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv270: R.Tensor((320, 1280), dtype="float32") = params[452]
            lv271: R.Tensor((1280,), dtype="float32") = params[44]
            lv47 = R.call_tir(fused_matmul14_add11_gelu, (lv205_1, lv270, lv271), out_sinfo=R.Tensor((2, 4096, 1280), dtype="float32"))
            lv272: R.Tensor((320, 1280), dtype="float32") = params[451]
            lv273: R.Tensor((1280,), dtype="float32") = params[43]
            lv48 = R.call_tir(fused_matmul14_add11_multiply7, (lv205_1, lv272, lv273, lv47), out_sinfo=R.Tensor((2, 4096, 1280), dtype="float32"))
            lv274: R.Tensor((1280, 320), dtype="float32") = params[453]
            lv275: R.Tensor((320,), dtype="float32") = params[45]
            lv49_1 = R.call_tir(fused_matmul15_add9_add10, (lv48, lv274, lv275, lv46), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv50 = R.call_tir(fused_reshape20_transpose17, (lv49_1,), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv276: R.Tensor((320, 320, 1, 1), dtype="float32") = params[46]
            lv277: R.Tensor((1, 320, 1, 1), dtype="float32") = params[454]
            lv51_1 = R.call_tir(fused_conv2d2_add5_add8, (lv50, lv276, lv277, lv32), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv278: R.Tensor((320, 320, 3, 3), dtype="float32") = params[47]
            lv279: R.Tensor((1, 320, 1, 1), dtype="float32") = params[455]
            lv52 = R.call_tir(fused_conv2d3_add12, (lv51_1, lv278, lv279), out_sinfo=R.Tensor((2, 320, 32, 32), dtype="float32"))
            lv280: R.Tensor((320,), dtype="float32") = params[48]
            lv281: R.Tensor((320,), dtype="float32") = params[49]
            lv53_1 = R.call_tir(fused_group_norm2_silu2, (lv52, lv280, lv281), out_sinfo=R.Tensor((2, 320, 32, 32), dtype="float32"))
            lv232_1 = R.call_tir(silu, (lv2,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv282: R.Tensor((1280, 640), dtype="float32") = params[457]
            lv283: R.Tensor((640,), dtype="float32") = params[51]
            lv54 = R.call_tir(fused_matmul16_add14_strided_slice4, (lv232_1, lv282, lv283), out_sinfo=R.Tensor((2, 640), dtype="float32"))
            lv237_1 = R.call_tir(reshape22, (lv54,), out_sinfo=R.Tensor((2, 640, 1, 1), dtype="float32"))
            lv284: R.Tensor((640, 320, 3, 3), dtype="float32") = params[50]
            lv285: R.Tensor((1, 640, 1, 1), dtype="float32") = params[456]
            lv55_1 = R.call_tir(fused_conv2d4_add13_add15, (lv53_1, lv284, lv285, lv237_1), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv286: R.Tensor((640,), dtype="float32") = params[52]
            lv287: R.Tensor((640,), dtype="float32") = params[53]
            lv56 = R.call_tir(fused_group_norm3_silu3, (lv55_1, lv286, lv287), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv288: R.Tensor((640, 320, 1, 1), dtype="float32") = params[55]
            lv289: R.Tensor((1, 640, 1, 1), dtype="float32") = params[458]
            lv57 = R.call_tir(fused_conv2d6_add13, (lv52, lv288, lv289), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv290: R.Tensor((640, 640, 3, 3), dtype="float32") = params[54]
            lv291: R.Tensor((1, 640, 1, 1), dtype="float32") = params[459]
            lv58 = R.call_tir(fused_conv2d5_add13_add16_divide1, (lv56, lv290, lv291, lv57), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv292: R.Tensor((640,), dtype="float32") = params[63]
            lv293: R.Tensor((640,), dtype="float32") = params[64]
            lv249_1 = R.call_tir(group_norm4, (lv58, lv292, lv293), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv294: R.Tensor((640, 640, 1, 1), dtype="float32") = params[65]
            lv295: R.Tensor((1, 640, 1, 1), dtype="float32") = params[460]
            lv59 = R.call_tir(fused_conv2d7_add13, (lv249_1, lv294, lv295), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv60 = R.call_tir(fused_transpose19_reshape23, (lv59,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv296: R.Tensor((640,), dtype="float32") = params[66]
            lv297: R.Tensor((640,), dtype="float32") = params[67]
            lv255_1 = R.call_tir(layer_norm2, (lv60, lv296, lv297), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv298: R.Tensor((640, 640), dtype="float32") = params[461]
            lv257_1 = R.call_tir(matmul17, (lv255_1, lv298), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv299: R.Tensor((640, 640), dtype="float32") = params[462]
            lv259_1 = R.call_tir(matmul17, (lv255_1, lv299), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv300: R.Tensor((640, 640), dtype="float32") = params[463]
            lv261_1 = R.call_tir(matmul17, (lv255_1, lv300), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv61 = R.call_tir(fused_reshape24_transpose21_reshape25, (lv257_1,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv62 = R.call_tir(fused_reshape24_transpose21_reshape25_transpose22, (lv259_1,), out_sinfo=R.Tensor((16, 80, 1024), dtype="float32"))
            lv63 = R.call_tir(fused_reshape24_transpose21_reshape25, (lv261_1,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv64 = R.call_tir(fused_matmul18_multiply8, (lv61, lv62), out_sinfo=R.Tensor((16, 1024, 1024), dtype="float32"))
            lv274_1 = R.call_tir(softmax3, (lv64,), out_sinfo=R.Tensor((16, 1024, 1024), dtype="float32"))
            lv275_1 = R.call_tir(matmul19, (lv274_1, lv63), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv65 = R.call_tir(fused_reshape26_transpose23_reshape27, (lv275_1,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv301: R.Tensor((640, 640), dtype="float32") = params[464]
            lv302: R.Tensor((640,), dtype="float32") = params[68]
            lv66 = R.call_tir(fused_matmul17_add17_add18, (lv65, lv301, lv302, lv60), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv303: R.Tensor((640,), dtype="float32") = params[69]
            lv304: R.Tensor((640,), dtype="float32") = params[70]
            lv283_1 = R.call_tir(layer_norm2, (lv66, lv303, lv304), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv305: R.Tensor((640, 640), dtype="float32") = params[465]
            lv285_1 = R.call_tir(matmul17, (lv283_1, lv305), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv306: R.Tensor((768, 640), dtype="float32") = params[466]
            lv287_1 = R.call_tir(matmul20, (inp_2, lv306), out_sinfo=R.Tensor((2, 77, 640), dtype="float32"))
            lv307: R.Tensor((768, 640), dtype="float32") = params[467]
            lv289_1 = R.call_tir(matmul20, (inp_2, lv307), out_sinfo=R.Tensor((2, 77, 640), dtype="float32"))
            lv67 = R.call_tir(fused_reshape24_transpose21_reshape25, (lv285_1,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv68_1 = R.call_tir(fused_reshape28_transpose25_reshape29_transpose26, (lv287_1,), out_sinfo=R.Tensor((16, 80, 77), dtype="float32"))
            lv69_1 = R.call_tir(fused_reshape28_transpose25_reshape29, (lv289_1,), out_sinfo=R.Tensor((16, 77, 80), dtype="float32"))
            lv70 = R.call_tir(fused_matmul21_multiply9, (lv67, lv68_1), out_sinfo=R.Tensor((16, 1024, 77), dtype="float32"))
            lv302_1 = R.call_tir(softmax4, (lv70,), out_sinfo=R.Tensor((16, 1024, 77), dtype="float32"))
            lv303_1 = R.call_tir(matmul22, (lv302_1, lv69_1), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv71 = R.call_tir(fused_reshape26_transpose23_reshape27, (lv303_1,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv308: R.Tensor((640, 640), dtype="float32") = params[468]
            lv309: R.Tensor((640,), dtype="float32") = params[71]
            lv72 = R.call_tir(fused_matmul17_add17_add18, (lv71, lv308, lv309, lv66), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv310: R.Tensor((640,), dtype="float32") = params[72]
            lv311: R.Tensor((640,), dtype="float32") = params[73]
            lv311_1 = R.call_tir(layer_norm2, (lv72, lv310, lv311), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv312: R.Tensor((640, 2560), dtype="float32") = params[470]
            lv313: R.Tensor((2560,), dtype="float32") = params[75]
            lv73 = R.call_tir(fused_matmul23_add19_gelu1, (lv311_1, lv312, lv313), out_sinfo=R.Tensor((2, 1024, 2560), dtype="float32"))
            lv314: R.Tensor((640, 2560), dtype="float32") = params[469]
            lv315: R.Tensor((2560,), dtype="float32") = params[74]
            lv74 = R.call_tir(fused_matmul23_add19_multiply10, (lv311_1, lv314, lv315, lv73), out_sinfo=R.Tensor((2, 1024, 2560), dtype="float32"))
            lv316: R.Tensor((2560, 640), dtype="float32") = params[471]
            lv317: R.Tensor((640,), dtype="float32") = params[76]
            lv75 = R.call_tir(fused_matmul24_add17_add18, (lv74, lv316, lv317, lv72), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv76 = R.call_tir(fused_reshape30_transpose29, (lv75,), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv318: R.Tensor((640, 640, 1, 1), dtype="float32") = params[77]
            lv319: R.Tensor((1, 640, 1, 1), dtype="float32") = params[472]
            lv77_1 = R.call_tir(fused_conv2d7_add13_add16, (lv76, lv318, lv319, lv58), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv320: R.Tensor((640,), dtype="float32") = params[56]
            lv321: R.Tensor((640,), dtype="float32") = params[57]
            lv78 = R.call_tir(fused_group_norm3_silu3, (lv77_1, lv320, lv321), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv335 = R.call_tir(silu, (lv2,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv322: R.Tensor((1280, 640), dtype="float32") = params[473]
            lv323: R.Tensor((640,), dtype="float32") = params[59]
            lv79_1 = R.call_tir(fused_matmul16_add14_strided_slice4, (lv335, lv322, lv323), out_sinfo=R.Tensor((2, 640), dtype="float32"))
            lv340 = R.call_tir(reshape22, (lv79_1,), out_sinfo=R.Tensor((2, 640, 1, 1), dtype="float32"))
            lv324: R.Tensor((640, 640, 3, 3), dtype="float32") = params[58]
            lv325: R.Tensor((1, 640, 1, 1), dtype="float32") = params[474]
            lv80 = R.call_tir(fused_conv2d5_add13_add15, (lv78, lv324, lv325, lv340), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv326: R.Tensor((640,), dtype="float32") = params[60]
            lv327: R.Tensor((640,), dtype="float32") = params[61]
            lv81_1 = R.call_tir(fused_group_norm3_silu3, (lv80, lv326, lv327), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv328: R.Tensor((640, 640, 3, 3), dtype="float32") = params[62]
            lv329: R.Tensor((1, 640, 1, 1), dtype="float32") = params[475]
            lv82 = R.call_tir(fused_conv2d5_add13_add16_divide1, (lv81_1, lv328, lv329, lv77_1), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv330: R.Tensor((640,), dtype="float32") = params[78]
            lv331: R.Tensor((640,), dtype="float32") = params[79]
            lv349 = R.call_tir(group_norm4, (lv82, lv330, lv331), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv332: R.Tensor((640, 640, 1, 1), dtype="float32") = params[80]
            lv333: R.Tensor((1, 640, 1, 1), dtype="float32") = params[476]
            lv83_1 = R.call_tir(fused_conv2d7_add13, (lv349, lv332, lv333), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv84 = R.call_tir(fused_transpose19_reshape23, (lv83_1,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv334: R.Tensor((640,), dtype="float32") = params[81]
            lv335_1: R.Tensor((640,), dtype="float32") = params[82]
            lv355 = R.call_tir(layer_norm2, (lv84, lv334, lv335_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv336: R.Tensor((640, 640), dtype="float32") = params[477]
            lv357 = R.call_tir(matmul17, (lv355, lv336), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv337: R.Tensor((640, 640), dtype="float32") = params[478]
            lv359 = R.call_tir(matmul17, (lv355, lv337), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv338: R.Tensor((640, 640), dtype="float32") = params[479]
            lv361 = R.call_tir(matmul17, (lv355, lv338), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv85 = R.call_tir(fused_reshape24_transpose21_reshape25, (lv357,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv86 = R.call_tir(fused_reshape24_transpose21_reshape25_transpose22, (lv359,), out_sinfo=R.Tensor((16, 80, 1024), dtype="float32"))
            lv87 = R.call_tir(fused_reshape24_transpose21_reshape25, (lv361,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv88 = R.call_tir(fused_matmul18_multiply8, (lv85, lv86), out_sinfo=R.Tensor((16, 1024, 1024), dtype="float32"))
            lv374 = R.call_tir(softmax3, (lv88,), out_sinfo=R.Tensor((16, 1024, 1024), dtype="float32"))
            lv375 = R.call_tir(matmul19, (lv374, lv87), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv89 = R.call_tir(fused_reshape26_transpose23_reshape27, (lv375,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv339: R.Tensor((640, 640), dtype="float32") = params[480]
            lv340_1: R.Tensor((640,), dtype="float32") = params[83]
            lv90 = R.call_tir(fused_matmul17_add17_add18, (lv89, lv339, lv340_1, lv84), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv341: R.Tensor((640,), dtype="float32") = params[84]
            lv342: R.Tensor((640,), dtype="float32") = params[85]
            lv383 = R.call_tir(layer_norm2, (lv90, lv341, lv342), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv343: R.Tensor((640, 640), dtype="float32") = params[481]
            lv385 = R.call_tir(matmul17, (lv383, lv343), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv344: R.Tensor((768, 640), dtype="float32") = params[482]
            lv387 = R.call_tir(matmul20, (inp_2, lv344), out_sinfo=R.Tensor((2, 77, 640), dtype="float32"))
            lv345: R.Tensor((768, 640), dtype="float32") = params[483]
            lv389 = R.call_tir(matmul20, (inp_2, lv345), out_sinfo=R.Tensor((2, 77, 640), dtype="float32"))
            lv91 = R.call_tir(fused_reshape24_transpose21_reshape25, (lv385,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv92 = R.call_tir(fused_reshape28_transpose25_reshape29_transpose26, (lv387,), out_sinfo=R.Tensor((16, 80, 77), dtype="float32"))
            lv93 = R.call_tir(fused_reshape28_transpose25_reshape29, (lv389,), out_sinfo=R.Tensor((16, 77, 80), dtype="float32"))
            lv94 = R.call_tir(fused_matmul21_multiply9, (lv91, lv92), out_sinfo=R.Tensor((16, 1024, 77), dtype="float32"))
            lv402 = R.call_tir(softmax4, (lv94,), out_sinfo=R.Tensor((16, 1024, 77), dtype="float32"))
            lv403 = R.call_tir(matmul22, (lv402, lv93), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv95 = R.call_tir(fused_reshape26_transpose23_reshape27, (lv403,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv346: R.Tensor((640, 640), dtype="float32") = params[484]
            lv347: R.Tensor((640,), dtype="float32") = params[86]
            lv96_1 = R.call_tir(fused_matmul17_add17_add18, (lv95, lv346, lv347, lv90), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv348: R.Tensor((640,), dtype="float32") = params[87]
            lv349_1: R.Tensor((640,), dtype="float32") = params[88]
            lv411 = R.call_tir(layer_norm2, (lv96_1, lv348, lv349_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv350: R.Tensor((640, 2560), dtype="float32") = params[486]
            lv351: R.Tensor((2560,), dtype="float32") = params[90]
            lv97_1 = R.call_tir(fused_matmul23_add19_gelu1, (lv411, lv350, lv351), out_sinfo=R.Tensor((2, 1024, 2560), dtype="float32"))
            lv352: R.Tensor((640, 2560), dtype="float32") = params[485]
            lv353: R.Tensor((2560,), dtype="float32") = params[89]
            lv98 = R.call_tir(fused_matmul23_add19_multiply10, (lv411, lv352, lv353, lv97_1), out_sinfo=R.Tensor((2, 1024, 2560), dtype="float32"))
            lv354: R.Tensor((2560, 640), dtype="float32") = params[487]
            lv355_1: R.Tensor((640,), dtype="float32") = params[91]
            lv99 = R.call_tir(fused_matmul24_add17_add18, (lv98, lv354, lv355_1, lv96_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv100 = R.call_tir(fused_reshape30_transpose29, (lv99,), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv356: R.Tensor((640, 640, 1, 1), dtype="float32") = params[92]
            lv357_1: R.Tensor((1, 640, 1, 1), dtype="float32") = params[488]
            lv101 = R.call_tir(fused_conv2d7_add13_add16, (lv100, lv356, lv357_1, lv82), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv358: R.Tensor((640, 640, 3, 3), dtype="float32") = params[93]
            lv359_1: R.Tensor((1, 640, 1, 1), dtype="float32") = params[489]
            lv102 = R.call_tir(fused_conv2d8_add20, (lv101, lv358, lv359_1), out_sinfo=R.Tensor((2, 640, 16, 16), dtype="float32"))
            lv360: R.Tensor((640,), dtype="float32") = params[94]
            lv361_1: R.Tensor((640,), dtype="float32") = params[95]
            lv103 = R.call_tir(fused_group_norm5_silu4, (lv102, lv360, lv361_1), out_sinfo=R.Tensor((2, 640, 16, 16), dtype="float32"))
            lv438 = R.call_tir(silu, (lv2,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv362: R.Tensor((1280, 1280), dtype="float32") = params[490]
            lv363: R.Tensor((1280,), dtype="float32") = params[97]
            lv104 = R.call_tir(fused_matmul6_add4_strided_slice5, (lv438, lv362, lv363), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv443 = R.call_tir(reshape32, (lv104,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv364: R.Tensor((1280, 640, 3, 3), dtype="float32") = params[96]
            lv365: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[491]
            lv105_1 = R.call_tir(fused_conv2d9_add21_add22, (lv103, lv364, lv365, lv443), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv366: R.Tensor((1280,), dtype="float32") = params[98]
            lv367: R.Tensor((1280,), dtype="float32") = params[99]
            lv106 = R.call_tir(fused_group_norm6_silu5, (lv105_1, lv366, lv367), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv368: R.Tensor((1280, 640, 1, 1), dtype="float32") = params[101]
            lv369: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[492]
            lv107 = R.call_tir(fused_conv2d11_add21, (lv102, lv368, lv369), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv370: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[100]
            lv371: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[493]
            lv108 = R.call_tir(fused_conv2d10_add21_add23_divide2, (lv106, lv370, lv371, lv107), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv372: R.Tensor((1280,), dtype="float32") = params[109]
            lv373: R.Tensor((1280,), dtype="float32") = params[110]
            lv455 = R.call_tir(group_norm7, (lv108, lv372, lv373), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv374_1: R.Tensor((1280, 1280, 1, 1), dtype="float32") = params[111]
            lv375_1: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[494]
            lv109 = R.call_tir(fused_conv2d12_add21, (lv455, lv374_1, lv375_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv110 = R.call_tir(fused_transpose30_reshape33, (lv109,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv376: R.Tensor((1280,), dtype="float32") = params[112]
            lv377: R.Tensor((1280,), dtype="float32") = params[113]
            lv461 = R.call_tir(layer_norm3, (lv110, lv376, lv377), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv378: R.Tensor((1280, 1280), dtype="float32") = params[495]
            lv463 = R.call_tir(matmul25, (lv461, lv378), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv379: R.Tensor((1280, 1280), dtype="float32") = params[496]
            lv465 = R.call_tir(matmul25, (lv461, lv379), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv380: R.Tensor((1280, 1280), dtype="float32") = params[497]
            lv467 = R.call_tir(matmul25, (lv461, lv380), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv111 = R.call_tir(fused_reshape34_transpose31_reshape35, (lv463,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv112 = R.call_tir(fused_reshape34_transpose31_reshape35_transpose32, (lv465,), out_sinfo=R.Tensor((16, 160, 256), dtype="float32"))
            lv113 = R.call_tir(fused_reshape34_transpose31_reshape35, (lv467,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv114 = R.call_tir(fused_matmul26_multiply11, (lv111, lv112), out_sinfo=R.Tensor((16, 256, 256), dtype="float32"))
            lv480 = R.call_tir(softmax5, (lv114,), out_sinfo=R.Tensor((16, 256, 256), dtype="float32"))
            lv481 = R.call_tir(matmul27, (lv480, lv113), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv115 = R.call_tir(fused_reshape36_transpose33_reshape37, (lv481,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv381: R.Tensor((1280, 1280), dtype="float32") = params[498]
            lv382: R.Tensor((1280,), dtype="float32") = params[114]
            lv116 = R.call_tir(fused_matmul25_add24_add25, (lv115, lv381, lv382, lv110), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv383_1: R.Tensor((1280,), dtype="float32") = params[115]
            lv384: R.Tensor((1280,), dtype="float32") = params[116]
            lv489 = R.call_tir(layer_norm3, (lv116, lv383_1, lv384), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv385_1: R.Tensor((1280, 1280), dtype="float32") = params[499]
            lv491 = R.call_tir(matmul25, (lv489, lv385_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv386: R.Tensor((768, 1280), dtype="float32") = params[500]
            lv493 = R.call_tir(matmul28, (inp_2, lv386), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv387_1: R.Tensor((768, 1280), dtype="float32") = params[501]
            lv495 = R.call_tir(matmul28, (inp_2, lv387_1), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv117 = R.call_tir(fused_reshape34_transpose31_reshape35, (lv491,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv118 = R.call_tir(fused_reshape38_transpose35_reshape39_transpose36, (lv493,), out_sinfo=R.Tensor((16, 160, 77), dtype="float32"))
            lv119 = R.call_tir(fused_reshape38_transpose35_reshape39, (lv495,), out_sinfo=R.Tensor((16, 77, 160), dtype="float32"))
            lv120 = R.call_tir(fused_matmul29_multiply12, (lv117, lv118), out_sinfo=R.Tensor((16, 256, 77), dtype="float32"))
            lv508 = R.call_tir(softmax6, (lv120,), out_sinfo=R.Tensor((16, 256, 77), dtype="float32"))
            lv509 = R.call_tir(matmul30, (lv508, lv119), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv121 = R.call_tir(fused_reshape36_transpose33_reshape37, (lv509,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv388: R.Tensor((1280, 1280), dtype="float32") = params[502]
            lv389_1: R.Tensor((1280,), dtype="float32") = params[117]
            lv122 = R.call_tir(fused_matmul25_add24_add25, (lv121, lv388, lv389_1, lv116), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv390: R.Tensor((1280,), dtype="float32") = params[118]
            lv391: R.Tensor((1280,), dtype="float32") = params[119]
            lv517 = R.call_tir(layer_norm3, (lv122, lv390, lv391), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv392: R.Tensor((1280, 5120), dtype="float32") = params[504]
            lv393: R.Tensor((5120,), dtype="float32") = params[121]
            lv123 = R.call_tir(fused_matmul31_add26_gelu2, (lv517, lv392, lv393), out_sinfo=R.Tensor((2, 256, 5120), dtype="float32"))
            lv394: R.Tensor((1280, 5120), dtype="float32") = params[503]
            lv395: R.Tensor((5120,), dtype="float32") = params[120]
            lv124 = R.call_tir(fused_matmul31_add26_multiply13, (lv517, lv394, lv395, lv123), out_sinfo=R.Tensor((2, 256, 5120), dtype="float32"))
            lv396: R.Tensor((5120, 1280), dtype="float32") = params[505]
            lv397: R.Tensor((1280,), dtype="float32") = params[122]
            lv125 = R.call_tir(fused_matmul32_add24_add25, (lv124, lv396, lv397, lv122), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv126 = R.call_tir(fused_reshape40_transpose39, (lv125,), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv398: R.Tensor((1280, 1280, 1, 1), dtype="float32") = params[123]
            lv399: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[506]
            lv127 = R.call_tir(fused_conv2d12_add21_add23, (lv126, lv398, lv399, lv108), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv400: R.Tensor((1280,), dtype="float32") = params[102]
            lv401: R.Tensor((1280,), dtype="float32") = params[103]
            lv128 = R.call_tir(fused_group_norm6_silu5, (lv127, lv400, lv401), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv541 = R.call_tir(silu, (lv2,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv402_1: R.Tensor((1280, 1280), dtype="float32") = params[507]
            lv403_1: R.Tensor((1280,), dtype="float32") = params[105]
            lv129_1 = R.call_tir(fused_matmul6_add4_strided_slice5, (lv541, lv402_1, lv403_1), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv546 = R.call_tir(reshape32, (lv129_1,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv404: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[104]
            lv405: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[508]
            lv130 = R.call_tir(fused_conv2d10_add21_add22, (lv128, lv404, lv405, lv546), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv406: R.Tensor((1280,), dtype="float32") = params[106]
            lv407: R.Tensor((1280,), dtype="float32") = params[107]
            lv131 = R.call_tir(fused_group_norm6_silu5, (lv130, lv406, lv407), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv408: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[108]
            lv409: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[509]
            lv132 = R.call_tir(fused_conv2d10_add21_add23_divide2, (lv131, lv408, lv409, lv127), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv410: R.Tensor((1280,), dtype="float32") = params[124]
            lv411_1: R.Tensor((1280,), dtype="float32") = params[125]
            lv555 = R.call_tir(group_norm7, (lv132, lv410, lv411_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv412: R.Tensor((1280, 1280, 1, 1), dtype="float32") = params[126]
            lv413: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[510]
            lv133 = R.call_tir(fused_conv2d12_add21, (lv555, lv412, lv413), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv134_1 = R.call_tir(fused_transpose30_reshape33, (lv133,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv414: R.Tensor((1280,), dtype="float32") = params[127]
            lv415: R.Tensor((1280,), dtype="float32") = params[128]
            lv561 = R.call_tir(layer_norm3, (lv134_1, lv414, lv415), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv416: R.Tensor((1280, 1280), dtype="float32") = params[511]
            lv563 = R.call_tir(matmul25, (lv561, lv416), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv417: R.Tensor((1280, 1280), dtype="float32") = params[512]
            lv565 = R.call_tir(matmul25, (lv561, lv417), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv418: R.Tensor((1280, 1280), dtype="float32") = params[513]
            lv567 = R.call_tir(matmul25, (lv561, lv418), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv135 = R.call_tir(fused_reshape34_transpose31_reshape35, (lv563,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv136 = R.call_tir(fused_reshape34_transpose31_reshape35_transpose32, (lv565,), out_sinfo=R.Tensor((16, 160, 256), dtype="float32"))
            lv137 = R.call_tir(fused_reshape34_transpose31_reshape35, (lv567,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv138 = R.call_tir(fused_matmul26_multiply11, (lv135, lv136), out_sinfo=R.Tensor((16, 256, 256), dtype="float32"))
            lv580 = R.call_tir(softmax5, (lv138,), out_sinfo=R.Tensor((16, 256, 256), dtype="float32"))
            lv581 = R.call_tir(matmul27, (lv580, lv137), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv139 = R.call_tir(fused_reshape36_transpose33_reshape37, (lv581,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv419: R.Tensor((1280, 1280), dtype="float32") = params[514]
            lv420: R.Tensor((1280,), dtype="float32") = params[129]
            lv140 = R.call_tir(fused_matmul25_add24_add25, (lv139, lv419, lv420, lv134_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv421: R.Tensor((1280,), dtype="float32") = params[130]
            lv422: R.Tensor((1280,), dtype="float32") = params[131]
            lv589 = R.call_tir(layer_norm3, (lv140, lv421, lv422), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv423: R.Tensor((1280, 1280), dtype="float32") = params[515]
            lv591 = R.call_tir(matmul25, (lv589, lv423), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv424: R.Tensor((768, 1280), dtype="float32") = params[516]
            lv593 = R.call_tir(matmul28, (inp_2, lv424), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv425: R.Tensor((768, 1280), dtype="float32") = params[517]
            lv595 = R.call_tir(matmul28, (inp_2, lv425), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv141 = R.call_tir(fused_reshape34_transpose31_reshape35, (lv591,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv142 = R.call_tir(fused_reshape38_transpose35_reshape39_transpose36, (lv593,), out_sinfo=R.Tensor((16, 160, 77), dtype="float32"))
            lv143_1 = R.call_tir(fused_reshape38_transpose35_reshape39, (lv595,), out_sinfo=R.Tensor((16, 77, 160), dtype="float32"))
            lv144 = R.call_tir(fused_matmul29_multiply12, (lv141, lv142), out_sinfo=R.Tensor((16, 256, 77), dtype="float32"))
            lv608 = R.call_tir(softmax6, (lv144,), out_sinfo=R.Tensor((16, 256, 77), dtype="float32"))
            lv609 = R.call_tir(matmul30, (lv608, lv143_1), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv145 = R.call_tir(fused_reshape36_transpose33_reshape37, (lv609,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv426: R.Tensor((1280, 1280), dtype="float32") = params[518]
            lv427: R.Tensor((1280,), dtype="float32") = params[132]
            lv146 = R.call_tir(fused_matmul25_add24_add25, (lv145, lv426, lv427, lv140), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv428: R.Tensor((1280,), dtype="float32") = params[133]
            lv429: R.Tensor((1280,), dtype="float32") = params[134]
            lv617 = R.call_tir(layer_norm3, (lv146, lv428, lv429), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv430: R.Tensor((1280, 5120), dtype="float32") = params[520]
            lv431: R.Tensor((5120,), dtype="float32") = params[136]
            lv147 = R.call_tir(fused_matmul31_add26_gelu2, (lv617, lv430, lv431), out_sinfo=R.Tensor((2, 256, 5120), dtype="float32"))
            lv432: R.Tensor((1280, 5120), dtype="float32") = params[519]
            lv433: R.Tensor((5120,), dtype="float32") = params[135]
            lv148 = R.call_tir(fused_matmul31_add26_multiply13, (lv617, lv432, lv433, lv147), out_sinfo=R.Tensor((2, 256, 5120), dtype="float32"))
            lv434: R.Tensor((5120, 1280), dtype="float32") = params[521]
            lv435: R.Tensor((1280,), dtype="float32") = params[137]
            lv149_1 = R.call_tir(fused_matmul32_add24_add25, (lv148, lv434, lv435, lv146), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv150 = R.call_tir(fused_reshape40_transpose39, (lv149_1,), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv436: R.Tensor((1280, 1280, 1, 1), dtype="float32") = params[138]
            lv437: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[522]
            lv151_1 = R.call_tir(fused_conv2d12_add21_add23, (lv150, lv436, lv437, lv132), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv438_1: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[139]
            lv439: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[523]
            lv152 = R.call_tir(fused_conv2d13_add27, (lv151_1, lv438_1, lv439), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv440: R.Tensor((1280,), dtype="float32") = params[140]
            lv441: R.Tensor((1280,), dtype="float32") = params[141]
            lv153_1 = R.call_tir(fused_group_norm8_silu6, (lv152, lv440, lv441), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv644 = R.call_tir(silu, (lv2,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv442: R.Tensor((1280, 1280), dtype="float32") = params[525]
            lv443_1: R.Tensor((1280,), dtype="float32") = params[143]
            lv154 = R.call_tir(fused_matmul6_add4_strided_slice5, (lv644, lv442, lv443_1), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv649 = R.call_tir(reshape32, (lv154,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv444: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[142]
            lv445: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[524]
            lv155_1 = R.call_tir(fused_conv2d14_add27_add28, (lv153_1, lv444, lv445, lv649), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv446: R.Tensor((1280,), dtype="float32") = params[144]
            lv447: R.Tensor((1280,), dtype="float32") = params[145]
            lv156 = R.call_tir(fused_group_norm8_silu6, (lv155_1, lv446, lv447), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv448: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[146]
            lv449: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[526]
            lv157 = R.call_tir(fused_conv2d14_add27_add29_divide3, (lv156, lv448, lv449, lv152), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv450: R.Tensor((1280,), dtype="float32") = params[147]
            lv451: R.Tensor((1280,), dtype="float32") = params[148]
            lv158 = R.call_tir(fused_group_norm8_silu6, (lv157, lv450, lv451), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv663 = R.call_tir(silu, (lv2,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv452: R.Tensor((1280, 1280), dtype="float32") = params[527]
            lv453: R.Tensor((1280,), dtype="float32") = params[150]
            lv159 = R.call_tir(fused_matmul6_add4_strided_slice5, (lv663, lv452, lv453), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv668 = R.call_tir(reshape32, (lv159,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv454: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[149]
            lv455_1: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[528]
            lv160 = R.call_tir(fused_conv2d14_add27_add28, (lv158, lv454, lv455_1, lv668), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv456: R.Tensor((1280,), dtype="float32") = params[151]
            lv457: R.Tensor((1280,), dtype="float32") = params[152]
            lv161 = R.call_tir(fused_group_norm8_silu6, (lv160, lv456, lv457), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv458: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[153]
            lv459: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[529]
            lv162 = R.call_tir(fused_conv2d14_add27_add29_divide3, (lv161, lv458, lv459, lv157), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv460: R.Tensor((1280,), dtype="float32") = params[154]
            lv461_1: R.Tensor((1280,), dtype="float32") = params[155]
            lv163 = R.call_tir(fused_group_norm8_silu6, (lv162, lv460, lv461_1), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv682 = R.call_tir(silu, (lv2,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv462: R.Tensor((1280, 1280), dtype="float32") = params[531]
            lv463_1: R.Tensor((1280,), dtype="float32") = params[157]
            lv164 = R.call_tir(fused_matmul6_add4_strided_slice5, (lv682, lv462, lv463_1), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv687 = R.call_tir(reshape32, (lv164,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv464: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[156]
            lv465_1: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[530]
            lv165 = R.call_tir(fused_conv2d14_add27_add28, (lv163, lv464, lv465_1, lv687), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv466: R.Tensor((1280,), dtype="float32") = params[158]
            lv467_1: R.Tensor((1280,), dtype="float32") = params[159]
            lv166 = R.call_tir(fused_group_norm8_silu6, (lv165, lv466, lv467_1), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv468: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[160]
            lv469: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[532]
            lv167 = R.call_tir(fused_conv2d14_add27_add29_divide3, (lv166, lv468, lv469, lv162), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv470: R.Tensor((1280,), dtype="float32") = params[168]
            lv471: R.Tensor((1280,), dtype="float32") = params[169]
            lv696 = R.call_tir(group_norm9, (lv167, lv470, lv471), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv472: R.Tensor((1280, 1280, 1, 1), dtype="float32") = params[170]
            lv473: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[533]
            lv168_1 = R.call_tir(fused_conv2d15_add27, (lv696, lv472, lv473), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv169_1 = R.call_tir(fused_transpose40_reshape41, (lv168_1,), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv474: R.Tensor((1280,), dtype="float32") = params[171]
            lv475: R.Tensor((1280,), dtype="float32") = params[172]
            lv702 = R.call_tir(layer_norm4, (lv169_1, lv474, lv475), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv476: R.Tensor((1280, 1280), dtype="float32") = params[534]
            lv704 = R.call_tir(matmul33, (lv702, lv476), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv477: R.Tensor((1280, 1280), dtype="float32") = params[535]
            lv706 = R.call_tir(matmul33, (lv702, lv477), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv478: R.Tensor((1280, 1280), dtype="float32") = params[536]
            lv708 = R.call_tir(matmul33, (lv702, lv478), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv170 = R.call_tir(fused_reshape42_transpose41_reshape43, (lv704,), out_sinfo=R.Tensor((16, 64, 160), dtype="float32"))
            lv171 = R.call_tir(fused_reshape42_transpose41_reshape43_transpose42, (lv706,), out_sinfo=R.Tensor((16, 160, 64), dtype="float32"))
            lv172 = R.call_tir(fused_reshape42_transpose41_reshape43, (lv708,), out_sinfo=R.Tensor((16, 64, 160), dtype="float32"))
            lv173 = R.call_tir(fused_matmul34_multiply14, (lv170, lv171), out_sinfo=R.Tensor((16, 64, 64), dtype="float32"))
            lv721 = R.call_tir(softmax7, (lv173,), out_sinfo=R.Tensor((16, 64, 64), dtype="float32"))
            lv722 = R.call_tir(matmul35, (lv721, lv172), out_sinfo=R.Tensor((16, 64, 160), dtype="float32"))
            lv174 = R.call_tir(fused_reshape44_transpose43_reshape45, (lv722,), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv479: R.Tensor((1280, 1280), dtype="float32") = params[537]
            lv480_1: R.Tensor((1280,), dtype="float32") = params[173]
            lv175 = R.call_tir(fused_matmul33_add30_add31, (lv174, lv479, lv480_1, lv169_1), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv481_1: R.Tensor((1280,), dtype="float32") = params[174]
            lv482: R.Tensor((1280,), dtype="float32") = params[175]
            lv730 = R.call_tir(layer_norm4, (lv175, lv481_1, lv482), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv483: R.Tensor((1280, 1280), dtype="float32") = params[538]
            lv732 = R.call_tir(matmul33, (lv730, lv483), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv484: R.Tensor((768, 1280), dtype="float32") = params[539]
            lv734 = R.call_tir(matmul28, (inp_2, lv484), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv485: R.Tensor((768, 1280), dtype="float32") = params[540]
            lv736 = R.call_tir(matmul28, (inp_2, lv485), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv176 = R.call_tir(fused_reshape42_transpose41_reshape43, (lv732,), out_sinfo=R.Tensor((16, 64, 160), dtype="float32"))
            lv177_1 = R.call_tir(fused_reshape38_transpose35_reshape39_transpose36, (lv734,), out_sinfo=R.Tensor((16, 160, 77), dtype="float32"))
            lv178 = R.call_tir(fused_reshape38_transpose35_reshape39, (lv736,), out_sinfo=R.Tensor((16, 77, 160), dtype="float32"))
            lv179_1 = R.call_tir(fused_matmul36_multiply15, (lv176, lv177_1), out_sinfo=R.Tensor((16, 64, 77), dtype="float32"))
            lv749 = R.call_tir(softmax8, (lv179_1,), out_sinfo=R.Tensor((16, 64, 77), dtype="float32"))
            lv750 = R.call_tir(matmul37, (lv749, lv178), out_sinfo=R.Tensor((16, 64, 160), dtype="float32"))
            lv180 = R.call_tir(fused_reshape44_transpose43_reshape45, (lv750,), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv486: R.Tensor((1280, 1280), dtype="float32") = params[541]
            lv487: R.Tensor((1280,), dtype="float32") = params[176]
            lv181_1 = R.call_tir(fused_matmul33_add30_add31, (lv180, lv486, lv487, lv175), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv488: R.Tensor((1280,), dtype="float32") = params[177]
            lv489_1: R.Tensor((1280,), dtype="float32") = params[178]
            lv758 = R.call_tir(layer_norm4, (lv181_1, lv488, lv489_1), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv490: R.Tensor((1280, 5120), dtype="float32") = params[543]
            lv491_1: R.Tensor((5120,), dtype="float32") = params[180]
            lv182 = R.call_tir(fused_matmul38_add32_gelu3, (lv758, lv490, lv491_1), out_sinfo=R.Tensor((2, 64, 5120), dtype="float32"))
            lv492: R.Tensor((1280, 5120), dtype="float32") = params[542]
            lv493_1: R.Tensor((5120,), dtype="float32") = params[179]
            lv183_1 = R.call_tir(fused_matmul38_add32_multiply16, (lv758, lv492, lv493_1, lv182), out_sinfo=R.Tensor((2, 64, 5120), dtype="float32"))
            lv494: R.Tensor((5120, 1280), dtype="float32") = params[544]
            lv495_1: R.Tensor((1280,), dtype="float32") = params[181]
            lv184 = R.call_tir(fused_matmul39_add30_add31, (lv183_1, lv494, lv495_1, lv181_1), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv185 = R.call_tir(fused_reshape46_transpose44, (lv184,), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv496: R.Tensor((1280, 1280, 1, 1), dtype="float32") = params[182]
            lv497: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[545]
            lv186 = R.call_tir(fused_conv2d15_add27_add29, (lv185, lv496, lv497, lv167), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv498: R.Tensor((1280,), dtype="float32") = params[161]
            lv499: R.Tensor((1280,), dtype="float32") = params[162]
            lv187 = R.call_tir(fused_group_norm8_silu6, (lv186, lv498, lv499), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv782 = R.call_tir(silu, (lv2,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv500: R.Tensor((1280, 1280), dtype="float32") = params[546]
            lv501: R.Tensor((1280,), dtype="float32") = params[164]
            lv188 = R.call_tir(fused_matmul6_add4_strided_slice5, (lv782, lv500, lv501), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv787 = R.call_tir(reshape32, (lv188,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv502: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[163]
            lv503: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[547]
            lv189 = R.call_tir(fused_conv2d14_add27_add28, (lv187, lv502, lv503, lv787), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv504: R.Tensor((1280,), dtype="float32") = params[165]
            lv505: R.Tensor((1280,), dtype="float32") = params[166]
            lv190 = R.call_tir(fused_group_norm8_silu6, (lv189, lv504, lv505), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv506: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[167]
            lv507: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[548]
            lv191 = R.call_tir(fused_conv2d14_add27_add29_divide3, (lv190, lv506, lv507, lv186), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv796 = R.call_tir(concatenate1, (lv191, lv162), out_sinfo=R.Tensor((2, 2560, 8, 8), dtype="float32"))
            lv508_1: R.Tensor((2560,), dtype="float32") = params[183]
            lv509_1: R.Tensor((2560,), dtype="float32") = params[184]
            lv192 = R.call_tir(fused_group_norm10_silu7, (lv796, lv508_1, lv509_1), out_sinfo=R.Tensor((2, 2560, 8, 8), dtype="float32"))
            lv802 = R.call_tir(silu, (lv2,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv510: R.Tensor((1280, 1280), dtype="float32") = params[549]
            lv511: R.Tensor((1280,), dtype="float32") = params[186]
            lv193 = R.call_tir(fused_matmul6_add4_strided_slice5, (lv802, lv510, lv511), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv807 = R.call_tir(reshape32, (lv193,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv512: R.Tensor((1280, 2560, 3, 3), dtype="float32") = params[185]
            lv513: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[550]
            lv194 = R.call_tir(fused_conv2d16_add27_add28, (lv192, lv512, lv513, lv807), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv514: R.Tensor((1280,), dtype="float32") = params[187]
            lv515: R.Tensor((1280,), dtype="float32") = params[188]
            lv195 = R.call_tir(fused_group_norm8_silu6, (lv194, lv514, lv515), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv516: R.Tensor((1280, 2560, 1, 1), dtype="float32") = params[190]
            lv517_1: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[552]
            lv196_2 = R.call_tir(fused_conv2d17_add27, (lv796, lv516, lv517_1), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv518: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[189]
            lv519: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[551]
            lv197_2 = R.call_tir(fused_conv2d14_add27_add29_divide3, (lv195, lv518, lv519, lv196_2), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv819 = R.call_tir(concatenate1, (lv197_2, lv157), out_sinfo=R.Tensor((2, 2560, 8, 8), dtype="float32"))
            lv520: R.Tensor((2560,), dtype="float32") = params[191]
            lv521: R.Tensor((2560,), dtype="float32") = params[192]
            lv198_1 = R.call_tir(fused_group_norm10_silu7, (lv819, lv520, lv521), out_sinfo=R.Tensor((2, 2560, 8, 8), dtype="float32"))
            lv825 = R.call_tir(silu, (lv2,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv522: R.Tensor((1280, 1280), dtype="float32") = params[553]
            lv523: R.Tensor((1280,), dtype="float32") = params[194]
            lv199_1 = R.call_tir(fused_matmul6_add4_strided_slice5, (lv825, lv522, lv523), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv830 = R.call_tir(reshape32, (lv199_1,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv524: R.Tensor((1280, 2560, 3, 3), dtype="float32") = params[193]
            lv525: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[554]
            lv200_1 = R.call_tir(fused_conv2d16_add27_add28, (lv198_1, lv524, lv525, lv830), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv526: R.Tensor((1280,), dtype="float32") = params[195]
            lv527: R.Tensor((1280,), dtype="float32") = params[196]
            lv201_1 = R.call_tir(fused_group_norm8_silu6, (lv200_1, lv526, lv527), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv528: R.Tensor((1280, 2560, 1, 1), dtype="float32") = params[198]
            lv529: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[555]
            lv202_1 = R.call_tir(fused_conv2d17_add27, (lv819, lv528, lv529), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv530: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[197]
            lv531: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[556]
            lv203_1 = R.call_tir(fused_conv2d14_add27_add29_divide3, (lv201_1, lv530, lv531, lv202_1), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv842 = R.call_tir(concatenate1, (lv203_1, lv152), out_sinfo=R.Tensor((2, 2560, 8, 8), dtype="float32"))
            lv532: R.Tensor((2560,), dtype="float32") = params[199]
            lv533: R.Tensor((2560,), dtype="float32") = params[200]
            lv204_1 = R.call_tir(fused_group_norm10_silu7, (lv842, lv532, lv533), out_sinfo=R.Tensor((2, 2560, 8, 8), dtype="float32"))
            lv848 = R.call_tir(silu, (lv2,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv534: R.Tensor((1280, 1280), dtype="float32") = params[558]
            lv535: R.Tensor((1280,), dtype="float32") = params[202]
            lv205_2 = R.call_tir(fused_matmul6_add4_strided_slice5, (lv848, lv534, lv535), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv853 = R.call_tir(reshape32, (lv205_2,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv536: R.Tensor((1280, 2560, 3, 3), dtype="float32") = params[201]
            lv537: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[557]
            lv206_1 = R.call_tir(fused_conv2d16_add27_add28, (lv204_1, lv536, lv537, lv853), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv538: R.Tensor((1280,), dtype="float32") = params[203]
            lv539: R.Tensor((1280,), dtype="float32") = params[204]
            lv207_1 = R.call_tir(fused_group_norm8_silu6, (lv206_1, lv538, lv539), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv540: R.Tensor((1280, 2560, 1, 1), dtype="float32") = params[206]
            lv541_1: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[559]
            lv208_1 = R.call_tir(fused_conv2d17_add27, (lv842, lv540, lv541_1), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv542: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[205]
            lv543: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[560]
            lv209_1 = R.call_tir(fused_conv2d14_add27_add29_divide3, (lv207_1, lv542, lv543, lv208_1), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv865 = R.call_tir(resize2d, (lv209_1,), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv544: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[207]
            lv545: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[561]
            lv210_1 = R.call_tir(fused_conv2d10_add21, (lv865, lv544, lv545), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv869 = R.call_tir(concatenate2, (lv210_1, lv151_1), out_sinfo=R.Tensor((2, 2560, 16, 16), dtype="float32"))
            lv546_1: R.Tensor((2560,), dtype="float32") = params[208]
            lv547: R.Tensor((2560,), dtype="float32") = params[209]
            lv211_1 = R.call_tir(fused_group_norm11_silu8, (lv869, lv546_1, lv547), out_sinfo=R.Tensor((2, 2560, 16, 16), dtype="float32"))
            lv875 = R.call_tir(silu, (lv2,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv548: R.Tensor((1280, 1280), dtype="float32") = params[562]
            lv549: R.Tensor((1280,), dtype="float32") = params[211]
            lv212_1 = R.call_tir(fused_matmul6_add4_strided_slice5, (lv875, lv548, lv549), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv880 = R.call_tir(reshape32, (lv212_1,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv550: R.Tensor((1280, 2560, 3, 3), dtype="float32") = params[210]
            lv551: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[563]
            lv213_1 = R.call_tir(fused_conv2d18_add21_add22, (lv211_1, lv550, lv551, lv880), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv552: R.Tensor((1280,), dtype="float32") = params[212]
            lv553: R.Tensor((1280,), dtype="float32") = params[213]
            lv214_1 = R.call_tir(fused_group_norm6_silu5, (lv213_1, lv552, lv553), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv554: R.Tensor((1280, 2560, 1, 1), dtype="float32") = params[215]
            lv555_1: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[564]
            lv215_1 = R.call_tir(fused_conv2d19_add21, (lv869, lv554, lv555_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv556: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[214]
            lv557: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[565]
            lv216_1 = R.call_tir(fused_conv2d10_add21_add23_divide2, (lv214_1, lv556, lv557, lv215_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv558: R.Tensor((1280,), dtype="float32") = params[232]
            lv559: R.Tensor((1280,), dtype="float32") = params[233]
            lv892 = R.call_tir(group_norm7, (lv216_1, lv558, lv559), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv560: R.Tensor((1280, 1280, 1, 1), dtype="float32") = params[234]
            lv561_1: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[566]
            lv217_1 = R.call_tir(fused_conv2d12_add21, (lv892, lv560, lv561_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv218_1 = R.call_tir(fused_transpose30_reshape33, (lv217_1,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv562: R.Tensor((1280,), dtype="float32") = params[235]
            lv563_1: R.Tensor((1280,), dtype="float32") = params[236]
            lv898 = R.call_tir(layer_norm3, (lv218_1, lv562, lv563_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv564: R.Tensor((1280, 1280), dtype="float32") = params[567]
            lv900 = R.call_tir(matmul25, (lv898, lv564), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv565_1: R.Tensor((1280, 1280), dtype="float32") = params[568]
            lv902 = R.call_tir(matmul25, (lv898, lv565_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv566: R.Tensor((1280, 1280), dtype="float32") = params[569]
            lv904 = R.call_tir(matmul25, (lv898, lv566), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv219_1 = R.call_tir(fused_reshape34_transpose31_reshape35, (lv900,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv220_1 = R.call_tir(fused_reshape34_transpose31_reshape35_transpose32, (lv902,), out_sinfo=R.Tensor((16, 160, 256), dtype="float32"))
            lv221_1 = R.call_tir(fused_reshape34_transpose31_reshape35, (lv904,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv222_1 = R.call_tir(fused_matmul26_multiply11, (lv219_1, lv220_1), out_sinfo=R.Tensor((16, 256, 256), dtype="float32"))
            lv917 = R.call_tir(softmax5, (lv222_1,), out_sinfo=R.Tensor((16, 256, 256), dtype="float32"))
            lv918 = R.call_tir(matmul27, (lv917, lv221_1), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv223_1 = R.call_tir(fused_reshape36_transpose33_reshape37, (lv918,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv567_1: R.Tensor((1280, 1280), dtype="float32") = params[570]
            lv568: R.Tensor((1280,), dtype="float32") = params[237]
            lv224_1 = R.call_tir(fused_matmul25_add24_add25, (lv223_1, lv567_1, lv568, lv218_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv569: R.Tensor((1280,), dtype="float32") = params[238]
            lv570: R.Tensor((1280,), dtype="float32") = params[239]
            lv926 = R.call_tir(layer_norm3, (lv224_1, lv569, lv570), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv571: R.Tensor((1280, 1280), dtype="float32") = params[571]
            lv928 = R.call_tir(matmul25, (lv926, lv571), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv572: R.Tensor((768, 1280), dtype="float32") = params[572]
            lv930 = R.call_tir(matmul28, (inp_2, lv572), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv573: R.Tensor((768, 1280), dtype="float32") = params[573]
            lv932 = R.call_tir(matmul28, (inp_2, lv573), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv225_1 = R.call_tir(fused_reshape34_transpose31_reshape35, (lv928,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv226_1 = R.call_tir(fused_reshape38_transpose35_reshape39_transpose36, (lv930,), out_sinfo=R.Tensor((16, 160, 77), dtype="float32"))
            lv227_1 = R.call_tir(fused_reshape38_transpose35_reshape39, (lv932,), out_sinfo=R.Tensor((16, 77, 160), dtype="float32"))
            lv228_1 = R.call_tir(fused_matmul29_multiply12, (lv225_1, lv226_1), out_sinfo=R.Tensor((16, 256, 77), dtype="float32"))
            lv945 = R.call_tir(softmax6, (lv228_1,), out_sinfo=R.Tensor((16, 256, 77), dtype="float32"))
            lv946 = R.call_tir(matmul30, (lv945, lv227_1), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv229_1 = R.call_tir(fused_reshape36_transpose33_reshape37, (lv946,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv574: R.Tensor((1280, 1280), dtype="float32") = params[574]
            lv575: R.Tensor((1280,), dtype="float32") = params[240]
            lv230_1 = R.call_tir(fused_matmul25_add24_add25, (lv229_1, lv574, lv575, lv224_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv576: R.Tensor((1280,), dtype="float32") = params[241]
            lv577: R.Tensor((1280,), dtype="float32") = params[242]
            lv954 = R.call_tir(layer_norm3, (lv230_1, lv576, lv577), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv578: R.Tensor((1280, 5120), dtype="float32") = params[576]
            lv579: R.Tensor((5120,), dtype="float32") = params[244]
            lv231_1 = R.call_tir(fused_matmul31_add26_gelu2, (lv954, lv578, lv579), out_sinfo=R.Tensor((2, 256, 5120), dtype="float32"))
            lv580_1: R.Tensor((1280, 5120), dtype="float32") = params[575]
            lv581_1: R.Tensor((5120,), dtype="float32") = params[243]
            lv232_2 = R.call_tir(fused_matmul31_add26_multiply13, (lv954, lv580_1, lv581_1, lv231_1), out_sinfo=R.Tensor((2, 256, 5120), dtype="float32"))
            lv582: R.Tensor((5120, 1280), dtype="float32") = params[577]
            lv583: R.Tensor((1280,), dtype="float32") = params[245]
            lv233_1 = R.call_tir(fused_matmul32_add24_add25, (lv232_2, lv582, lv583, lv230_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv234_1 = R.call_tir(fused_reshape40_transpose39, (lv233_1,), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv584: R.Tensor((1280, 1280, 1, 1), dtype="float32") = params[246]
            lv585: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[578]
            lv235_1 = R.call_tir(fused_conv2d12_add21_add23, (lv234_1, lv584, lv585, lv216_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv973 = R.call_tir(concatenate2, (lv235_1, lv127), out_sinfo=R.Tensor((2, 2560, 16, 16), dtype="float32"))
            lv586: R.Tensor((2560,), dtype="float32") = params[216]
            lv587: R.Tensor((2560,), dtype="float32") = params[217]
            lv236_1 = R.call_tir(fused_group_norm11_silu8, (lv973, lv586, lv587), out_sinfo=R.Tensor((2, 2560, 16, 16), dtype="float32"))
            lv979 = R.call_tir(silu, (lv2,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv588: R.Tensor((1280, 1280), dtype="float32") = params[579]
            lv589_1: R.Tensor((1280,), dtype="float32") = params[219]
            lv237_2 = R.call_tir(fused_matmul6_add4_strided_slice5, (lv979, lv588, lv589_1), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv984 = R.call_tir(reshape32, (lv237_2,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv590: R.Tensor((1280, 2560, 3, 3), dtype="float32") = params[218]
            lv591_1: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[580]
            lv238_1 = R.call_tir(fused_conv2d18_add21_add22, (lv236_1, lv590, lv591_1, lv984), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv592: R.Tensor((1280,), dtype="float32") = params[220]
            lv593_1: R.Tensor((1280,), dtype="float32") = params[221]
            lv239_1 = R.call_tir(fused_group_norm6_silu5, (lv238_1, lv592, lv593_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv594: R.Tensor((1280, 2560, 1, 1), dtype="float32") = params[223]
            lv595_1: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[581]
            lv240_1 = R.call_tir(fused_conv2d19_add21, (lv973, lv594, lv595_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv596: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[222]
            lv597: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[582]
            lv241_1 = R.call_tir(fused_conv2d10_add21_add23_divide2, (lv239_1, lv596, lv597, lv240_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv598: R.Tensor((1280,), dtype="float32") = params[247]
            lv599: R.Tensor((1280,), dtype="float32") = params[248]
            lv996 = R.call_tir(group_norm7, (lv241_1, lv598, lv599), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv600: R.Tensor((1280, 1280, 1, 1), dtype="float32") = params[249]
            lv601: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[583]
            lv242_1 = R.call_tir(fused_conv2d12_add21, (lv996, lv600, lv601), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv243_1 = R.call_tir(fused_transpose30_reshape33, (lv242_1,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv602: R.Tensor((1280,), dtype="float32") = params[250]
            lv603: R.Tensor((1280,), dtype="float32") = params[251]
            lv1002 = R.call_tir(layer_norm3, (lv243_1, lv602, lv603), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv604: R.Tensor((1280, 1280), dtype="float32") = params[584]
            lv1004 = R.call_tir(matmul25, (lv1002, lv604), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv605: R.Tensor((1280, 1280), dtype="float32") = params[585]
            lv1006 = R.call_tir(matmul25, (lv1002, lv605), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv606: R.Tensor((1280, 1280), dtype="float32") = params[586]
            lv1008 = R.call_tir(matmul25, (lv1002, lv606), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv244_1 = R.call_tir(fused_reshape34_transpose31_reshape35, (lv1004,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv245_1 = R.call_tir(fused_reshape34_transpose31_reshape35_transpose32, (lv1006,), out_sinfo=R.Tensor((16, 160, 256), dtype="float32"))
            lv246_1 = R.call_tir(fused_reshape34_transpose31_reshape35, (lv1008,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv247_1 = R.call_tir(fused_matmul26_multiply11, (lv244_1, lv245_1), out_sinfo=R.Tensor((16, 256, 256), dtype="float32"))
            lv1021 = R.call_tir(softmax5, (lv247_1,), out_sinfo=R.Tensor((16, 256, 256), dtype="float32"))
            lv1022 = R.call_tir(matmul27, (lv1021, lv246_1), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv248_1 = R.call_tir(fused_reshape36_transpose33_reshape37, (lv1022,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv607: R.Tensor((1280, 1280), dtype="float32") = params[587]
            lv608_1: R.Tensor((1280,), dtype="float32") = params[252]
            lv249_2 = R.call_tir(fused_matmul25_add24_add25, (lv248_1, lv607, lv608_1, lv243_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv609_1: R.Tensor((1280,), dtype="float32") = params[253]
            lv610: R.Tensor((1280,), dtype="float32") = params[254]
            lv1030 = R.call_tir(layer_norm3, (lv249_2, lv609_1, lv610), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv611: R.Tensor((1280, 1280), dtype="float32") = params[588]
            lv1032 = R.call_tir(matmul25, (lv1030, lv611), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv612: R.Tensor((768, 1280), dtype="float32") = params[589]
            lv1034 = R.call_tir(matmul28, (inp_2, lv612), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv613: R.Tensor((768, 1280), dtype="float32") = params[590]
            lv1036 = R.call_tir(matmul28, (inp_2, lv613), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv250_1 = R.call_tir(fused_reshape34_transpose31_reshape35, (lv1032,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv251_1 = R.call_tir(fused_reshape38_transpose35_reshape39_transpose36, (lv1034,), out_sinfo=R.Tensor((16, 160, 77), dtype="float32"))
            lv252_1 = R.call_tir(fused_reshape38_transpose35_reshape39, (lv1036,), out_sinfo=R.Tensor((16, 77, 160), dtype="float32"))
            lv253_1 = R.call_tir(fused_matmul29_multiply12, (lv250_1, lv251_1), out_sinfo=R.Tensor((16, 256, 77), dtype="float32"))
            lv1049 = R.call_tir(softmax6, (lv253_1,), out_sinfo=R.Tensor((16, 256, 77), dtype="float32"))
            lv1050 = R.call_tir(matmul30, (lv1049, lv252_1), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv254_1 = R.call_tir(fused_reshape36_transpose33_reshape37, (lv1050,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv614: R.Tensor((1280, 1280), dtype="float32") = params[591]
            lv615: R.Tensor((1280,), dtype="float32") = params[255]
            lv255_2 = R.call_tir(fused_matmul25_add24_add25, (lv254_1, lv614, lv615, lv249_2), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv616: R.Tensor((1280,), dtype="float32") = params[256]
            lv617_1: R.Tensor((1280,), dtype="float32") = params[257]
            lv1058 = R.call_tir(layer_norm3, (lv255_2, lv616, lv617_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv618: R.Tensor((1280, 5120), dtype="float32") = params[593]
            lv619: R.Tensor((5120,), dtype="float32") = params[259]
            lv256_1 = R.call_tir(fused_matmul31_add26_gelu2, (lv1058, lv618, lv619), out_sinfo=R.Tensor((2, 256, 5120), dtype="float32"))
            lv620: R.Tensor((1280, 5120), dtype="float32") = params[592]
            lv621: R.Tensor((5120,), dtype="float32") = params[258]
            lv257_2 = R.call_tir(fused_matmul31_add26_multiply13, (lv1058, lv620, lv621, lv256_1), out_sinfo=R.Tensor((2, 256, 5120), dtype="float32"))
            lv622: R.Tensor((5120, 1280), dtype="float32") = params[594]
            lv623: R.Tensor((1280,), dtype="float32") = params[260]
            lv258_1 = R.call_tir(fused_matmul32_add24_add25, (lv257_2, lv622, lv623, lv255_2), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv259_2 = R.call_tir(fused_reshape40_transpose39, (lv258_1,), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv624: R.Tensor((1280, 1280, 1, 1), dtype="float32") = params[261]
            lv625: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[595]
            lv260_1 = R.call_tir(fused_conv2d12_add21_add23, (lv259_2, lv624, lv625, lv241_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv1077 = R.call_tir(concatenate3, (lv260_1, lv102), out_sinfo=R.Tensor((2, 1920, 16, 16), dtype="float32"))
            lv626: R.Tensor((1920,), dtype="float32") = params[224]
            lv627: R.Tensor((1920,), dtype="float32") = params[225]
            lv261_2 = R.call_tir(fused_group_norm12_silu9, (lv1077, lv626, lv627), out_sinfo=R.Tensor((2, 1920, 16, 16), dtype="float32"))
            lv1083 = R.call_tir(silu, (lv2,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv628: R.Tensor((1280, 1280), dtype="float32") = params[596]
            lv629: R.Tensor((1280,), dtype="float32") = params[227]
            lv262_1 = R.call_tir(fused_matmul6_add4_strided_slice5, (lv1083, lv628, lv629), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv1088 = R.call_tir(reshape32, (lv262_1,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv630: R.Tensor((1280, 1920, 3, 3), dtype="float32") = params[226]
            lv631: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[597]
            lv263_1 = R.call_tir(fused_conv2d20_add21_add22, (lv261_2, lv630, lv631, lv1088), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv632: R.Tensor((1280,), dtype="float32") = params[228]
            lv633: R.Tensor((1280,), dtype="float32") = params[229]
            lv264_1 = R.call_tir(fused_group_norm6_silu5, (lv263_1, lv632, lv633), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv634: R.Tensor((1280, 1920, 1, 1), dtype="float32") = params[231]
            lv635: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[598]
            lv265_1 = R.call_tir(fused_conv2d21_add21, (lv1077, lv634, lv635), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv636: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[230]
            lv637: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[599]
            lv266_1 = R.call_tir(fused_conv2d10_add21_add23_divide2, (lv264_1, lv636, lv637, lv265_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv638: R.Tensor((1280,), dtype="float32") = params[262]
            lv639: R.Tensor((1280,), dtype="float32") = params[263]
            lv1100 = R.call_tir(group_norm7, (lv266_1, lv638, lv639), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv640: R.Tensor((1280, 1280, 1, 1), dtype="float32") = params[264]
            lv641: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[600]
            lv267_1 = R.call_tir(fused_conv2d12_add21, (lv1100, lv640, lv641), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv268_1 = R.call_tir(fused_transpose30_reshape33, (lv267_1,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv642: R.Tensor((1280,), dtype="float32") = params[265]
            lv643: R.Tensor((1280,), dtype="float32") = params[266]
            lv1106 = R.call_tir(layer_norm3, (lv268_1, lv642, lv643), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv644_1: R.Tensor((1280, 1280), dtype="float32") = params[601]
            lv1108 = R.call_tir(matmul25, (lv1106, lv644_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv645: R.Tensor((1280, 1280), dtype="float32") = params[602]
            lv1110 = R.call_tir(matmul25, (lv1106, lv645), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv646: R.Tensor((1280, 1280), dtype="float32") = params[603]
            lv1112 = R.call_tir(matmul25, (lv1106, lv646), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv269_1 = R.call_tir(fused_reshape34_transpose31_reshape35, (lv1108,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv270_1 = R.call_tir(fused_reshape34_transpose31_reshape35_transpose32, (lv1110,), out_sinfo=R.Tensor((16, 160, 256), dtype="float32"))
            lv271_1 = R.call_tir(fused_reshape34_transpose31_reshape35, (lv1112,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv272_1 = R.call_tir(fused_matmul26_multiply11, (lv269_1, lv270_1), out_sinfo=R.Tensor((16, 256, 256), dtype="float32"))
            lv1125 = R.call_tir(softmax5, (lv272_1,), out_sinfo=R.Tensor((16, 256, 256), dtype="float32"))
            lv1126 = R.call_tir(matmul27, (lv1125, lv271_1), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv273_1 = R.call_tir(fused_reshape36_transpose33_reshape37, (lv1126,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv647: R.Tensor((1280, 1280), dtype="float32") = params[604]
            lv648: R.Tensor((1280,), dtype="float32") = params[267]
            lv274_2 = R.call_tir(fused_matmul25_add24_add25, (lv273_1, lv647, lv648, lv268_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv649_1: R.Tensor((1280,), dtype="float32") = params[268]
            lv650: R.Tensor((1280,), dtype="float32") = params[269]
            lv1134 = R.call_tir(layer_norm3, (lv274_2, lv649_1, lv650), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv651: R.Tensor((1280, 1280), dtype="float32") = params[605]
            lv1136 = R.call_tir(matmul25, (lv1134, lv651), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv652: R.Tensor((768, 1280), dtype="float32") = params[606]
            lv1138 = R.call_tir(matmul28, (inp_2, lv652), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv653: R.Tensor((768, 1280), dtype="float32") = params[607]
            lv1140 = R.call_tir(matmul28, (inp_2, lv653), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv275_2 = R.call_tir(fused_reshape34_transpose31_reshape35, (lv1136,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv276_1 = R.call_tir(fused_reshape38_transpose35_reshape39_transpose36, (lv1138,), out_sinfo=R.Tensor((16, 160, 77), dtype="float32"))
            lv277_1 = R.call_tir(fused_reshape38_transpose35_reshape39, (lv1140,), out_sinfo=R.Tensor((16, 77, 160), dtype="float32"))
            lv278_1 = R.call_tir(fused_matmul29_multiply12, (lv275_2, lv276_1), out_sinfo=R.Tensor((16, 256, 77), dtype="float32"))
            lv1153 = R.call_tir(softmax6, (lv278_1,), out_sinfo=R.Tensor((16, 256, 77), dtype="float32"))
            lv1154 = R.call_tir(matmul30, (lv1153, lv277_1), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv279_1 = R.call_tir(fused_reshape36_transpose33_reshape37, (lv1154,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv654: R.Tensor((1280, 1280), dtype="float32") = params[608]
            lv655: R.Tensor((1280,), dtype="float32") = params[270]
            lv280_1 = R.call_tir(fused_matmul25_add24_add25, (lv279_1, lv654, lv655, lv274_2), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv656: R.Tensor((1280,), dtype="float32") = params[271]
            lv657: R.Tensor((1280,), dtype="float32") = params[272]
            lv1162 = R.call_tir(layer_norm3, (lv280_1, lv656, lv657), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv658: R.Tensor((1280, 5120), dtype="float32") = params[610]
            lv659: R.Tensor((5120,), dtype="float32") = params[274]
            lv281_1 = R.call_tir(fused_matmul31_add26_gelu2, (lv1162, lv658, lv659), out_sinfo=R.Tensor((2, 256, 5120), dtype="float32"))
            lv660: R.Tensor((1280, 5120), dtype="float32") = params[609]
            lv661: R.Tensor((5120,), dtype="float32") = params[273]
            lv282_1 = R.call_tir(fused_matmul31_add26_multiply13, (lv1162, lv660, lv661, lv281_1), out_sinfo=R.Tensor((2, 256, 5120), dtype="float32"))
            lv662: R.Tensor((5120, 1280), dtype="float32") = params[611]
            lv663_1: R.Tensor((1280,), dtype="float32") = params[275]
            lv283_2 = R.call_tir(fused_matmul32_add24_add25, (lv282_1, lv662, lv663_1, lv280_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv284_1 = R.call_tir(fused_reshape40_transpose39, (lv283_2,), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv664: R.Tensor((1280, 1280, 1, 1), dtype="float32") = params[276]
            lv665: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[612]
            lv285_2 = R.call_tir(fused_conv2d12_add21_add23, (lv284_1, lv664, lv665, lv266_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv1181 = R.call_tir(resize2d1, (lv285_2,), out_sinfo=R.Tensor((2, 1280, 32, 32), dtype="float32"))
            lv666: R.Tensor((1280, 1280, 3, 3), dtype="float32") = params[277]
            lv667: R.Tensor((1, 1280, 1, 1), dtype="float32") = params[613]
            lv286_1 = R.call_tir(fused_conv2d22_add33, (lv1181, lv666, lv667), out_sinfo=R.Tensor((2, 1280, 32, 32), dtype="float32"))
            lv1185 = R.call_tir(concatenate4, (lv286_1, lv101), out_sinfo=R.Tensor((2, 1920, 32, 32), dtype="float32"))
            lv668_1: R.Tensor((1920,), dtype="float32") = params[278]
            lv669: R.Tensor((1920,), dtype="float32") = params[279]
            lv287_2 = R.call_tir(fused_group_norm13_silu10, (lv1185, lv668_1, lv669), out_sinfo=R.Tensor((2, 1920, 32, 32), dtype="float32"))
            lv1191 = R.call_tir(silu, (lv2,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv670: R.Tensor((1280, 640), dtype="float32") = params[615]
            lv671: R.Tensor((640,), dtype="float32") = params[281]
            lv288_1 = R.call_tir(fused_matmul16_add14_strided_slice4, (lv1191, lv670, lv671), out_sinfo=R.Tensor((2, 640), dtype="float32"))
            lv1196 = R.call_tir(reshape22, (lv288_1,), out_sinfo=R.Tensor((2, 640, 1, 1), dtype="float32"))
            lv672: R.Tensor((640, 1920, 3, 3), dtype="float32") = params[280]
            lv673: R.Tensor((1, 640, 1, 1), dtype="float32") = params[614]
            lv289_2 = R.call_tir(fused_conv2d23_add13_add15, (lv287_2, lv672, lv673, lv1196), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv674: R.Tensor((640,), dtype="float32") = params[282]
            lv675: R.Tensor((640,), dtype="float32") = params[283]
            lv290_1 = R.call_tir(fused_group_norm3_silu3, (lv289_2, lv674, lv675), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv676: R.Tensor((640, 1920, 1, 1), dtype="float32") = params[285]
            lv677: R.Tensor((1, 640, 1, 1), dtype="float32") = params[616]
            lv291_1 = R.call_tir(fused_conv2d24_add13, (lv1185, lv676, lv677), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv678: R.Tensor((640, 640, 3, 3), dtype="float32") = params[284]
            lv679: R.Tensor((1, 640, 1, 1), dtype="float32") = params[617]
            lv292_1 = R.call_tir(fused_conv2d5_add13_add16_divide1, (lv290_1, lv678, lv679, lv291_1), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv680: R.Tensor((640,), dtype="float32") = params[302]
            lv681: R.Tensor((640,), dtype="float32") = params[303]
            lv1208 = R.call_tir(group_norm4, (lv292_1, lv680, lv681), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv682_1: R.Tensor((640, 640, 1, 1), dtype="float32") = params[304]
            lv683: R.Tensor((1, 640, 1, 1), dtype="float32") = params[618]
            lv293_1 = R.call_tir(fused_conv2d7_add13, (lv1208, lv682_1, lv683), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv294_1 = R.call_tir(fused_transpose19_reshape23, (lv293_1,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv684: R.Tensor((640,), dtype="float32") = params[305]
            lv685: R.Tensor((640,), dtype="float32") = params[306]
            lv1214 = R.call_tir(layer_norm2, (lv294_1, lv684, lv685), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv686: R.Tensor((640, 640), dtype="float32") = params[619]
            lv1216 = R.call_tir(matmul17, (lv1214, lv686), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv687_1: R.Tensor((640, 640), dtype="float32") = params[620]
            lv1218 = R.call_tir(matmul17, (lv1214, lv687_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv688: R.Tensor((640, 640), dtype="float32") = params[621]
            lv1220 = R.call_tir(matmul17, (lv1214, lv688), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv295_1 = R.call_tir(fused_reshape24_transpose21_reshape25, (lv1216,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv296_1 = R.call_tir(fused_reshape24_transpose21_reshape25_transpose22, (lv1218,), out_sinfo=R.Tensor((16, 80, 1024), dtype="float32"))
            lv297_1 = R.call_tir(fused_reshape24_transpose21_reshape25, (lv1220,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv298_1 = R.call_tir(fused_matmul18_multiply8, (lv295_1, lv296_1), out_sinfo=R.Tensor((16, 1024, 1024), dtype="float32"))
            lv1233 = R.call_tir(softmax3, (lv298_1,), out_sinfo=R.Tensor((16, 1024, 1024), dtype="float32"))
            lv1234 = R.call_tir(matmul19, (lv1233, lv297_1), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv299_1 = R.call_tir(fused_reshape26_transpose23_reshape27, (lv1234,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv689: R.Tensor((640, 640), dtype="float32") = params[622]
            lv690: R.Tensor((640,), dtype="float32") = params[307]
            lv300_1 = R.call_tir(fused_matmul17_add17_add18, (lv299_1, lv689, lv690, lv294_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv691: R.Tensor((640,), dtype="float32") = params[308]
            lv692: R.Tensor((640,), dtype="float32") = params[309]
            lv1242 = R.call_tir(layer_norm2, (lv300_1, lv691, lv692), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv693: R.Tensor((640, 640), dtype="float32") = params[623]
            lv1244 = R.call_tir(matmul17, (lv1242, lv693), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv694: R.Tensor((768, 640), dtype="float32") = params[624]
            lv1246 = R.call_tir(matmul20, (inp_2, lv694), out_sinfo=R.Tensor((2, 77, 640), dtype="float32"))
            lv695: R.Tensor((768, 640), dtype="float32") = params[625]
            lv1248 = R.call_tir(matmul20, (inp_2, lv695), out_sinfo=R.Tensor((2, 77, 640), dtype="float32"))
            lv301_1 = R.call_tir(fused_reshape24_transpose21_reshape25, (lv1244,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv302_2 = R.call_tir(fused_reshape28_transpose25_reshape29_transpose26, (lv1246,), out_sinfo=R.Tensor((16, 80, 77), dtype="float32"))
            lv303_2 = R.call_tir(fused_reshape28_transpose25_reshape29, (lv1248,), out_sinfo=R.Tensor((16, 77, 80), dtype="float32"))
            lv304_1 = R.call_tir(fused_matmul21_multiply9, (lv301_1, lv302_2), out_sinfo=R.Tensor((16, 1024, 77), dtype="float32"))
            lv1261 = R.call_tir(softmax4, (lv304_1,), out_sinfo=R.Tensor((16, 1024, 77), dtype="float32"))
            lv1262 = R.call_tir(matmul22, (lv1261, lv303_2), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv305_1 = R.call_tir(fused_reshape26_transpose23_reshape27, (lv1262,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv696_1: R.Tensor((640, 640), dtype="float32") = params[626]
            lv697: R.Tensor((640,), dtype="float32") = params[310]
            lv306_1 = R.call_tir(fused_matmul17_add17_add18, (lv305_1, lv696_1, lv697, lv300_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv698: R.Tensor((640,), dtype="float32") = params[311]
            lv699: R.Tensor((640,), dtype="float32") = params[312]
            lv1270 = R.call_tir(layer_norm2, (lv306_1, lv698, lv699), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv700: R.Tensor((640, 2560), dtype="float32") = params[628]
            lv701: R.Tensor((2560,), dtype="float32") = params[314]
            lv307_1 = R.call_tir(fused_matmul23_add19_gelu1, (lv1270, lv700, lv701), out_sinfo=R.Tensor((2, 1024, 2560), dtype="float32"))
            lv702_1: R.Tensor((640, 2560), dtype="float32") = params[627]
            lv703: R.Tensor((2560,), dtype="float32") = params[313]
            lv308_1 = R.call_tir(fused_matmul23_add19_multiply10, (lv1270, lv702_1, lv703, lv307_1), out_sinfo=R.Tensor((2, 1024, 2560), dtype="float32"))
            lv704_1: R.Tensor((2560, 640), dtype="float32") = params[629]
            lv705: R.Tensor((640,), dtype="float32") = params[315]
            lv309_1 = R.call_tir(fused_matmul24_add17_add18, (lv308_1, lv704_1, lv705, lv306_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv310_1 = R.call_tir(fused_reshape30_transpose29, (lv309_1,), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv706_1: R.Tensor((640, 640, 1, 1), dtype="float32") = params[316]
            lv707: R.Tensor((1, 640, 1, 1), dtype="float32") = params[630]
            lv311_2 = R.call_tir(fused_conv2d7_add13_add16, (lv310_1, lv706_1, lv707, lv292_1), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv1289 = R.call_tir(concatenate5, (lv311_2, lv77_1), out_sinfo=R.Tensor((2, 1280, 32, 32), dtype="float32"))
            lv708_1: R.Tensor((1280,), dtype="float32") = params[286]
            lv709: R.Tensor((1280,), dtype="float32") = params[287]
            lv312_1 = R.call_tir(fused_group_norm14_silu11, (lv1289, lv708_1, lv709), out_sinfo=R.Tensor((2, 1280, 32, 32), dtype="float32"))
            lv1295 = R.call_tir(silu, (lv2,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv710: R.Tensor((1280, 640), dtype="float32") = params[631]
            lv711: R.Tensor((640,), dtype="float32") = params[289]
            lv313_1 = R.call_tir(fused_matmul16_add14_strided_slice4, (lv1295, lv710, lv711), out_sinfo=R.Tensor((2, 640), dtype="float32"))
            lv1300 = R.call_tir(reshape22, (lv313_1,), out_sinfo=R.Tensor((2, 640, 1, 1), dtype="float32"))
            lv712: R.Tensor((640, 1280, 3, 3), dtype="float32") = params[288]
            lv713: R.Tensor((1, 640, 1, 1), dtype="float32") = params[632]
            lv314_1 = R.call_tir(fused_conv2d25_add13_add15, (lv312_1, lv712, lv713, lv1300), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv714: R.Tensor((640,), dtype="float32") = params[290]
            lv715: R.Tensor((640,), dtype="float32") = params[291]
            lv315_1 = R.call_tir(fused_group_norm3_silu3, (lv314_1, lv714, lv715), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv716: R.Tensor((640, 1280, 1, 1), dtype="float32") = params[293]
            lv717: R.Tensor((1, 640, 1, 1), dtype="float32") = params[633]
            lv316_1 = R.call_tir(fused_conv2d26_add13, (lv1289, lv716, lv717), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv718: R.Tensor((640, 640, 3, 3), dtype="float32") = params[292]
            lv719: R.Tensor((1, 640, 1, 1), dtype="float32") = params[634]
            lv317_1 = R.call_tir(fused_conv2d5_add13_add16_divide1, (lv315_1, lv718, lv719, lv316_1), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv720: R.Tensor((640,), dtype="float32") = params[317]
            lv721_1: R.Tensor((640,), dtype="float32") = params[318]
            lv1312 = R.call_tir(group_norm4, (lv317_1, lv720, lv721_1), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv722_1: R.Tensor((640, 640, 1, 1), dtype="float32") = params[319]
            lv723: R.Tensor((1, 640, 1, 1), dtype="float32") = params[635]
            lv318_1 = R.call_tir(fused_conv2d7_add13, (lv1312, lv722_1, lv723), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv319_1 = R.call_tir(fused_transpose19_reshape23, (lv318_1,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv724: R.Tensor((640,), dtype="float32") = params[320]
            lv725: R.Tensor((640,), dtype="float32") = params[321]
            lv1318 = R.call_tir(layer_norm2, (lv319_1, lv724, lv725), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv726: R.Tensor((640, 640), dtype="float32") = params[636]
            lv1320 = R.call_tir(matmul17, (lv1318, lv726), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv727: R.Tensor((640, 640), dtype="float32") = params[637]
            lv1322 = R.call_tir(matmul17, (lv1318, lv727), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv728: R.Tensor((640, 640), dtype="float32") = params[638]
            lv1324 = R.call_tir(matmul17, (lv1318, lv728), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv320_1 = R.call_tir(fused_reshape24_transpose21_reshape25, (lv1320,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv321_1 = R.call_tir(fused_reshape24_transpose21_reshape25_transpose22, (lv1322,), out_sinfo=R.Tensor((16, 80, 1024), dtype="float32"))
            lv322_1 = R.call_tir(fused_reshape24_transpose21_reshape25, (lv1324,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv323_1 = R.call_tir(fused_matmul18_multiply8, (lv320_1, lv321_1), out_sinfo=R.Tensor((16, 1024, 1024), dtype="float32"))
            lv1337 = R.call_tir(softmax3, (lv323_1,), out_sinfo=R.Tensor((16, 1024, 1024), dtype="float32"))
            lv1338 = R.call_tir(matmul19, (lv1337, lv322_1), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv324_1 = R.call_tir(fused_reshape26_transpose23_reshape27, (lv1338,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv729: R.Tensor((640, 640), dtype="float32") = params[639]
            lv730_1: R.Tensor((640,), dtype="float32") = params[322]
            lv325_1 = R.call_tir(fused_matmul17_add17_add18, (lv324_1, lv729, lv730_1, lv319_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv731: R.Tensor((640,), dtype="float32") = params[323]
            lv732_1: R.Tensor((640,), dtype="float32") = params[324]
            lv1346 = R.call_tir(layer_norm2, (lv325_1, lv731, lv732_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv733: R.Tensor((640, 640), dtype="float32") = params[640]
            lv1348 = R.call_tir(matmul17, (lv1346, lv733), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv734_1: R.Tensor((768, 640), dtype="float32") = params[641]
            lv1350 = R.call_tir(matmul20, (inp_2, lv734_1), out_sinfo=R.Tensor((2, 77, 640), dtype="float32"))
            lv735: R.Tensor((768, 640), dtype="float32") = params[642]
            lv1352 = R.call_tir(matmul20, (inp_2, lv735), out_sinfo=R.Tensor((2, 77, 640), dtype="float32"))
            lv326_1 = R.call_tir(fused_reshape24_transpose21_reshape25, (lv1348,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv327_1 = R.call_tir(fused_reshape28_transpose25_reshape29_transpose26, (lv1350,), out_sinfo=R.Tensor((16, 80, 77), dtype="float32"))
            lv328_1 = R.call_tir(fused_reshape28_transpose25_reshape29, (lv1352,), out_sinfo=R.Tensor((16, 77, 80), dtype="float32"))
            lv329_1 = R.call_tir(fused_matmul21_multiply9, (lv326_1, lv327_1), out_sinfo=R.Tensor((16, 1024, 77), dtype="float32"))
            lv1365 = R.call_tir(softmax4, (lv329_1,), out_sinfo=R.Tensor((16, 1024, 77), dtype="float32"))
            lv1366 = R.call_tir(matmul22, (lv1365, lv328_1), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv330_1 = R.call_tir(fused_reshape26_transpose23_reshape27, (lv1366,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv736_1: R.Tensor((640, 640), dtype="float32") = params[643]
            lv737: R.Tensor((640,), dtype="float32") = params[325]
            lv331_1 = R.call_tir(fused_matmul17_add17_add18, (lv330_1, lv736_1, lv737, lv325_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv738: R.Tensor((640,), dtype="float32") = params[326]
            lv739: R.Tensor((640,), dtype="float32") = params[327]
            lv1374 = R.call_tir(layer_norm2, (lv331_1, lv738, lv739), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv740: R.Tensor((640, 2560), dtype="float32") = params[645]
            lv741: R.Tensor((2560,), dtype="float32") = params[329]
            lv332_1 = R.call_tir(fused_matmul23_add19_gelu1, (lv1374, lv740, lv741), out_sinfo=R.Tensor((2, 1024, 2560), dtype="float32"))
            lv742: R.Tensor((640, 2560), dtype="float32") = params[644]
            lv743: R.Tensor((2560,), dtype="float32") = params[328]
            lv333_1 = R.call_tir(fused_matmul23_add19_multiply10, (lv1374, lv742, lv743, lv332_1), out_sinfo=R.Tensor((2, 1024, 2560), dtype="float32"))
            lv744: R.Tensor((2560, 640), dtype="float32") = params[646]
            lv745: R.Tensor((640,), dtype="float32") = params[330]
            lv334_1 = R.call_tir(fused_matmul24_add17_add18, (lv333_1, lv744, lv745, lv331_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv335_2 = R.call_tir(fused_reshape30_transpose29, (lv334_1,), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv746: R.Tensor((640, 640, 1, 1), dtype="float32") = params[331]
            lv747: R.Tensor((1, 640, 1, 1), dtype="float32") = params[647]
            lv336_1 = R.call_tir(fused_conv2d7_add13_add16, (lv335_2, lv746, lv747, lv317_1), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv1393 = R.call_tir(concatenate6, (lv336_1, lv52), out_sinfo=R.Tensor((2, 960, 32, 32), dtype="float32"))
            lv748: R.Tensor((960,), dtype="float32") = params[294]
            lv749_1: R.Tensor((960,), dtype="float32") = params[295]
            lv337_1 = R.call_tir(fused_group_norm15_silu12, (lv1393, lv748, lv749_1), out_sinfo=R.Tensor((2, 960, 32, 32), dtype="float32"))
            lv1399 = R.call_tir(silu, (lv2,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv750_1: R.Tensor((1280, 640), dtype="float32") = params[648]
            lv751: R.Tensor((640,), dtype="float32") = params[297]
            lv338_1 = R.call_tir(fused_matmul16_add14_strided_slice4, (lv1399, lv750_1, lv751), out_sinfo=R.Tensor((2, 640), dtype="float32"))
            lv1404 = R.call_tir(reshape22, (lv338_1,), out_sinfo=R.Tensor((2, 640, 1, 1), dtype="float32"))
            lv752: R.Tensor((640, 960, 3, 3), dtype="float32") = params[296]
            lv753: R.Tensor((1, 640, 1, 1), dtype="float32") = params[649]
            lv339_1 = R.call_tir(fused_conv2d27_add13_add15, (lv337_1, lv752, lv753, lv1404), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv754: R.Tensor((640,), dtype="float32") = params[298]
            lv755: R.Tensor((640,), dtype="float32") = params[299]
            lv340_2 = R.call_tir(fused_group_norm3_silu3, (lv339_1, lv754, lv755), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv756: R.Tensor((640, 960, 1, 1), dtype="float32") = params[301]
            lv757: R.Tensor((1, 640, 1, 1), dtype="float32") = params[650]
            lv341_1 = R.call_tir(fused_conv2d28_add13, (lv1393, lv756, lv757), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv758_1: R.Tensor((640, 640, 3, 3), dtype="float32") = params[300]
            lv759: R.Tensor((1, 640, 1, 1), dtype="float32") = params[651]
            lv342_1 = R.call_tir(fused_conv2d5_add13_add16_divide1, (lv340_2, lv758_1, lv759, lv341_1), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv760: R.Tensor((640,), dtype="float32") = params[332]
            lv761: R.Tensor((640,), dtype="float32") = params[333]
            lv1416 = R.call_tir(group_norm4, (lv342_1, lv760, lv761), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv762: R.Tensor((640, 640, 1, 1), dtype="float32") = params[334]
            lv763: R.Tensor((1, 640, 1, 1), dtype="float32") = params[652]
            lv343_1 = R.call_tir(fused_conv2d7_add13, (lv1416, lv762, lv763), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv344_1 = R.call_tir(fused_transpose19_reshape23, (lv343_1,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv764: R.Tensor((640,), dtype="float32") = params[335]
            lv765: R.Tensor((640,), dtype="float32") = params[336]
            lv1422 = R.call_tir(layer_norm2, (lv344_1, lv764, lv765), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv766: R.Tensor((640, 640), dtype="float32") = params[653]
            lv1424 = R.call_tir(matmul17, (lv1422, lv766), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv767: R.Tensor((640, 640), dtype="float32") = params[654]
            lv1426 = R.call_tir(matmul17, (lv1422, lv767), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv768: R.Tensor((640, 640), dtype="float32") = params[655]
            lv1428 = R.call_tir(matmul17, (lv1422, lv768), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv345_1 = R.call_tir(fused_reshape24_transpose21_reshape25, (lv1424,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv346_1 = R.call_tir(fused_reshape24_transpose21_reshape25_transpose22, (lv1426,), out_sinfo=R.Tensor((16, 80, 1024), dtype="float32"))
            lv347_1 = R.call_tir(fused_reshape24_transpose21_reshape25, (lv1428,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv348_1 = R.call_tir(fused_matmul18_multiply8, (lv345_1, lv346_1), out_sinfo=R.Tensor((16, 1024, 1024), dtype="float32"))
            lv1441 = R.call_tir(softmax3, (lv348_1,), out_sinfo=R.Tensor((16, 1024, 1024), dtype="float32"))
            lv1442 = R.call_tir(matmul19, (lv1441, lv347_1), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv349_2 = R.call_tir(fused_reshape26_transpose23_reshape27, (lv1442,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv769: R.Tensor((640, 640), dtype="float32") = params[656]
            lv770: R.Tensor((640,), dtype="float32") = params[337]
            lv350_1 = R.call_tir(fused_matmul17_add17_add18, (lv349_2, lv769, lv770, lv344_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv771: R.Tensor((640,), dtype="float32") = params[338]
            lv772: R.Tensor((640,), dtype="float32") = params[339]
            lv1450 = R.call_tir(layer_norm2, (lv350_1, lv771, lv772), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv773: R.Tensor((640, 640), dtype="float32") = params[657]
            lv1452 = R.call_tir(matmul17, (lv1450, lv773), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv774: R.Tensor((768, 640), dtype="float32") = params[658]
            lv1454 = R.call_tir(matmul20, (inp_2, lv774), out_sinfo=R.Tensor((2, 77, 640), dtype="float32"))
            lv775: R.Tensor((768, 640), dtype="float32") = params[659]
            lv1456 = R.call_tir(matmul20, (inp_2, lv775), out_sinfo=R.Tensor((2, 77, 640), dtype="float32"))
            lv351_1 = R.call_tir(fused_reshape24_transpose21_reshape25, (lv1452,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv352_1 = R.call_tir(fused_reshape28_transpose25_reshape29_transpose26, (lv1454,), out_sinfo=R.Tensor((16, 80, 77), dtype="float32"))
            lv353_1 = R.call_tir(fused_reshape28_transpose25_reshape29, (lv1456,), out_sinfo=R.Tensor((16, 77, 80), dtype="float32"))
            lv354_1 = R.call_tir(fused_matmul21_multiply9, (lv351_1, lv352_1), out_sinfo=R.Tensor((16, 1024, 77), dtype="float32"))
            lv1469 = R.call_tir(softmax4, (lv354_1,), out_sinfo=R.Tensor((16, 1024, 77), dtype="float32"))
            lv1470 = R.call_tir(matmul22, (lv1469, lv353_1), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv355_2 = R.call_tir(fused_reshape26_transpose23_reshape27, (lv1470,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv776: R.Tensor((640, 640), dtype="float32") = params[660]
            lv777: R.Tensor((640,), dtype="float32") = params[340]
            lv356_1 = R.call_tir(fused_matmul17_add17_add18, (lv355_2, lv776, lv777, lv350_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv778: R.Tensor((640,), dtype="float32") = params[341]
            lv779: R.Tensor((640,), dtype="float32") = params[342]
            lv1478 = R.call_tir(layer_norm2, (lv356_1, lv778, lv779), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv780: R.Tensor((640, 2560), dtype="float32") = params[662]
            lv781: R.Tensor((2560,), dtype="float32") = params[344]
            lv357_2 = R.call_tir(fused_matmul23_add19_gelu1, (lv1478, lv780, lv781), out_sinfo=R.Tensor((2, 1024, 2560), dtype="float32"))
            lv782_1: R.Tensor((640, 2560), dtype="float32") = params[661]
            lv783: R.Tensor((2560,), dtype="float32") = params[343]
            lv358_1 = R.call_tir(fused_matmul23_add19_multiply10, (lv1478, lv782_1, lv783, lv357_2), out_sinfo=R.Tensor((2, 1024, 2560), dtype="float32"))
            lv784: R.Tensor((2560, 640), dtype="float32") = params[663]
            lv785: R.Tensor((640,), dtype="float32") = params[345]
            lv359_2 = R.call_tir(fused_matmul24_add17_add18, (lv358_1, lv784, lv785, lv356_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv360_1 = R.call_tir(fused_reshape30_transpose29, (lv359_2,), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv786: R.Tensor((640, 640, 1, 1), dtype="float32") = params[346]
            lv787_1: R.Tensor((1, 640, 1, 1), dtype="float32") = params[664]
            lv361_2 = R.call_tir(fused_conv2d7_add13_add16, (lv360_1, lv786, lv787_1, lv342_1), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv1497 = R.call_tir(resize2d2, (lv361_2,), out_sinfo=R.Tensor((2, 640, 64, 64), dtype="float32"))
            lv788: R.Tensor((640, 640, 3, 3), dtype="float32") = params[347]
            lv789: R.Tensor((1, 640, 1, 1), dtype="float32") = params[665]
            lv362_1 = R.call_tir(fused_conv2d29_add34, (lv1497, lv788, lv789), out_sinfo=R.Tensor((2, 640, 64, 64), dtype="float32"))
            lv1501 = R.call_tir(concatenate7, (lv362_1, lv51_1), out_sinfo=R.Tensor((2, 960, 64, 64), dtype="float32"))
            lv790: R.Tensor((960,), dtype="float32") = params[348]
            lv791: R.Tensor((960,), dtype="float32") = params[349]
            lv363_1 = R.call_tir(fused_group_norm16_silu13, (lv1501, lv790, lv791), out_sinfo=R.Tensor((2, 960, 64, 64), dtype="float32"))
            lv1507 = R.call_tir(silu, (lv2,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv792: R.Tensor((1280, 320), dtype="float32") = params[667]
            lv793: R.Tensor((320,), dtype="float32") = params[351]
            lv364_1 = R.call_tir(fused_matmul7_add6_strided_slice3, (lv1507, lv792, lv793), out_sinfo=R.Tensor((2, 320), dtype="float32"))
            lv1512 = R.call_tir(reshape12, (lv364_1,), out_sinfo=R.Tensor((2, 320, 1, 1), dtype="float32"))
            lv794: R.Tensor((320, 960, 3, 3), dtype="float32") = params[350]
            lv795: R.Tensor((1, 320, 1, 1), dtype="float32") = params[666]
            lv365_1 = R.call_tir(fused_conv2d30_add5_add7, (lv363_1, lv794, lv795, lv1512), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv796_1: R.Tensor((320,), dtype="float32") = params[352]
            lv797: R.Tensor((320,), dtype="float32") = params[353]
            lv366_1 = R.call_tir(fused_group_norm_silu1, (lv365_1, lv796_1, lv797), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv798: R.Tensor((320, 960, 1, 1), dtype="float32") = params[355]
            lv799: R.Tensor((1, 320, 1, 1), dtype="float32") = params[668]
            lv367_1 = R.call_tir(fused_conv2d31_add5, (lv1501, lv798, lv799), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv800: R.Tensor((320, 320, 3, 3), dtype="float32") = params[354]
            lv801: R.Tensor((1, 320, 1, 1), dtype="float32") = params[669]
            lv368_1 = R.call_tir(fused_conv2d1_add5_add8_divide, (lv366_1, lv800, lv801, lv367_1), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv802_1: R.Tensor((320,), dtype="float32") = params[372]
            lv803: R.Tensor((320,), dtype="float32") = params[373]
            lv1524 = R.call_tir(group_norm1, (lv368_1, lv802_1, lv803), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv804: R.Tensor((320, 320, 1, 1), dtype="float32") = params[374]
            lv805: R.Tensor((1, 320, 1, 1), dtype="float32") = params[670]
            lv369_1 = R.call_tir(fused_conv2d2_add5, (lv1524, lv804, lv805), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv370_1 = R.call_tir(fused_transpose9_reshape13, (lv369_1,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv806: R.Tensor((320,), dtype="float32") = params[375]
            lv807_1: R.Tensor((320,), dtype="float32") = params[376]
            lv1530 = R.call_tir(layer_norm1, (lv370_1, lv806, lv807_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv808: R.Tensor((320, 320), dtype="float32") = params[671]
            lv1532 = R.call_tir(matmul8, (lv1530, lv808), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv809: R.Tensor((320, 320), dtype="float32") = params[672]
            lv1534 = R.call_tir(matmul8, (lv1530, lv809), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv810: R.Tensor((320, 320), dtype="float32") = params[673]
            lv1536 = R.call_tir(matmul8, (lv1530, lv810), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv371_1 = R.call_tir(fused_reshape14_transpose11_reshape15, (lv1532,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv372_1 = R.call_tir(fused_reshape14_transpose11_reshape15_transpose12, (lv1534,), out_sinfo=R.Tensor((16, 40, 4096), dtype="float32"))
            lv373_1 = R.call_tir(fused_reshape14_transpose11_reshape15, (lv1536,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv374_2 = R.call_tir(fused_matmul9_multiply5, (lv371_1, lv372_1), out_sinfo=R.Tensor((16, 4096, 4096), dtype="float32"))
            lv1549 = R.call_tir(softmax1, (lv374_2,), out_sinfo=R.Tensor((16, 4096, 4096), dtype="float32"))
            lv1550 = R.call_tir(matmul10, (lv1549, lv373_1), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv375_2 = R.call_tir(fused_reshape16_transpose13_reshape17, (lv1550,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv811: R.Tensor((320, 320), dtype="float32") = params[674]
            lv812: R.Tensor((320,), dtype="float32") = params[377]
            lv376_1 = R.call_tir(fused_matmul8_add9_add10, (lv375_2, lv811, lv812, lv370_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv813: R.Tensor((320,), dtype="float32") = params[378]
            lv814: R.Tensor((320,), dtype="float32") = params[379]
            lv1558 = R.call_tir(layer_norm1, (lv376_1, lv813, lv814), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv815: R.Tensor((320, 320), dtype="float32") = params[675]
            lv1560 = R.call_tir(matmul8, (lv1558, lv815), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv816: R.Tensor((768, 320), dtype="float32") = params[676]
            lv1562 = R.call_tir(matmul11, (inp_2, lv816), out_sinfo=R.Tensor((2, 77, 320), dtype="float32"))
            lv817: R.Tensor((768, 320), dtype="float32") = params[677]
            lv1564 = R.call_tir(matmul11, (inp_2, lv817), out_sinfo=R.Tensor((2, 77, 320), dtype="float32"))
            lv377_1 = R.call_tir(fused_reshape14_transpose11_reshape15, (lv1560,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv378_1 = R.call_tir(fused_reshape18_transpose15_reshape19_transpose16, (lv1562,), out_sinfo=R.Tensor((16, 40, 77), dtype="float32"))
            lv379_1 = R.call_tir(fused_reshape18_transpose15_reshape19, (lv1564,), out_sinfo=R.Tensor((16, 77, 40), dtype="float32"))
            lv380_1 = R.call_tir(fused_matmul12_multiply6, (lv377_1, lv378_1), out_sinfo=R.Tensor((16, 4096, 77), dtype="float32"))
            lv1577 = R.call_tir(softmax2, (lv380_1,), out_sinfo=R.Tensor((16, 4096, 77), dtype="float32"))
            lv1578 = R.call_tir(matmul13, (lv1577, lv379_1), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv381_1 = R.call_tir(fused_reshape16_transpose13_reshape17, (lv1578,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv818: R.Tensor((320, 320), dtype="float32") = params[678]
            lv819_1: R.Tensor((320,), dtype="float32") = params[380]
            lv382_1 = R.call_tir(fused_matmul8_add9_add10, (lv381_1, lv818, lv819_1, lv376_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv820: R.Tensor((320,), dtype="float32") = params[381]
            lv821: R.Tensor((320,), dtype="float32") = params[382]
            lv1586 = R.call_tir(layer_norm1, (lv382_1, lv820, lv821), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv822: R.Tensor((320, 1280), dtype="float32") = params[680]
            lv823: R.Tensor((1280,), dtype="float32") = params[384]
            lv383_2 = R.call_tir(fused_matmul14_add11_gelu, (lv1586, lv822, lv823), out_sinfo=R.Tensor((2, 4096, 1280), dtype="float32"))
            lv824: R.Tensor((320, 1280), dtype="float32") = params[679]
            lv825_1: R.Tensor((1280,), dtype="float32") = params[383]
            lv384_1 = R.call_tir(fused_matmul14_add11_multiply7, (lv1586, lv824, lv825_1, lv383_2), out_sinfo=R.Tensor((2, 4096, 1280), dtype="float32"))
            lv826: R.Tensor((1280, 320), dtype="float32") = params[681]
            lv827: R.Tensor((320,), dtype="float32") = params[385]
            lv385_2 = R.call_tir(fused_matmul15_add9_add10, (lv384_1, lv826, lv827, lv382_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv386_1 = R.call_tir(fused_reshape20_transpose17, (lv385_2,), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv828: R.Tensor((320, 320, 1, 1), dtype="float32") = params[386]
            lv829: R.Tensor((1, 320, 1, 1), dtype="float32") = params[682]
            lv387_2 = R.call_tir(fused_conv2d2_add5_add8, (lv386_1, lv828, lv829, lv368_1), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv1605 = R.call_tir(concatenate8, (lv387_2, lv27), out_sinfo=R.Tensor((2, 640, 64, 64), dtype="float32"))
            lv830_1: R.Tensor((640,), dtype="float32") = params[356]
            lv831: R.Tensor((640,), dtype="float32") = params[357]
            lv388_1 = R.call_tir(fused_group_norm17_silu14, (lv1605, lv830_1, lv831), out_sinfo=R.Tensor((2, 640, 64, 64), dtype="float32"))
            lv1611 = R.call_tir(silu, (lv2,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv832: R.Tensor((1280, 320), dtype="float32") = params[684]
            lv833: R.Tensor((320,), dtype="float32") = params[359]
            lv389_2 = R.call_tir(fused_matmul7_add6_strided_slice3, (lv1611, lv832, lv833), out_sinfo=R.Tensor((2, 320), dtype="float32"))
            lv1616 = R.call_tir(reshape12, (lv389_2,), out_sinfo=R.Tensor((2, 320, 1, 1), dtype="float32"))
            lv834: R.Tensor((320, 640, 3, 3), dtype="float32") = params[358]
            lv835: R.Tensor((1, 320, 1, 1), dtype="float32") = params[683]
            lv390_1 = R.call_tir(fused_conv2d32_add5_add7, (lv388_1, lv834, lv835, lv1616), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv836: R.Tensor((320,), dtype="float32") = params[360]
            lv837: R.Tensor((320,), dtype="float32") = params[361]
            lv391_1 = R.call_tir(fused_group_norm_silu1, (lv390_1, lv836, lv837), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv838: R.Tensor((320, 640, 1, 1), dtype="float32") = params[363]
            lv839: R.Tensor((1, 320, 1, 1), dtype="float32") = params[685]
            lv392_1 = R.call_tir(fused_conv2d33_add5, (lv1605, lv838, lv839), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv840: R.Tensor((320, 320, 3, 3), dtype="float32") = params[362]
            lv841: R.Tensor((1, 320, 1, 1), dtype="float32") = params[686]
            lv393_1 = R.call_tir(fused_conv2d1_add5_add8_divide, (lv391_1, lv840, lv841, lv392_1), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv842_1: R.Tensor((320,), dtype="float32") = params[387]
            lv843: R.Tensor((320,), dtype="float32") = params[388]
            lv1628 = R.call_tir(group_norm1, (lv393_1, lv842_1, lv843), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv844: R.Tensor((320, 320, 1, 1), dtype="float32") = params[389]
            lv845: R.Tensor((1, 320, 1, 1), dtype="float32") = params[687]
            lv394_1 = R.call_tir(fused_conv2d2_add5, (lv1628, lv844, lv845), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv395_1 = R.call_tir(fused_transpose9_reshape13, (lv394_1,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv846: R.Tensor((320,), dtype="float32") = params[390]
            lv847: R.Tensor((320,), dtype="float32") = params[391]
            lv1634 = R.call_tir(layer_norm1, (lv395_1, lv846, lv847), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv848_1: R.Tensor((320, 320), dtype="float32") = params[688]
            lv1636 = R.call_tir(matmul8, (lv1634, lv848_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv849: R.Tensor((320, 320), dtype="float32") = params[689]
            lv1638 = R.call_tir(matmul8, (lv1634, lv849), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv850: R.Tensor((320, 320), dtype="float32") = params[690]
            lv1640 = R.call_tir(matmul8, (lv1634, lv850), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv396_1 = R.call_tir(fused_reshape14_transpose11_reshape15, (lv1636,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv397_1 = R.call_tir(fused_reshape14_transpose11_reshape15_transpose12, (lv1638,), out_sinfo=R.Tensor((16, 40, 4096), dtype="float32"))
            lv398_1 = R.call_tir(fused_reshape14_transpose11_reshape15, (lv1640,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv399_1 = R.call_tir(fused_matmul9_multiply5, (lv396_1, lv397_1), out_sinfo=R.Tensor((16, 4096, 4096), dtype="float32"))
            lv1653 = R.call_tir(softmax1, (lv399_1,), out_sinfo=R.Tensor((16, 4096, 4096), dtype="float32"))
            lv1654 = R.call_tir(matmul10, (lv1653, lv398_1), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv400_1 = R.call_tir(fused_reshape16_transpose13_reshape17, (lv1654,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv851: R.Tensor((320, 320), dtype="float32") = params[691]
            lv852: R.Tensor((320,), dtype="float32") = params[392]
            lv401_1 = R.call_tir(fused_matmul8_add9_add10, (lv400_1, lv851, lv852, lv395_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv853_1: R.Tensor((320,), dtype="float32") = params[393]
            lv854: R.Tensor((320,), dtype="float32") = params[394]
            lv1662 = R.call_tir(layer_norm1, (lv401_1, lv853_1, lv854), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv855: R.Tensor((320, 320), dtype="float32") = params[692]
            lv1664 = R.call_tir(matmul8, (lv1662, lv855), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv856: R.Tensor((768, 320), dtype="float32") = params[693]
            lv1666 = R.call_tir(matmul11, (inp_2, lv856), out_sinfo=R.Tensor((2, 77, 320), dtype="float32"))
            lv857: R.Tensor((768, 320), dtype="float32") = params[694]
            lv1668 = R.call_tir(matmul11, (inp_2, lv857), out_sinfo=R.Tensor((2, 77, 320), dtype="float32"))
            lv402_2 = R.call_tir(fused_reshape14_transpose11_reshape15, (lv1664,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv403_2 = R.call_tir(fused_reshape18_transpose15_reshape19_transpose16, (lv1666,), out_sinfo=R.Tensor((16, 40, 77), dtype="float32"))
            lv404_1 = R.call_tir(fused_reshape18_transpose15_reshape19, (lv1668,), out_sinfo=R.Tensor((16, 77, 40), dtype="float32"))
            lv405_1 = R.call_tir(fused_matmul12_multiply6, (lv402_2, lv403_2), out_sinfo=R.Tensor((16, 4096, 77), dtype="float32"))
            lv1681 = R.call_tir(softmax2, (lv405_1,), out_sinfo=R.Tensor((16, 4096, 77), dtype="float32"))
            lv1682 = R.call_tir(matmul13, (lv1681, lv404_1), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv406_1 = R.call_tir(fused_reshape16_transpose13_reshape17, (lv1682,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv858: R.Tensor((320, 320), dtype="float32") = params[695]
            lv859: R.Tensor((320,), dtype="float32") = params[395]
            lv407_1 = R.call_tir(fused_matmul8_add9_add10, (lv406_1, lv858, lv859, lv401_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv860: R.Tensor((320,), dtype="float32") = params[396]
            lv861: R.Tensor((320,), dtype="float32") = params[397]
            lv1690 = R.call_tir(layer_norm1, (lv407_1, lv860, lv861), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv862: R.Tensor((320, 1280), dtype="float32") = params[697]
            lv863: R.Tensor((1280,), dtype="float32") = params[399]
            lv408_1 = R.call_tir(fused_matmul14_add11_gelu, (lv1690, lv862, lv863), out_sinfo=R.Tensor((2, 4096, 1280), dtype="float32"))
            lv864: R.Tensor((320, 1280), dtype="float32") = params[696]
            lv865_1: R.Tensor((1280,), dtype="float32") = params[398]
            lv409_1 = R.call_tir(fused_matmul14_add11_multiply7, (lv1690, lv864, lv865_1, lv408_1), out_sinfo=R.Tensor((2, 4096, 1280), dtype="float32"))
            lv866: R.Tensor((1280, 320), dtype="float32") = params[698]
            lv867: R.Tensor((320,), dtype="float32") = params[400]
            lv410_1 = R.call_tir(fused_matmul15_add9_add10, (lv409_1, lv866, lv867, lv407_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv411_2 = R.call_tir(fused_reshape20_transpose17, (lv410_1,), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv868: R.Tensor((320, 320, 1, 1), dtype="float32") = params[401]
            lv869_1: R.Tensor((1, 320, 1, 1), dtype="float32") = params[699]
            lv412_1 = R.call_tir(fused_conv2d2_add5_add8, (lv411_2, lv868, lv869_1, lv393_1), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv1709 = R.call_tir(concatenate8, (lv412_1, lv3), out_sinfo=R.Tensor((2, 640, 64, 64), dtype="float32"))
            lv870: R.Tensor((640,), dtype="float32") = params[364]
            lv871: R.Tensor((640,), dtype="float32") = params[365]
            lv413_1 = R.call_tir(fused_group_norm17_silu14, (lv1709, lv870, lv871), out_sinfo=R.Tensor((2, 640, 64, 64), dtype="float32"))
            lv1715 = R.call_tir(silu, (lv2,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv872: R.Tensor((1280, 320), dtype="float32") = params[700]
            lv873: R.Tensor((320,), dtype="float32") = params[367]
            lv414_1 = R.call_tir(fused_matmul7_add6_strided_slice3, (lv1715, lv872, lv873), out_sinfo=R.Tensor((2, 320), dtype="float32"))
            lv1720 = R.call_tir(reshape12, (lv414_1,), out_sinfo=R.Tensor((2, 320, 1, 1), dtype="float32"))
            lv874: R.Tensor((320, 640, 3, 3), dtype="float32") = params[366]
            lv875_1: R.Tensor((1, 320, 1, 1), dtype="float32") = params[701]
            lv415_1 = R.call_tir(fused_conv2d32_add5_add7, (lv413_1, lv874, lv875_1, lv1720), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv876: R.Tensor((320,), dtype="float32") = params[368]
            lv877: R.Tensor((320,), dtype="float32") = params[369]
            lv416_1 = R.call_tir(fused_group_norm_silu1, (lv415_1, lv876, lv877), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv878: R.Tensor((320, 640, 1, 1), dtype="float32") = params[371]
            lv879: R.Tensor((1, 320, 1, 1), dtype="float32") = params[703]
            lv417_1 = R.call_tir(fused_conv2d33_add5, (lv1709, lv878, lv879), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv880_1: R.Tensor((320, 320, 3, 3), dtype="float32") = params[370]
            lv881: R.Tensor((1, 320, 1, 1), dtype="float32") = params[702]
            lv418_1 = R.call_tir(fused_conv2d1_add5_add8_divide, (lv416_1, lv880_1, lv881, lv417_1), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv882: R.Tensor((320,), dtype="float32") = params[402]
            lv883: R.Tensor((320,), dtype="float32") = params[403]
            lv1732 = R.call_tir(group_norm1, (lv418_1, lv882, lv883), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv884: R.Tensor((320, 320, 1, 1), dtype="float32") = params[404]
            lv885: R.Tensor((1, 320, 1, 1), dtype="float32") = params[704]
            lv419_1 = R.call_tir(fused_conv2d2_add5, (lv1732, lv884, lv885), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv420_1 = R.call_tir(fused_transpose9_reshape13, (lv419_1,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv886: R.Tensor((320,), dtype="float32") = params[405]
            lv887: R.Tensor((320,), dtype="float32") = params[406]
            lv1738 = R.call_tir(layer_norm1, (lv420_1, lv886, lv887), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv888: R.Tensor((320, 320), dtype="float32") = params[705]
            lv1740 = R.call_tir(matmul8, (lv1738, lv888), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv889: R.Tensor((320, 320), dtype="float32") = params[706]
            lv1742 = R.call_tir(matmul8, (lv1738, lv889), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv890: R.Tensor((320, 320), dtype="float32") = params[707]
            lv1744 = R.call_tir(matmul8, (lv1738, lv890), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv421_1 = R.call_tir(fused_reshape14_transpose11_reshape15, (lv1740,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv422_1 = R.call_tir(fused_reshape14_transpose11_reshape15_transpose12, (lv1742,), out_sinfo=R.Tensor((16, 40, 4096), dtype="float32"))
            lv423_1 = R.call_tir(fused_reshape14_transpose11_reshape15, (lv1744,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv424_1 = R.call_tir(fused_matmul9_multiply5, (lv421_1, lv422_1), out_sinfo=R.Tensor((16, 4096, 4096), dtype="float32"))
            lv1757 = R.call_tir(softmax1, (lv424_1,), out_sinfo=R.Tensor((16, 4096, 4096), dtype="float32"))
            lv1758 = R.call_tir(matmul10, (lv1757, lv423_1), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv425_1 = R.call_tir(fused_reshape16_transpose13_reshape17, (lv1758,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv891: R.Tensor((320, 320), dtype="float32") = params[708]
            lv892_1: R.Tensor((320,), dtype="float32") = params[407]
            lv426_1 = R.call_tir(fused_matmul8_add9_add10, (lv425_1, lv891, lv892_1, lv420_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv893: R.Tensor((320,), dtype="float32") = params[408]
            lv894: R.Tensor((320,), dtype="float32") = params[409]
            lv1766 = R.call_tir(layer_norm1, (lv426_1, lv893, lv894), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv895: R.Tensor((320, 320), dtype="float32") = params[709]
            lv1768 = R.call_tir(matmul8, (lv1766, lv895), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv896: R.Tensor((768, 320), dtype="float32") = params[710]
            lv1770 = R.call_tir(matmul11, (inp_2, lv896), out_sinfo=R.Tensor((2, 77, 320), dtype="float32"))
            lv897: R.Tensor((768, 320), dtype="float32") = params[711]
            lv1772 = R.call_tir(matmul11, (inp_2, lv897), out_sinfo=R.Tensor((2, 77, 320), dtype="float32"))
            lv427_1 = R.call_tir(fused_reshape14_transpose11_reshape15, (lv1768,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv428_1 = R.call_tir(fused_reshape18_transpose15_reshape19_transpose16, (lv1770,), out_sinfo=R.Tensor((16, 40, 77), dtype="float32"))
            lv429_1 = R.call_tir(fused_reshape18_transpose15_reshape19, (lv1772,), out_sinfo=R.Tensor((16, 77, 40), dtype="float32"))
            lv430_1 = R.call_tir(fused_matmul12_multiply6, (lv427_1, lv428_1), out_sinfo=R.Tensor((16, 4096, 77), dtype="float32"))
            lv1785 = R.call_tir(softmax2, (lv430_1,), out_sinfo=R.Tensor((16, 4096, 77), dtype="float32"))
            lv1786 = R.call_tir(matmul13, (lv1785, lv429_1), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv431_1 = R.call_tir(fused_reshape16_transpose13_reshape17, (lv1786,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv898_1: R.Tensor((320, 320), dtype="float32") = params[712]
            lv899: R.Tensor((320,), dtype="float32") = params[410]
            lv432_1 = R.call_tir(fused_matmul8_add9_add10, (lv431_1, lv898_1, lv899, lv426_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv900_1: R.Tensor((320,), dtype="float32") = params[411]
            lv901: R.Tensor((320,), dtype="float32") = params[412]
            lv1794 = R.call_tir(layer_norm1, (lv432_1, lv900_1, lv901), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv902_1: R.Tensor((320, 1280), dtype="float32") = params[714]
            lv903: R.Tensor((1280,), dtype="float32") = params[414]
            lv433_1 = R.call_tir(fused_matmul14_add11_gelu, (lv1794, lv902_1, lv903), out_sinfo=R.Tensor((2, 4096, 1280), dtype="float32"))
            lv904_1: R.Tensor((320, 1280), dtype="float32") = params[713]
            lv905: R.Tensor((1280,), dtype="float32") = params[413]
            lv434_1 = R.call_tir(fused_matmul14_add11_multiply7, (lv1794, lv904_1, lv905, lv433_1), out_sinfo=R.Tensor((2, 4096, 1280), dtype="float32"))
            lv906: R.Tensor((1280, 320), dtype="float32") = params[715]
            lv907: R.Tensor((320,), dtype="float32") = params[415]
            lv435_1 = R.call_tir(fused_matmul15_add9_add10, (lv434_1, lv906, lv907, lv432_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv436_1 = R.call_tir(fused_reshape20_transpose17, (lv435_1,), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv908: R.Tensor((320, 320, 1, 1), dtype="float32") = params[416]
            lv909: R.Tensor((1, 320, 1, 1), dtype="float32") = params[716]
            lv437_1 = R.call_tir(fused_conv2d2_add5_add8, (lv436_1, lv908, lv909, lv418_1), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv910: R.Tensor((320,), dtype="float32") = params[417]
            lv911: R.Tensor((320,), dtype="float32") = params[418]
            lv438_2 = R.call_tir(fused_group_norm_silu1, (lv437_1, lv910, lv911), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv912: R.Tensor((4, 320, 3, 3), dtype="float32") = params[419]
            lv913: R.Tensor((1, 4, 1, 1), dtype="float32") = params[717]
            lv439_1 = R.call_tir(fused_conv2d34_add35, (lv438_2, lv912, lv913), out_sinfo=R.Tensor((2, 4, 64, 64), dtype="float32"))
            gv: R.Tensor((2, 4, 64, 64), dtype="float32") = lv439_1
            R.output(gv)
        return gv

    @R.function
    def vae(inp_0: R.Tensor((1, 4, 64, 64), dtype="float32"), params: R.Tuple(R.Tensor((4, 4, 1, 1), dtype="float32"), R.Tensor((512, 4, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((256, 512, 3, 3), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256, 256, 3, 3), dtype="float32"), R.Tensor((256, 512, 1, 1), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256, 256, 3, 3), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256, 256, 3, 3), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256, 256, 3, 3), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256, 256, 3, 3), dtype="float32"), R.Tensor((256, 256, 3, 3), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((128, 256, 3, 3), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128, 128, 3, 3), dtype="float32"), R.Tensor((128, 256, 1, 1), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128, 128, 3, 3), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128, 128, 3, 3), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128, 128, 3, 3), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128, 128, 3, 3), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((3, 128, 3, 3), dtype="float32"), R.Tensor((1, 4, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((512, 512), dtype="float32"), R.Tensor((512, 512), dtype="float32"), R.Tensor((512, 512), dtype="float32"), R.Tensor((512, 512), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 256, 1, 1), dtype="float32"), R.Tensor((1, 256, 1, 1), dtype="float32"), R.Tensor((1, 256, 1, 1), dtype="float32"), R.Tensor((1, 256, 1, 1), dtype="float32"), R.Tensor((1, 256, 1, 1), dtype="float32"), R.Tensor((1, 256, 1, 1), dtype="float32"), R.Tensor((1, 256, 1, 1), dtype="float32"), R.Tensor((1, 256, 1, 1), dtype="float32"), R.Tensor((1, 128, 1, 1), dtype="float32"), R.Tensor((1, 128, 1, 1), dtype="float32"), R.Tensor((1, 128, 1, 1), dtype="float32"), R.Tensor((1, 128, 1, 1), dtype="float32"), R.Tensor((1, 128, 1, 1), dtype="float32"), R.Tensor((1, 128, 1, 1), dtype="float32"), R.Tensor((1, 128, 1, 1), dtype="float32"), R.Tensor((1, 3, 1, 1), dtype="float32"))) -> R.Tensor((1, 3, 512, 512), dtype="float32"):
        with R.dataflow():
            lv914: R.Tensor((4, 4, 1, 1), dtype="float32") = params[0]
            lv915: R.Tensor((1, 4, 1, 1), dtype="float32") = params[100]
            lv574 = R.call_tir(fused_conv2d35_add36, (inp_0, lv914, lv915), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
            lv916: R.Tensor((512, 4, 3, 3), dtype="float32") = params[1]
            lv917: R.Tensor((1, 512, 1, 1), dtype="float32") = params[101]
            lv575 = R.call_tir(fused_conv2d36_add37, (lv574, lv916, lv917), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv918: R.Tensor((512,), dtype="float32") = params[2]
            lv919: R.Tensor((512,), dtype="float32") = params[3]
            lv576 = R.call_tir(fused_group_norm18_silu15, (lv575, lv918, lv919), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv920: R.Tensor((512, 512, 3, 3), dtype="float32") = params[4]
            lv921: R.Tensor((1, 512, 1, 1), dtype="float32") = params[102]
            lv577 = R.call_tir(fused_conv2d37_add37, (lv576, lv920, lv921), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv922: R.Tensor((512,), dtype="float32") = params[5]
            lv923: R.Tensor((512,), dtype="float32") = params[6]
            lv578 = R.call_tir(fused_group_norm18_silu15, (lv577, lv922, lv923), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv924: R.Tensor((512, 512, 3, 3), dtype="float32") = params[7]
            lv925: R.Tensor((1, 512, 1, 1), dtype="float32") = params[103]
            lv579 = R.call_tir(fused_conv2d37_add37_add38_divide4, (lv578, lv924, lv925, lv575), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv926: R.Tensor((512,), dtype="float32") = params[14]
            lv927: R.Tensor((512,), dtype="float32") = params[15]
            lv18 = R.call_tir(group_norm18, (lv579, lv926, lv927), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv580 = R.call_tir(fused_reshape49_transpose45, (lv18,), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv928: R.Tensor((512, 512), dtype="float32") = params[104]
            lv929: R.Tensor((512,), dtype="float32") = params[16]
            lv581 = R.call_tir(fused_matmul40_add39, (lv580, lv928, lv929), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv930: R.Tensor((512, 512), dtype="float32") = params[105]
            lv931: R.Tensor((512,), dtype="float32") = params[17]
            lv582 = R.call_tir(fused_matmul40_add39, (lv580, lv930, lv931), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv932: R.Tensor((512, 512), dtype="float32") = params[106]
            lv933: R.Tensor((512,), dtype="float32") = params[18]
            lv583 = R.call_tir(fused_matmul40_add39, (lv580, lv932, lv933), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv584 = R.call_tir(fused_reshape50_transpose47_reshape51, (lv581,), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv585 = R.call_tir(fused_reshape50_transpose47_reshape51_transpose48, (lv582,), out_sinfo=R.Tensor((1, 512, 4096), dtype="float32"))
            lv586 = R.call_tir(fused_reshape50_transpose47_reshape51, (lv583,), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv587 = R.call_tir(fused_matmul41_multiply17_cast2, (lv584, lv585), out_sinfo=R.Tensor((1, 4096, 4096), dtype="float32"))
            lv588 = R.call_tir(fused_softmax9_cast2, (lv587,), out_sinfo=R.Tensor((1, 4096, 4096), dtype="float32"))
            lv45 = R.call_tir(matmul42, (lv588, lv586), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv589 = R.call_tir(fused_reshape52_transpose49_reshape53, (lv45,), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv934: R.Tensor((512, 512), dtype="float32") = params[107]
            lv935: R.Tensor((512,), dtype="float32") = params[19]
            lv590 = R.call_tir(fused_matmul40_add39, (lv589, lv934, lv935), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv591 = R.call_tir(fused_transpose48_reshape54_add38_divide4, (lv590, lv579), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv936: R.Tensor((512,), dtype="float32") = params[8]
            lv937: R.Tensor((512,), dtype="float32") = params[9]
            lv592 = R.call_tir(fused_group_norm18_silu15, (lv591, lv936, lv937), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv938: R.Tensor((512, 512, 3, 3), dtype="float32") = params[10]
            lv939: R.Tensor((1, 512, 1, 1), dtype="float32") = params[108]
            lv593 = R.call_tir(fused_conv2d37_add37, (lv592, lv938, lv939), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv940: R.Tensor((512,), dtype="float32") = params[11]
            lv941: R.Tensor((512,), dtype="float32") = params[12]
            lv594 = R.call_tir(fused_group_norm18_silu15, (lv593, lv940, lv941), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv942: R.Tensor((512, 512, 3, 3), dtype="float32") = params[13]
            lv943: R.Tensor((1, 512, 1, 1), dtype="float32") = params[109]
            lv595 = R.call_tir(fused_conv2d37_add37_add38_divide4, (lv594, lv942, lv943, lv591), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv944: R.Tensor((512,), dtype="float32") = params[20]
            lv945: R.Tensor((512,), dtype="float32") = params[21]
            lv596 = R.call_tir(fused_group_norm18_silu15, (lv595, lv944, lv945), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv946: R.Tensor((512, 512, 3, 3), dtype="float32") = params[22]
            lv947: R.Tensor((1, 512, 1, 1), dtype="float32") = params[110]
            lv597 = R.call_tir(fused_conv2d37_add37, (lv596, lv946, lv947), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv948: R.Tensor((512,), dtype="float32") = params[23]
            lv949: R.Tensor((512,), dtype="float32") = params[24]
            lv598 = R.call_tir(fused_group_norm18_silu15, (lv597, lv948, lv949), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv950: R.Tensor((512, 512, 3, 3), dtype="float32") = params[25]
            lv951: R.Tensor((1, 512, 1, 1), dtype="float32") = params[111]
            lv599 = R.call_tir(fused_conv2d37_add37_add38_divide4, (lv598, lv950, lv951, lv595), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv952: R.Tensor((512,), dtype="float32") = params[26]
            lv953: R.Tensor((512,), dtype="float32") = params[27]
            lv600 = R.call_tir(fused_group_norm18_silu15, (lv599, lv952, lv953), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv954: R.Tensor((512, 512, 3, 3), dtype="float32") = params[28]
            lv955: R.Tensor((1, 512, 1, 1), dtype="float32") = params[112]
            lv601 = R.call_tir(fused_conv2d37_add37, (lv600, lv954, lv955), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv956: R.Tensor((512,), dtype="float32") = params[29]
            lv957: R.Tensor((512,), dtype="float32") = params[30]
            lv602 = R.call_tir(fused_group_norm18_silu15, (lv601, lv956, lv957), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv958: R.Tensor((512, 512, 3, 3), dtype="float32") = params[31]
            lv959: R.Tensor((1, 512, 1, 1), dtype="float32") = params[113]
            lv603 = R.call_tir(fused_conv2d37_add37_add38_divide4, (lv602, lv958, lv959, lv599), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv960: R.Tensor((512,), dtype="float32") = params[32]
            lv961: R.Tensor((512,), dtype="float32") = params[33]
            lv604 = R.call_tir(fused_group_norm18_silu15, (lv603, lv960, lv961), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv962: R.Tensor((512, 512, 3, 3), dtype="float32") = params[34]
            lv963: R.Tensor((1, 512, 1, 1), dtype="float32") = params[114]
            lv605 = R.call_tir(fused_conv2d37_add37, (lv604, lv962, lv963), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv964: R.Tensor((512,), dtype="float32") = params[35]
            lv965: R.Tensor((512,), dtype="float32") = params[36]
            lv606 = R.call_tir(fused_group_norm18_silu15, (lv605, lv964, lv965), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv966: R.Tensor((512, 512, 3, 3), dtype="float32") = params[37]
            lv967: R.Tensor((1, 512, 1, 1), dtype="float32") = params[115]
            lv607 = R.call_tir(fused_conv2d37_add37_add38_divide4, (lv606, lv966, lv967, lv603), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv104 = R.call_tir(resize2d3, (lv607,), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv968: R.Tensor((512, 512, 3, 3), dtype="float32") = params[38]
            lv969: R.Tensor((1, 512, 1, 1), dtype="float32") = params[116]
            lv608 = R.call_tir(fused_conv2d38_add40, (lv104, lv968, lv969), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv970: R.Tensor((512,), dtype="float32") = params[39]
            lv971: R.Tensor((512,), dtype="float32") = params[40]
            lv609 = R.call_tir(fused_group_norm19_silu16, (lv608, lv970, lv971), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv972: R.Tensor((512, 512, 3, 3), dtype="float32") = params[41]
            lv973: R.Tensor((1, 512, 1, 1), dtype="float32") = params[117]
            lv610 = R.call_tir(fused_conv2d38_add40, (lv609, lv972, lv973), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv974: R.Tensor((512,), dtype="float32") = params[42]
            lv975: R.Tensor((512,), dtype="float32") = params[43]
            lv611 = R.call_tir(fused_group_norm19_silu16, (lv610, lv974, lv975), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv976: R.Tensor((512, 512, 3, 3), dtype="float32") = params[44]
            lv977: R.Tensor((1, 512, 1, 1), dtype="float32") = params[118]
            lv612 = R.call_tir(fused_conv2d38_add40_add41_divide5, (lv611, lv976, lv977, lv608), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv978: R.Tensor((512,), dtype="float32") = params[45]
            lv979: R.Tensor((512,), dtype="float32") = params[46]
            lv613 = R.call_tir(fused_group_norm19_silu16, (lv612, lv978, lv979), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv980: R.Tensor((512, 512, 3, 3), dtype="float32") = params[47]
            lv981: R.Tensor((1, 512, 1, 1), dtype="float32") = params[119]
            lv614 = R.call_tir(fused_conv2d38_add40, (lv613, lv980, lv981), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv982: R.Tensor((512,), dtype="float32") = params[48]
            lv983: R.Tensor((512,), dtype="float32") = params[49]
            lv615 = R.call_tir(fused_group_norm19_silu16, (lv614, lv982, lv983), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv984: R.Tensor((512, 512, 3, 3), dtype="float32") = params[50]
            lv985: R.Tensor((1, 512, 1, 1), dtype="float32") = params[120]
            lv616 = R.call_tir(fused_conv2d38_add40_add41_divide5, (lv615, lv984, lv985, lv612), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv986: R.Tensor((512,), dtype="float32") = params[51]
            lv987: R.Tensor((512,), dtype="float32") = params[52]
            lv617 = R.call_tir(fused_group_norm19_silu16, (lv616, lv986, lv987), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv988: R.Tensor((512, 512, 3, 3), dtype="float32") = params[53]
            lv989: R.Tensor((1, 512, 1, 1), dtype="float32") = params[121]
            lv618 = R.call_tir(fused_conv2d38_add40, (lv617, lv988, lv989), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv990: R.Tensor((512,), dtype="float32") = params[54]
            lv991: R.Tensor((512,), dtype="float32") = params[55]
            lv619 = R.call_tir(fused_group_norm19_silu16, (lv618, lv990, lv991), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv992: R.Tensor((512, 512, 3, 3), dtype="float32") = params[56]
            lv993: R.Tensor((1, 512, 1, 1), dtype="float32") = params[122]
            lv620 = R.call_tir(fused_conv2d38_add40_add41_divide5, (lv619, lv992, lv993, lv616), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv144 = R.call_tir(resize2d4, (lv620,), out_sinfo=R.Tensor((1, 512, 256, 256), dtype="float32"))
            lv994: R.Tensor((512, 512, 3, 3), dtype="float32") = params[57]
            lv995: R.Tensor((1, 512, 1, 1), dtype="float32") = params[123]
            lv621 = R.call_tir(fused_conv2d39_add42, (lv144, lv994, lv995), out_sinfo=R.Tensor((1, 512, 256, 256), dtype="float32"))
            lv996: R.Tensor((512,), dtype="float32") = params[58]
            lv997: R.Tensor((512,), dtype="float32") = params[59]
            lv622 = R.call_tir(fused_group_norm20_silu17, (lv621, lv996, lv997), out_sinfo=R.Tensor((1, 512, 256, 256), dtype="float32"))
            lv998: R.Tensor((256, 512, 3, 3), dtype="float32") = params[60]
            lv999: R.Tensor((1, 256, 1, 1), dtype="float32") = params[124]
            lv623 = R.call_tir(fused_conv2d40_add43, (lv622, lv998, lv999), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv1000: R.Tensor((256,), dtype="float32") = params[61]
            lv1001: R.Tensor((256,), dtype="float32") = params[62]
            lv624 = R.call_tir(fused_group_norm21_silu18, (lv623, lv1000, lv1001), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv1002: R.Tensor((256, 512, 1, 1), dtype="float32") = params[64]
            lv1003: R.Tensor((1, 256, 1, 1), dtype="float32") = params[125]
            lv625 = R.call_tir(fused_conv2d42_add43, (lv621, lv1002, lv1003), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv1004: R.Tensor((256, 256, 3, 3), dtype="float32") = params[63]
            lv1005: R.Tensor((1, 256, 1, 1), dtype="float32") = params[126]
            lv626 = R.call_tir(fused_conv2d41_add43_add44_divide6, (lv624, lv1004, lv1005, lv625), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv1006: R.Tensor((256,), dtype="float32") = params[65]
            lv1007: R.Tensor((256,), dtype="float32") = params[66]
            lv627 = R.call_tir(fused_group_norm21_silu18, (lv626, lv1006, lv1007), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv1008: R.Tensor((256, 256, 3, 3), dtype="float32") = params[67]
            lv1009: R.Tensor((1, 256, 1, 1), dtype="float32") = params[127]
            lv628 = R.call_tir(fused_conv2d41_add43, (lv627, lv1008, lv1009), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv1010: R.Tensor((256,), dtype="float32") = params[68]
            lv1011: R.Tensor((256,), dtype="float32") = params[69]
            lv629 = R.call_tir(fused_group_norm21_silu18, (lv628, lv1010, lv1011), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv1012: R.Tensor((256, 256, 3, 3), dtype="float32") = params[70]
            lv1013: R.Tensor((1, 256, 1, 1), dtype="float32") = params[128]
            lv630 = R.call_tir(fused_conv2d41_add43_add44_divide6, (lv629, lv1012, lv1013, lv626), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv1014: R.Tensor((256,), dtype="float32") = params[71]
            lv1015: R.Tensor((256,), dtype="float32") = params[72]
            lv631 = R.call_tir(fused_group_norm21_silu18, (lv630, lv1014, lv1015), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv1016: R.Tensor((256, 256, 3, 3), dtype="float32") = params[73]
            lv1017: R.Tensor((1, 256, 1, 1), dtype="float32") = params[129]
            lv632 = R.call_tir(fused_conv2d41_add43, (lv631, lv1016, lv1017), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv1018: R.Tensor((256,), dtype="float32") = params[74]
            lv1019: R.Tensor((256,), dtype="float32") = params[75]
            lv633 = R.call_tir(fused_group_norm21_silu18, (lv632, lv1018, lv1019), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv1020: R.Tensor((256, 256, 3, 3), dtype="float32") = params[76]
            lv1021: R.Tensor((1, 256, 1, 1), dtype="float32") = params[130]
            lv634 = R.call_tir(fused_conv2d41_add43_add44_divide6, (lv633, lv1020, lv1021, lv630), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv187 = R.call_tir(resize2d5, (lv634,), out_sinfo=R.Tensor((1, 256, 512, 512), dtype="float32"))
            lv1022: R.Tensor((256, 256, 3, 3), dtype="float32") = params[77]
            lv1023: R.Tensor((1, 256, 1, 1), dtype="float32") = params[131]
            lv635 = R.call_tir(fused_conv2d43_add45, (lv187, lv1022, lv1023), out_sinfo=R.Tensor((1, 256, 512, 512), dtype="float32"))
            lv1024: R.Tensor((256,), dtype="float32") = params[78]
            lv1025: R.Tensor((256,), dtype="float32") = params[79]
            lv636 = R.call_tir(fused_group_norm22_silu19, (lv635, lv1024, lv1025), out_sinfo=R.Tensor((1, 256, 512, 512), dtype="float32"))
            lv1026: R.Tensor((128, 256, 3, 3), dtype="float32") = params[80]
            lv1027: R.Tensor((1, 128, 1, 1), dtype="float32") = params[132]
            lv637 = R.call_tir(fused_conv2d44_add46, (lv636, lv1026, lv1027), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1028: R.Tensor((128,), dtype="float32") = params[81]
            lv1029: R.Tensor((128,), dtype="float32") = params[82]
            lv638 = R.call_tir(fused_group_norm23_silu20, (lv637, lv1028, lv1029), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1030: R.Tensor((128, 256, 1, 1), dtype="float32") = params[84]
            lv1031: R.Tensor((1, 128, 1, 1), dtype="float32") = params[133]
            lv639 = R.call_tir(fused_conv2d46_add46, (lv635, lv1030, lv1031), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1032: R.Tensor((128, 128, 3, 3), dtype="float32") = params[83]
            lv1033: R.Tensor((1, 128, 1, 1), dtype="float32") = params[134]
            lv640 = R.call_tir(fused_conv2d45_add46_add47_divide7, (lv638, lv1032, lv1033, lv639), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1034: R.Tensor((128,), dtype="float32") = params[85]
            lv1035: R.Tensor((128,), dtype="float32") = params[86]
            lv641 = R.call_tir(fused_group_norm23_silu20, (lv640, lv1034, lv1035), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1036: R.Tensor((128, 128, 3, 3), dtype="float32") = params[87]
            lv1037: R.Tensor((1, 128, 1, 1), dtype="float32") = params[135]
            lv642 = R.call_tir(fused_conv2d45_add46, (lv641, lv1036, lv1037), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1038: R.Tensor((128,), dtype="float32") = params[88]
            lv1039: R.Tensor((128,), dtype="float32") = params[89]
            lv643 = R.call_tir(fused_group_norm23_silu20, (lv642, lv1038, lv1039), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1040: R.Tensor((128, 128, 3, 3), dtype="float32") = params[90]
            lv1041: R.Tensor((1, 128, 1, 1), dtype="float32") = params[136]
            lv644 = R.call_tir(fused_conv2d45_add46_add47_divide7, (lv643, lv1040, lv1041, lv640), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1042: R.Tensor((128,), dtype="float32") = params[91]
            lv1043: R.Tensor((128,), dtype="float32") = params[92]
            lv645 = R.call_tir(fused_group_norm23_silu20, (lv644, lv1042, lv1043), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1044: R.Tensor((128, 128, 3, 3), dtype="float32") = params[93]
            lv1045: R.Tensor((1, 128, 1, 1), dtype="float32") = params[137]
            lv646 = R.call_tir(fused_conv2d45_add46, (lv645, lv1044, lv1045), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1046: R.Tensor((128,), dtype="float32") = params[94]
            lv1047: R.Tensor((128,), dtype="float32") = params[95]
            lv647 = R.call_tir(fused_group_norm23_silu20, (lv646, lv1046, lv1047), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1048: R.Tensor((128, 128, 3, 3), dtype="float32") = params[96]
            lv1049: R.Tensor((1, 128, 1, 1), dtype="float32") = params[138]
            lv648 = R.call_tir(fused_conv2d45_add46_add47_divide7, (lv647, lv1048, lv1049, lv644), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1050: R.Tensor((128,), dtype="float32") = params[97]
            lv1051: R.Tensor((128,), dtype="float32") = params[98]
            lv649 = R.call_tir(fused_group_norm23_silu20, (lv648, lv1050, lv1051), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1052: R.Tensor((3, 128, 3, 3), dtype="float32") = params[99]
            lv1053: R.Tensor((1, 3, 1, 1), dtype="float32") = params[139]
            lv650 = R.call_tir(fused_conv2d47_add48, (lv649, lv1052, lv1053), out_sinfo=R.Tensor((1, 3, 512, 512), dtype="float32"))
            gv: R.Tensor((1, 3, 512, 512), dtype="float32") = lv650
            R.output(gv)
        return gv
